[{"content":"Nginx 进程模型 Nginx其实有两种进程结构，一种是单进程结构，一种是多进程结构。单进程结构只适合我们做开发调试，在生产环境下，为了保持 Nginx 足够健壮，以及可以利用到 CPU 的多核特性，我们用到的是多进程架构的Nginx。\n多进程架构的Nginx，有一个父进程 master process，master 会有很多子进程，这些子进程分为两类，一类是worker 进程，一类是 cache 相关的进程。\n在一个四核的Linux服务器上查看Nginx进程：\n1 2 3 4 5 6 7 8 9 $ ps -ef --forest | grep nginx mazhen 20875 13073 0 15:25 pts/0 00:00:00 | \\_ grep --color=auto nginx mazhen 20862 1 0 15:25 ? 00:00:00 nginx: master process ./sbin/nginx mazhen 20863 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20864 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20865 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20866 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20867 20862 0 15:25 ? 00:00:00 \\_ nginx: cache manager process mazhen 20868 20862 0 15:25 ? 00:00:00 \\_ nginx: cache loader process 可以看到，Nginx 的 master 进程创建了4个 worker 进程，以及用来管理磁盘内容缓存的缓存helper进程。\n为什么Nginx使用的是多进程结构，而不是多线程结构呢？因为多线程结构，线程之间是共享同一个进程地址空间，当某一个第三方模块出现了地址空间的断错误时，会导致整个Nginx进程挂掉，而多进程模型就不会出现这样的问题，Nginx的第三方模块通常不会在 master 进程中加入自己的功能代码。\nmaster 进程执行一些特权操作，比如读取配置以及绑定端口，它管理 worker 进程的，负责监控每个 worke进程是否在正常工作，是否需要重载配置文件，以及做热部署等。\nworker 进程处理真正的请求，从磁盘读取内容或往磁盘中写入内容，以及与上游服务器通信。\ncache manager 进程会周期性地运行，从磁盘缓存中删除条目，以保证缓存没有超过配置的大小。\ncache loader 进程在启动时运行，用于将磁盘上的缓存加载到内存中，随后退出。\nNginx 采用了事件驱动的模型，它希望 worker 进程的数量和 CPU 一致，并且每一个 worker 进程与某一颗CPU绑定，worker 进程以非阻塞的方式处理多个连接，减少了上下文切换，同时更好的利用到了 CPU 缓存，减少缓存失效。\n请求处理流程 Nginx 使用的是非阻塞的事件驱动处理引擎，需要用状态机来把这个请求正确的识别和处理。Nginx 内部有三个状态机，分别是处理4层 TCP 流量的传输层状态机，处理7层流量的HTTP状态机和处理邮件的email 状态机。\nworker 进程首先等待监听套接字上的事件，新接入的连接会触发事件，然后连接分配到一个状态机。\n状态机本质上是告诉 Nginx 如何处理请求的指令集。解析出的请求是要访问静态资源，那么就去磁盘加载静态资源，更多的时候 Nginx 是作为负载均衡或者反向代理使用，这个时候请求会通过4层或7层协议，传输到上游服务器。对于每一个处理完成的请求，Nginx会记录 access 日志和 error 日志。\nNginx 进程管理 Linux 上多进程之间进行通讯，可以使用共享内存和信号。Nginx 在做进程间的管理时，使用了信号。我们可以使用 kill 命令直接向 master 进程和 worker 进程发送信号，也可以使用 nginx 命令行。\nmaster 进程接收处理的信号：\nCHLD 在 Linux 系统中，当子进程终止的时候，会向父进程发送 CHLD 信号。master 进程启动的 worker 进程，所以 master 是 worker 的父进程。如果 worker 进程由于一些原因意外退出，那么 master 进程会立刻收到通知，可以重新启动一个新的 worker进程。\nTERM 和 INT 立刻终止 worker 和 master 进程。\nQUIT 优雅的停止 worker 和 master 进程。worker 不会向客户端发送 reset 立即结束连接。\nHUP 重新加载配置文件\nUSR1 重新打开日志文件，做日志文件的切割\nUSR2 通知 master 开始进行热部署\nWINCH 在热部署过程中，通知旧的 master ，让它优雅关闭 worker 进程\n我们也可以通过 nginx -s 命令向 master 进程发送信号。在 Nginx 启动过程中， Nginx 会把 master 的 PID 记录在文件中，这个文件的默认位置是 $nginx/logs/nginx.pid 。 当我们执行 nginx -s 命令时，nginx 命令会去读取 nginx.pid 文件中 master 进程的 PID，然后向 master 进程发送对应的信号。下面是 nginx -s 命令对应的信号：\nreload - HUP reopen - USR1 stop - TERM quit - QUIT 使用 nginx -s 和 直接使用 kill 命令向 master 进程发送信号，效果是一样的。\n注意，USR2 和 WINCH 没有对应的 nginx -s 命令，只能通过 kill 命令直接向 master 进程发送。\nworker 进程能接收的信号：\nTERM 和 INT QUIT USR1 WINCH worker 进程收到这些信号，会产生和发给 master 一样的效果。但我们通常不会直接向 worker 进程发送信号，而是通过 master 进程来管理 worker 进程，master 进程收到信号以后，会再把信号转发给 worker 进程。\nNginx 配置更新流程 当更改了 Nginx 配置文件后，我们都会执行 nginx -s reload 命令重新加载配置文件。Nginx 不会停止服务，在处理新的请求的同时，平滑的进行配置文件的更新。\n执行 nginx -s reload 命令，会向 master 进程发送 SIGHUP 信号。当 master 进程接收 SIGHUP信号后，会做如下处理：\n检查配置文件语法是否正确。 master 加载配置，启动一组新的 worker 进程。这些 worker 进程马上开始接收新连接和处理网络请求。子进程可以共享使用父进程已经打开的端口，所以新的 worker 可以和老的worker监听同样的端口。 master 向旧的 worker 发送 QUIT 信号，让旧的 worker 优雅退出。 旧的 worker 进程停止接收新连接，完成现有连接的处理后结束进程。 Nginx 热部署流程 Nginx 支持热部署，在升级的过程中也实现了高可用性，不导致任何连接丢失，停机时间或服务中断。热部署的流程如下：\n备份旧的 nginx 二进制文件，将新的nginx二进制文件拷贝到 $nginx_home/sbin目录。 向 master 进程发送 USR2 信号。 master 进程用新的nginx文件启动新的master进程，新的master进程会启动新的worker进程。 向旧的 master 进程发送 WINCH 信号，让它优雅的关闭旧的 worker 进程。此时旧的 master 仍然在运行。 如果想回滚到旧版本，可以向旧的 master 发送 HUP 信号，向新的master 发送QUIT信号。 如果一切正常，可以向旧的 master 发送 QUIT 信号，关闭旧的 master。 ","date":"2022-11-18T11:16:48+08:00","permalink":"https://mazhen.tech/p/nginx%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/","title":"Nginx架构基础"},{"content":"反向代理（reverse proxy）是指用代理服务器来接受外部的访问请求，然后将请求转发给内网的上游服务器，并将从上游服务器上得到的结果返回外部客户端。作为反向代理是 Nginx 的一种常见用法。\n这里的负载均衡是指选择一种策略，尽量把请求平均地分布到每一台上游服务器上。下面介绍负载均衡的配置项。\nupstream 作为反向代理，一般都需要向上游服务器的集群转发请求。upstream 块定义了一个上游服务器的集群，便于反向代理中的 proxy_pass使用。\n1 2 3 4 5 6 7 http { ... upstream backend { server 127.0.0.1:8080; } ... } upstream 定义了一组上游服务器，并命名为 backend。\nproxy_pass proxy_pass 指令设置代理服务器的协议和地址。协议可以指定 \u0026ldquo;http \u0026ldquo;或 \u0026ldquo;https\u0026rdquo;。地址可以指定为域名或IP地址，也可以配置为 upstream 定义的上游服务器：\n1 2 3 4 5 6 7 8 9 10 http { server { listen 6888; server_name localhost; location / { proxy_pass http://backend; } } } proxy_set_header 在传递给上游服务器的请求头中，可以使用proxy_set_header 重新定义或添加字段。一般我们使用 proxy_set_header 向上游服务器传递一些必要的信息。\n1 2 3 4 5 6 location / { proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://backend; } 上面的配置使用 proxy_set_header 添加了三个 HTTP header：\nHost Host 是表明请求的主机名。默认情况下，Nginx 向上游服务器发送请求时，请求头中的 Host 字段是上游真实服务器的IP和端口号。如果我们想让传递给上游服务器的 Host 字段，包含的是用户访问反向代理时使用的域名，就需要通过 proxy_set_header 设置 Host 字段，值可以为 $host 或 $http_host，区别是前者只包含IP，而后者包含IP和端口号。\nX-Real-IP 经过反向代理后，上游服务器无法直接拿到客户端的 ip，也就是说，在应用中使用request.getRemoteAddr() 获得的是 Nginx 的地址。通过 proxy_set_header X-Real-IP $remote_addr;，将客户端的 ip 添加到了 HTTP header中，让应用可以使用 request.getHeader(“X-Real-IP”) 获取客户端的真实ip。\nX-Forwarded-For 如果配置了多层反向代理，当一个请求经过多层代理到达上游服务器时，上游服务器通过 X-Real-IP 获得的就不是客户端的真实IP了。那么这个时候就要用到 X-Forwarded-For ，设置 X-Forwarded-For 时是增加，而不是覆盖，从客户的真实IP为起点，穿过多层级代理 ，最终到达上游服务器，都会被记录下来。\nproxy_cache Nginx 作为反向代理支持的所有特性和内置变量都可以在 ngx_http_proxy_module 的文档页面找到：\n其中一个比较重要的特性是 proxy cache，对访问上游服务器的请求进行缓存，极大减轻了对上游服务的压力。\n配置示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 http { ... proxy_cache_path /tmp/nginx/cache levels=1:2 keys_zone=myzone:10m inactive=1h max_size=10g use_temp_path=off; server { ... location / { ... proxy_cache myzone; proxy_cache_key $host$uri$is_args$args; proxy_cache_valid 200 304 302 12h; } } } 配置说明：\nproxy_cache_path 缓存路径，要把缓存放在哪里\nlevels=1:2：缓存的目录结构 keys_zone=myzone:10m：定义一块用于存放缓存key的共享内存区，命名为myzone，并分配 10MB 的内存；配至10MB的zone 大约可以存放 80000个key。 inactive=1d：不活跃的缓存文件 1 小时后将被清除 max_size=10g：缓存所占磁盘空间的上限 use_temp_path=off：不另设临时目录 proxy_cache myzone;：代表要使用上面定义的 myzone\nproxy_cache_key：用于生成缓存键，区分不同的资源。key 是决定缓存命中率的因素之一。\n$host：request header中的 Host字段 $uri：请求的uri $is_args 反映请求的 URI 是否带参数，若没有即为空值。 $args：请求中的参数 proxy_cache_valid：控制缓存有效期，可以针对不同的 HTTP 状态码可以设定不同的有效期。示例针对 200，304，302 状态码的缓存有效期为12小时。\n检验缓存配置的效果。\n首先查看缓存路径，没有存放任何内容：\n1 2 3 4 $ tree /tmp/nginx/cache/ /tmp/nginx/cache/ 0 directories, 0 files 然后访问Nginx反向代理服务器：\n1 2 3 ❯ curl -v http://172.21.32.84:6888/ ... 再次查看缓存路径：\n1 2 3 4 5 6 7 $ tree /tmp/nginx/cache/ /tmp/nginx/cache/ └── 6 └── ed └── 5e9596b7783c532f541535dd1a60eed6 2 directories, 1 file 经过请求后，缓存路径中已经有内容，并且目录结构是我们配置的 level=1:2。\n","date":"2022-11-16T10:56:47+08:00","permalink":"https://mazhen.tech/p/nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE/","title":"Nginx反向代理配置"},{"content":"配置文件语法 Nginx的配置文件是一个文本文件，由指令和指令块构成。\n指令 指令以分号 ; 结尾，指令和参数间以空格分割。\n指令块作为容器，将相关的指令组合在一起，用大括号 {} 将它们包围起来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 http { include mime.types; default_type application/octet-stream; server { listen 8080; server_name localhost; location / { root html; index index.html index.htm; } } } 上面配置中的http、server、location等都是指令块。指令块配置项之后是否如参数（例如 location /），取决于解析这个块配置项的模块。\n指令块配置项是可以嵌套的。内层块会继承父级块包含的指令的设置。有些指令可以出现在多层指令块内，你可以通过在内层指令块包含该指令，来覆盖从父级继承的设置。\nContext 一些 top-level 指令被称为 context，将适用于不同流量类型的指令组合在一起。\nevents – 通用的连接处理 http – HTTP流量 mail – Mail 流量 stream – TCP 和 UDP 流量 放在这些 context 之外的指令是在 main context中。\n在每个流量处理 context 中，可以包括一个或多个 server 块，用来定义控制请求处理的虚拟服务器。\n对于HTTP流量，每个 server 指令块是对特定域名或IP地址访问的控制。通过一个活多个 location 定义如何处理特定的URI。\n对于 Mail 和 TCP/UDP 流量，server 指令块是对特定 TCP 端口流量的控制。\n静态资源服务 将个人网站的静态资源 clone 到 nginx 根目录：\n1 git clone https://github.com/mz1999/mazhen.git 在 conf/nginx.conf 文件中配置监听端口和 location：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 http { server { listen 8080; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { alias mazhen/; #index index.html index.htm; } } location 的语法格式为：\n1 location [ = | ~ | ~* | ^~ ] uri { ... } location 会尝试根据用户请求中的 URI 来匹配上面的 uri 表达式，如果可以匹配，就选择这个 location 块中的配置来处理用户请求。\nlocation 指定文件路径有两种方式：root和alias。\nroot 与alias 会以不同的方式将请求映射到服务器的文件上，它们的主要区别在于如何解释 location 后面的 uri 。\nroot的处理结果是，root＋location uri。 alias的处理结果是，使用 alias 替换 location uri。 alias 作为一个目录别名的定义。 例如：\n1 2 3 location /i/ { root /data/w3; } 如果一个请求的 URI 是 /i/top.gif ，Nginx 将会返回服务器上的 /data/w3/i/top.gif 文件。\n1 2 3 location /i/ { alias /data/w3/images/; } 如果一个请求的 URI 是 /i/top.gif，Nginx 将会返回服务器上的 /data/w3/images/top.gif文件。alias 会把 location 后面配置的 uri 替换为 alias 定义的目录。\n最后要注意，使用 alias 时，目录名后面一定要加 /。\n开启gzip Nginx 的 ngx_http_gzip_module 模块是一个过滤器，它使用 \u0026ldquo;gzip \u0026ldquo;方法压缩响应。可以在 http context 下配置 gzip：\n1 2 3 4 5 6 7 8 9 10 11 http { ... gzip on; gzip_min_length 1000; gzip_comp_level 2; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; server { ... } } gzip_min_length：设置允许压缩的页面最小字节数 gzip_comp_level： 设置 gzip 压缩比，1 压缩比最小处理速度最快，9 压缩比最大但处理最慢 gzip_types：匹配MIME类型进行压缩。 更多的配置项，可以参考官方文档。\nautoindex Nginx 的 ngx_http_autoindex_module 模块处理以斜线字符 / 结尾的请求，并产生一个目录列表。通常情况下，当 ngx_http_index_module 模块找不到index文件时，请求会被传递给 ngx_http_autoindex_module 模块。\nautoindex 的配置很简单：\n1 2 3 4 location / { alias mazhen/; autoindex on; } 注意，只有 index 模块找不到index文件时，请求才会被 autoindex 模块处理。我们可以把 mazhen 目录下的 index 文件删掉，或者为 index 指令配置一个不存在的文件。\nlimit_rate 由于带宽的限制，我们有时候需要限制某些资源向客户端传输响应的速率，例如可以对大文件限速，避免传输大文件占用过多带宽，从而影响其他更重要的小文件（css，js）的传输。我们可以使用 set 指令配合内置变量 $limit_rate 实现这个功能：\n1 2 3 4 location / { ... set $limit_rate 1k; } 上面的指令限制了Nginx向客户端发送响应的速率为 1k/秒。\n$limit_rate是Nginx的内置变量，Nginx的文档详细列出了每个模块的内置变量。以 ngx_http_core_module 为例，在 Nginx文档首页的 Modules reference 部分，点击进入 ngx_http_core_module ：\n在 ngx_http_core_module 文档目录的最下方，点击 Embedded Variables ，会跳转到 ngx_http_core_module 内置变量列表：\n这里有 http module 所有内置变量的说明，包括我们刚才使用 $limit_rate。\naccess log Nginx 的 access log 功能由 ngx_http_log_module 模块提供。ngx_http_log_module 提供了两个指令：\nlog_format 指定日志格式 access_log 设置日志写入的路径 举例说明：\n1 2 3 4 5 6 7 8 9 10 http { ... log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; server { ... access_log logs/mazhen.access.log main; } } log_format 使用内置变量定义日志格式，示例中的 log_format 可以使用 http module 定义的内置变量。log_format 还指定了这个日志格式的名称为 main，这样让我们定义多种格式的日志，为不同的 server 配置特定的日志格式。\naccess_log 设置了日志路径为 logs/mazhen.access.log，并指定了日志格式为 main。示例中的 access_log 定义在 server 下，那所有发往这个 server 的请求日志都使用 main 格式，被记录在 logs/mazhen.access.log文件中。\n","date":"2022-11-15T14:40:19+08:00","permalink":"https://mazhen.tech/p/nginx%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E6%9C%8D%E5%8A%A1%E7%9A%84%E9%85%8D%E7%BD%AE/","title":"Nginx静态资源服务的配置"},{"content":"Nginx 常用命令 Nginx的指令格式为 nginx [options argument]。\n查看帮助 1 ./sbin/nginx -? 使用指定的配置文件 1 ./sbin/nginx -c filename 指定运行目录 1 ./sbin/nginx -p /home/mazhen/nginx/ 设置配置指令，覆盖配置文件中的指令 1 ./sbin/nginx -g directives 向 Nginx 发送信号 我们可以向 Nginx 进程发送信号，控制运行中的 Nginx。一种方法是使用 kill 命令，也可以使用 nginx -s ：\n1 2 3 4 5 6 7 8 9 10 11 # 重新加载配置 $ ./sbin/nginx -s reload # 立即停止服务 $ ./sbin/nginx -s stop # 优雅停止服务 $ ./sbin/nginx -s quit # 重新开始记录日志文件 $ ./sbin/nginx -s reopen 测试配置文件是否有语法错误 1 ./sbin/nginx -t/-T 打印nginx版本 1 ./sbin/nginx -v/-V 热部署 在不停机的情况下升级正在运行的 Nginx 版本，就是热部署。\n首先查看正在运行的 Nginx：\n1 2 3 4 5 6 7 $ ps aux | grep nginx mazhen 4376 0.0 0.0 9896 2372 ? Ss 16:47 0:00 nginx: master process ./sbin/nginx mazhen 4402 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4403 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4404 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4405 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4407 0.0 0.0 12184 2316 pts/0 S+ 16:51 0:00 grep --color=auto nginx 备份现有 Nginx 的二进制文件：\n1 cp nginx nginx.old 将构建好的最新版 Nginx 的二进制文件拷贝到 $nginx/sbin 目录：\n1 cp ~/works/nginx-1.22.1/objs/nginx ~/nginx/sbin/ -f 给正在运行的Nginx的 master 进程发送信号，通知它我们要开始进行热部署：\n1 kill -USR2 4376 这时候 Nginx master 进程会使用新的二进制文件，启动新的 master 进程。新的 master 会生成新的 worker，同时，老的worker并没有退出，也在运行中，但不再监听 80/443 端口，请求会平滑的过度到新 worker 中。\n1 2 3 4 5 6 7 8 9 10 11 12 $ ps aux | grep nginx mazhen 4376 0.0 0.0 9896 2536 ? Ss 16:47 0:00 nginx: master process ./sbin/nginx mazhen 4402 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4403 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4404 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4405 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4454 0.0 0.0 9768 6024 ? S 16:59 0:00 nginx: master process ./sbin/nginx mazhen 4455 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4456 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4457 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4458 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4461 0.0 0.0 12184 2436 pts/0 S+ 16:59 0:00 grep --color=auto nginx 向老的 Nginx master 发送信号，让它优雅关闭 worker 进程。\n1 kill -WINCH 4376 这时候再查看 Nginx 进程：\n1 2 3 4 5 6 7 8 $ ps aux | grep nginx mazhen 4376 0.0 0.0 9896 2536 ? Ss 16:47 0:00 nginx: master process ./sbin/nginx mazhen 4454 0.0 0.0 9768 6024 ? S 16:59 0:00 nginx: master process ./sbin/nginx mazhen 4455 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4456 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4457 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4458 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4475 0.0 0.0 12184 2292 pts/0 S+ 17:07 0:00 grep --color=auto nginx 老的 worker 已经优雅退出，所有的请求已经切换到了新升级的 Nginx 中。\n老的 master 仍然在运行，如果需要，我们可以向它发送 reload 信号，回退到老版本的 Nginx。\n日志切割 首先使用 mv 命令，备份旧的日志：\n1 mv access.log bak.log Linux 文件系统中，改名并不会影响已经打开文件的写入操作，因为内核 inode 不变，这样操作不会出现丢日志的情况。\n然后给运行中的 Nginx 发送 reopen 信号：\n1 ./nginx -s reopen Nginx 会重新生成 access.log 日志文件。\n一般会写一个 bash 脚本，通过配置 crontab，每日进行日志切割。\n","date":"2022-11-11T17:26:02+08:00","permalink":"https://mazhen.tech/p/nginx%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","title":"Nginx的基本使用"},{"content":"Nginx 是最流行的Web服务器，根据 W3Techs 最新的统计，世界上三分之一的网站在使用Nginx。\n准备工作 Linux 版本 Nginx 需要 Linux 的内核为 2.6 及以上的版本，因为Linux 内核从 2.6 开始支持 epoll。可以使用 uname -a 查看 Linux 内核版本：\n1 2 $ uname -a Linux mazhen-laptop 5.15.0-52-generic #58-Ubuntu SMP Thu Oct 13 08:03:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux 从输出看到内核版本为 5.15.0，满足要求。现在已经很难找到内核 2.6 以下的服务器了吧。\n安装依赖 为了编译 Nginx 源码，需要安装一些依赖包。本文以 Ubuntu 为例。\nGCC编译器 GCC（GNU Compiler Collection）是必需的编译工具。使用下面的命令安装： 1 sudo apt install build-essential build-essential 是所谓的 meta-package，包含了 g++/GNU 编译器集合，GNU调试器，以及一些编译程序所需的工具和库。\nPCRE库 PCRE库支持正则表达式。如果我们在配置文件nginx.conf中使用了正则表达式，那么在编译Nginx时就必须把PCRE库编译进 Nginx，因为 Nginx的 HTTP 模块需要靠它来解析正则表达式。另外，pcre-devel 是使用PCRE做二次开发时所需要的开发库，包括头文件等，这也是编译Nginx所必须使用的。使用下面的命令安装： 1 sudo apt install libpcre3 libpcre3-dev zlib库 zlib库用于对 HTTP 包的内容做 gzip 格式的压缩，如果我们在 nginx.conf 中配置了gzip on，并指定对于某些类型（content-type）的 HTTP 响应使用 gzip 来进行压缩以减少网络传输量，则在编译时就必须把 zlib 编译进 Nginx。zlib-devel 是二次开发所需要的库。使用下面的命令安装：\n1 sudo apt install zlib1g-dev OpenSSL库 如果我们需要 Nginx 支持 SSL 加密传输，需要安装 OpenSSL 库。另外，如果我们想使用MD5、SHA1等散列函数，那么也需要安装它。使用下面的命令安装：\n1 sudo apt install openssl libssl-dev 下载Nginx源码 从 http://nginx.org/en/download.html下载当前稳定版本的源码。\n当前稳定版为 1.22.1：\n1 wget https://nginx.org/download/nginx-1.22.1.tar.gz Nginx配置文件的语法高亮 为了 Nginx 的配置文件在 vim 中能语法高亮，需要经过如下配置。\n解压 Nginx 源码：\n1 tar -zxvf nginx-1.22.1.tar.gz 将 Nginx 源码目录 contrib/vim/ 下的所有内容，复制到 $HOME/.vim 目录：\n1 2 mkdir ~/.vim cp -r contrib/vim/* ~/.vim/ 现在使用 vim 打开 nginx.conf，可以看到配置文件已经可以语法高亮了。\n编译前的配置 编译前需要使用 configure 命令进行相关参数的配置。\n使用 configure --help 查看编译配置支持的参数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ ./configure --help | more --help print this message --prefix=PATH set installation prefix --sbin-path=PATH set nginx binary pathname --modules-path=PATH set modules path --conf-path=PATH set nginx.conf pathname --error-log-path=PATH set error log pathname --pid-path=PATH set nginx.pid pathname --lock-path=PATH set nginx.lock pathname ...... --with-libatomic force libatomic_ops library usage --with-libatomic=DIR set path to libatomic_ops library sources --with-openssl=DIR set path to OpenSSL library sources --with-openssl-opt=OPTIONS set additional build options for OpenSSL --with-debug enable debug logging --with开头的模块缺省不包括在编译结果中，如果想使用需要在编译配置时显示的指定。--without开头的模块则相反，如果不想包含在编译结果中需要显示设定。\n例如我们可以这样进行编译前设置：\n1 ./configure --prefix=/home/mazhen/nginx --with-http_ssl_module 设置了Nginx的安装目录，以及需要http_ssl模块。\nconfigure命令执行完后，会生成中间文件，放在目录objs下。其中最重要的是ngx_modules.c文件，它决定了最终那些模块会编译进nginx。\n编译和安装 执行编译 在nginx目录下执行make编译：\n1 $ make 编译成功的nginx二进制文件在objs目录下。如果是做nginx的升级，可以直接将这个二进制文件copy到nginx的安装目录中。\n安装 在nginx目录下执行make install进行安装：\n1 $ make install 安装完成后，我们到 --prefix 指定的目录中查看安装结果：\n1 2 3 4 5 6 $ tree -L 1 /home/mazhen/nginx nginx/ ├── conf ├── html ├── logs └── sbin 验证安装结果 编辑 nginx/conf/nginx.conf 文件，设置监听端口为8080：\n1 2 3 4 5 6 7 http { ... server { listen 8080; server_name localhost; ... 启动 nginx\n1 ./sbin/nginx 访问默认首页：\n1 2 3 4 5 6 7 8 9 10 $ curl -I http://localhost:8080 HTTP/1.1 200 OK Server: nginx/1.22.1 Date: Fri, 11 Nov 2022 08:04:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 08 Nov 2022 09:54:09 GMT Connection: keep-alive ETag: \u0026#34;636a2741-267\u0026#34; Accept-Ranges: bytes ","date":"2022-11-11T16:06:29+08:00","permalink":"https://mazhen.tech/p/nginx%E7%9A%84%E7%BC%96%E8%AF%91%E5%92%8C%E5%AE%89%E8%A3%85/","title":"Nginx的编译和安装"},{"content":"Tower 是一个专注于对网络编程进行抽象的框架，最核心的抽象为 Service trait。Service::call 接受一个 request 进行处理，成功则返回 response，否则返回 error。\n1 fn call(Request) -\u0026gt; Result\u0026lt;Response, Error\u0026gt; Service trait 的定义 我们希望 Service 是异步编程风格，也就是 call 为 async 方法：\n1 async fn call(Request) -\u0026gt; Result\u0026lt;Response, Error\u0026gt; 我们就可以在 Service::call 上 await：\n1 service.call(request).await 然而当前 Rust 不支持 async trait 方法。\n我们可以让 call 作为普通方法，返回一个实现了 Future 的类型：\n1 2 3 4 5 6 7 trait Service\u0026lt;Request\u0026gt; { type Response; type Error; // ERROR: `impl Trait` not allowed outside of function and inherent // method return types fn call(\u0026amp;mut self, req: Request) -\u0026gt; impl Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;; } 还是不行，目前 Rust 也不支持在 Trait 中使用 impl Trait 做返回值，上一篇文章\u0026lt;impl Trait 的使用\u0026gt;分析过原因。\nTower 在定义 Service 时，使用了关联类型 type Future ，其实是将问题留给了 Service 的实现者，由用户选择 type Future 的实际类型：\n1 2 3 4 5 6 7 8 9 pub trait Service\u0026lt;Request\u0026gt; { type Response; type Error; type Future: Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;; fn poll_ready(\u0026amp;mut self, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Result\u0026lt;(), Self::Error\u0026gt;\u0026gt;; fn call(\u0026amp;mut self, req: Request) -\u0026gt; Self::Future; } 实现自己的 Future 类型 我们在实现 Service 时，仍然需要为type Future 设置具体的类型。\n既然没法让 call 直接返回 impl Future，一种方法是定义自己的 Future 类型，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 pub struct HttpRequest { url: String, } pub struct HttpResponse { code: u32, } pub struct ResponseFuture { request: HttpRequest, } impl Future for ResponseFuture { type Output = Result\u0026lt;HttpResponse, Error\u0026gt;; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut std::task::Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; { println!(\u0026#34;process url:{}\u0026#34;, \u0026amp;self.as_ref().get_ref().request.url); Poll::Ready(Ok(HttpResponse { code: 200 })) } } 在实现 Service 时，关联类型 type Future 设置为我们手工实现的 Future：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 struct RequestHandler; impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { type Response = HttpResponse; type Error = Error; type Future = ResponseFuture; fn poll_ready(\u0026amp;mut self, cx: \u0026amp;mut std::task::Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Result\u0026lt;(), Self::Error\u0026gt;\u0026gt; { Poll::Ready(Ok(())) } fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { ResponseFuture { request: req } } } 然后就可以像正常的 Future 一样，使用 Service ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 #[tokio::main] async fn main() { let mut service = RequestHandler {}; match service .call(HttpRequest { url: \u0026#34;/user/mazhen\u0026#34;.to_owned(), }) .await { Ok(r) =\u0026gt; println!(\u0026#34;Response code: {}\u0026#34;, r.code), Err(e) =\u0026gt; println!(\u0026#34;process failed. {:?}\u0026#34;, e), } } 使用 async blocks 手工实现 Future 会比较麻烦，我们一般都是用 async fn/async blocks语法糖生成 Future，那么这时 Service::call 返回什么类型呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 struct RequestHandler; impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { ... type Future = ??? fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { async move { println!(\u0026#34;process url {:?}\u0026#34;, \u0026amp;req.url); Ok(HttpResponse { code: 200 }) } } } 既然不能返回 impl Trait ，可以让 call 返回 trait object，用 trait object 统一返回值的类型。\n1 2 3 4 5 6 7 8 9 10 11 impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { ... type Future = Box\u0026lt;dyn Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;\u0026gt;; fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { Box::new(async move { println!(\u0026#34;process url {:?}\u0026#34;, \u0026amp;req.url); Ok(HttpResponse { code: 200 }) }) } } 这时候会报错，说 dyn Future 没有实现 Unpin：\n1 2 3 4 | 51 | type Future = Box\u0026lt;dyn Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;\u0026gt;; | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ the trait `Unpin` is not implemented for `(dyn std::future::Future\u0026lt;Output = Result\u0026lt;HttpResponse, std::io::Error\u0026gt;\u0026gt; + \u0026#39;static)` | 编译错误提示的没有实现 Unpin trait 是什么意思？那要先从 Pin 说起。\n一句话解释Pin Pin 本质上解决的问题是保证 Pin\u0026lt;P\u0026lt;T\u0026gt;\u0026gt; 中的 T 不会被 move，除非 T 满足 T: Unpin。\n什么是move 所有权转移的这个过程就是 move，例如：\n1 2 let s1 = \u0026#34;Hello, world\u0026#34;.to_owned(); let s2 = s1; s2 浅copy s1 的内容，同时 String 的所有权转移给了 s2。\n通过std::mem::swap()方法交换了两个可变借用 \u0026amp;mut 的内容，也发生了move。\n为什么要Pin 自引用结构体，move了以后会出问题。\n所以需要 Pin，不能move。\n怎么 Pin 住的 保证 T 不会被move，需要避免两种情况：\n不能暴露 T ，否则赋值、方法调用等都会move 不能暴露 \u0026amp;mut T，开发者可以调用 std::mem::swap() 或 std::mem::replace() 这类方法来 move 掉 T Pin\u0026lt;P\u0026lt;T\u0026gt;\u0026gt;没有暴露T，而且没法让你获得 \u0026amp;mut T，所以就 Pin 住了T。但注意有个前提条件：T 没有实现 Unpin\n谁没有实现 Unpin Unpin 是一个auto trait，编译器默认会给所有类型实现 Unpin。唯独有几个例外，他们实现的是 !Unpin。\nPhantomPinned 1 2 3 4 5 6 7 8 9 /// A marker type which does not implement `Unpin`. /// /// If a type contains a `PhantomPinned`, it will not implement `Unpin` by default. #[stable(feature = \u0026#34;pin\u0026#34;, since = \u0026#34;1.33.0\u0026#34;)] #[derive(Debug, Default, Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)] pub struct PhantomPinned; #[stable(feature = \u0026#34;pin\u0026#34;, since = \u0026#34;1.33.0\u0026#34;)] impl !Unpin for PhantomPinned {} 如果你使用了 PhantomPinned，你的类型自动实现 !Unpin 1 2 3 4 5 6 7 8 9 10 use std::marker::PhantomPinned; #[derive(Debug)] struct SelfReference { name: String, // 在初始化后指向 name name_ptr: *const String, // PhantomPinned 占位符 _marker: PhantomPinned, } 编译器为 async fn 生成的匿名结构体实现的是 !Unpin async fn async fn 是语法糖，在编译时，编译器使用Generator为 async fn 生成匿名结构体，这个结构体实现了 Future。\n这个匿名结构体是自引用的。原因是，如果 async fn 内有多个 await，执行到 await 可能因为资源没准备好而让出 CPU 暂停执行，随后该 Future 可能被调度到其他线程接着执行。所以这个匿名结构体需要保存跨 await 的数据，形成了自引用结构。\n由于为 async fn 生成的结构体是自引用的，所以这个结构体实现了 !Unpin，表示它不能被 move。\n这也是为什么不能给Future::poll 直接传 \u0026amp;mut Self 的原因：生成的匿名结构体不能被move，而拿到 \u0026amp;mut Self就可以使用 swap 或 replace之类的方法进行move，这样不安全，所以必须使用 Pin\u0026lt;\u0026amp;mut Self\u0026gt;。\nFuture 都是 !Unpin 的吗 不一定。\nasync fn 语法糖生成的实现了 Future 的匿名结构，内部包含自引用，它会明确实现 !Unpin，不能 move。\n但如果你自己实现的 Future，内部没有自引用，它就不是 !Unpin，当然可以 move。\n也就是说，Future 和 !Unpin 是两个 trait，虽然它们经常联系在一起，但并不是实现了 Future 的类型都必须同时实现 !Unpin，没有包含自引用的 Future当然可以安全的 move 了。\n实现了 Unpin 的 Future 如果是可以 move 的 Future，也就是实现了 Unpin 的 Future，在调用 Future::poll 的时候，要求传入 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，会不会有什么问题呢？\n首先，如果 T：Unpin，那么 Pin\u0026lt;\u0026amp;mut T\u0026gt; 就完全等同于 \u0026amp;mut T。换句话说，Unpin 意味着这个类型可以被移动，即使是在 Pin 住的情况下，所以 Pin 对这样的类型没有影响。因为 Pin 是智能指针，它实现了 Deref/DerefMut，只要满足 T: Unpin，你就能拿到\u0026amp;mut T：\n1 2 3 4 5 6 #[stable(feature = \u0026#34;pin\u0026#34;, since = \u0026#34;1.33.0\u0026#34;)] impl\u0026lt;P: DerefMut\u0026lt;Target: Unpin\u0026gt;\u0026gt; DerefMut for Pin\u0026lt;P\u0026gt; { fn deref_mut(\u0026amp;mut self) -\u0026gt; \u0026amp;mut P::Target { Pin::get_mut(Pin::as_mut(self)) } } 其次，为了用户方便，FutureExt 提供了 poll_unpin，让你直接在 Unpin 的 Future 上 poll：\n1 2 3 4 5 6 7 8 9 pub trait FutureExt: Future { /// A convenience for calling `Future::poll` on `Unpin` future types. fn poll_unpin(\u0026amp;mut self, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; where Self: Unpin, { Pin::new(self).poll(cx) } } 所以，如果你的 Future 是 Unpin，那么即使Future::poll 要求传入的是 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，对你也没有任何影响。\nBox\u0026lt;dyn Futrue\u0026lt;\u0026gt;\u0026gt; 的问题 回到上面的问题，我们想让 Service::call 返回 trait object，也就是 Box\u0026lt;dyn Futrue\u0026lt;\u0026gt;\u0026gt;，会编译不过，为什么呢？\n因为标准库为 Box 实现了 Future，但要求 Box 包装的 Future 必须同时实现了 Unpin ：\n1 2 3 4 impl\u0026lt;F, A\u0026gt; Future for Box\u0026lt;F, A\u0026gt; where F: Future + Unpin + ?Sized, A: Allocator + \u0026#39;static, 前面已经讲过，async 解语法糖生成的 Future 没有实现 Unpin，所以Box::new(async{...}) 不满足类型约束，它没有实现 Future，不能在它上面await。\nPin 实现了 Future 前面讲过，在 Future 上 poll 的时候，不能直接传入\u0026amp;mut Self，而需要传入 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，需要这样调用 Future::poll(Pin::new(\u0026amp;mut future), ctx)。如果 Pin 实现了 Future ，我们就可以直接这样 poll 了： Pin::new(\u0026amp;mut future).poll(ctx)。\n标准库也确实为 Pin 实现了 Future：\n1 2 3 4 impl\u0026lt;P\u0026gt; Future for Pin\u0026lt;P\u0026gt; where P: DerefMut, \u0026lt;P as Deref\u0026gt;::Target: Future, 我们来看对 P 的约束，P 可解引用为 Future，也就是说，P是 Future 的引用 \u0026amp;mut future，或者是智能指针 Box\u0026lt;dyn Future\u0026gt; 都可以满足约束。因为 Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt; 实现了 Future，我们可以用它作为 Service::call 返回值类型。\nBox 提供了 pin 方法，让用户构建 Pin\u0026lt;Box\u0026lt;T\u0026gt;\u0026gt;：\n1 pub fn pin(x: T) -\u0026gt; Pin\u0026lt;Box\u0026lt;T, Global\u0026gt;\u0026gt; 使用 Box::pin，RequestHanlder 可以这么修改：\n1 2 3 4 5 6 7 8 9 10 11 impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { ... type Future = Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;\u0026gt;\u0026gt;; fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { Box::pin(async move { println!(\u0026#34;process url {:?}\u0026#34;, \u0026amp;req.url); Ok(HttpResponse { code: 200 }) }) } } 这回终于可以了。事实上，在异步场景下，我们经常会看到使用 Box::pin 去包装 async block。\nPin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt; Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt; 除了实现了Future，也实现了 Unpin。\n因为 Pin 实现了 Unpin，只要 P 是 Unpin 的：\n1 2 3 impl\u0026lt;P\u0026gt; Unpin for Pin\u0026lt;P\u0026gt; where P: Unpin 而 Box 正好是 Unpin 的：\n1 2 3 4 impl\u0026lt;T, A\u0026gt; Unpin for Box\u0026lt;T, A\u0026gt; where A: Allocator + \u0026#39;static, T: ?Sized, 因此 Pin\u0026lt;Box\u0026lt;T\u0026gt;\u0026gt;是 Unpin 的。可以这么理解，Pin 钉住了 T，但 Pin 本身是 Unpin的，可以安全的 move。\n很多异步方法需要你的 Future 同时实现了 Unpin ，例如tokio::select!()，而 async fn 返回的 Future 显然不满足 Unpin，这个时候仍然可以用 Box::pin把 Future pin 住，得到的Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;...\u0026gt;\u0026gt;\u0026gt; 同时实现了 Future 和 Unpin，满足你的要求。\n简单总结，在异步编程场景，我们经常会用Box::pin 包装 async block，获得同时实现了 Future 和 Unpin 的Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt;。\n","date":"2022-10-12T09:14:10+08:00","permalink":"https://mazhen.tech/p/pinboxdyn-future%E8%A7%A3%E6%9E%90/","title":"Pin\u003cBox\u003cdyn Future\u003c\u003e\u003e\u003e解析"},{"content":"Rust 通过 RFC conservative impl trait 增加了新的语法 impl Trait，它被用在函数返回值的位置上，表示返回的类型将实现这个 Trait。随后的 RFC expanding impl Trait 更进一步，允许 impl Trait 用在函数参数的位置，表示由调用者决定参数的具体类型，其实就等价于函数的泛型参数。\nimpl Trait 作为函数参数 根据 RFC on expanding impl Trait， impl Trait 可以用在函数参数中，作用是作为函数的匿名泛型参数。\nExpand impl Trait to allow use in arguments, where it behaves like an anonymous generic parameter.\n也就是说，impl Trait 作为函数参数，和泛型参数是等价的：\n1 2 3 // These two are equivalent fn map\u0026lt;U\u0026gt;(self, f: impl FnOnce(T) -\u0026gt; U) -\u0026gt; Option\u0026lt;U\u0026gt; fn map\u0026lt;U, F\u0026gt;(self, f: F) -\u0026gt; Option\u0026lt;U\u0026gt; where F: FnOnce(T) -\u0026gt; U 不过，impl Trait和泛型参数有一个不同的地方，impl Trait 作为参数，不能明确指定它的类型：\n1 2 3 4 5 fn foo\u0026lt;T: Trait\u0026gt;(t: T) fn bar(t: impl Trait) foo::\u0026lt;u32\u0026gt;(0) // this is allowed bar::\u0026lt;u32\u0026gt;(0) // this is not 除了这个差别，可以认为impl Trait 作为函数参数，和使用泛型参数是等价的。\nimpl Trait 作为函数返回值 impl Trait 作为函数的返回值，表示返回的类型将实现这个 Trait。\n1 2 3 4 5 6 7 8 fn foo(n: u32) -\u0026gt; impl Iterator\u0026lt;Item = u32\u0026gt; { (0..n).map(|x| x * 100) } fn main() { for x in foo(10) { println!(\u0026#34;{}\u0026#34;, x); } } 在这种情况下，需要注意函数的所有返回路径必须返回完全相同的具体类型。\n1 2 3 4 5 6 7 8 // 编译错误，即使这两个类型都实现了Bar fn f(a: bool) -\u0026gt; impl Bar { if a { Foo { ... } } else { Baz { ... } } } 可以把函数返回值位置的 impl Trait 替换为泛型吗？\n1 2 3 4 // 不能编译 fn bar\u0026lt;T: Iterator\u0026lt;Item = u32\u0026gt;\u0026gt;(n: u32) -\u0026gt; T { (0..n).map(|x| x * 100) } 编译器给的错误信息是，期待返回值的类型是泛型类型 T，却实际却返回了一个具体类型。编译器很智能的给出了使用 impl Iterator\u0026lt;Item = u32\u0026gt;作为返回类型的建议：\n1 2 3 4 5 6 7 8 9 10 11 12 --\u0026gt; src/main.rs:6:5 | 5 | fn bar\u0026lt;T: Iterator\u0026lt;Item = u32\u0026gt;\u0026gt;(n: u32) -\u0026gt; T { | - - | | | | | expected `T` because of return type | this type parameter help: consider using an impl return type: `impl Iterator\u0026lt;Item = u32\u0026gt;` 6 | (0..n).map(|x| x * 100) | ^^^^^^^^^^^^^^^^^^^^^^^ expected type parameter `T`, found struct `Map` | = note: expected type parameter `T` found struct `Map\u0026lt;std::ops::Range\u0026lt;u32\u0026gt;, [closure@src/main.rs:6:16: 6:27]\u0026gt;` Universals vs. Existentials 在 RFC on expanding impl Trait 中使用了两个术语，Universal 和 Existential：\nUniversal quantification, i.e. \u0026ldquo;for any type T\u0026rdquo;, i.e. \u0026ldquo;caller chooses\u0026rdquo;. This is how generics work today. When you write fn foo\u0026lt;T\u0026gt;(t: T), you\u0026rsquo;re saying that the function will work for any choice of T, and leaving it to your caller to choose the T. Existential quantification, i.e. \u0026ldquo;for some type T\u0026rdquo;, i.e. \u0026ldquo;callee chooses\u0026rdquo;. This is how impl Trait works today (which is in return position only). When you write fn foo() -\u0026gt; impl Iterator, you\u0026rsquo;re saying that the function will produce some type T that implements Iterator, but the caller is not allowed to assume anything else about that type. 简单来说：\nimpl Trait 用在参数位置是 universal type，也就是泛型类型，它可以是任意类型，由函数的调用者指定具体的类型。\nimpl Trait 用在返回值位置是 existential type，它不能是任意类型，而是由函数的实现者指定，一个实现了 Trait 的具体类型。调用者不能对这个类型做任何假设。\n也就是说，impl Trait 用在返回位置不是泛型，编译时不需要单态化，抽象类型可以简单地替换为调用代码中的具体类型。\n在 Trait 中使用 impl Trait Rust 目前还不支持在 Trait 里使用 impl Trait 做返回值：\n1 2 3 4 5 trait Foo { // ERROR: `impl Trait` not allowed outside of function and inherent // method return types fn foo(\u0026amp;self) -\u0026gt; impl Iterator\u0026lt;Item=u8\u0026gt;; } 因为 impl Trait 用在返回值位置是 existential type，意味着这个函数将返回一个实现了这个 Trait 的单一类型，而函数定义在 Trait 中，意味着每个实现了 Trait 的类型，都可以让这个函数返回不同类型，对编译器来说这很难处理，因为它需要知道被返回类型的具体大小。\n一个简单的解决方法是让函数返回 trait object：\n1 2 3 trait Foo { fn foo(\u0026amp;self) -\u0026gt; Box\u0026lt;dyn Iterator\u0026lt;Item=u8\u0026gt;\u0026gt;; } 带有 trait object 的函数不是泛型函数，它只带有单一类型，这个类型就是 trait object 类型。Trait object 本身被实现为胖指针，其中，一个指针指向数据本身，另一个则指向虚函数表（vtable）。\n这样定义在 Trait 中的函数，返回的不再是泛型，而是一个单一的 trait object 类型，大小固定（两个指针大小），编译器可以处理。\n","date":"2022-10-08T17:58:36+08:00","permalink":"https://mazhen.tech/p/impl-trait-%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"impl Trait 的使用"},{"content":"Rust开发生态最重要的两个工具\nrustup Rust 官方的跨平台安装工具 Cargo Rust 构建系统和包管理器 它们的下载源都位于国外，为了改善在国内的使用体验，可以为它们配置国内源。\nrustup 国内源\n目前国内 Rust 工具链镜像源有清华大学源、中国科学技术大学源、上海交通大学源等，以清华大学源为例，设置环境变量：\n1 2 export RUSTUP_DIST_SERVER=https://mirrors.tuna.tsinghua.edu.cn/rustup export RUSTUP_UPDATE_ROOT=https://mirrors.tuna.tsinghua.edu.cn/rustup/rustup crates.io 国内源\nCargo 默认的源服务器为 crates.io，同样可以配置为国内的镜像源，以清华大学源为例，编辑 ~/.cargo/config 文件，添加以下内容：\n1 2 3 4 5 [source.crates-io] replace-with = \u0026#39;tuna\u0026#39; [source.tuna] registry = \u0026#34;https://mirrors.tuna.tsinghua.edu.cn/git/crates.io-index.git\u0026#34; 这样可加快 Cargo 读取软件包索引的速度。\n","date":"2022-09-30T22:36:25+08:00","permalink":"https://mazhen.tech/p/%E9%85%8D%E7%BD%AErustup%E5%92%8Ccargo%E5%9B%BD%E5%86%85%E6%BA%90/","title":"配置rustup和Cargo国内源"},{"content":"原文链接\nLinux 是内核，由Linus在90年代创造。不包括驱动程序，内核本身在编译时只有几兆字节。\nGNU是一个自由软件的大集合，可以和操作系统一起使用，包括你非常熟悉的 grep、sed、gcc等等。GNU 还包括 glibc，C 语言标准库的实现。\nGNU/Linux 是任何基于 GNU 集合的Linux发行版（内核+用户态应用）。Debian、Ubuntu、CentOS，甚至RHEL在技术上都是 GNU/Linux。因为有共同的C语言标准库和系统工具，通常你可以在 GNU/Linux 发行版之间跳来跳去，不会有什么麻烦。\nAlpine Linux 是另一个类型 Linux 发行版，它不是基于 GNU 集合。\n为了替代 GNU，Alpine 使用：\nBusyBox：一个小型软件套件 (~2MB)，在单个可执行文件中提供了多个 Unix 实用程序。 musl：一种现代且更强大的 C 标准库实现 因此，Alpine Linux 不是 Debian 或 CentOS 等 GNU/Linux 发行版的直接替代品，musl 的行为可能与 glibc 不同！\n","date":"2022-09-30T10:23:23+08:00","permalink":"https://mazhen.tech/p/linux-gnu/linux%E4%BB%A5%E5%8F%8Aalpine-linux/","title":"Linux, GNU/Linux以及Alpine Linux"},{"content":"Future trait Rust 异步编程最核心的是 Future trait：\n1 2 3 4 5 6 7 8 pub trait Future { type Output; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt;; } pub enum Poll\u0026lt;T\u0026gt; { Ready(T), Pending, } Future 代表一个异步计算，它会产生一个值。通过调用 poll 方法来推进 Future 的运行，如果 Future 完成了，它将返回 Poll::Ready(result)，我们拿到运算结果。如果 Future 还不能完成，可能是因为需要等待其他资源，它返回 Poll::Pending。等条件具备，如资源已经准备好，这个 Future 将被唤醒，再次进入 poll，直到计算完成获得结果。\nasync/.await 如果产生一个 Future 呢，使用 async 是产生 Future 最方便的方法。使用 async 有两种方式：async fn 和 async blocks。每种方法都返回一个实现了Future trait 的匿名结构：\n1 2 3 4 5 6 7 8 9 10 // `foo()` returns a type that implements `Future\u0026lt;Output = u8\u0026gt;`. async fn foo() -\u0026gt; u8 { 5 } fn bar() -\u0026gt; impl Future\u0026lt;Output = u8\u0026gt; { // This `async` block results in a type that implements // `Future\u0026lt;Output = u8\u0026gt;`. async { 5 } } 这两种方式是等价的，都返回了 impl Future\u0026lt;Output = u8\u0026gt;。async 关键字相当于一个返回 impl Future\u0026lt;Output\u0026gt; 的语法糖。\n调用 async fn 并不会让函数执行，而是返回 impl Future\u0026lt;Output\u0026gt;，你只有在返回值上使用 .await，才能触发函数的实际执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 async fn say_world() { println!(\u0026#34;world\u0026#34;); } #[tokio::main] async fn main() { // Calling `say_world()` does not execute the body of `say_world()`. let op = say_world(); // This println! comes first println!(\u0026#34;hello\u0026#34;); // Calling `.await` on `op` starts executing `say_world`. op.await; } 上面的程序输出为：\n1 2 hello world 在 Future 上调用 await，相当于执行 Future::poll。如果 Future 被某些条件阻塞，它将放弃对当前线程的控制。当条件准备好后， Future会被唤醒恢复执行。\n简单总结，我们用async 生成 Future，用 await 来触发 Future 的执行。尽管其他语言也实现了async/.await，但 Rust 的 async 是 lazy 的，只有在主动 await 后才开始执行。\n我们当然也可以手工为数据结构实现 Future：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 struct Delay { when: Instant, } impl Future for Delay { type Output = \u0026amp;\u0026#39;static str; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;\u0026amp;\u0026#39;static str\u0026gt; { if Instant::now() \u0026gt;= self.when { println!(\u0026#34;Hello world\u0026#34;); Poll::Ready(\u0026#34;done\u0026#34;) } else { // Ignore this line for now. cx.waker().wake_by_ref(); Poll::Pending } } } 同样用 await 触发 Future 的实际执行：\n1 2 3 4 5 6 7 8 #[tokio::main] async fn main() { let when = Instant::now() + Duration::from_millis(10); let future = Delay { when }; let out = future.await; assert_eq!(out, \u0026#34;done\u0026#34;); } Pin Future 被每个 await 分成多段，执行到 await 可能因为资源没准备好而让出 CPU 暂停执行，随后该 future 可能被调度到其他线程接着执行。所以 future 结构中需要保存跨await的数据，形成了自引用结构。\n自引用结构不能被移动，否则内部引用因为指向移动前的地址，引发不可控的问题。所以future需要被pin住，不能移动。\n如何让 future 不被move？ 方法调用时只传递引用，那么就没有移动 future。但是通过可变引用仍然可以使用 replace，swap 等方式移动数据。那么用 Pin 包装可变引用 Pin\u0026lt;\u0026amp;mut T\u0026gt;，让用户没法拿到 \u0026amp;mut T，就把这个漏洞堵上了。\n总之 Pin\u0026lt;\u0026amp;mut T\u0026gt; 不是数据的 owner，也没法获得 \u0026amp;mut T，所以就不能移动 T。\n注意，Pin 拿住的是一个可以解引用成 T 的指针类型 P，而不是直接拿原本的类型 T。Pin 本身是可 move 的，T 被 pin 住，不能 move。\nasync runtime的内部实现 要运行异步函数，必须将最外层的 Future 提交给 executor。 executor 负责调用Future::poll，推动异步计算的前进。\nexecutor 内部会有一个 Task 队列，executor 在 run 方法内，不停的从 receiver 获取 Task，然后执行。\nTask 包装了一个 future，同时内部持有一个 sender，用于将自身放回 executor 的 Task 队列。\nFuture 的 poll 方法，接收的是 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，而不是 \u0026amp;mut Self。所以在向 executor 提交 Future 时，需要先 pin 住，然后才能用来初始化 Task：\n1 2 3 4 5 6 7 8 9 10 11 fn spawn\u0026lt;F\u0026gt;(future: F, sender: \u0026amp;channel::Sender\u0026lt;Arc\u0026lt;Task\u0026gt;\u0026gt;) where F: Future\u0026lt;Output = ()\u0026gt; + Send + \u0026#39;static, { let task = Arc::new(Task { future: Mutex::new(Box::pin(future)), executor: sender.clone(), }); let _ = sender.send(task); } 保存在 Task 字段中的 Future 是 Pin\u0026lt;Box\u0026lt;Future\u0026gt;\u0026gt;，保证了以后每次调用 poll 传入的是 Pin\u0026lt;\u0026amp;mut Self\u0026gt;。注意，Pin 是可以移动的，Task 也是可以移动的，只是 Future 不能移动。\n在执行 Future 时，如果遇到资源未准备好，需要让出 CPU，那么 Task 可以将自己放入 Reactor。Task 实现了 ArcWake trait，实际上放入 Reactor 的 Waker 就是 Task 的包装：\n1 2 3 4 5 6 7 8 9 10 11 12 fn poll(self: Arc\u0026lt;Self\u0026gt;) { // Get a waker referencing the task. let waker = task::waker(self.clone()); // Initialize the task context with the waker. let mut cx = Context::from_waker(\u0026amp;waker); // This will never block as only a single thread ever locks the future. let mut future = self.future.try_lock().unwrap(); // Poll the future let _ = future.as_mut().poll(\u0026amp;mut cx); } 当 Reactor 得到了满足条件的事件，它会调用 Waker.wake() 唤醒之前挂起的任务。Waker.wake 会调用 Task::wake_by_ref 方法，将 Task 放回 executor 的任务队列：\n1 2 3 4 5 impl ArcWake for Task { fn wake_by_ref(arc_self: \u0026amp;Arc\u0026lt;Self\u0026gt;) { let _ = arc_self.executor.send(arc_self.clone()); } } Stream trait 对于 Iterator，可以不断调用其 next() 方法，获得新的值，直到 Iterator 返回 None。Iterator 是阻塞式返回数据的，每次调用 next()，必然独占 CPU 直到得到一个结果，而异步的 Stream 是非阻塞的，在等待的过程中会空出 CPU 做其他事情。\nStream::poll_next() 方法和 Future::poll() 类似, 除了它可以被重复调用，以便从 Stream 中接收多个值。然而，poll_next() 调用起来不方便，我们需要自己处理 Poll 状态。也就是说，await 语法糖只能应用在 Future 上，没法使用 stream.await 。所以，我们要想办法用 Future 包装 Stream，在 Future::poll() 中调用 Stream::poll_next()，这样就可以使用 await。 StreamExt 提供了 next() 方法，返回一个实现了 Future trait 的 Next 结构，这样，我们就可以直接通过 stream.next().await 来获取下一个值了。看一下 next() 方法以及 Next 结构的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 pub trait StreamExt: Stream { fn next(\u0026amp;mut self) -\u0026gt; Next\u0026lt;\u0026#39;_, Self\u0026gt; where Self: Unpin { assert_future::\u0026lt;Option\u0026lt;Self::Item\u0026gt;, _\u0026gt;(Next::new(self)) } } // next 返回的 Next 结构 pub struct Next\u0026lt;\u0026#39;a, St: ?Sized\u0026gt; { stream: \u0026amp;\u0026#39;a mut St, } // Next 实现了 Future，每次 poll() 实际上就是从 stream 中 poll_next() impl\u0026lt;St: ?Sized + Stream + Unpin\u0026gt; Future for Next\u0026lt;\u0026#39;_, St\u0026gt; { type Output = Option\u0026lt;St::Item\u0026gt;; fn poll(mut self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; { self.stream.poll_next_unpin(cx) } } 当手动实现一个 stream 时，它通常是通过合成 futures 和其他stream 来完成的。例如下面的例子，将 Lines 封装为 Stream，在 Stream::poll_next() 中利用了 Lines::poll_next_line()：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #[pin_project] struct LineStream\u0026lt;R\u0026gt; { #[pin] lines: Lines\u0026lt;BufReader\u0026lt;R\u0026gt;\u0026gt;, } impl\u0026lt;R: AsyncRead\u0026gt; LineStream\u0026lt;R\u0026gt; { /// 从 BufReader 创建一个 LineStream pub fn new(reader: BufReader\u0026lt;R\u0026gt;) -\u0026gt; Self { Self { lines: reader.lines(), } } } /// 为 LineStream 实现 Stream trait impl\u0026lt;R: AsyncRead\u0026gt; Stream for LineStream\u0026lt;R\u0026gt; { type Item = std::io::Result\u0026lt;String\u0026gt;; fn poll_next(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Option\u0026lt;Self::Item\u0026gt;\u0026gt; { self.project() .lines .poll_next_line(cx) .map(Result::transpose) } } Stream 可以是 unpin 的，Future 也可以是 unpin 的，如果他们内部包含了其他 !Unpin 的 Stream 或 Future，只需要把他们用 pin 包装，外面的 Stream 和 Future 就可以是 unpin 的。\n一般我们使用的 Stream 都是 unpin 的，如果不是，就用 pin 把它变成 unpin 的。为啥我们用的都是 unpin 的？因为能 move 的 Stream 更加灵活，可以作为参数和返回值。\nAsyncRead 和 AsyncWrite 所有同步的 Read / Write / Seek trait，前面加一个 Async，就构成了对应的异步 IO 接口。\nAsyncRead / AsyncWrite 的方法会返回一个实现了 Future 的 struct，这样我们才能使用 await ，将 future 提交到 async runtime，触发 future 的执行。例如 AsyncReadExt::read_to_end()方法，返回 ReadToEnd 结构，而 ReadToEnd 实现了 Future：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 pub trait AsyncReadExt: AsyncRead { ... fn read_to_end\u0026lt;\u0026#39;a\u0026gt;(\u0026amp;\u0026#39;a mut self, buf: \u0026amp;\u0026#39;a mut Vec\u0026lt;u8\u0026gt;) -\u0026gt; ReadToEnd\u0026lt;\u0026#39;a, Self\u0026gt; where Self: Unpin, { read_to_end(self, buf) } } pin_project! { #[derive(Debug)] #[must_use = \u0026#34;futures do nothing unless you `.await` or poll them\u0026#34;] pub struct ReadToEnd\u0026lt;\u0026#39;a, R: ?Sized\u0026gt; { reader: \u0026amp;\u0026#39;a mut R, buf: VecWithInitialized\u0026lt;\u0026amp;\u0026#39;a mut Vec\u0026lt;u8\u0026gt;\u0026gt;, // The number of bytes appended to buf. This can be less than buf.len() if // the buffer was not empty when the operation was started. read: usize, // Make this future `!Unpin` for compatibility with async trait methods. #[pin] _pin: PhantomPinned, } } impl\u0026lt;A\u0026gt; Future for ReadToEnd\u0026lt;\u0026#39;_, A\u0026gt; where A: AsyncRead + ?Sized + Unpin, { type Output = io::Result\u0026lt;usize\u0026gt;; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; { let me = self.project(); read_to_end_internal(me.buf, Pin::new(*me.reader), me.read, cx) } } ","date":"2022-09-24T16:48:29+08:00","permalink":"https://mazhen.tech/p/rust-%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/","title":"Rust 异步编程笔记"},{"content":"读写流程 Ledger 的元数据信息保存在zookeeper中。\n写入流程 当写入发生时，首先 entry 被写入一个 journal 文件。journal 是一个 write-ahead log（WAL），它帮助 BookKeeper 在发生故障时避免数据丢失。这与关系型数据库实现数据持久化的机制相同。\n同时 entry 会被加入到 write cache 中。write cache 中累积的 entry 会定期排序，异步刷盘到 entry log 文件中。同一个 ledger 的 entry 排序后会被放在一起，这样有利于提高读取的性能。\nwrite cache 中的 entry 也将被写入 RocksDB， RocksDB 记录了每个 entry 在 entry log 文件中的位置，是 (ledgerId, entryId) 到 (entry log file, offset) 的映射，即可以通过(ledgerId, entryId)，在 entry log 文件中定位到 entry。\n读取流程 读取时会首先查看 write cache ，因为 write cache 中有最新的 entry。如果 write cache 中没有，那么接着查看 read cache。如果 read cache 中还是没有，那么就通过 (ledgerId, entryId) 在 RocksDB 中查找到该 entry 所在的 entry log 文件及偏移量，然后从 entry log 文件中读取该 entry ，并更新到 read cache 中，以便后续请求能在 read cache 中命中。两层缓存让绝大多数的读取通常是从内存获取。\n读写隔离 BookKeeper 中的写入都是顺序写入 journal 文件，该文件可以存储在专用磁盘上，可以获得更大的吞吐量。write cache 中的 entry 会异步批量写入 entry log 文件和 RocksDB，通常配置在另外一块磁盘。因此，一个磁盘用于同步写入（ journal 文件），另一个磁盘用于异步优化写入，以及所有的读取。\n数据一致性 Bookie 操作基本都是在客户端完成和实现的，比如副本复制、读写 entry 等操作。这些有助于确保 BookKeeper 的一致性。\n客户端在创建 ledger 时，会出现 Ensemble、Write Quorum 和 Ack Quorum 这些数据指标。\nEnsemble —— 用哪几台 bookie 去存储 ledger 对应的 entry\nWrite Quorum ——对于一条 entry，需要存多少副本\nAck Quorum —— 在写 entry 时，要等几个 response\n我们会用（5,3,2）的实例进行讲述\n（5,3,2) 代表了对于一个 ledger ，会挑 5 台 bookie 去存储所有的 entry。所以当 entry 0 生成时，可以根据hash模型计算出应该放置到哪台 bookie。比如 E0 对应 B1，B2，B3，那 E1 就对应 B2，B3，B4，以此类推。\n虽然总体是需要 5 台 bookie，但是每条 entry 只会占用 3 台 bookie 去存放，并只需等待其中的 2 台 bookie 给出应答即可。\nLastAddConfirm LAC（LastAddConfirm）是由 LAP（LastAddPush） 延伸而来是根据客户端进行维护并发布的最后一条 entry id，从 0 开始递增。所以 LAC 是由应答确认回来的最后一条 entry id 构成，如下图右侧显示。 LAC 以前的 entry ID （比它本身小的）都已确认过，它其实是一致性的边界，LAC 之前和之后的都是一致的。 同时 LAC 作为 bookie 的元数据，可以根据此来判断 entry 的确认与否。这样做的好处是，LAC 不会受限于一个集中的元数据管理，可以分散存储在存储节点。 Ensemble change 当其中的某个 bookie 挂掉时，客户端会进行一个 ensemble change 的操作，用新的 bookie 替换挂掉的旧 bookie。比如 当bookie 4 挂掉时，可以使用 bookie 6 进行替换。 整个过程，只要有新的存储节点出现，就会保证不会中断读写操作是，即过程中随时补新。\nEnsemble change 对应到元数据存储，即对元数据的修改。之前的 E0-E6 是写在 B1～B5 上，E7 以后写在了 B1、B2、B3、B6、B5 上。这样就可以通过元数据的方式，看到数据到底存储在那个bookie上。 Bookie Fencing BookKeeper 有一个极其重要的功能，叫做 fencing。fencing 功能让 BookKeeper 保证只有一个写入者（Pulsar broker）可以写入一个 ledger。\nBroker（B1）挂掉，Broker（B2）接管 B1 上topic X的流程：\n当前拥有 topic X 所有权的Pulsar Broker（B1）被认为已经挂掉或不可用（通过ZooKeeper）。\n另一个Broker（B2）将topic X 的当前 ledger 的状态从 OPEN 更新为 IN_RECOVERY。\nB2 向当前 ledger 的所有 bookie 发送 fencing LAC read 请求，并等待(Write Quorum - Ack Quorum)+1的响应。一旦收到这个数量的回应，ledger 就已经被 fencing。B1就算还活着，也不能再写入这个ledger，因为B1无法得到 Ack Quorum 的确认。\nB2采取最高的LAC响应，然后开始执行从 LAC+1 的恢复性读取。它确保从该点开始的所有 entry 都被复制到 Write Quorum 个bookie。当 B2 不能再读取和复制任何entry，ledger 完成恢复。\nB2将 ledger 的状态改为 CLOSED。\nB2打开一个新的 ledger，现在可以接受对Topic X的写入。\n整个失效恢复的过程，是没有回头复用 ledger 的。因为复用意味着所有元素都处于相同状态且都同意后才能继续去读写，这个是很难控制的。\n我们从主从复制方式进行切入，将其定义为物理文件。数据从主复制到从，由于复制过程的速度差异，为了保证所有的一致性，需要做一些「删除/清空类」的操作。但是这个过程中一旦包含覆盖的操作，就会在过程中更改文件状态，容易出现 bug。\nBookKeeper 在运行的过程中，不是一个物理文件，而是逻辑上的序。同时在失效恢复过程中，没有进行任何的复用，使得数据恢复变得简单又清晰。其次它在整个修复过程中，没有去额外动用 ledger X 的数据。\n自动恢复 当一个 bookie 失败时，这个 bookie 上所有 ledger 的 fragments 都将被复制到其他节点，以确保每个 ledger 的复制系数（Write quorum）得到保证。\nrecovery 不是 BookKeeper 复制协议的一部分，而是在 BookKeeper 外部运行，作为一种异步修复机制使用。\n有两种类型的recovery：手动或自动。\n自动恢复进程 AutoRecoveryMain 可以在独立的服务器上运行，也可以和bookie跑在一起。其中一个自动恢复进程被选为审核员，然后进行如下操作：\n从 zookeeper 读取所有的 ledger 列表，并找到位于失败 bookie上的 ledger。\n对于第一步找到的所有ledger，在ZooKeeper上创建一个复制任务。\nAutoRecoveryMain 进程会发现 ZooKeeper 上的复制任务，锁定任务，进行复制，满足Write quorum，最后删除任务。\n通过自动恢复，Pulsar集群能够在面对存储层故障时进行自我修复，只需确保部署了适量的bookie就可以了。\n如果审计节点失败，那么另一个节点就会被提升为审计员。\n","date":"2022-09-17T16:42:59+08:00","permalink":"https://mazhen.tech/p/bookkeeper%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/","title":"BookKeeper实现分析"},{"content":" Previous Next \u0026nbsp; \u0026nbsp; Page: / ","date":"2022-08-22T16:24:52+08:00","permalink":"https://mazhen.tech/p/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/","title":"深入浅出容器技术"},{"content":"GCC 会为不同 CPU 架构预定义宏，如 __x86_64__ 代表Intel 64位CPU， __aarch64__代表 ARM64。 网上已经有文档对 GCC 为 CPU 的预定义宏进行了总结。\n这些预定义的宏有什么用呢？我们在代码中可以判断出当前的 CPU 架构，那么可以针对 不同CPU的特性，进行优化实现。例如RocksDB 对于获取当前时间，在 x86 平台上，会用到 Time Stamp Counter (TSC) 寄存器，使用 RDTSC 指令提取 TSC 中值。对于 ARM 64 也有类似的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Get the value of tokutime for right now. We want this to be fast, so we // expose the implementation as RDTSC. static inline tokutime_t toku_time_now(void) { #if defined(__x86_64__) || defined(__i386__) uint32_t lo, hi; __asm__ __volatile__(\u0026#34;rdtsc\u0026#34; : \u0026#34;=a\u0026#34;(lo), \u0026#34;=d\u0026#34;(hi)); return (uint64_t)hi \u0026lt;\u0026lt; 32 | lo; #elif defined(__aarch64__) uint64_t result; __asm __volatile__(\u0026#34;mrs %[rt], cntvct_el0\u0026#34; : [ rt ] \u0026#34;=r\u0026#34;(result)); return result; #elif defined(__powerpc__) return __ppc_get_timebase(); #elif defined(__s390x__) uint64_t result; asm volatile(\u0026#34;stckf %0\u0026#34; : \u0026#34;=Q\u0026#34;(result) : : \u0026#34;cc\u0026#34;); return result; #else #error No timer implementation for this platform #endif } 而在将 RocksDB 移植到龙芯的过程中，需要修改上面的代码，判断出当前是龙芯 loongarch64 架构。\n网上没有搜到 GCC 对龙芯 CPU 的预定宏的文档说明，只能从源码中找答案：\n1 2 3 4 5 6 7 void loongarch_cpu_cpp_builtins (cpp_reader *pfile) { ... builtin_define (\u0026#34;__loongarch__\u0026#34;); ... } 可以看到，__loongarch__代表龙芯CPU。 在暂时不知道龙芯是否支持RDTSC的情况下，只能给出通用的实现，以后再查龙芯的CPU手册进行优化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 #if defined(__x86_64__) || defined(__i386__) ... #elif defined(__aarch64__) ... #elif defined(__powerpc__) ... #elif defined(__loongarch__) struct timeval tv; gettimeofday(\u0026amp;tv,NULL); return tv.tv_sec*(uint64_t)1000000+tv.tv_usec; #else #error No implementation for this platform #endif ","date":"2022-06-17T16:19:54+08:00","permalink":"https://mazhen.tech/p/gcc-%E4%B8%BA%E9%BE%99%E8%8A%AF-cpu%E7%9A%84%E9%A2%84%E5%AE%9A%E4%B9%89%E5%AE%8F/","title":"GCC 为龙芯 CPU的预定义宏"},{"content":"我们经常会使用 kill 命令杀掉运行中的进程，对多次杀不死的进程进一步用 kill -9 干掉它。你可能知道这是在用 kill 命令向进程发送信号，优雅或粗暴的让进程退出。我们能向进程发送很多类型的信号，其中一些常见的信号 SIGINT 、SIGQUIT、 SIGTERM 和 SIGKILL 都是通知进程退出，但它们有什么区别呢？很多人经常把它们搞混，这篇文章会让你了解 Linux 的信号机制，以及一些常见信号的作用。\n什么是信号 信号（Signal）是 Linux 进程收到的一个通知。当进程收到一个信号时，该进程会中断其执行，并执行收到信号对应的处理程序。\n信号机制作为 Linux 进程间通信的一种方法。Linux 进程间通信常用的方法还有管道、消息、共享内存等。\n信号的产生有多种来源：\n硬件来源，例如 CPU 内存访问出错，当前进程会收到信号 SIGSEGV；按下 Ctrl+C 键，当前运行的进程会收到信号 SIGINT 而退出； 软件来源，例如用户通过命令 kill [pid]，直接向一个进程发送信号。进程使用系统调用 int kill(pid_t pid, int sig) 显示的向另一个进程发送信号。内核在某些情况下，也会给进程发送信号，例如当子进程退出时，内核给父进程发送 SIGCHLD 信号。 你可以使用 kill -l 命令查看系统实现了哪些信号：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ kill -l 1) SIGHUP\t2) SIGINT\t3) SIGQUIT\t4) SIGILL\t5) SIGTRAP 6) SIGABRT\t7) SIGBUS\t8) SIGFPE\t9) SIGKILL\t10) SIGUSR1 11) SIGSEGV\t12) SIGUSR2\t13) SIGPIPE\t14) SIGALRM\t15) SIGTERM 16) SIGSTKFLT\t17) SIGCHLD\t18) SIGCONT\t19) SIGSTOP\t20) SIGTSTP 21) SIGTTIN\t22) SIGTTOU\t23) SIGURG\t24) SIGXCPU\t25) SIGXFSZ 26) SIGVTALRM\t27) SIGPROF\t28) SIGWINCH\t29) SIGIO\t30) SIGPWR 31) SIGSYS\t34) SIGRTMIN\t35) SIGRTMIN+1\t36) SIGRTMIN+2\t37) SIGRTMIN+3 38) SIGRTMIN+4\t39) SIGRTMIN+5\t40) SIGRTMIN+6\t41) SIGRTMIN+7\t42) SIGRTMIN+8 43) SIGRTMIN+9\t44) SIGRTMIN+10\t45) SIGRTMIN+11\t46) SIGRTMIN+12\t47) SIGRTMIN+13 48) SIGRTMIN+14\t49) SIGRTMIN+15\t50) SIGRTMAX-14\t51) SIGRTMAX-13\t52) SIGRTMAX-12 53) SIGRTMAX-11\t54) SIGRTMAX-10\t55) SIGRTMAX-9\t56) SIGRTMAX-8\t57) SIGRTMAX-7 58) SIGRTMAX-6\t59) SIGRTMAX-5\t60) SIGRTMAX-4\t61) SIGRTMAX-3\t62) SIGRTMAX-2 63) SIGRTMAX-1\t64) SIGRTMAX 使用 man 7 signal 命令查看系统对每个信号作用的描述：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Signal Standard Action Comment ──────────────────────────────────────────────────────────────────────── SIGABRT P1990 Core Abort signal from abort(3) SIGALRM P1990 Term Timer signal from alarm(2) SIGBUS P2001 Core Bus error (bad memory access) SIGCHLD P1990 Ign Child stopped or terminated SIGCLD - Ign A synonym for SIGCHLD SIGCONT P1990 Cont Continue if stopped SIGEMT - Term Emulator trap SIGFPE P1990 Core Floating-point exception SIGHUP P1990 Term Hangup detected on controlling terminal or death of controlling process SIGILL P1990 Core Illegal Instruction ... 信号和中断 信号处理是一种典型的异步事件处理方式：进程需要提前向内核注册信号处理函数，当某个信号到来时，内核会就执行相应的信号处理函数。\n我们知道，硬件中断也是一种内核的异步事件处理方式。当外部设备出现一个必须由 CPU 处理的事件，如键盘敲击、数据到达网卡等，内核会收到中断通知，暂时打断当前程序的执行，跳转到该中断类型对应的中断处理程序。中断处理程序是由 BIOS 和操作系统在系统启动过程中预先注册在内核中的。\n中断和信号通知都是在内核产生。中断是完全在内核里完成处理，而信号的处理则是在用户态完成的。也就是说，内核只是将信号保存在进程相关的数据结构里面，在执行信号处理程序之前，需要从内核态切换到用户态，执行完信号处理程序之后，又回到内核态，再恢复进程正常的运行。\n可以看出，中断和信号的严重程度不一样。信号影响的是一个进程，信号处理出了问题，最多是这个进程被干掉。而中断影响的是整个系统，一旦中断处理程序出了问题，可能整个系统都会挂掉。\n信号处理 一旦有信号产生，进程对它的处理都有下面三个选择。\n执行缺省操作（Default）。Linux 为每个信号都定义了一个缺省的行为。例如，信号 SIGKILL 的缺省操作是 Term，也就是终止进程的意思。信号 SIGQUIT 的缺省操作是 Core，即终止进程后，通过 Core Dump 将当前进程的运行状态保存在文件里面。 捕捉信号（Catch）。这个是指让用户进程可以注册自己针对这个信号的处理函数。当信号发生时，就执行我们注册的信号处理函数。 忽略信号（Ignore）。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。 有两个信号例外，对于 SIGKILL 和 SIGSTOP 这个两个信号，进程是无法捕捉和忽略，它们用于在任何时候中断或结束某一进程。SIGKILL 和 SIGSTOP 为内核和超级用户提供了删除任意进程的特权。\n如果我们不想让信号执行缺省操作，可以对特定的信号注册信号处理函数：\n1 2 3 4 5 6 #include \u0026lt;signal.h\u0026gt; typedef void (*sighandler_t)(int); sighandler_t signal(int signum, sighandler_t handler); 例如下面的例子，程序捕获了信号 SIGINT ，并且只是输出不做其他处理，这样在键盘上按 Ctrl+C 并不能让程序退出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; void sig_handler(int signo) { if (signo == SIGINT) { printf(\u0026#34;received SIGINT\\n\u0026#34;); } } int main(int argc, char *argv[]) { signal(SIGINT, sig_handler); printf(\u0026#34;Process is sleeping\\n\u0026#34;); while (1) { sleep(1000); } return 0; } 通过 signal 注册的信号处理函数，会保存在进程内核的数据结构 task_struct 中。由于信号都发给进程，并由进程在用户态处理，所以发送给进程的信号也保存在 task_struct 中。\ntask_struct-\u0026gt;sighand 和 task_struct-\u0026gt;signal 是线程组内共享，而 task_struct-\u0026gt;pending 是线程私有的。\nstask_struct-\u0026gt;sighand 里面有一个 action，这是一个数组，下标是信号，数组内容就是注册的信号处理函数。\ntask_struct-\u0026gt;pending 内包含了一个链表，保存了本线程所有的待处理信号。task_struct-\u0026gt;signal-\u0026gt;shared_pending 上也有一个待处理信号链表，这个链表保存的是线程组内共享的信号。\n常见信号 下面的列表列举了一些常见的信号。\nSingal Value Action comment key binding SIGHUP 1 Term Hangup detected on controlling terminal or death of controlling process SIGINT 2 Term Interrupt from keyboard Ctrl-c SIGQUIT 3 Core Quit from keyboard Ctrl-\\ SIGKILL 9 Term Kill signal SIGSEGV 11 Core Invalid memory reference SIGPIPE 13 Term Broken pipe: write to pipe with no readers SIGTERM 15 Term Termination signal SIGCHLD 17 Ign Child stopped or terminated SIGCONT 18 Cont Continue if stopped SIGSTOP 19 Stop Stop process SIGTSTP 20 Stop Stop typed at terminal Ctrl-z SIGTTIN 21 Stop Terminal input for background process SIGTTOU 22 Stop Terminal output for background process 第一列是信号名称，第二列是信号编号。使用 kill 向进程发送信号时，用信号名称和编号都可以，例如：\n1 2 kill -1 [pid] kill -SIGHUP [pid] Action 列是信号的缺省行为，主要有如下几个：\nTerm 终止进程 Core 终止进程并core dump Ign 忽略信号 Stop 停止进程 Cont 如果进程是已停止，则恢复进程执行 有一些信号在 TTY 终端做了键盘按键绑定，例如 CTRL+c 会向终端上运行的前台进程发送 SIGINT 信号。\nSIGHUP 运行在终端中，由 bash 启动的进程，都是 bash 的子进程。终端退出结束时会向 bash 的每一个子进程发送 SIGHUP 信号。由于 SIGHUP 的缺省行为是 Term，因此，即使运行在后台的进程也会和终端一起结束。\n使用 nohup 命令可解决这个问题，它的作用是让进程忽略 SIGHUP 信号：\n1 $ nohup command \u0026gt;cmd.log 2\u0026gt;\u0026amp;1 \u0026amp; 这样，即使我们退出了终端，运行在后台的程序会忽视 SIGHUP 信号而继续运行。由于作为父进程的 bash 进程已经结束，因此 /sbin/init 就成为孤儿进程新的父进程。\nSIGINT, SIGQUIT, SIGTERM 和 SIGKILL SIGTERM 和 SIGKILL 是通用的终止进程请求，SIGINT 和 SIGQUIT 是专门用于来自终端的终止进程请求。他们的关键不同点是：SIGINT 和 SIGQUIT 可以是用户在终端使用快捷键生成的，而 SIGTERM 和 SIGKILL 必须由另一个程序以某种方式生成（例如通过 kill 命令）。\n当用户按下 ctrl-c 时，终端将发送 SIGINT 到前台进程。 SIGINT 的缺省行为是终止进程（Term），但它可以被捕获或忽略。 信号 SIGINT 的目的是为进程提供一种有序、优雅的关闭机制。\n当用户按下 ctrl-\\ 时，终端将发送 SIGQUIT 到前台进程。 SIGQUIT 的缺省行为是终止进程并 core dump，它同样可以被捕获或忽略。 你可以认为 SIGINT 是用户发起的愉快的终止，而 SIGQUIT 是用户发起的不愉快终止，需要生成 Core Dump ，方便用户事后进行分析问题在哪里。\n在 ubuntu 上由 systemd-coredump 系统服务处理 core dump。我们可以使用 coredumpctl 命令行工具查询和处理 core dump 文件。\n1 2 3 $ coredumpctl list TIME PID UID GID SIG COREFILE EXE SIZE Tue 2022-04-12 22:09:52 CST 6754 1000 1000 SIGQUIT present /usr/bin/cat 17.1K core dump 文件缺省保存在 /var/lib/systemd/coredump 目录下。\nSIGTERM 默认行为是终止进程，但它也可以被捕获或忽略。SIGTERM 的目的是杀死进程，它允许进程有机会在终止前进行清理，优雅的退出。当我们使用 kill 命令时，SIGTERM 是默认信号。\nSIGKILL 唯一的行为是立即终止进程。 由于 SIGKILL 是特权信号，进程无法捕获和忽略，因此进程在收到该信号后无法进行清理，立刻退出。\n例如 docker 在停止容器的时候，先给容器里的1号进程发送 SIGTERM，如果不起作用，那么等待30秒后会会发送 SIGKILL，保证容器最终会被停止。\nSIGSTOP 、 SIGTSTP 和 SIGCONT SIGSTOP 和 SIGTSTP 这两个信号都是为了暂停一个进程，但 SIGSTOP 是特权信息，不能被捕获或忽略。\nSIGSTOP 必须由另一个程序以某种方式生成（例如：kill -SIGSTOP pid），而SIGTSTP 也可以由用户在键盘上键入快捷键 Ctrl-z 生成。\n被暂停的进程通过信号 SIGCONT 恢复。当用户调用 fg 命令时，SIGCONT 由 shell 显式发送给被暂停的进程。\nLinux 使用他们进行作业控制，让你能够手动干预和停止正在运行的应用程序，并在未来某个时间恢复程序的执行。\nSIGTTOU 和 SIGTTIN Linux 系统中可以有多个会话（session），每个会话可以包含多个进程组，每个进程组可以包含多个进程。\n会话是用户登录系统到退出的所有活动，从登录到结束前创建的所有进程都属于这次会话。会话有一个前台进程组，还可以有一个或多个后台进程组。只有前台进程可以从终端接收输入，也只有前台进程才被允许向终端输出。如果一个后台作业中的进程试图进行终端读写操作，终端会向整个作业发送 SIGTTOU 或 SIGTTIN 信号，默认的行为是暂停进程。\nJVM 对信号的处理 如果你使用 strace 追踪 Java 应用，发现 Java 程序会抛出大量 SIGSEGV。\n1 2 3 4 5 6 7 8 9 10 11 $ strace -fe \u0026#39;trace=!all\u0026#39; java [app] ... [pid 21746] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061680} --- [pid 21872] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061480} --- [pid 21943] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061500} --- [pid 21844] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061780} --- [pid 21728] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061c00} --- [pid 21906] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061980} --- [pid 21738] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061100} --- [pid 21729] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061e00} --- ... SIGSEGV 信号的意思是 “分段错误”（segmentation fault），是当系统检测到进程试图访问不属于它的内存地址时，内核向进程发送的信号。SIGSEGV 对于一般应用来说是很严重的错误，但 Java 进程中的 SIGSEGV 几乎总是正常和安全的。\n在常规的 C/C++ 程序中，当你期望指针是指向某个结构，但实际指向的是 NULL，会导致应用程序崩溃。这种崩溃实际上是内核向进程发送了信号 SIGSEGV。如果应用程序没有为该信号注册信号处理程序，则信号会返回到内核，然后内核会终止应用。实际上 JVM 为 SIGSEGV 注册了一个信号处理程序，因为 JVM 想使用 SIGSEGV 和其他一些信号来实现自己的目的。\n这篇文档 描述了 JVM 对信号的特殊处理：\nSignal Description SIGSEGV, SIGBUS, SIGFPE, SIGPIPE, SIGILL These signals are used in the implementation for implicit null check, and so forth. SIGQUIT This signal is used to dump Java stack traces to the standard error stream. (Optional) SIGTERM, SIGINT, SIGHUP These signals are used to support the shutdown hook mechanism (java.lang.Runtime.addShutdownHook) when the VM is terminated abnormally. (Optional) SIGUSR1 This signal is used in the implementation of the java.lang.Thread.interrupt method. Not used starting with Oracle Solaris 10 reserved on Linux. (Configurable) SIGUSR2 This signal is used internally. Not used starting with Oracle Solaris 10 operating system. (Configurable) SIGABRT The HotSpot VM does not handle this signal. Instead it calls the abort function after fatal error handling. If an application uses this signal then it should terminate the process to preserve the expected semantics. 实际上，JVM 是使用 SIGSEGV、SIGBUS、SIGPIPE 等进行代码中的各种 NULL 检查。\n同样，我们在终端上键入 ctrl-\\，也不会让前台运行的 Java 进程终止并 core dump，而是会将 Java 进程的 stack traces 输出到终端的标准错误流。\n那么如何对 Java 进程进行 core dump 呢？需要在 Java 的启动命令里增加 JVM 选项 -Xrs ，它会让 JVM 不自己处理 SIGQUIT 信号，这样 SIGQUIT 会触发缺省行为 core dump。\n一般 Java 进程的运行时内存占用都比较大，在进行 core dump 时很容易超过缺省大小而被truncated，因此需要修改配置文件 /etc/systemd/coredump.conf，合理设置 ProcessSizeMax 和 ExternalSizeMax 的大小。\n","date":"2022-04-13T16:17:35+08:00","permalink":"https://mazhen.tech/p/linux-%E4%BF%A1%E5%8F%B7signal/","title":"Linux 信号(Signal)"},{"content":"长时间运行的Linux服务器，通常 free 的内存越来越少，让人觉得 Linux 特别能“吃”内存，甚至有人专门做了个网站 LinuxAteMyRam.com解释这个现象。实际上 Linux 内核会尽可能的对访问过的文件进行缓存，来弥补磁盘和内存之间巨大的延迟差距。缓存文件内容的内存就是 Page Cache。\nGoogle 的大神 Jeffrey Dean总结过一个Latency numbers every programmer should know，其中提到从磁盘读取 1MB 数据的耗时是内存的80倍，即使换成 SSD 也是内存延迟的 4 倍。\n我在本机做了实验，来体会一下 Page Cache 的作用。首先生成一个 1G 大小的文件：\n1 # dd if=/dev/zero of=/root/dd.out bs=4096 count=262144 清空 Page Cache：\n1 # sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches 统计第一次读取文件的耗时：\n1 2 3 4 5 # time cat /root/dd.out \u0026amp;\u0026gt; /dev/null real\t0m2.097s user\t0m0.010s sys\t0m0.638s 再此读取同一个文件，由于系统已经将读取过的文件内容放入了 Page Cache ，这次耗时大大缩短：\n1 2 3 4 5 # time cat /root/dd.out \u0026amp;\u0026gt; /dev/null real\t0m0.186s user\t0m0.004s sys\t0m0.182s Page Cache 不仅能加速对文件内容的访问，对共享库建立 Page Cache，可以在多个进程间共享，避免每个进程都单独加载，造成宝贵内存资源的浪费。\nPage Cache 是什么 Page Cache 是由内核管理的内存，位于 VFS(Virtual File System) 层和具体文件系统层（例如ext4，ext3）之间。应用进程使用 read/write 等文件操作，通过系统调用进入到 VFS 层，根据 O_DIRECT 标志，可以使用 Page Cache 作为文件内容的缓存，也可以跳过 Page Cache 不使用内核提供的缓存功能。\n另外，应用程序可以使用 mmap ，将文件内容映射到进程的虚拟地址空间，可以像读写内存一样直接读写硬盘上的文件。进程的虚拟内存直接和 Page Cache 映射。\n为了了解内核是怎么管理 Page Cache 的，我们先看一下 VFS 的几个核心对象：\nfile 存放已打开的文件信息，是进程访问文件的接口； dentry 使用 dentry 将文件组织成目录树结构； inode 唯一标识文件系统中的文件。对于同一个文件，内核中只会有一个 inode 结构。 对于每一个进程，打开的文件都有一个文件描述符，内核中进程数据结构 task_struct 中有一个类型为 files_struct 的 files 字段，保存着该进程打开的所有文件。files_struct 结构的fd_array 字段是 file 数组， 数组的下标是文件描述符，内容指向一个 file 结构，表示该进程打开的文件。file 与打开文件的进程相关联，如果多个进程打开同一个文件，那么每个进程都有自己的 file ，但这些 file 指向同一个 inode。\n如上图所示，进程通过文件描述符与 VFS 中的 file 产生联系， 每个 file 对象又与一个 dentry 对应，根据 dentry 能找到 inode，而 inode 则代表文件本身。上图中进程 A 和进程 B 打开了同一个文件，进程 A 和进程 B 都维护着各自的 file ，但它们指向同一个 inode。\ninode 通过 address_space 管理着文件已加载到内存中的内容，也就是 Page Cache。address_space 的字段 i_pages 指向一棵 xarray 树，与这个文件相关的 Page Cache 页都挂在这颗树上。我们在访问文件内容的时候，根据指定文件和相应的页偏移量，就可以通过 xarray 树快速判断该页是否已经在 Page Cache 中。如果该页存在，说明文件内容已经被读取到了内存，也就是存在于 Page Cache 中；如果该页不存在，就说明内容不在 Page Cache 中，需要从磁盘中去读取。\n由于文件和 inode 一一对应，我们可以认为 inode 是 Page Cache 的宿主（host），内核通过 inode-\u0026gt;imapping-\u0026gt;i_pages 指向的树，管理维护着 Page Cache。\nPage Cache 是如何产生和释放，又是如何与进程相关联的呢？我们需要先了解进程虚拟地址空间。\n进程虚拟地址空间 Linux 是多任务系统，它支持多个进程的的并发执行。操作系统和 CPU 联手制造了一个假象：每个进程都独享连续的虚拟内存空间，并且各个进程的地址空间是完全隔离的，因此进程并不会意识到彼此的存在。从进程的角度来看，它会认为自己是系统中唯一的进程。\n进程看到的是虚拟内存的地址空间，它也不能直接访问物理地址。当进程访问某个虚拟地址的时候，该虚拟地址由内核负责转换成物理内存地址，即完成虚拟地址到物理地址的映射。这样不同进程在运行的时候，即使访问相同的虚拟地址，但内核会将它们映射到不同的物理地址，因此不会发生冲突。\n进程在 Linux 内核由 task_struct 所描述。估计 task_struct 是你学习内核时第一个熟悉的数据结构，因为它实在太重要了。 task_struct描述了进程相关的所有信息，包括进程状态，运行时统计信息，进程亲缘关系，进度调度信息，信号处理，进程内存管理，进程打开的文件等等。我们这里关注的进程虚拟内存空间，是由 task_struct 中的 mm 字段指向的 mm_struct 所描述，它是一个进程内存的运行时摘要信息。\n进程的虚拟地址是线性的，使用结构体 vm_area_struct 来描述。内核将每一段具有相同属性的内存区域当作一个 vm_area_struct 进行管理，每个 vm_area_struct 是一个连续的虚拟地址范围，这些区域不会互相重叠。 mm_struct 里面有一个单链表 mmap，用于将 vm_area_struct 串联起来，另外还有一颗红黑树 mm_rb ，vm_area_struct 根据起始地址挂在这颗树上。使用红黑树可以根据地址，快速查找一个内存区域。\nvm_area_struct 可以直接映射到物理内存，也可以关联文件。如果 vm_area_struct 是文件映射，由成员 vm_file 指向对应的文件指针。一个没有关联文件的 vm_area_struct 是匿名内存。\n开发者使用 malloc 等 glibc 库函数分配内存的时候，不是直接分配物理内存，而是在进程的虚拟内存空间中申请一段虚拟内存，生成相应的数据结构 vm_area_struct ，然后将它插进 mm_struct 的链表 mmap，同时挂在红黑树 mm_rb 上，就算完成了工作，根本没有涉及到物理内存的分配。只有当第一次对这块虚拟内存进行读写时，发现该内存区域没有映射到物理内存，这时会触发缺页中断，然后由内核填写页表，完成虚拟内存到物理内存的映射。\n当开发者使用 mmap 进行文件映射时，内核根据 vm_area_struct 中代表文件映射关系 vm_file，将文件内容从磁盘加载到物理内存，也就是 Page Cache 中，最后建立这段虚拟地址到物理地址的映射。\n另外，在虚拟内存中连续的页面，在物理内存中不必是连续的。只要维护好从虚拟内存页到物理内存页的映射关系，你就能正确地使用内存。由于每个进程都有独立的地址空间，为了完成虚拟地址到物理地址的映射，每个进程都要有独立的进程页表。在一个实际的进程里面，虚拟内存占用的地址空间，通常是两段连续的空间，而不是完全散落的随机的内存地址。基于这个特点，内核使用多级页表保存映射关系，可以大大减少页表本身的空间占用。最顶级的页表保存在 mm_struct 的 pgd 字段中。\n好了，我们对进程虚拟地址空间有了基本的了解，下面看看 Page Cache 的产生和释放，以及如何与进程空间发生联系的。\nPage Cache 的产生和释放 Page Cache 的产生有两种不同的方式：\nBuffered I/O Memory-Mapped file 使用这两种方式访问磁盘上的文件时，内核会根据指定的文件和相应的页偏移量，判断文件内容是否已经在 Page Cache 中，如果内容不存在，需要从磁盘中去读取并创建 Page Cache 页。\n这两种方式的不同之处在于，使用 Buffered I/O，要先将数据从 Page Cache 拷贝到用户缓冲区，应用才能从用户缓冲区读取数据。而对于 Memory-Mapped file 而言，则是直接将 Page Cache 页映射到进程虚拟地址空间，用户可以直接读写 Page Cache 中的内容。由于少了一次 copy，使用 Memory-Mapped file 要比 Buffered I/O 的效率高一些。\n随着服务器运行时间的增加，系统中空闲内存会越来越少，其中很大一部分都被 Page Cache 占用。访问过的文件都被 Page Cache 缓存，内存最终会被耗尽，那什么时候回收 Page Cache 呢？ 内核认为，Page Cache 是可回收内存，当应用在申请内存时，如果没有足够的空闲内存，就会先回收 Page Cache，再尝试申请。回收的方式主要是两种：直接回收和后台回收。\n使用 Buffered I/O 时，Page Cache 并没有和进程的虚拟内存空间产生直接的关联，而是通过用户缓冲区作为中转。效率更好的Memory-Mapped file方式，看着比较简单，但背后的实现却有些复杂。下面我们看一下内核是如何实现 Memory-Mapped file 的。\n内存文件映射 前面我们介绍过， inode 是 Page Cache 的宿主（host），内核通过 inode-\u0026gt;imapping-\u0026gt;i_pages 指向的树，管理维护着 Page Cache。那么内核是如何完成内存文件映射，直接把缓存了文件内容的 Page Cache 映射到进程虚拟内存空间的呢？\n我们知道，进程结构体 task_struct 中的字段 mm 指向该进程的虚拟地址空间 mm_struct ，而一段虚拟内存由结构体 vm_area_struct 所描述，将 vm_area_struct 串在一起的链表 mmap 就代表了已经申请分配的虚拟内存。\n如果是进行内存文件映射，那么映射了文件的虚拟内存区域 vm_area_struct ，它的 vm_file 会指向被映射的文件结构体 file。file 表示进程打开的文件，它的成员 f_mapping 指向 address_space，这样就和管理文件着 Page Cache 的 address_space 关联了起来。\n当第一次访问文件映射的虚拟内存区域时，这段虚拟内存并没有映射到物理内存，这时会触发缺页中断。内核在处理缺页中断时，发现代表这段虚拟内存的 vm_area_struct 有关联的文件，即 vm_file 字段指向一个文件结构体 file。内核拿到该文件的 address_space，根据要访问内容的页偏移量，对 address_space-\u0026gt;i_pages 指向的 xarray 树进行查找。这颗树上挂的都是页偏移量对应的内存页，如果没找到，就说明文件内容还没加载进内存，就会分配内存页，将文件内容加载到内存中，然后把内存页挂在 xarray 树上。下次再访问同样的页偏移量时，文件内容已经在树上，可直接返回。 address_space-\u0026gt;i_pages 指向的树就是内核管理的 Page Cache。\n将文件内容加载到 Page Cache 后，内核就可以填写进程相关的页表项，将这块文件映射的虚拟地址区域，直接映射到 Page Cache 页，完成缺页中断的处理。\n当内存紧张需要回收 Page Cache 时，内核需要知道这些 Page Cache 页映射到了哪些进程，这样才能修改进程的页表，解除虚拟内存和物理内存的映射。我们知道，同一个文件可以映射到多个进程空间，所以需要保存反向映射关系，即根据 Page Cache 页找到进程。\nPage Cache 页的反向映射关系保存在 address_space 维护的另一颗树 i_mmap。address_space-\u0026gt;i_mmap 是一个优先查找树（Priority Search Tree），关联了这个文件 Page Cache 页的 vm_area_struct 就挂在这棵树上，而这些 vm_area_struct都将指向各自的进程空间描述符 mm_struct，从而建立了 Page Cache 页到进程的联系。\n当需要解除一个 Page Cache 页的映射时，利用 address_space-\u0026gt;i_mmap 指向的树，查找 Page Cache 页映射到哪些进程的哪些 vm_area_struct，从而确定需要修改的进程页表项内容。\n简单总结一下，一个文件对应的 address_space 主要管理着两颗树：i_pages 指向的 xarray 树，维护着的所有 Page Cache 页；i_mmap 指向的 PST 树，维护着文件映射所形成的 vm_area_struct 虚拟内存区域，用来在释放 Page Cache 页时，查找映射了该文件的进程。如果文件没有被映射到进程空间，那么 i_mmap 对应的 PST 树为空。\nPage Cache 的观测 可以通过查看 /proc/meminfo 文件获知 Page Cache 相关的各种指标。\n/proc 是伪文件系统（Pseudo filesystems ）。Linux 通过伪文件系统，让系统和内核信息在用户空间可用。使用 free、vmstat等命令查看到的内存信息，数据实际上都来自 /proc/meminfo。\n我们看一个示例：\n关于 /proc/meminfo 每一项的详细解释，可以查看 [Linux 内核文档 - The /proc Filesystem](The /proc Filesystem — The Linux Kernel documentation)。我们重点看一下 Page Cache 相关的字段。\n当前系统 Page Cache 等于 Buffers + Cached 之和 ：\n1 Buffers + Cached = 5072756 kB 前面讨论过，如果 vm_area_struct 关联到文件，那么这段内存区域就是 File-backed 内存。没有关联文件的 vm_area_struct 内存区域是匿名内存。我们是否可以认为，和磁盘文件相关联的 File-backed 内存总和，应该等于 Page Cache 呢？\n1 Active(file) + Inactive(file) = 4908664 kB 好像有点对不上，还差了一些，差的这部分是共享内存（Shmem）。\nLinux 为了实现“共享内存”（shared memory）功能，即多个进程共同使用同一内存中的内容，需要使用虚拟文件系统。虚拟文件并不是真实存在于磁盘上的文件，它只是由内核模拟出来的。但虚拟文件也有自己的 inode 和 address_space结构。内核在创建共享匿名映射区域时，会创建出一个虚拟文件，并将这个文件与 vm_area_struct关联起来，这样多个进程的 vm_area_struct 会关联到同一个虚拟文件，最终映射到同样的物理内存页，从而实现了共享内存功能。这就是共享内存（Shmem）的实现原理。\n由于 Shmem 没有关联磁盘上的文件，因此它不属于 File-backed 内存，而是被记录在匿名内存（Active(anon) 或 Inactive(anon)）部分。但因为 Shmem 有自己的 inode ，inode-\u0026gt;address_sapce 维护的 Page Cache 页挂在 address_space-\u0026gt;i_pages 指向的 xarray 树上，因此 Shmem 部分的内存也应该算在 Page Cache 里。\n此外 File-backed 内存还有 Active 和 Inactive 的区别。刚被使用过的数据的内存空间被认为是 Active 的，长时间未被使用过的数据的内存空间则被认为是 Inactive 的。当物理内存不足，不得不释放正在使用的内存时，会首先释放 Inactive 的内存。\nPage Cache 和 匿名内存以及 File-backed 内存等之间的关系，如图下图所示。虽然难免存在误差，但大体来说下面的关系式是成立的：\n值得注意的是，AnonPages != Active(anon) + Inactive(anon)。Active(anon) 和 Inactive(anon) 是用来表示不可回收但是可以被交换到 swap 分区的内存，而 AnonPages 则是指没有对应文件的内存，两者的角度不一样。 Shmem 虽然属于Active(anon) 或者 Inactive(anon)，但是 Shmem 有对应的内存虚拟文件，所以它不属于 AnonPages。\n总之，Page Cache 肯定关联了文件，不管是真实存在的磁盘文件，还是虚拟内存文件。AnonPages 则没有关联任何文件。Shmem 关联了虚拟文件，它属于 Active(anon) 或者 Inactive(anon)，同时也算在 Page Cache 中。\n如果我们想知道某个文件有多少内容被缓存在 Page Cache ，可以使用 [fincore](fincore（1） - Linux 手册页 (man7.org)) 命令。例如：\n1 2 3 $ fincore /usr/lib/x86_64-linux-gnu/libc.so.6 RES PAGES SIZE FILE 2.1M 542 2.1M /usr/lib/x86_64-linux-gnu/libc.so.6 RES 是文件内容被加载进物理内存占用的内存空间大小。PAGES 是换算成文件内容占用了多少内存页。 在上面的例子中，文件 /usr/lib/x86_64-linux-gnu/libc.so.6 的全部内容，都被加载进了 Page Cache。\n结合 lsof 命令，我们可以查看某一进程打开的文件占用了多少 Page Cache：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ sudo lsof -p 1270 | grep REG | awk \u0026#39;{print $9}\u0026#39; | xargs sudo fincore RES PAGES SIZE FILE 64.8M 16580 89.9M /usr/bin/dockerd 32K 8 32K /var/lib/docker/buildkit/cache.db 16K 4 16K /var/lib/docker/buildkit/metadata_v2.db 16K 4 16K /var/lib/docker/buildkit/snapshots.db 16K 4 16K /var/lib/docker/buildkit/containerdmeta.db 284K 71 282.4K /usr/lib/x86_64-linux-gnu/libnss_systemd.so.2 244K 61 594.7K /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0.10.2 156K 39 154.2K /usr/lib/x86_64-linux-gnu/libgpg-error.so.0.29.0 24K 6 20.6K /usr/lib/x86_64-linux-gnu/libpthread.so.0 908K 227 906.5K /usr/lib/x86_64-linux-gnu/libm.so.6 ... 另外，对于所有缓存类型，缓存命中率都是一个非常重要的指标。我们可以使用 bcc 内置的工具 cachestat 追踪整个系统的 Page Cache 命中率：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ sudo cachestat-bpfcc HITS MISSES DIRTIES HITRATIO BUFFERS_MB CACHED_MB 2059 0 32 100.00% 74 1492 522 0 0 100.00% 74 1492 32 0 7 100.00% 74 1492 135 0 69 100.00% 74 1492 97 1 3 98.98% 74 1492 512 0 82 100.00% 74 1492 303 0 86 100.00% 74 1492 2474 7 1028 99.72% 74 1494 815 0 964 100.00% 74 1497 2786 0 1 100.00% 74 1497 1051 0 0 100.00% 74 1497 ^C 502 0 0 100.00% 74 1497 Detaching... 使用 cachetop 可以按进程追踪 Page Cache 命中率：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ sudo cachetop-bpfcc 14:20:41 Buffers MB: 86 / Cached MB: 2834 / Sort: HITS / Order: descending PID UID CMD HITS MISSES DIRTIES READ_HIT% WRITE_HIT% 14237 mazhen java 12823 4594 3653 52.6% 13.2% 14370 mazhen ldd 869 0 0 100.0% 0.0% 14371 mazhen grep 596 0 0 100.0% 0.0% 14376 mazhen ldd 536 0 0 100.0% 0.0% 14369 mazhen env 468 0 0 100.0% 0.0% 14377 mazhen ldd 467 0 0 100.0% 0.0% 14551 mazhen grpc-default-ex 466 0 0 100.0% 0.0% 14375 mazhen ldd 435 0 0 100.0% 0.0% 14479 mazhen ldconfig 421 0 0 100.0% 0.0% 14475 mazhen BookieJournal-3 417 58 132 60.0% 6.1% ... mmap系统调用 系统调用 mmap是最重要的内存管理接口。使用 mmap 可以创建文件映射，从而产生 Page Cache。使用 mmap 还可以用来申请堆内存。glibc 提供的 malloc，内部使用的就是 mmap 系统调用。 由于 mmap 系统调用分配内存的效率比较低， malloc 会先使用 mmap 向操作系统申请一块比较大的内存，然后再通过各种优化手段让内存分配的效率最大化。\nmmap 根据参数的不同， 可以从是不是文件映射，以及是不是私有内存这两个不同的维度来进行组合：\n私有匿名映射 在调用 mmap(MAP_ANON | MAP_PRIVATE) 时，只需要在进程虚拟内存空间分配一块内存，然后创建这块内存所对应的 vm_area_struct 结构，这次调用就结束了。当访问到这块虚拟内存时，由于这块虚拟内存都没有映射到物理内存上，就会发生缺页中断。 vm_area_struct关联文件属性为空，所以是匿名映射。内核会分配一个物理内存，然后在页表里建立起虚拟地址到物理地址的映射关系。\n私有文件映射 进程通过 mmap(MAP_FILE | MAP_PRIVATE) 这种方式来申请的内存，比如进程将共享库（Shared libraries）和可执行文件的代码段（Text Segment）映射到自己的地址空间就是通过这种方式。\n如果文件是只读的话，那这个文件在物理页的层面上其实是共享的。也就是进程 A 和进程 B 都有一页虚拟内存被映射到了相同的物理页上。但如果要写文件的时候，因为这一段内存区域的属性是私有的，所以内核就会做一次写时复制，为写文件的进程单独地创建一份副本。这样，一个进程在写文件时，并不会影响到其他进程的读。\n私有文件映射的只读页是多进程间共享的，可写页是每个进程都有一个独立的副本，创建副本的时机仍然是写时复制。\n共享文件映射 进程通过 mmap(MAP_FILE | MAP_SHARED) 这种方式来申请的内存。在私有文件映射的基础上，共享文件映射就很简单了：对于可写的页面，在写的时候不进行复制就可以了。这样的话，无论何时，也无论是读还是写，多个进程在访问同一个文件的同一个页时，访问的都是相同的物理页面。\n共享匿名映射 进程通过 mmap(MAP_ANON | MAP_SHARED) 这种方式来申请的内存。借助虚拟文件系统，多个进程的 vm_area_struct 会关联到同一个虚拟文件，最终映射到同样的物理内存页，实现进程间共享内存的功能。\nmmap 的四种映射类型，和上面介绍的 /proc/meminfo 内存指标之间的关系：\n私有映射都属于 AnonPages，共享映射都是 Page cache。前面讨论过，共享的匿名映射 Shmem，虽然没有关联真实的磁盘文件，但是关联了虚拟内存文件，所以也属于 Page Cache。\n私有文件映射，如果文件是只读的话，这块内存属于 Page Cache。如果有进程写文件，因为这一段内存区域的属性是私有的，所以内核就会做一次写时复制，为写文件的进程单独地创建一份副本，这个副本就属于 AnonPages 了。\n写在最后 Page Cache 机制涉及了进程空间，文件系统，内存管理等多个内核功能，Page Cache 就像一条线将这几部分串在了一起。因此深入理解 Page Cache 机制，对学习内核会有很大的帮助。\n","date":"2022-04-01T16:06:54+08:00","permalink":"https://mazhen.tech/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-page-cache/","title":"深入理解 Page Cache"},{"content":"进程ID 进程相关的 ID 有多种，除了进程标识 PID 外，还包括：线程组标识 TGID，进程组标识 PGID，回话标识 SID。TGID/PGID/SID 分别是相关线程组长/进程组长/回话 leader 进程的 PID。\n下面分别介绍这几种ID。\nPID 进程总是会被分配一个唯一标识它们的进程ID号，简称 PID。\n用 fork 或 clone 产生的每个进程都由内核自动地分配了一个唯一的 PID 。\nPID 保存在 task_struct-\u0026gt;pid中。\nTGID 进程以 CLONE_THREAD 标志调用 clone 方法，创建与该进程共享资源的线程。线程有独立的task_struct，但它 task_struct内的 files_struct、fs_struct 、sighand_struct、signal_struct和mm_struct 等数据结构仅仅是对进程相应数据结构的引用。\n由进程创建的所有线程都有相同的线程组ID(TGID)。线程有自己的 PID，它的TGID 就是进程的主线程的 PID。如果进程没有使用线程，则其 PID 和 TGID 相同。\n在内核中进程和线程都用 task_struct表示，而有了 TGID，我们就可以知道 task_struct 代表的是一个进程还是一个线程。\nTGID 保存在 task_struct-\u0026gt;tgid 中。\n当 task_struct 代表一个线程时，task_struct-\u0026gt;group_leader 指向主线程的 task_struct。\nPGID 如果 shell 具有作业管理能力，则它所创建的相关进程构成一个进程组，同一进程组的进程都有相同的 PGID。例如，用管道连接的进程包含在同一个进程组中。\n进程组简化了向组的所有成员发送信号的操作。进程组提供了一种机制，让信号可以发送给组内的所有进程，这使得作业控制变得简单。\n当 task_struct 代表一个进程，且该进程属于某一个进程组，则 task_struct-\u0026gt;group_leader 指向组长进程的 task_struct。\nPGID 保存在 task_struct-\u0026gt;signal-\u0026gt;pids[PIDTYPE_PGID].pid中。 pids[] 的数组下标是枚举类型，在 include/linux/pid.h 中定义了 PID 的类型：\n1 2 3 4 5 6 7 8 enum pid_type { PIDTYPE_PID, PIDTYPE_TGID, PIDTYPE_PGID, PIDTYPE_SID, PIDTYPE_MAX, }; task_struce-\u0026gt;signal 是 signal_struct 类型，维护了进程收到的信号，task_struce-\u0026gt;signal 被该进程的所有线程共享。从 PGID 保存在 task_struct-\u0026gt;signal-\u0026gt;pids[PIDTYPE_PGID]中可以看出进程组和信号处理相关。 SID 用户一次登录所涉及所有活动称为一个会话（session），其间产生的所有进程都有相同的会话ID（SID），等于会话 leader 进程的 PID。\nSID 保存在 task_struct-\u0026gt;signal-\u0026gt;pids[PIDTYPE_SID].pid中。\nPID/TGID/PGID/SID总结 用一幅图来总结 PID/TGID/PGID/SID ：\n进程间关系 内核中所有进程的 task_struct 会形成多种组织关系。根据进程的创建过程会有亲属关系，进程间的父子关系组织成一个进程树；根据用户登录活动会有会话和进程组关系。\n亲属关系 进程通过 fork() 创建出一个子进程，就形成来父子关系，如果创建出多个子进程，那么这些子进程间属于兄弟关系。可以用 pstree 命令查看当前系统的进程树。\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ pstree -p systemd(1)─┬─ModemManager(759)─┬─{ModemManager}(802) │ └─{ModemManager}(806) ├─NetworkManager(685)─┬─{NetworkManager}(743) │ └─{NetworkManager}(750) ├─acpid(675) ├─agetty(814) ├─avahi-daemon(679)───avahi-daemon(712) ├─bluetoothd(680) ├─canonical-livep(754)─┬─{canonical-livep}(1224) │ ├─{canonical-livep}(1225) │ ├─{canonical-livep}(1226) ... 进程描述符 task_struct 的 parent 指向父进程，children指向子进程链表的头部，sibling 把当前进程插入到兄弟链表中。\n通常情况下，real_parent 和 parent 是一样的。如果在 bash 上使用 GDB 来 debug 一个进程，这时候进程的 parent 是 GDB ，进程的 real_parent 是 bash。\n当一个进程创建了子进程后，它应该通过系统调用 wait() 或者 waitpid() 等待子进程结束，回收子进程的资源。而子进程在结束时，会向它的父进程发送 SIGCHLD 信号。因此父进程还可以注册 SIGCHLD 信号的处理函数，异步回收资源。\n如果父进程提前结束，那么子进程将把1号进程 init 作为父进程。总之，进程都有父进程，负责进程结束后的资源回收。在子进程退出且父进程完成回收前，子进程变成僵尸进程。僵尸进程持续的时间通常比较短，在父进程回收它的资源后就会消亡。如果父进程没有处理子进程的终止，那么子进程就会一直处于僵尸状态。\n会话、进程组关系 Linux 系统中可以有多个会话（session），每个会话可以包含多个进程组，每个进程组可以包含多个进程。\n会话是用户登录系统到退出的所有活动，从登录到结束前创建的所有进程都属于这次会话。登录后第一个被创建的进程（通常是 shell），被称为 会话 leader。\n进程组用于作业控制。一个终端上可以启动多个作业，也就是进程组，并能控制哪个作业在前台，前台作业可以访问终端，哪些作业运行在后台，不能读写终端。\n我们来看一个会话和进程组的例子。\n1 2 3 4 5 6 7 8 9 10 11 12 $ cat | head hello hello ^Z [1]+ 已停止 cat | head $ ps j | more PPID PID PGID SID TTY TPGID STAT UID TIME COMMAND 1522 1532 1532 1532 pts/0 1762 Ss 1000 0:00 -bash 1532 1760 1760 1532 pts/0 1762 T 1000 0:00 cat 1532 1761 1760 1532 pts/0 1762 T 1000 0:00 head 1532 1762 1762 1532 pts/0 1762 R+ 1000 0:00 ps j 1532 1763 1762 1532 pts/0 1762 S+ 1000 0:00 more 上面的命令通过 cat | head 创建了第一个进程组，包含 cat 和 head 两个进程。这时这个作业是前台任务，可以控制终端。当我们按下 Ctrl + z，会发送信号 SIGTSTP 给前台进程组的所有进程，该信号的缺省行为是暂停作业执行。暂停的作业会让出终端，并且进程不会再被调度，直到它们收到 SIGCONT 信号恢复执行。\n然后我们通过 ps j | more 创建了另一个进程组，包含 ps 和 more 两个进程。ps 的参数 j 表示用任务格式显示进程。输出中的 STAT 列是进程的状态码，前面的大写字母表示进程状态，我们可以从 ps 的 man page 查看其含义：\n1 2 3 4 5 6 7 8 9 D uninterruptible sleep (usually IO) I Idle kernel thread R running or runnable (on run queue) S interruptible sleep (waiting for an event to complete) T stopped by job control signal t stopped by debugger during the tracing W paging (not valid since the 2.6.xx kernel) X dead (should never be seen) Z defunct (\u0026#34;zombie\u0026#34;) process, terminated but not reaped by its parent 某些进程除了大写字母代表的进程状态，还跟着一个附加符号：\ns ：进程是会话 leader 进程 + ：进程位于前台进程组中 从输出可以看出，bash 是这个会话的 leader 进程，它的 PID、PGID 和 SID 相同，都是1532 。这个会话其他所有进程的 SID 也都是 1532。\ncat | head 进程组的 PGID 是 1760，ps j | more 进程组的 PGID 是 1762。用管道连接的进程包含在同一个进程组中，每个进程组内第一个进程成为 Group Leader，并以 Group Leader 的 PID 作为组内进程的 PGID。\n会话有一个前台进程组，还可以有一个或多个后台进程组，只有前台作业可以从终端读写数据。示例的进程组关系如图：\n注意到上图中显示，终端设备可以向进程组发送信号。我们可以在终端输入特殊字符向前台进程发送信号：\nCtrl + c 发送 SIGINT 信号，默认行为是终止进程； Ctrl + \\ 发送 SIGQUIT 信号，默认行为是终止进程，并进行 core dump； Ctrl + z 发送 SIGTSTP 信号，暂停进程。 只有前台进程可以从终端接收输入，也只有前台进程才被允许向终端输出。如果一个后台作业中的进程试图进行终端读写操作，终端会向整个作业发送 SIGTTOU 或 SIGTTIN 信号，默认的行为是暂停进程。\n当终端关闭时，会向整个会话发送 SIGHUP 信号，通常情况下，这个会话的所有进程都会被终止。如果想让运用在后台的进程不随着 session 的结束而退出，可以使用 nohup 命令忽略 SIGHUP 信号：\n1 $ nohup command \u0026gt;cmd.log 2\u0026gt;\u0026amp;1 \u0026amp; 即使 shell 结束，运行于后台的进程也能无视 SIGHUP 信号继续执行。另外一个方法是可以让进程运行在 screen 或 tmux 这种终端多路复用器（terminal multiplexer）中。\n","date":"2021-11-14T16:02:22+08:00","permalink":"https://mazhen.tech/p/%E8%BF%9B%E7%A8%8Bid%E5%8F%8A%E8%BF%9B%E7%A8%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/","title":"进程ID及进程间的关系"},{"content":"我们通常会使用 load average 了解服务器的健康状况，检查服务器的负载是否正常。但 load average 有几个缺点：\nload average 的计算包含了 TASK_RUNNING 和 TASK_UNINTERRUPTIBLE 两种状态的进程。TASK_RUNNING 是进程处于运行、或等待分配 CPU 的准备运行状态。TASK_UNINTERRUPTIBLE 是进程处于不可中断的等待，一般是等待磁盘的输入输出。因此 load average 值飙高，可能是因为 CPU 资源不够，让很多处于 TASK_RUNNING 状态的进程等待 CPU，也可能是由于磁盘 I/O 资源紧张，造成很多进程因为等待 IO 而处于 TASK_UNINTERRUPTIBLE 状态。你可以通过 load average 发现系统很忙，但没法区分是因为争夺 CPU 还是 IO 引起的。 load average 最短的时间窗口为1分钟，没法观察更短窗口的负载平均值，例如想了解最近10秒的load average。 load average 报告的是活跃进程数的原始数据，你还需要知道可用的 CPU 核数，这样 load average 的值才有意义。 所以，当用户遇到服务器 load average 飙高的时候，还需要继续查看 CPU、I/O 和内存等资源的统计数据，才能进一步分析问题。\n于是，Facebook的工程师 Johannes Weiner 发明了一个新的指标 PSI(Pressure Stall Information)，并向内核提交了这个patch。\nPSI 概览 当 CPU、内存或 IO 设备争夺激烈的时候，系统会出现负载的延迟峰值、吞吐量下降，并可能触发内核的 OOM Killer。PSI(Pressure Stall Information) 字面意思就是由于资源（CPU、内存和 IO）压力造成的任务执行停顿。PSI 量化了由于硬件资源紧张造成的任务执行中断，统计了系统中任务等待硬件资源的时间。我们可以用 PSI 作为指标，来衡量硬件资源的压力情况。停顿的时间越长，说明资源面临的压力越大。\n如果持续监控 PSI 指标并绘制变化曲线图，可以发现吞吐量下降与资源短缺的关系，让用户在资源变得紧张前，采取更主动的措施，例如将任务迁移到其他服务器，杀死低优先级的任务等。\nPSI 已经包含在 4.20及以上版本的 Linux 内核中。\nPSI 接口文件 CPU、内存和 IO 的压力信息导出到了 /proc/pressure/ 目录下对应的文件，你可以使用 cat 命令查询资源的压力统计信息：\n1 2 3 4 5 6 7 8 9 10 $ cat /proc/pressure/cpu some avg10=0.03 avg60=0.07 avg300=0.06 total=8723835 $ cat /proc/pressure/io some avg10=0.00 avg60=0.00 avg300=0.00 total=56385169 full avg10=0.00 avg60=0.00 avg300=0.00 total=54915860 $ cat /proc/pressure/memory some avg10=0.00 avg60=0.00 avg300=0.00 total=149158 full avg10=0.00 avg60=0.00 avg300=0.00 total=34054 内存和 IO 显示了两行指标：some 和 full，CPU 只有一行指标 some。关于 some 和 full 的定义下一节解释。\navg 给出了任务由于硬件资源不可用而被停顿的时间百分比。avg10、avg60和avg300分别是最近10秒、60秒和300秒的停顿时间百分比。\n例如上面 /proc/pressure/cpu 的输出，avg10=0.03 意思是任务因为CPU资源的不可用，在最近的10秒内，有0.03%的时间停顿等待 CPU。如果 avg 大于 40 ，也就是有 40% 时间在等待硬件资源，就说明这种资源的压力已经比较大了。\ntotal 是任务停顿的总时间，以微秒（microseconds）为单位。通过 total 可以检测出停顿持续太短而无法影响平均值的情况。\nsome 和 full 的定义 some 指标说明一个或多个任务由于等待资源而被停顿的时间百分比。在下图的例子中，在最近的60秒内，任务A的运行没有停顿，而由于内存紧张，任务B在运行过程中花了30秒等待内存，则 some 的值为50%。\nsome 表明了由于缺乏资源而造成至少一个任务的停顿。\nfull 指标表示所有的任务由于等待资源而被停顿的时间百分比。在下图的例子中，在最近的60秒内，任务 B 等待了 30 秒的内存，任务 A 等待了 10 秒内存，并且和任务 B 的等待时间重合。在这个重合的时间段10秒内，任务 A 和 任务 B 都在等待内存，结果是 some 指标为 50%，full 指标为 10/60 = 16.66%。\nfull 表明了总吞吐量的损失，在这种状态下，所有任务都在等待资源，CPU 周期将被浪费。\n请注意，some 和 full 的计算是用整个时间窗口内累计的等待时间，等待时间可以是连续的，也可能是离散的。\n理解了 some 和 full 的含义，就明白了 CPU 为什么没有 full 指标，因为不可能所有的任务都同时饿死在 CPU 上，CPU 总是在执行一个任务。\nPSI 阈值监控 用户可以向 PSI 注册触发器，在资源压力超过自定义的阈值时获得通知。一个触发器定义了特定时间窗口内最大累积停顿时间，例如，在任何 500ms 的窗口内，累计 100ms 的停顿时间会产生一个通知事件。\n如何向 PSI 注册触发器呢？打开 /proc/pressure/ 目录下资源对应的 PSI 接口文件，写入想要的阈值和时间窗口，然后在打开的文件描述符上使用 select()、poll() 或 epoll() 方法等待通知事件。写入 PSI 接口文件的数据格式为：\n1 \u0026lt;some|full\u0026gt; \u0026lt;停顿阈值\u0026gt; \u0026lt;时间窗口\u0026gt; 阈值和时间窗口的单位都是微秒（us）。内核接受的窗口大小范围为500ms到10秒。\n举个例子，向 /proc/pressure/io 写入 \u0026ldquo;some 500000 1000000\u0026rdquo;，代表着在任何 1 秒的时间窗口内，如果一个或多个进程因为等待 IO 而造成的时间停顿超过了阈值 500ms，将触发通知事件。\n当用于定义触发器的 PSI 接口文件描述符被关闭时，触发器将被取消注册。\n我们通过一个例子演示触发器的使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 #include \u0026lt;errno.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;poll.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main() { const char trig[] = \u0026#34;some 500000 1000000\u0026#34;; struct pollfd fds; int n; fds.fd = open(\u0026#34;/proc/pressure/io\u0026#34;, O_RDWR | O_NONBLOCK); if (fds.fd \u0026lt; 0) { printf(\u0026#34;/proc/pressure/io open error: %s\\n\u0026#34;, strerror(errno)); return 1; } fds.events = POLLPRI; if (write(fds.fd, trig, strlen(trig) + 1) \u0026lt; 0) { printf(\u0026#34;/proc/pressure/io write error: %s\\n\u0026#34;, strerror(errno)); return 1; } printf(\u0026#34;waiting for events...\\n\u0026#34;); while (1) { n = poll(\u0026amp;fds, 1, -1); if (n \u0026lt; 0) { printf(\u0026#34;poll error: %s\\n\u0026#34;, strerror(errno)); return 1; } if (fds.revents \u0026amp; POLLERR) { printf(\u0026#34;got POLLERR, event source is gone\\n\u0026#34;); return 0; } if (fds.revents \u0026amp; POLLPRI) { printf(\u0026#34;event triggered!\\n\u0026#34;); } else { printf(\u0026#34;unknown event received: 0x%x\\n\u0026#34;, fds.revents); return 1; } } return 0; } 在服务器上编译并运行该程序，如果当前服务器比较空闲，我们会看到程序一直在等待 IO 压力超过阈值的通知：\n1 2 $ sudo ./monitor waiting for events... 我们为服务器制造点 IO 压力，生成一个5G大小的文件：\n1 $ dd if=/dev/zero of=/home/mazhen/testfile bs=4096 count=1310720 再回到示例程序的运行窗口，会发现已经收到事件触发的通知：\n1 2 3 4 5 6 7 8 $ sudo ./monitor waiting for events... event triggered! event triggered! event triggered! event triggered! event triggered! ... PSI 应用案例 Facebook 是因为一些实际的需求开发了 PSI。其中一个案例是为了避免内核 OOM(Out-Of-Memory) killer 的触发。\n应用在申请内存的时候，如果没有足够的 free 内存，可以通过回收 Page Cache 释放内存，如果这时 free 内存还是不够，就会触发内核的 OOM Killer，挑选一个进程 kill 掉释放内存。这个过程是同步的，申请分配内存的进程一直被阻塞等待，而且内核选择 kill 掉哪个进程释放内存，用户不可控。因此，Facebook 开发了用户空间的 OOM Killer 工具 oomd。\noomd 使用 PSI 阈值作为触发器，在内存压力增加到一定程度时，执行指定的动作，避免最终 OOM 的发生。oomd 作为第一道防线，确保服务器工作负载的健康，并能自定义复杂的清除策略，这些都是内核做不到的。\ncgroup2 也支持 group 内任务的 PSI 指标追踪，这样就可以知道容器内 CPU、内存和 IO 的真实压力情况，进行更精细化的容器调度，在资源利用率最大化的同时保证任务的延迟和吞吐量。\n","date":"2021-08-01T11:51:05+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8psipressure-stall-information%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%B5%84%E6%BA%90/","title":"使用PSI（Pressure Stall Information）监控服务器资源"},{"content":"你可能听说过 TTY 和 PTY 这些缩写，也在 /dev 目录下看到过 /dev/tty[n] 设备，大概知道它们和 Linux 终端的概念有关。可是你清楚 TTY、PTY 具体指的是什么，它们有什么区别，以及它们和 shell 又是什么关系呢？为了理解这些，我们需要先回顾一下历史。\n回顾历史 在计算机诞生之前，人们发明了 Teleprinter(电传打字机)，通过长长的电线点对点连接，发送和接收打印的信息，用于远距离传输电报信息。\nTeleprinter 也可以写成 teletypewriter 或 teletype。\n后来人们将 Teleprinter 连接到早期的大型计算机上，作为输入和输出设备，将输入的数据发送到计算机，并打印出响应。\n在今天你很难想象程序的运行结果需要等到打印出来才能看到，Teleprinter 设备已经进了计算机博物馆。现在我们用 TTY 代表计算机终端（terminal），只是沿用了历史习惯，电传打字机（teletypewriter）曾经是计算机的终端，它的缩写便是 TTY(TeleTYpewriter)。\n为了把不同型号的电传打字机接入计算机，需要在操作系统内核安装驱动，为上层应用屏蔽所有的低层细节。\n电传打字机通过两根电缆连接：一根用于向计算机发送指令，一根用于接收计算机的输出。这两根电缆插入 UART （Universal Asynchronous Receiver and Transmitter，通用异步接收和发送器）的串行接口连接到计算机。\n操作系统包含一个 UART 驱动程序，管理字节的物理传输，包括奇偶校验和流量控制。然后输入的字符序列被传递给 TTY 驱动，该驱动包含一个 line discipline。\nline discipline 负责转换特殊字符（如退格、擦除字、清空行），并将收到的内容回传给电传打字机，以便用户可以看到输入的内容。line discipline 还负责对字符进行缓冲，当按下回车键时，缓冲的数据被传递给与 TTY 相关的前台用户进程。用户可以并行的执行几个进程，但每次只与一个进程交互，其他进程在后台工作。\n终端模拟器(terminal emulator) 今天电传打字机已经进了博物馆，但 Linux/Unix 仍然保留了当初 TTY驱动和 line discipline 的设计和功能。终端不再是一个需要通过 UART 连接到计算机上物理设备。终端成为内核的一个模块，它可以直接向 TTY 驱动发送字符，并从 TTY 驱动读取响应然后打印到屏幕上。也就是说，用内核模块模拟物理终端设备，因此被称为终端模拟器(terminal emulator)。\n上图是一个典型的Linux桌面系统。终端模拟器就像过去的物理终端一样，它监听来自键盘的事件将其发送到 TTY 驱动，并从 TTY 驱动读取响应，通过显卡驱动将结果渲染到显示器上。TTY驱动 和 line discipline的行为与原先一样，但不再有 UART 和 物理终端参与。\n如何看到一个终端模拟器呢？在 Ubuntu 20 桌面系统上，按 Ctrl+Alt+F3 就会得到一个由内核模拟的 TTY。Linux上这种模拟的文本终端也被称为虚拟终端（Virtual consoles）。每个虚拟终端都由一个特殊的设备文件 /dev/tty[n] 所表示，与这个虚拟终端的交互，是通过对这个设备文件的读写操作，以及使用ioctl系统调用操作这个设备文件进行的。通过执行 tty 命令可以查看代表当前虚拟终端的设备文件：\n1 2 $ tty /dev/tty3 可以看到，当前终端的设备文件是 /dev/tty3，也就是通过 Ctrl+Alt+F3 得到的虚拟终端。\n你可以通过 Ctrl+Alt+F3 到 Ctrl+Alt+F6 在几个虚拟终端之间切换。按 Ctrl+Alt+F2 回到桌面环境。X 系统也是运行在一个终端模拟器上，在 Ubuntu 20 上它对应的设备是 /dev/tty2，这也是为什么使用 Ctrl+Alt+F2 可以切换到 X 系统的原因。\n我们可以看看 X 系统打开的文件中是否包含了设备文件 /dev/tty2。先查找 X 系统的 PID：\n1 2 # ps aux | grep Xorg mazhen 1404 0.1 0.6 741884 49996 tty2 Sl+ 08:07 0:13 /usr/lib/xorg/Xorg vt2 -displayfd 3 -auth /run/user/1000/gdm/Xauthority -background none -noreset -keeptty -verbose 3 再看看这个进程(1404)打开了哪些文件：\n1 2 3 4 5 6 7 8 # ll /proc/1404/fd 总用量 0 dr-x------ 2 mazhen mazhen 0 7月 10 08:07 ./ dr-xr-xr-x 9 mazhen mazhen 0 7月 10 08:07 ../ lrwx------ 1 mazhen mazhen 64 7月 10 08:07 0 -\u0026gt; /dev/tty2 lrwx------ 1 mazhen mazhen 64 7月 10 08:07 1 -\u0026gt; \u0026#39;socket:[39965]\u0026#39; lrwx------ 1 mazhen mazhen 64 7月 10 10:09 10 -\u0026gt; \u0026#39;socket:[34615]\u0026#39; ... 可以看到，X 系统确实打开了 /dev/tty2。\n再做一个有趣的实验，在 tty3 下以 root 用户身份执行 echo 命令：\n1 # echo \u0026#34;hello from tty3\u0026#34; \u0026gt; /dev/tty4 再按 Ctrl+Alt+F4 切换到 tty4，能看到从 tty3 发送来的信息。\n伪终端（pseudo terminal, PTY） 终端模拟器(terminal emulator) 是运行在内核的模块，我们也可以让终端模拟程序运行在用户区。运行在用户区的终端模拟程序，就被称为伪终端（pseudo terminal, PTY）。\nPTY 运行在用户区，更加安全和灵活，同时仍然保留了 TTY 驱动和 line discipline 的功能。常用的伪终端有 xterm，gnome-terminal，以及远程终端 ssh。我们以 Ubuntu 桌面版提供的 gnome-terminal 为例，介绍伪终端如何与 TTY 驱动交互。\nPTY 是通过打开特殊的设备文件 /dev/ptmx 创建，由一对双向的字符设备构成，称为 PTY master 和 PTY slave。\ngnome-terminal 持有 PTY master 的文件描述符 /dev/ptmx。 gnome-terminal 负责监听键盘事件，通过PTY master接收或发送字符到 PTY slave，还会在屏幕上绘制来自PTY master的字符输出。\ngnome-terminal 会 fork 一个 shell 子进程，并让 shell 持有 PTY slave 的设备文件 /dev/pts/[n]，shell 通过 PTY slave 接收字符，并输出处理结果。\nPTY master 和 PTY slave 之间是 TTY 驱动，会在 master 和 slave 之间复制数据，并进行会话管理和提供 line discipline 功能。\n在 gnome-terminal 中执行 tty 命令，可以看到代表PTY slave的设备文件：\n1 2 $ tty /dev/pts/0 执行 ps -l 命令，也可以确认 shell 关联的伪终端是 pts/0：\n1 2 3 4 $ ps -l F S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD 0 S 1000 1842 1832 0 80 0 - 3423 do_wai pts/0 00:00:00 bash 0 R 1000 1897 1842 0 80 0 - 3626 - pts/0 00:00:00 ps 注意到 TTY 这一列指出了当前进程的终端是 pts/0。\n我们以实际的例子，看看在 terminal 执行一个命令的全过程。\n我们在桌面启动终端程序 gnome-terminal，它向操作系统请求一个PTY master，并把 GUI 绘制在显示器上 gnome-terminal 启动子进程 bash bash 的标准输入、标准输出和标准错误都设置为 PTY slave gnome-terminal 监听键盘事件，并将输入的字符发送到PTY master line discipline 收到字符，进行缓冲。只有当你按下回车键时，它才会把缓冲的字符复制到PTY slave。 line discipline 在接收到字符的同时，也会把字符写回给PTY master。gnome-terminal 只会在屏幕上显示来自 PTY master 的东西。因此，line discipline 需要回传字符，以便让你看到你刚刚输入的内容。 当你按下回车键时，TTY 驱动负责将缓冲的数据复制到PTY slave bash 从标准输入读取输入的字符（例如 ls -l ）。注意，bash 在启动时已经将标准输入被设置为了PTY slave bash 解释从输入读取的字符，发现需要运行 ls bash fork 出 ls 进程。bash fork 出的进程拥有和 bash 相同的标准输入、标准输出和标准错误，也就是PTY slave ls 运行，结果打印到标准输出，也就是PTY slave TTY 驱动将字符复制到PTY master gnome-terminal 循环从 PTY master 读取字节，绘制到用户界面上。 Shell 我们经常不去区分 terminal 和 Shell，会说打开一个 terminal，或打开一个 Shell。从前面介绍的命令执行过程可以看出，Shell 不处理键盘事件，也不负责字符的显示，这是 terminal 要为它处理好的。\nShell是用户空间的应用程序，通常由 terminal fork出来，是 terminal 的子进程。Shell用来提示用户输入，解释用户输入的字符，然后处理来自底层操作系统的输出。\n通常我们使用较多的 shell 有 Bash、Zsh 和 sh。\n配置 TTY 设备 内核将使用 TTY 驱动来处理 terminal 和 Shell 之间的通信。line discipline 是 TTY 驱动的一个逻辑组件。line discipline 主要有以下功能：\n当用户输入时，字符会被回传到PTY master line discipline 会在内存中缓冲这些字符。当用户按回车键时，它才将这些字符发送到PTY slave line discipline 可以拦截处理一些特殊的功能键，例如： 当用户按 CTRL+c 时，它向连接到 PTY slave 的进程发送 kill -2（SIGINT） 信号 当用户按 CTRL+w 时，它删除用户输入的最后一个字 当用户按 CTRL+z 时，它向连接到 PTY slave 的进程发送 kill -STOP信号 当用户按退格键时，它从缓冲区中删除该字符，并向PTY master发送删除最后一个字符的指令 我们可以使用命令行工具 stty 查询和配置 TTY，包括 line discipline 规则。在 terminal 执行 stty -a 命令：\n1 2 3 4 5 6 7 8 9 10 11 $ stty -a speed 38400 baud; rows 40; columns 80; line = 0; intr = ^C; quit = ^\\; erase = ^?; kill = ^U; eof = ^D; eol = \u0026lt;undef\u0026gt;; eol2 = \u0026lt;undef\u0026gt;; swtch = \u0026lt;undef\u0026gt;; start = ^Q; stop = ^S; susp = ^Z; rprnt = ^R; werase = ^W; lnext = ^V; discard = ^O; min = 1; time = 0; -parenb -parodd -cmspar cs8 -hupcl -cstopb cread -clocal -crtscts -ignbrk -brkint -ignpar -parmrk -inpck -istrip -inlcr -igncr icrnl ixon -ixoff -iuclc -ixany -imaxbel iutf8 opost -olcuc -ocrnl onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0 isig icanon iexten echo echoe echok -echonl -noflsh -xcase -tostop -echoprt echoctl echoke -flusho -extproc -a 标志告诉 stty 返回所有的设置，包括TTY 的特征和 line discipline 规则。\n让我们看一下第一行：\nspeed 表示波特率。当 terminal 和计算机通过物理线路连接时，speed 后的数字表示物理线路的波特率。波特率对 PTY 来说是没有意义。 rows, columns 表示 terminal 的行数和列数，以字符为单位。 line 表示line discipline 的类型。0 是 N_TTY。 stty 能够对 terminal 进行设置，让我们做个简单的测试验证一下。在第一个 terminal 中使用 vi 编辑一个文件。vi 在启动时会查询当前 terminal 的大小，以便 vi 能填满整个窗口。这时候我们在另一个 terminal 中输入：\n1 # stty -F /dev/pts/0 rows 20 这个命令将终端 pts/0 的行数设置为原来的一半，这将更新内核中 TTY 的数据结构，并向 vi 发送一个 SIGWINCH 信号，vi 接收到该信号后将根据 TTY 新的行列数重新绘制自己，这时 vi 就只使用了可用窗口区域的上半部分。\nstty -a 输出的第二行给出了 line discipline 能处理的所有特殊字符，包含了键的绑定。例如 intr = ^C 是指将 CTRL+c 映射到 kill -2 (SIGINT) 信号。你也可以更改这个绑定，例如执行 stty intr o 命令，将发送 SIGINT 信号的键从 CTRL+c 换成了字符 o。\n最后，stty -a 列出了一系列 line discipline 规则的开关。- 表示开关是关闭的，否则开关就是打开的。所有的开关在 man stty中都有解释。我举其中一个简单的例子，echo 是指示 line discipline 将字符回传的规则，我们可以执行命令关闭 echo 规则：\n1 $ stty -echo 这时候你再输入一些东西，屏幕上什么也不会出现。line discipline 不会将字符回传给 PTY master，因此 terminal 不会再显示我们输入的内容。然而其他一切都照常进行。例如你输入 ls，在输入时看不到字符 ls，然后你输入回车后，仍然会看到 ls 的输出。执行命令恢复 echo 规则：\n1 $ stty echo 可以通过 stty raw 命令来禁用所有的 line discipline 规则，这样的终端被称为 raw terminal。像 vi 这样的编辑器会将终端设置为 raw ，因为它需要自己处理字符。后面介绍的远程终端也是需要一个 raw terminal，同样会禁用所有的 line discipline 规则。\n远程终端 我们经常通过 ssh 连接到一个远程主机，这时候远程主机上的 ssh server 就是一个伪终端 PTY，它同样持有 PTY master，但 ssh server 不再监听键盘事件，以及在屏幕上绘制输出结果，而是通过 TCP 连接，向 ssh client 发送或接收字符。\n我们简单梳理一下远程终端是如何执行命令的。\n用户在客户端的 terminal 中输入 ssh 命令，经过 PTY master、TTY 驱动，到达 PTY slave。bash 的标准输入已经设置为了 PTY slave，它从标准输入读取字符序列并解释执行，发现需要启动 ssh 客户端，并请求和远程服务器建 TCP 连接。\n服务器端接收客户端的 TCP 连接请求，向内核申请创建 PTY，获得一对设备文件描述符。让 ssh server 持有 PTY master，ssh server fork 出的子进程 bash 持有 PTY slave。bash 的标准输入、标准输出和标准错误都设置为了PTY slave。\n当用户在客户端的 terminal 中输入命令 ls -l 和回车键，这些字符经过 PTY master 到达 TTY 驱动。我们需要禁用客户端 line discipline 的所有规则，也就是说客户端的 line discipline 不会对特殊字符回车键做处理，而是让命令 ls -l 和回车键一起到达 PTY slave。ssh client 从 PTY slave 读取字符序列，通过网络，发送给 ssh server。\nssh server 将从 TCP 连接上接收到的字节写入PTY master。TTY 驱动对字节进行缓冲，直到收到特殊字符回车键。\n由于服务器端的 line discipline 没有禁用 echo 规则，所以 TTY 驱动还会将收到的字符写回PTY master，ssh server 从 PTY master 读取字符，将这些字符通过 TCP 连接发回客户端。注意，这是发回的字符不是 ls -l 命令的执行结果，而是 ls -l 本身的回显，让客户端能看到自己的输入。\n在服务器端 TTY 驱动将字符序列传送给 PTY slave，bash 从 PTY slave读取字符，解释并执行命令 ls -l。bash fork 出 ls 子进程，该子进程的标准输入、标准输出和标准错误同样设置为了 PTY slave。ls -l 命令的执行结果写入标准输出 PTY slave，然后执行结果通过 TTY 驱动到达 PTY master，再由 ssh server 通过 TCP 连接发送给 ssh client。\n注意在客户端，我们在屏幕上看到的所有字符都来自于远程服务器。包括我们输入的内容，也是远程服务器上的 line discipline 应用 echo 规则的结果，将这些字符回显了回来。表面看似简单的在远程终端上执行了一条命令，实际上底下确是波涛汹涌。\n写在最后 简单回顾总结一下本文的主要内容：\n电传打字机（TTY）是物理设备，最初是为电报设计的，后来被连接到计算机上，发送输入和获取输出。 电传打字机（TTY）现在被运行在内核中的模块所模拟，被称为终端模拟器(terminal emulator)。 伪终端（pseudo terminal, PTY） 是运行在用户区的终端模拟程序。 Shell 由 terminal fork 出来，是 terminal 的子进程。Shell 不处理键盘事件，也不负责字符的显示，这些是由 terminal 处理。Shell 负责解释执行用户输入的字符。 可以使用 stty 命令对 TTY 设备进行配置。 远程终端 ssh 也是一种伪终端 PTY。 相信通过这篇文章，你已经能够理解终端、终端模拟器和伪终端的区别和联系。如果想进一步探究低层实现，可以阅读 TTY 驱动的源码 drivers/tty/tty_io.c和 line discipline 的源码 drivers/tty/n_tty.c。\n","date":"2021-07-12T11:46:39+08:00","permalink":"https://mazhen.tech/p/%E7%90%86%E8%A7%A3linux-%E7%BB%88%E7%AB%AF%E7%BB%88%E7%AB%AF%E6%A8%A1%E6%8B%9F%E5%99%A8%E5%92%8C%E4%BC%AA%E7%BB%88%E7%AB%AF/","title":"理解Linux 终端、终端模拟器和伪终端"},{"content":"操作系统内核对应用开发工程师来说就像一个黑盒，似乎很难窥探到其内部的运行机制。其实Linux内核很早就内置了一个强大的tracing工具：Ftrace，它几乎可以跟踪内核的所有函数，不仅可以用于调试和分析，还可以用于观察学习Linux内核的内部运行。虽然Ftrace在2008年就加入了内核，但很多应用开发工程师仍然不知道它的存在。本文就给你介绍一下Ftrace的基本使用。\nFtrace初体验 先用一个例子体验一下Ftrace的使用简单，且功能强大。使用 root 用户进入/sys/kernel/debug/tracing目录，执行 echo 和 cat 命令：\n我们使用Ftrace的function_graph功能显示了内核函数 _do_fork() 所有子函数调用。左边的第一列是执行函数的 CPU，第二列 DURATION 显示在相应函数中花费的时间。我们注意到最后一行的耗时之前有个 + 号，提示用户注意延迟高的函数。+ 代表耗时大于 10 μs。如果耗时大于 100 μs，则显示 ! 号。\n我们知道，fork 是建立父进程的一个完整副本，然后作为子进程执行。那么_do_fork()的第一件大事就是调用 copy_process() 复制父进程的数据结构，从上面输出的调用链信息也验证了这一点。\n使用完后执行下面的命令关闭function_graph：\n1 2 # echo nop \u0026gt; current_tracer # echo \u0026gt; set_graph_function 使用 Ftrace 的 function_graph 功能，可以查看内核函数的子函数调用链，帮助我们理解复杂的代码流程，而这只是 Ftrace 的功能之一。这么强大的功能，我们不必安装额外的用户空间工具，只要使用 echo 和 cat 命令访问特定的文件就能实现。Ftrace 对用户的使用接口正是tracefs文件系统。\ntracefs 文件系统 用户通过tracefs文件系统使用Ftrace，这很符合一切皆文件的Linux哲学。tracefs文件系统一般挂载在/sys/kernel/tracing目录。由于Ftrace最初是debugfs文件系统的一部分，后来才被拆分为自己的tracefs。所以如果系统已经挂载了debugfs，那么仍然会保留原来的目录结构，将tracefs挂载到debugfs的子目录下。我们可以使用 mount 命令查看当前系统debugfs和tracefs挂载点：\n1 2 3 4 # mount -t debugfs,tracefs debugfs on /sys/kernel/debug type debugfs (rw,nosuid,nodev,noexec,relatime) tracefs on /sys/kernel/tracing type tracefs (rw,nosuid,nodev,noexec,relatime) tracefs on /sys/kernel/debug/tracing type tracefs (rw,nosuid,nodev,noexec,relatime) 我使用的系统是Ubuntu 20.04.2 LTS，可以看到，为了保持兼容，tracefs同时挂载到了/sys/kernel/tracing和/sys/kernel/debug/tracing。\ntracefs下的文件主要分两类：控制文件和输出文件。这些文件的名字都很直观，像前面例子通过 current_tracer 设置当前要使用的 tracer，然后从 trace中读取结果。还有像 available_tracers 包含了当前内核可用的 tracer，可以设置 trace_options 自定义输出。\n本文后面的示例假定你已经处在了/sys/kernel/tracing或/sys/kernel/debug/tracing目录下。\n函数跟踪 Ftrace 实际上代表的就是function trace（函数跟踪），因此函数追踪是Ftrace最初的一个主要功能。\nFtrace 可以跟踪几乎所有内核函数调用的详细信息，这是怎么做到的呢？简单来说，在编译内核的时候使用了 gcc 的 -pg 选项，编译器会在每个内核函数的入口处调用一个特殊的汇编函数“mcount” 或 “__fentry__”，如果跟踪功能被打开，mcount/fentry 会调用当前设置的 tracer，tracer将不同的数据写入ring buffer。\n从上图可以看出，Ftrace 提供的 function hooks 机制在内核函数入口处埋点，根据配置调用特定的 tracer， tracer将数据写入ring buffer。Ftrace实现了一个无锁的ring buffer，所有的跟踪信息都存储在ring buffer中。用户通过 tracefs 文件系统接口访问函数跟踪的输出结果。\n你可能已经意识到，如果每个内核函数入口都加入跟踪代码，必然会非常影响内核的性能，幸好Ftrace支持动态跟踪功能。如果启用了CONFIG_DYNAMIC_FTRACE选项，编译内核时所有的mcount/fentry调用点都会被收集记录。在内核的初始化启动过程中，会根据编译期记录的列表，将mcount/fentry调用点替换为NOP指令。NOP就是 no-operation，不做任何事，直接转到下一条指令。因此在没有开启跟踪功能的情况下，Ftrace不会对内核性能产生任何影响。在开启追踪功能时，Ftrace才会将NOP指令替换为mcount/fentry。\n启用函数追踪功能，只需要将 current_tracer 文件的内容设置为 \u0026ldquo;function\u0026rdquo;：\n文件头已经很好的解释了每一列的含义。前两项是被追踪的任务名称和 PID，大括号内是执行跟踪的CPU。TIMESTAMP 是启动后的时间，后面是被追踪的函数，它的调用者在 \u0026lt;- 之后。\n我们可以设置 set_ftrace_filter 选择想要跟踪的函数：\ntrace_pipe 包含了与 trace 相同的输出，从这个文件的读取会返回一个无尽的事件流，它也会消耗事件，所以在读取一次后，它们就不再在跟踪缓冲区中了。\n也许你只想跟踪一个特定的进程，可以通过设置 set_ftrace_pid 内容为PID指定想追踪的特定进程。让 tracer 只追踪PID列在这个文件中的线程：\n如果设置了 function-fork 选项，那么当一个 PID 被列在 set_ftrace_pid 这个文件中时，其子任务的 PID 将被自动添加到这个文件中，并且子任务也将被 tracer 追踪。\n取消function-fork 选项：\n取消 set_ftrace_pid 的设置：\nFtrace function_graph 文章开始例子已经展示过，function_graph 可以打印出函数的调用图，揭示代码的流程。function_graph 不仅跟踪函数的输入，而且跟踪函数的返回，这使得 tracer 能够知道被调用的函数的深度。function_graph 可以让人更容易跟踪内核的执行流程。\n我们再看一个例子：\n前面提到过，函数耗时大于 10 μs，前面会有 + 号提醒用户注意，其他的符号还有：\n$ ：延迟大于1秒 @ ：延迟大于 100 ms * ：延迟大于 10 ms # ：延迟大于 1 ms ! ：延迟大于 100 μs + ：延迟大于 10 μs 函数Profiler 函数Profiler提供了内核函数调用的统计数据，可以观察哪些内核函数正在被使用，并能发现哪些函数的执行耗时最长。\n这里有一个要注意的地方，确保使用的是 0 \u0026gt;，而不是 0\u0026gt;。这两者的含义不一样，0\u0026gt;是对文件描述符 0 的重定向。同样要避免使用 1\u0026gt;，因为这是对文件描述符 1 的重定向。\n现在可以从 trace_stat 目录中读取 profile 的统计数据。在这个目录中，profile 数据按照 CPU 保存在名为 function[n] 文件中。我使用的4核CPU，看一下profile 结果：\n第一行是每一列的名称，分别是函数名称（Function），调用次数（Hit），函数的总时间（Time）、平均函数时间（Avg）和标准差（s^2）。输出结果显示，tcp_sendmsg() 在3个 CPU 上都是最频繁的，tcp_v4_rcv() 在 CPU2 上被调用了1618次，平均延迟为 17.218 us。\n最后要注意一点，在使用 Ftrace Profiler 时，尽量通过 set_ftrace_filter 限制 profile 的范围，避免对所有的内核函数都进行 profile。\n追踪点 Tracepoints Tracepoints是内核的静态埋点。内核维护者在他认为重要的位置放置静态 tracepoints 记录上下文信息，方便后续排查问题。例如系统调用的开始和结束，中断被触发，网络数据包发送等等。\n在Linux的早期，内核维护者就一直想在内核中加入静态 tracepoints，尝试过各种策略。Ftrace 创造了Event Tracing 基础设施，让开发者使用 TRACE_EVENT() 宏添加内核 tracepoints，不用创建自定义内核模块，使用 Event Tracing 基础设施来注册埋点函数。\n现在内核中的Tracepoints都使用了 TRACE_EVENT() 宏来定义，tracepoints 记录的上下文信息作为 Trace events 进入 Event Tracing 基础设施，这样我们就可以复用 Ftrace 的 tracefs ，通过文件接口来配置 tracepoint events，并使用 trace 或 trace_pipe 文件查看事件输出。\n所有的 tracepoint events 的控制文件都在 events 目录下，按照类别以子目录形式组织：\n我们以 events/sched/sched_process_fork 事件为例，该事件是在 include/trace/events/sched.h 中由 TRACE_EVENT 宏所定义：\n1 2 3 4 5 6 7 8 9 10 /* * Tracepoint for do_fork: */ TRACE_EVENT(sched_process_fork, TP_PROTO(struct task_struct *parent, struct task_struct *child), TP_ARGS(parent, child), ... ); TRACE_EVENT 宏会根据事件名称 sched_process_fork 生成 tracepoint 方法 trace_sched_process_fork()。你会在 kernel/fork.c 的 _do_fork() 中看到调用这个 tracepoint 方法。_do_fork() 是进程 fork 的主流程，在这里放置 tracepoint 是一个合适的位置，trace_sched_process_fork(current, p) 记录当前进程和 fork 出的子进程信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 /* * Ok, this is the main fork-routine. * * It copies the process, and if successful kick-starts * it and waits for it to finish using the VM if required. * * args-\u0026gt;exit_signal is expected to be checked for sanity by the caller. */ long _do_fork(struct kernel_clone_args *args) { ... p = copy_process(NULL, trace, NUMA_NO_NODE, args); add_latent_entropy(); /* * Do this prior waking up the new thread - the thread pointer * might get invalid after that point, if the thread exits quickly. */ trace_sched_process_fork(current, p); pid = get_task_pid(p, PIDTYPE_PID); ... } 在 events/sched/sched_process_fork 目录下，有这个事件的控制文件：\n我们演示如何通过 enable 文件开启和关闭这个 tracepoint 事件：\n前五列分别是进程名称，PID，CPU ID，irqs-off 等标志位，timestamp 和 tracepoint 事件名称。其余部分是 tracepoint 格式字符串，包含当前这个 tracepoint 记录的重要信息。格式字符串可以在 events/sched/sched_process_fork/format 文件中查看：\n通过这个 format 文件，我们可以了解这个 tracepoint 事件每个字段的含义。\n我们再演示一个使用 trigger 控制文件的例子：\n这个例子使用了 hist triggers，通过 sched_process_fork 事件来统计 _do_fork 的次数，并按照进程ID生成直方图。输出显示了 PID 24493 在追踪期间 fork 了24个子进程，最后几行显示了统计数据。\n关于 Hist Triggers 的详细介绍可以参考文档 Event Histograms。\n我的系统内核版本是 5.8.0-59-generic，当前可用的 tracepoints events 有2547个：\nEvent Tracing 基础设施应该是 Ftrace 的另一大贡献，它提供的 TRACE_EVENT 宏统一了内核 tracepoint 的实现方式，为 tracepoint events 提供了基础支持。perf 的 tracepoint events 也是基于 Ftrace 实现的。\n利用 Tracepoints 理解内核代码 由于 tracepoints 是内核维护者在流程重要位置设置的埋点，因此我们可以从 tracepoints 入手来学习内核代码。所有的 tracepoints 都定义在 include/trace/events/ 目录下的头文件中，例如进程调度相关的 tracepoints 定义在 include/trace/events/sched.h中，我们以 sched_switch 为例：\n1 2 3 4 5 6 7 8 9 10 /* * Tracepoint for task switches, performed by the scheduler: */ TRACE_EVENT(sched_switch, TP_PROTO(bool preempt, struct task_struct *prev, struct task_struct *next), TP_ARGS(preempt, prev, next), TRACE_EVENT 宏会根据事件名称 sched_switch 生成 tracepoint 方法 trace_sched_switch()，在源码中查找该方法，发现在 kernel/sched/core.c 的 __schedule()中调用了trace_sched_switch() ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /* * __schedule() is the main scheduler function. *... */ static void __sched notrace __schedule(bool preempt) { ... if (likely(prev != next)) { rq-\u0026gt;nr_switches++; ... trace_sched_switch(preempt, prev, next); ... else { ... } balance_callback(rq); } 这样我们就找到了 scheduler 的主流程，可以从这里开始阅读进程调度的源码。\n写在最后 Ftrace 就包含在内核源码中 kernel/trace，理解了 Ftrace 内核不再是黑箱，你会有豁然开朗的感觉，内核源码忽然有条理了起来。让我们从 Ftrace 开始内核探索之旅吧。\n","date":"2021-06-21T11:44:02+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8Eftrace%E5%BC%80%E5%A7%8B%E5%86%85%E6%A0%B8%E6%8E%A2%E7%B4%A2%E4%B9%8B%E6%97%85/","title":"从Ftrace开始内核探索之旅"},{"content":"GDB（GNU Debugger）是Linux上的调试程序，可用于C/C++、Go、Rust等多种语言。GDB可以让你在被调试程序执行时看到它的”内部“情况，观察程序在特定断点上的状态，并逐行运行代码。\nGDB还提供了“远程”模式，使用GDB协议通过网络或串行设备与被调试程序进行通信。程序需要链接GDB提供的stub，这个stub实现了GDB协议。或者可以使用GDBserver，这时程序不需要进行任何更改。\n类似的，Linux内核开发者可以使用GDB的远程模式，与调试应用程序几乎相同的方式来调试Linux内核。KGDB是Linux内核的源代码级调试器，你可以使用GDB作为KGDB的前端，在我们熟悉且功能强大的GDB调试界面中调试内核。\n使用KGDB需要两台机器，一台作为开发机，另一台是目标机器，要调试的内核在目标机器上运行。在开发机上使用gdb运行包含符号信息的vmlinux，然后通过指定网络地址和端口，连接到目标机器的KGDB。我们也可以使用QEMU/KVM虚拟机作为目标机器，让待调试的内核运行在虚拟机中，然后在宿主机上运行gdb，连接到虚拟机中的KGDB。\n本文将介绍如何在本机搭建Linux内核调试环境，步骤比较繁琐，还会涉及到编译内核。作为内核小白，我会尽量写的详细些，毕竟我折腾了很久才成功。\n本机环境 我使用的Ubuntu 20.04.2 LTS，gdb版本为9.2。\n安装QEMU/KVM和Virsh Virsh是Virtual Shell的缩写，是一个用于管理虚拟机的命令行工具。你可以使用Virsh创建、编辑、启动、停止、关闭和删除VM。Virsh目前支持KVM，LXC，Xen，QEMU，OpenVZ，VirtualBox和VMware ESX。这里我们使用Virsh管理QEMU/KVM虚拟机。\n在安装之前，首先要确认你的CPU是否支持虚拟化技术。使用grep查看cpuinfo是否有\u0026quot;vmx\u0026quot;(Intel-VT 技术)或\u0026quot;svm\u0026quot;(AMD-V 支持)输出：\n1 egrep \u0026#34;(svm|vmx)\u0026#34; /proc/cpuinfo 某些CPU型号在默认情况下，BIOS中可能禁用了VT支持。我们需要再检查BIOS设置是否启用了VT的支持。使用kvm-ok命令进行检查：\n1 2 $ sudo apt install cpu-checker $ kvm-ok 如果输出为：\n1 2 INFO: /dev/kvm exists KVM acceleration can be used 证明CPU的虚拟化支持已经在BIOS中启用。\n运行下面的命令安装QEMU/KVM和Virsh：\n1 $ sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager 检查libvirt守护程序是否已经启动：\n1 2 $ sudo systemctl is-active libvirtd active 如果没有输出active，运行下面的命令启动libvertd服务：\n1 2 $ sudo systemctl enable libvirtd $ sudo systemctl start libvirtd 创建虚拟机镜像 创建一个虚拟机镜像，大小为40G，qcow2 格式为动态分配磁盘占用空间。\n1 qemu-img create -f qcow2 ubuntutest.img 40G 创建虚拟机，安装操作系统 使用下面的命令启动虚拟机，-cdrom参数为虚拟机挂载了Ubuntu的安装光盘：\n1 qemu-system-x86_64 -enable-kvm -name ubuntutest -m 4096 -hda ubuntutest.img -cdrom ubuntu-20.04.2-live-server-amd64.iso -boot d -vnc :19 我们使用VNC客户端连接进虚拟机，完成Ubuntu的安装。注意上面的命令通过-vnc :19设置了虚拟机的VNC监听端口为5919。\n我使用的VNC客户端是VNC Viewer，支持Windows、macOS和Linux等主流平台。按照正常步骤，完成Ubuntu在虚拟机上的安装。\n安装完成后，可以用ctrl+c退出qemu-system-x86_64命令的执行来停止虚拟机。再次启动虚拟机，需要把 -cdrom 参数去掉。\n1 qemu-system-x86_64 -enable-kvm -name ubuntutest -m 4096 -hda ubuntutest.img -boot d -vnc :19 配置虚拟机网络 为了让虚拟机能访问外部网络，我们需要形成下面的结构：\n在宿主机上创建网桥br0，并设置一个IP地址：\n1 2 3 $ sudo brctl addbr br0 $ sudo ip link set br0 up $ sudo ifconfig br0 192.168.57.1/24 编辑宿主机的/etc/sysctl.conf文件，设置IP转发生效：\n1 net.ipv4.ip_forward=1 使用sysctl -p重新加载sysctl.conf配置使其生效。\n在宿主机上增加SNAT规则。\n1 sudo iptables -t nat -A POSTROUTING -o wlp2s0 -j MASQUERADE 虚拟机的IP地址外部并不认识，如果它要访问外网，需要在数据包离开前将源地址替换为宿主机的IP，这样外部主机才能用宿主机的IP作为目的地址发回响应。\n上面的命令的含义是：在nat表的POSTROUTING链增加规则，出口设备为wlp2s0时，就执行MASQUERADE动作。MASQUERADE是一种源地址转换动作，它会动态选择宿主机的一个IP做源地址转换。\n注意上面命令中的 -o 参数，指定了数据包的出口设备为wlp2s0。你需要使用ip link命令在你的机器上查看具体设备的名称：\n如果想进一步了解iptables，可以参见我的另一篇文章《Docker单机网络模型动手实验》。\n接着我们需要将虚拟机的网卡连接到网桥br0。后面我们使用libvirt来管理QEMU/KVM虚拟机，这样可以把虚拟机的配置参数记录在XML文件中，易于维护。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \u0026lt;domain type=\u0026#39;kvm\u0026#39; xmlns:qemu=\u0026#39;http://libvirt.org/schemas/domain/qemu/1.0\u0026#39;\u0026gt; \u0026lt;name\u0026gt;ubuntutest\u0026lt;/name\u0026gt; \u0026lt;uuid\u0026gt;0f0806ab-531d-6134-5def-c5b4955292aa\u0026lt;/uuid\u0026gt; \u0026lt;memory unit=\u0026#39;GiB\u0026#39;\u0026gt;4\u0026lt;/memory\u0026gt; \u0026lt;currentMemory unit=\u0026#39;GiB\u0026#39;\u0026gt;4\u0026lt;/currentMemory\u0026gt; \u0026lt;vcpu placement=\u0026#39;static\u0026#39;\u0026gt;2\u0026lt;/vcpu\u0026gt; \u0026lt;os\u0026gt; \u0026lt;type arch=\u0026#39;x86_64\u0026#39; machine=\u0026#39;pc-i440fx-trusty\u0026#39;\u0026gt;hvm\u0026lt;/type\u0026gt; \u0026lt;boot dev=\u0026#39;hd\u0026#39;/\u0026gt; \u0026lt;/os\u0026gt; \u0026lt;features\u0026gt; \u0026lt;acpi/\u0026gt; \u0026lt;apic/\u0026gt; \u0026lt;pae/\u0026gt; \u0026lt;/features\u0026gt; \u0026lt;clock offset=\u0026#39;utc\u0026#39;/\u0026gt; \u0026lt;on_poweroff\u0026gt;destroy\u0026lt;/on_poweroff\u0026gt; \u0026lt;on_reboot\u0026gt;restart\u0026lt;/on_reboot\u0026gt; \u0026lt;on_crash\u0026gt;restart\u0026lt;/on_crash\u0026gt; \u0026lt;devices\u0026gt; \u0026lt;emulator\u0026gt;/usr/bin/kvm\u0026lt;/emulator\u0026gt; \u0026lt;disk type=\u0026#39;file\u0026#39; device=\u0026#39;disk\u0026#39;\u0026gt; \u0026lt;driver name=\u0026#39;qemu\u0026#39; type=\u0026#39;qcow2\u0026#39;/\u0026gt; \u0026lt;source file=\u0026#39;/home/mazhen/works/ubuntutest.img\u0026#39;/\u0026gt; \u0026lt;target dev=\u0026#39;vda\u0026#39; bus=\u0026#39;virtio\u0026#39;/\u0026gt; \u0026lt;/disk\u0026gt; \u0026lt;controller type=\u0026#39;pci\u0026#39; index=\u0026#39;0\u0026#39; model=\u0026#39;pci-root\u0026#39;/\u0026gt; \u0026lt;interface type=\u0026#39;bridge\u0026#39;\u0026gt; \u0026lt;mac address=\u0026#39;fa:16:3e:6e:89:ce\u0026#39;/\u0026gt; \u0026lt;source bridge=\u0026#39;br0\u0026#39;/\u0026gt; \u0026lt;target dev=\u0026#39;tap1\u0026#39;/\u0026gt; \u0026lt;model type=\u0026#39;virtio\u0026#39;/\u0026gt; \u0026lt;/interface\u0026gt; \u0026lt;serial type=\u0026#39;pty\u0026#39;\u0026gt; \u0026lt;target port=\u0026#39;0\u0026#39;/\u0026gt; \u0026lt;/serial\u0026gt; \u0026lt;console type=\u0026#39;pty\u0026#39;\u0026gt; \u0026lt;target type=\u0026#39;serial\u0026#39; port=\u0026#39;0\u0026#39;/\u0026gt; \u0026lt;/console\u0026gt; \u0026lt;graphics type=\u0026#39;vnc\u0026#39; port=\u0026#39;5919\u0026#39; autoport=\u0026#39;no\u0026#39; listen=\u0026#39;0.0.0.0\u0026#39;\u0026gt; \u0026lt;listen type=\u0026#39;address\u0026#39; address=\u0026#39;0.0.0.0\u0026#39;/\u0026gt; \u0026lt;/graphics\u0026gt; \u0026lt;video\u0026gt; \u0026lt;model type=\u0026#39;cirrus\u0026#39;/\u0026gt; \u0026lt;/video\u0026gt; \u0026lt;/devices\u0026gt; \u0026lt;qemu:commandline\u0026gt; \u0026lt;qemu:arg value=\u0026#39;-s\u0026#39;/\u0026gt; \u0026lt;/qemu:commandline\u0026gt; \u0026lt;/domain\u0026gt; 我们可以看到，source file指定的文件/home/mazhen/works/ubuntutest.img就是虚拟机镜像。devices中的interface定义了虚拟网卡，br0是我们前面创建的网桥，libvirt帮我们创建的虚拟网卡会连接到网桥br0上。\n将XML文件保存为domain.xml，然后在libvirt定义虚拟机：\n1 $ virsh define domain.xml 接着我们可以使用virsh list --all查看虚拟机列表：\n1 2 3 4 $ virsh list --all Id Name State ----------------------------- - ubuntutest shut off 使用命令virsh start ubuntutest启动虚拟机：\n1 2 3 4 5 6 7 $ virsh start ubuntutest Domain ubuntutest started $ virsh list Id Name State ---------------------------- 1 ubuntutest running 这时我们使用VNC Viewer连接进行虚拟机，为虚拟机配置IP地址。虚拟机安装的是ubuntu-20.04.2，编辑/etc/netplan/00-installer-config.yaml文件配置IP地址。\n1 2 3 4 5 6 7 8 9 10 network: ethernets: ens3: addresses: [192.168.57.100/24] gateway4: 192.168.57.1 dhcp4: no nameservers: addresses: [114.114.114.114] optional: true version: 2 我们可以看到，网关配置的就是br0的IP地址。然后，使用命令 netplan apply让配置生效。这样，虚拟机的网络就配置好了，可以在虚拟机里访问到外网。这时我们就可以在宿主机上使用ssh登录虚拟机，这样比使用VNC Viewer操作更方便一些。\n下载Linux内核源码 在虚拟机上下载Linux内核源码：\n1 $ sudo apt install linux-source-5.4.0 ubuntu-20.04.2对应的内核版本是5.4。可以使用uname -srm查看内核版本。\n源码被下载到来/usr/src/目录下，使用下面的命令解压缩：\n1 sudo tar vjxkf linux-source-5.4.0.tar.bz2 内核源码被解压缩到了/usr/src/linux-source-5.4.0目录下。\n编译Linux内核 首先我们需要安装编译内核用到的依赖包：\n1 $ sudo apt install libncurses5-dev libssl-dev bison flex libelf-dev gcc make openssl libc6-dev 编译前要定义内核编译选项。进入/usr/src/linux-source-5.4.0目录，运行下面的命令，会进入内核参数配置界面：\n1 $ sudo make menuconfig 为了构建能够调试的内核，我们需要配置以下几个参数。\nCONFIG_DEBUG_INFO 在内核和内核模块中包含调试信息，这个选项在幕后为gcc使用的编译器参数增加了-g选项。 这个选项的菜单路径为：\n1 2 3 Kernel hacking ---\u0026gt; Compile-time checks and compiler options ---\u0026gt; [*] Compile the kernel with debug info 实际上通过菜单进行设置比较麻烦。我们保存设置退出后，配置会保存在.config文件中。直接编辑这个文件会更方便一些。在.config中确认CONFIG_DEBUG_INFO的设置正确。\n1 CONFIG_DEBUG_INFO=y CONFIG_FRAME_POINTER 这个选项会将调用帧信息保存在寄存器或堆栈上的不同位置，使gdb在调试内核时可以更准确地构造堆栈回溯跟踪（stack back traces）。 在.config中设置：\n1 CONFIG_FRAME_POINTER=y 启用CONFIG_GDB_SCRIPTS，但要关闭CONFIG_DEBUG_INFO_REDUCED。 1 2 CONFIG_GDB_SCRIPTS=y CONFIG_DEBUG_INFO_REDUCED=n CONFIG_KGDB 启用内置的内核调试器，该调试器允许进行远程调试。 1 CONFIG_KGDB=y 关闭CONFIG_RANDOMIZE_BASE设置 1 CONFIG_RANDOMIZE_BASE=n KASLR会更改引导时放置内核代码的基地址。如果你在内核配置中启用了KASLR（CONFIG_RANDOMIZE_BASE=y），则无法从gdb设置断点。\n设置完必要的内核参数后，我们开始编译内核：\n1 2 3 sudo make -j8 sudo make modules_install sudo make install 编译的过程很漫长，可能需要数小时。当编译完毕之后，新内核的选项已经增加到了grub的配置中。我们可以查看配置文件/boot/grub/grub.cfg确认：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 submenu \u0026#39;Advanced options for Ubuntu\u0026#39; $menuentry_id_option \u0026#39;gnulinux-advanced-5506d28f-c9e7-46d4-a12e-42555d491eec\u0026#39; { menuentry \u0026#39;Ubuntu, with Linux 5.4.106\u0026#39; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option \u0026#39;gnulinux-5.4.106-advanced-5506d28f-c9e7-46d4-a12e-42555d491eec\u0026#39; { recordfail load_video gfxmode $linux_gfx_mode insmod gzio if [ x$grub_platform = xxen ]; then insmod xzio; insmod lzopio; fi insmod part_gpt insmod ext2 if [ x$feature_platform_search_hint = xy ]; then search --no-floppy --fs-uuid --set=root 5506d28f-c9e7-46d4-a12e-42555d491eec else search --no-floppy --fs-uuid --set=root 5506d28f-c9e7-46d4-a12e-42555d491eec fi echo \u0026#39;Loading Linux 5.4.106 ...\u0026#39; linux /boot/vmlinuz-5.4.106 root=UUID=5506d28f-c9e7-46d4-a12e-42555d491eec ro maybe-ubiquity echo \u0026#39;Loading initial ramdisk ...\u0026#39; initrd /boot/initrd.img-5.4.106 } vmlinuz-5.4.106就是我们新编译的内核。\n重启虚拟机。在GRUB界面选择 Ubuntu 高级选项，选择第一项进去，就进入了新的内核。\n启用gdb监听端口 QEMU有个命令行参数-s，它代表参数-gdb tcp::1234，意思是QEMU监听 1234端口，这样gdb 可以 attach 到这个端口上，调试QEMU里面的内核。\n实际上在前面的domain.xml中我们已经为QEMU加了-s参数。\n1 2 3 4 5 6 \u0026lt;domain type=\u0026#39;kvm\u0026#39; xmlns:qemu=\u0026#39;http://libvirt.org/schemas/domain/qemu/1.0\u0026#39;\u0026gt; ... \u0026lt;qemu:commandline\u0026gt; \u0026lt;qemu:arg value=\u0026#39;-s\u0026#39;/\u0026gt; \u0026lt;/qemu:commandline\u0026gt; \u0026lt;/domain\u0026gt; 所以这时运行在虚拟机里的内核已经可以被调试了。\n调试内核 在宿主机上运行gdb需要内核的二进制文件，这个文件就是在虚拟机GRUB里配置的/boot/vmlinuz-5.4.106。为了方便在调试过程中查看源代码，我们可以将虚拟机的/usr/src/linux-source-5.4.0整个目录都拷贝到宿主机上来。\n1 $ scp -r mazhen@virtual-node:/usr/src/linux-source-5.4.0 ./ 在/usr/src/linux-source-5.4.0目录下面的vmlinux文件也是内核的二进制文件。\n为了能让gdb在启动时能够加载Linux helper脚本，需要在~/.gdbinit文件中添加如下内容：\n1 add-auto-load-safe-path /path/to/linux-build /path/to/linux-build就是上面从虚拟机拷贝过来的Linux源码目录。\n必要的配置完成后，就可以启动gdb了。\n在宿主机的./linux-source-5.4.0目录下执行gdb vmlinux。\n然后在gdb的交互环境下使用target remote :1234命令attach到虚拟机的内核。\n1 2 3 4 5 6 $ gdb vmlinux ... Reading symbols from vmlinux... (gdb) target remote :1234 Remote debugging using :1234 0xffffffff81ade35e in native_safe_halt () at ./arch/x86/include/asm/irqflags.h:60 如果我们想调试进程fork的过程，可以用b _do_fork设置断点：\n1 2 (gdb) b _do_fork Breakpoint 1 at 0xffffffff81098450: file kernel/fork.c, line 2362. 我们可以看到，断点设置成功。如果你不确认fork的具体方法名，可以使用info functions命令搜索符号表：\n1 2 3 4 5 (gdb) info function do_fork All functions matching regular expression \u0026#34;do_fork\u0026#34;: File kernel/fork.c: 2361:\tlong _do_fork(struct kernel_clone_args *); 使用命令c让内核继续执行：\n1 2 (gdb) c Continuing. 这时在虚拟机里执行任意命令，例如ls，断点将被触发：\n1 2 3 4 5 6 (gdb) c Continuing. Thread 1 hit Breakpoint 1, _do_fork (args=0xffffc9000095fee0) at kernel/fork.c:2362 2362\t{ (gdb) 我们可以使用n执行下一条语句：\n1 2 3 4 5 6 7 (gdb) n 2376\tif (!(clone_flags \u0026amp; CLONE_UNTRACED)) { (gdb) n 2377\tif (clone_flags \u0026amp; CLONE_VFORK) (gdb) n 2379\telse if (args-\u0026gt;exit_signal != SIGCHLD) (gdb) l显示多行源码：\n1 2 3 4 5 6 7 8 9 10 11 (gdb) l 2374\t* for the type of forking is enabled. 2375\t*/ 2376\tif (!(clone_flags \u0026amp; CLONE_UNTRACED)) { 2377\tif (clone_flags \u0026amp; CLONE_VFORK) 2378\ttrace = PTRACE_EVENT_VFORK; 2379\telse if (args-\u0026gt;exit_signal != SIGCHLD) 2380\ttrace = PTRACE_EVENT_CLONE; 2381\telse 2382\ttrace = PTRACE_EVENT_FORK; 2383\tbt查看函数调用信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 (gdb) bt #0 _do_fork (args=0xffffc9000095fee0) at kernel/fork.c:2379 #1 0xffffffff810989f4 in __do_sys_clone (tls=\u0026lt;optimized out\u0026gt;, child_tidptr=\u0026lt;optimized out\u0026gt;, parent_tidptr=\u0026lt;optimized out\u0026gt;, newsp=\u0026lt;optimized out\u0026gt;, clone_flags=\u0026lt;optimized out\u0026gt;) at kernel/fork.c:2544 #2 __se_sys_clone (tls=\u0026lt;optimized out\u0026gt;, child_tidptr=\u0026lt;optimized out\u0026gt;, parent_tidptr=\u0026lt;optimized out\u0026gt;, newsp=\u0026lt;optimized out\u0026gt;, clone_flags=\u0026lt;optimized out\u0026gt;) at kernel/fork.c:2525 #3 __x64_sys_clone (regs=\u0026lt;optimized out\u0026gt;) at kernel/fork.c:2525 #4 0xffffffff81003fd7 in do_syscall_64 (nr=\u0026lt;optimized out\u0026gt;, regs=0xffffc9000095ff58) at arch/x86/entry/common.c:290 #5 0xffffffff81c0008c in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:175 #6 0x00005621191e2da0 in ?? () #7 0x000056211a7de450 in ?? () #8 0x00007ffc9f31a3e0 in ?? () #9 0x0000000000000000 in ?? () p用于打印内部变量值：\n1 2 (gdb) p clone_flags $1 = 18874368 你现在可以像调试普通应用程序一样，调试Linux内核了！\n写在最后 在本机搭建Linux内核调试环境的步骤有点繁杂，但使用GDB能调试内核，会成为我们学习内核的利器，进程管理、内存管理、文件系统，对源码有什么困惑就可以debug一下。 Enjoy it!\n","date":"2021-05-21T11:37:22+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8gdb%E8%B0%83%E8%AF%95linux%E5%86%85%E6%A0%B8/","title":"使用GDB调试Linux内核"},{"content":"Atomikos是一个轻量级的分布式事务管理器，实现了Java Transaction API (JTA)规范，可以很方便的和Spring Boot集成，支持微服务场景下跨节点的全局事务。\n本文为一个微服务的示例应用，通过引入Atomikos增加全局事务能力。\n示例代码可以在这里查看。\n用户访问Business服务，它通过RPC调用分别调用Order和Storage创建订单和减库存。三个服务需要加入到一个全局事务中，要么全部成功，任何一个服务失败，都会造成事务回滚，数据的状态始终保持一致性。\n蚂蚁金服开源的Seata就是为了解决这类问题，在微服务架构下提供分布式事务服务。传统的应用服务器通过JTA/JTS也能解决分布式场景下的事务问题，但需要和EJB绑定在一起才能使用。Atomikos是一个独立的分布式事务管理器，原先是为Spring和Tomcat提供事务服务，让用户不必只为了事务服务而引入应用服务器。\n现在Atomikos也能为微服务提供分布式事务服务，这时主要需要两个问题：\n事务上下文如何通过RPC在服务间传播 微服务如何参与进两阶段提交协议的过程 后面会结合示例应用介绍Atomikos是如何解决这两个问题。示例应用atomkos-sample的结构如下：\napi：定义了服务接口OrderService和StorageService order-service：OrderService的具体实现 storage-service：StorageService的具体实现 business-service：用户访问入口 事务上下文的传播 在项目主工程的pom文件中引入Atomikos依赖，注意要包括transactions-remoting，正是它才能让事务上下文在RPC调用时传递。\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.atomikos\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;transactions-remoting\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.0.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; transactions-remoting支持jaxrs，Spring Remoting和Spring rest等几种RPC方式，我们使用的是Spring Remoting。\n以order-service为例，通过TransactionalHttpInvokerServiceExporter将OrderService发布为远程服务：\n1 2 3 4 5 6 7 @Bean(name = \u0026#34;/services/order\u0026#34;) TransactionalHttpInvokerServiceExporter orderService(OrderServiceImpl orderService) { TransactionalHttpInvokerServiceExporter exporter = new TransactionalHttpInvokerServiceExporter(); exporter.setService(orderService); exporter.setServiceInterface(OrderService.class); return exporter; } OrderService的调用者business-service使用HttpInvokerProxyFactoryBean引入远程服务：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Bean public HttpInvokerProxyFactoryBean orderService() { HttpInvokerProxyFactoryBean orderService = new HttpInvokerProxyFactoryBean(); orderService.setHttpInvokerRequestExecutor(httpInvokerRequestExecutor()); orderService.setServiceUrl(\u0026#34;http://localhost:8082/services/order\u0026#34;); orderService.setServiceInterface(OrderService.class); return orderService; } @Bean public TransactionalHttpInvokerRequestExecutor httpInvokerRequestExecutor() { TransactionalHttpInvokerRequestExecutor httpInvokerRequestExecutor = new TransactionalHttpInvokerRequestExecutor(); return httpInvokerRequestExecutor; } business-service负责发起全局事务，它使用Spring标准的@Transactional标记方法开启事务：\n1 2 3 4 5 @Transactional public void createOrder(String userId, String commodityCode, Integer count) { orderService.create(userId, commodityCode, count); storageService.deduct(commodityCode, count); } Atomikos提供了TransactionalHttpInvokerRequestExecutor和TransactionalHttpInvokerServiceExporter拦截请求和响应，利用HTTP header传递事务上下文。\nbusiness-service在调用远程服务OrderService时，请求发送前会经过TransactionalHttpInvokerRequestExecutor.prepareConnection处理，增加HTTP header，携带事务上下文：\n1 2 3 4 5 6 7 @Override protected void prepareConnection(HttpURLConnection con, int contentLength) throws IOException { String propagation = template.onOutgoingRequest(); con.setRequestProperty(HeaderNames.PROPAGATION_HEADER_NAME, propagation); super.prepareConnection(con, contentLength); } OrderService会使用TransactionalHttpInvokerServiceExporter.decorateInputStream进行请求拦截，能从HTTP header中解析出事务上下文：\n1 2 3 4 5 6 7 8 9 10 11 @Override protected InputStream decorateInputStream(HttpServletRequest request, InputStream is) throws IOException { try { String propagation = request.getHeader(HeaderNames.PROPAGATION_HEADER_NAME); template.onIncomingRequest(propagation); } catch (IllegalArgumentException e) { ... } return super.decorateInputStream(request, is); } OrderService处理完成返回响应时，会将该节点加入全局事务包装成Event，放入HTTP header返回给business-service：\n1 2 3 4 5 6 7 8 9 10 11 12 @Override protected OutputStream decorateOutputStream(HttpServletRequest request, HttpServletResponse response, OutputStream os) throws IOException { ... response.addHeader(HeaderNames.EXTENT_HEADER_NAME, extent); ... return super.decorateOutputStream(request, response, os); } business-service接收到响应，利用TransactionalHttpInvokerRequestExecutor.validateResponse解析出Event，注册进事务管理器，这样在全局事务提交时，可以让该分支参与到两阶段提交协议：\n1 2 3 4 5 6 7 @Override protected void validateResponse(HttpInvokerClientConfiguration config, HttpURLConnection con) throws IOException { super.validateResponse(config, con); String extent = con.getHeaderField(HeaderNames.EXTENT_HEADER_NAME); template.onIncomingResponse(extent); } 两阶段提交过程 在处理RPC调用的响应时，Atomikos会将参与到全局事务的远程节点注册为Participants(Extent.addRemoteParticipants)，在事务提交时，所有的Participants都会参与到两阶段提交：\n1 2 3 4 5 6 7 8 9 10 11 12 13 synchronized ( fsm_ ) { if ( commit ) { if ( participants_.size () \u0026lt;= 1 ) { commit ( true ); } else { int prepareResult = prepare (); // make sure to only do commit if NOT read only if ( prepareResult != Participant.READ_ONLY ) commit ( false ); } } else { rollback (); } 可以看出，如果Participants大于1，会走prepare和commit两阶段提交的完整过程。那么OrderService和StorageService如何参与进两阶段提交呢？\nAtomikos提供了REST入口com.atomikos.remoting.twopc.AtomikosRestPort，你可以将AtomikosRestPort注册到JAX-RS，例如本示例选择的是Apache CFX，在application.properties进行配置：\n1 2 3 cxf.path=/api cxf.jaxrs.classes-scan=true cxf.jaxrs.classes-scan-packages=com.atomikos.remoting.twopc business-service在进行全局事务提交时，会访问所有Participants相应的REST接口进行两阶段提交：\nbusiness-service是怎么知道AtomikosRestPort的访问地址的呢？上面提到了，business-service在访问OrderService时，返回的响应header中包含了Event，地址就随着Event返回给了调用者。AtomikosRestPort的访问地址配置在jta.properties中：\n1 com.atomikos.icatch.rest_port_url=http://localhost:8082/api/atomikos 至此，我们解释清楚了Atomikos如何为微服务提供分布式事务服务的，主要解决了两个问题：事务上下文如何通过RPC在服务间传播，以及微服务如何参与进两阶段提交协议的过程。\n下一步我准备为Atomikos增加dubbo的支持，即事务上下文可以通过dubbo进行传播。\n","date":"2020-05-15T11:30:26+08:00","permalink":"https://mazhen.tech/p/atomikos%E5%9C%A8%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"Atomikos在微服务场景下的使用"},{"content":"应用服务器的分布式事务支持 我们先看一下分布式事务的需求是如何产生的，以及应用服务器是如何支持分布式事务管理的。\n单体应用 首先看单体应用，所有的模块部署在一个应用服务器上，业务数据都保存在单个数据库中，这种场景本地事务就可以满足需求。\n数据库水平拆分 如果数据库按照业务模块进行水平拆分，完成一个业务请求会涉及到跨库的资源访问和更新，这时候就需要使用应用服务器的JTA进行两阶段提交，保证跨库操作的事务完整性。\n应用模块拆分 应用按照业务模块进一步拆分，每一个模块都作为EJB，部署在独立的应用服务器中。完成一个业务请求会跨越多个应用服务器节点和资源，如何在这种场景保证业务操作的事务呢？当访问入口EJB时JTA会自动开启全局事务，事务上下文随着EJB的远程调用在应用服务器之间传播，让被调用的EJB也加入到全局事务中。\n这就是应用因拆分而遇到分布式事务的问题，以及应用服务器是如何解决这个问题的。\n分布式事务中间件 微服务时代，没人再使用沉重的EJB，都是将Spring Bean直接暴露为远程服务。完成一个业务请求需要跨越多个微服务，同样需要面对分布式事务的问题。这时就需要引入分布式事务中间件。我们以蚂蚁金服开源的Seata为例，看看它是怎么解决微服务场景下的分布式事务问题。\n将上一小节跑在应用服务器上的业务，使用微服务 + Seata的重构后，部署架构如下：\n上图中黄色方框（RM，TM，TC）是Seata的核心组件，它们配合完成对微服务的分布式事务支持。可以看出，和应用服务器的EJB方案架构上类似，只是多了一个独立运行的TC组件。\n我们再看看Seata各组件的具体作用。\nSeata的架构 Seata由三个组件构成：\nTransaction Coordinator (TC)： 事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚。 Transaction Manager (TM)： 控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议。 Resource Manager (RM)： 控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚。 Seata vs. 应用服务器 Seata和应用服务器的分布式事务支持主要有以下四个差异：\nSeata和应用服务器都可以实现业务无侵入分布式事务支持。但应用服务器的XA方案实现的是实时一致性，而Seata的AT 模式实现的是最终一致性。 Seata引入了独立运行的Transaction Coordinator，维护全局事务的运行状态。而应用服务器的访问入口节点承担了维护全局事务状态的职责。 Seata自己实现了Resource Manager，不需要依赖数据库的XA driver。这样就有可能将没有实现XA接口的资源加入的分布式事务中，例如NoSQL。同时，RM的实现要比JTA中的XAResource复杂很多。RM需要拦截并解析SQL，生成回滚语句，在事务rollback时自动进行数据还原。XAResource是对XA driver的包装，资源参与分布式事务的能力，都是由数据库提供的。 事务上下文的传播机制不同。应用服务器使用标准的RMI-IIOP协议进行事务上下文的跨节点传播。Seata是对各种RPC框架提供了插件，拦截请求和响应，事务上下文随着RPC调用进行跨节点传播。目前Seata已经支持了dubbo、gRPC、Motan和sofa-rpc等多种RPC框架。 Seata和应用服务器都支持在分布式场景下的全局事务，都可以做到对业务无侵入。Seata实现的是最终一致性，因此性能比应用服务器的XA方案好很多，具备海量并发处理能力，这也是互联网公司选择它的原因。由于Seata不依赖数据库的XA driver，只使用数据库的本地事务，就完成了对分布式事务的支持，相当于承担了部分数据库的职责，因此Seata的实现难度要比应用服务器的JTA大。\n应用服务器进入微服务时代 那么应用服务器的分布式事务支持在微服务时代还有用吗？或者说我们应该怎样改进，才能让应用服务器进入微服务时代？\n首先我们要看到JTA/XA的优势：支持数据的实时一致性，对业务开发更加友好。客户对原有的系统进行微服务改造时，如果把业务模型假定成数据最终一致性，客户就不得不做出很大的妥协和变更。特别是有些金融客户对一致性的要求会比较高。\n我们可以学习Seata的架构，抛弃掉沉重的EJB/RMI-IIOP，让Spring Bean通过dubbo等RPC框架直接对外暴露服务，同时事务上下文可以在RPC调用时进行传递：\n我们甚至可以将JTA独立出来，和Tomcat这样的Web容器整合，为微服务架构提供分布式事务支持。相信通过这样的改造，应用服务器的分布式事务能力在微服务时代又能焕发第二春。\n","date":"2020-04-21T11:25:09+08:00","permalink":"https://mazhen.tech/p/%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%94%AF%E6%8C%81%E5%92%8Cseata%E7%9A%84%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/","title":"应用服务器的分布式事务支持和Seata的对比分析"},{"content":"性能分析工具的分类 性能分析的技术和工具可以分为以下几类：\nCounters 内核维护着各种统计信息，被称为Counters，用于对事件进行计数。例如，接收的网络数据包数量，发出的磁盘I/O请求，执行的系统调用次数。常见的这类工具有：\nvmstat: 虚拟和物理内存统计 mpstat: CPU使用率统计 iostat：磁盘的I/O使用情况 netstat：网络接口统计信息，TCP/IP协议栈统计信息，连接统计信息 Tracing Tracing是收集每个事件的数据进行分析。Tracing会捕获所有的事件，因此有比较大的CPU开销，并且可能需要大量存储来保存数据。\n常见的Tracing工具有：\ntcpdump: network packet tracing blktrace: block I/O tracing perf: Linux Performance Events, 跟踪静态和动态探针 strace: 系统调用tracing gdb: 源代码级调试器 Profiling Profiling 是通过收集目标行为的样本或快照，来了解目标的特征。Profiling可以从多个方面对程序进行动态分析，如CPU、Memory、Thread、I/O等，其中对CPU进行Profiling的应用最为广泛。\nCPU Profiling原理是基于一定频率对运行的程序进行采样，来分析消耗CPU时间的代码路径。可以基于固定的时间间隔进行采样，例如每10毫秒采样一次。也可以设置固定速率采样，例如每秒采集100个样本。\nCPU Profiling经常被用于分析代码的热点，比如“哪个方法占用CPU的执行时间最长”、“每个方法占用CPU的比例是多少”等等，然后我们就可以针对热点瓶颈进行分析和性能优化。\nLinux上常用的CPU Profiling工具有：\nperf的 record 子命令 BPF profile Monitoring 系统性能监控会记录一段时间内的性能统计信息，以便能够基于时间周期进行比较。这对于容量规划，了解高峰期的使用情况都很有帮助。历史值还为我们理解当前的性能指标提供了上下文。\n监控单个操作系统最常用工具是sar（system activity reporter，系统活动报告）命令。sar通过一个定期执行的agent来记录系统计数器的状态，并可以使用sar命令查看它们，例如：\n1 2 3 4 5 6 7 8 9 10 $ sar Linux 4.15.0-88-generic (mazhen) 03/19/2020 _x86_64_\t(4 CPU) 12:53:08 PM LINUX RESTART 12:55:01 PM CPU %user %nice %system %iowait %steal %idle 01:05:01 PM all 14.06 0.00 10.97 0.11 0.00 74.87 01:15:01 PM all 9.60 0.00 7.49 0.09 0.00 82.83 01:25:01 PM all 0.04 0.00 0.02 0.02 0.00 99.92 01:35:01 PM all 0.03 0.00 0.02 0.01 0.00 99.94 本文主要讨论如何使用perf和BPF进行CPU Profiling。\nperf perf最初是使用Linux性能计数器子系统的工具，因此perf开始的名称是Performance Counters for Linux(PCL)。perf在Linux2.6.31合并进内核，位于tools/perf目录下。\n随后perf进行了各种增强，增加了tracing、profiling等能力，可用于性能瓶颈的查找和热点代码的定位。\nperf是一个面向事件（event-oriented）的性能剖析工具，因此它也被称为Linux perf events (LPE)，或perf_events。\nperf的整体架构如下：\nperf 由两部分组成：\nperf Tools：perf用户态命令，为用户提供了一系列工具集，用于收集、分析性能数据。 perf Event Subsystem：Perf Events是内核的子系统之一，和用户态工具共同完成数据的采集。 内核依赖的硬件，比如说CPU，一般会内置一些性能统计方面的寄存器（Hardware Performance Counter），通过软件读取这些特殊寄存器里的信息，我们也可以得到很多直接关于硬件的信息。perf最初就是用来监测CPU的性能监控单元（performance monitoring unit, PMU）的。\nperf Events分类 perf支持多种性能事件：\n这些性能事件分类为：\nHardware Events: CPU性能监控计数器performance monitoring counters（PMC），也被称为performance monitoring unit（PMU） Software Events: 基于内核计数器的底层事件。例如，CPU迁移，minor faults，major faults等。 Kernel Tracepoint Events: 内核的静态Tracepoint，已经硬编码在内核需要收集信息的位置。 User Statically-Defined Tracing (USDT): 用户级程序的静态Tracepoint。 Dynamic Tracing: 用户自定义事件，可以动态的插入到内核或正在运行中的程序。Dynamic Tracing技术分为两类： kprobes：对于kernel的动态追踪技术，可以动态地在指定的内核函数的入口和出口等位置上放置探针，并定义自己的探针处理程序。 uprobes：对于用户态软件的动态追踪技术，可以安全地在用户态函数的入口等位置设置动态探针，并执行自己的探针处理程序。 可以使用perf的list子命令查看当前可用的事件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ sudo perf list List of pre-defined events (to be used in -e): branch-instructions OR branches [Hardware event] branch-misses [Hardware event] bus-cycles [Hardware event] cache-misses [Hardware event] cache-references [Hardware event] cpu-cycles OR cycles [Hardware event] ... alignment-faults [Software event] bpf-output [Software event] context-switches OR cs [Software event] cpu-clock [Software event] cpu-migrations OR migrations [Software event] ... alarmtimer:alarmtimer_cancel [Tracepoint event] alarmtimer:alarmtimer_fired [Tracepoint event] alarmtimer:alarmtimer_start [Tracepoint event] alarmtimer:alarmtimer_suspend [Tracepoint event] block:block_bio_backmerge [Tracepoint event] block:block_bio_bounce [Tracepoint event] ... perf的使用 如果还没有安装perf，可以使用apt或yum进行安装：\n1 sudo apt install linux-tools-$(uname -r) linux-tools-generic perf的功能强大，支持硬件计数器统计，定时采样，静态和动态tracing等。本文只介绍几个常用的使用场景，如果想全面的了解perf的使用，可以参考perf.wiki。\nCPU Statistics 使用perf的stat命令可以收集性能计数器统计信息，精确统计一段时间内 CPU 相关硬件计数器数值的变化。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 -\u0026gt; % sudo perf stat dd if=/dev/zero of=/dev/null count=10000000 10000000+0 records in 10000000+0 records out 5120000000 bytes (5.1 GB, 4.8 GiB) copied, 12.2795 s, 417 MB/s Performance counter stats for \u0026#39;dd if=/dev/zero of=/dev/null count=10000000\u0026#39;: 12280.299325 task-clock (msec) # 1.000 CPUs utilized 16 context-switches # 0.001 K/sec 0 cpu-migrations # 0.000 K/sec 70 page-faults # 0.006 K/sec 41,610,802,323 cycles # 3.388 GHz 20,195,746,887 instructions # 0.49 insn per cycle 3,972,723,471 branches # 323.504 M/sec 90,061,565 branch-misses # 2.27% of all branches 12.280445133 seconds time elapsed CPU Profiling 可以使用perf record以任意频率收集快照。这通常用于CPU使用情况的分析。\nsudo perf record -F 99 -a -g sleep 10 对所有CPU（-a）进行call stacks（-g）采样，采样频率为99 Hertz（-F 99），即每秒99次，持续10秒（sleep 10）。\nsudo perf record -F 99 -a -g -p PID sleep 10 对指定进程（-p PID）进行采样。\nsudo perf record -F 99 -a -g -e context-switches -p PID sleep 10 perf可以和各种instrumentation points一起使用，以跟踪内核调度程序（scheduler）的活动。其中包括software events和tracepoint event（静态探针）。\n上面的例子对指定进程的上下文切换（-e context-switches）进行采样。\nreport perf record的运行结果保存在当前目录的perf.data文件中，采样结束后，我们使用perf report查看结果。\n交互式查看模式 1 $ sudo perf report 以+开头的行可以回车，展开详细信息。\n使用--stdio选项打印所有输出 1 $ sudo perf report --stdio context-switches的采样报告：\n后面我们会介绍火焰图，以可视化的方式展示stack traces，比perf report更加直观。\nBPF BPF是Berkeley Packet Filter的缩写，最初是为BSD开发，第一个版本于1992年发布，用于改进网络数据包捕获的性能。BPF是在内核级别进行过滤，不必将每个数据包拷贝到用户空间，从而提高了数据包过滤的性能。tcpdump使用的就是BPF。\n2013年BPF被重写，被称为Extended BPF (eBPF)，于2014年包含进Linux内核中。改进后的BPF成为了通用执行引擎，可用于多种用途，包括创建高级性能分析工具。\nBPF允许在内核中运行mini programs，来响应系统和应用程序事件（例如磁盘I/O事件）。这种运作机制和JavaScript类似：JavaScript是运行在浏览器引擎中的mini programs，响应鼠标点击等事件。BPF使内核可编程化，使用户（包括非内核开发人员）能够自定义和控制他们的系统，以解决实际问题。\nBPF可以被认为是一个虚拟机，由指令集，存储对象和helper函数三部分组成。BPF指令集由位于Linux内核的BPF runtime执行，BPF runtime包括了解释器和JIT编译器。BPF是一种灵活高效的技术，可以用于networking，tracing和安全等领域。我们重点关注它作为系统监测工具方面的应用。\n和perf一样，BPF能够监测多种性能事件源，同时可以通过调用perf_events，使用perf已有的功能：\nBPF可以在内核运行计算和统计汇总，这样大大减少了复制到用户空间的数据量：\nBPF已经内置在Linux内核中，因此你无需再安装任何新的内核组件，就可以在生产环境中使用BPF。\nBCC和bpftrace 直接使用BPF指令进行编程非常繁琐，因此很有必要提供高级语言前端方便用户使用，于是就出现了BCC和bpftrace。\nBCC（BPF Compiler Collection） 提供了一个C编程环境，使用LLVM工具链来把 C 代码编译为BPF虚拟机所接受的字节码。此外它还支持Python，Lua和C++作为用户接口。\nbpftrace 是一个比较新的前端，它为开发BPF工具提供了一种专用的高级语言。bpftrace适合单行代码和自定义短脚本，而BCC更适合复杂的脚本和守护程序。\nBCC和bpftrace没有在内核代码库，它们存放在GitHub上名为IO Visor的Linux Foundation项目中。\niovisor/bcc iovisor/bpftrace BCC的安装 BCC可以参考官方的安装文档。以Ubuntu 18.04 LTS为例，建议从源码build安装：\n安装build依赖 1 2 3 4 sudo apt-get -y install bison build-essential cmake flex git libedit-dev \\ libllvm6.0 llvm-6.0-dev libclang-6.0-dev python zlib1g-dev libelf-dev sudo apt-get -y install luajit luajit-5.1-dev 编译和安装 1 2 3 4 5 git clone https://github.com/iovisor/bcc.git mkdir bcc/build; cd bcc/build cmake .. make sudo make install build python3 binding 1 2 3 4 5 cmake -DPYTHON_CMD=python3 .. pushd src/python/ make sudo make install popd make install完成后，BCC自带的工具都安装在了/usr/share/bcc/tools目录下。BCC已经包含70多个BPF工具，用于性能分析和故障排查。这些工具都可以直接使用，无需编写任何BCC代码。\n我们试用其中一个工具biolatency，跟踪磁盘I/O延迟：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 -\u0026gt; % sudo /usr/share/bcc/tools/biolatency Tracing block device I/O... Hit Ctrl-C to end. ^C usecs : count distribution 0 -\u0026gt; 1 : 0 | | 2 -\u0026gt; 3 : 0 | | 4 -\u0026gt; 7 : 0 | | 8 -\u0026gt; 15 : 0 | | 16 -\u0026gt; 31 : 2 |*** | 32 -\u0026gt; 63 : 0 | | 64 -\u0026gt; 127 : 3 |***** | 128 -\u0026gt; 255 : 7 |*********** | 256 -\u0026gt; 511 : 6 |********** | 512 -\u0026gt; 1023 : 11 |****************** | 1024 -\u0026gt; 2047 : 16 |************************** | 2048 -\u0026gt; 4095 : 24 |****************************************| 4096 -\u0026gt; 8191 : 1 |* | 8192 -\u0026gt; 16383 : 6 |********** | 16384 -\u0026gt; 32767 : 3 |***** | biolatency展示的直方图比iostat的平均值能更好的理解磁盘I/O性能。\nBCC已经自带了CPU profiling工具：\ntools/profile: Profile CPU usage by sampling stack traces at a timed interval. 此外，BCC还提供了Off-CPU的分析工具：\ntools/offcputime: Summarize off-CPU time by kernel stack trace 一般的CPU profiling都是分析on-CPU，即CPU时间都花费在了哪些代码路径。off-CPU是指进程不在CPU上运行时所花费的时间，进程因为某种原因处于休眠状态，比如说等待锁，或者被进程调度器（scheduler）剥夺了 CPU 的使用。这些情况都会导致这个进程无法运行在 CPU 上，但是仍然花费了时间。\noff-CPU分析是对on-CPU的补充，让我们知道线程所有的时间花费，更全面的了解程序的运行情况。\n后面会介绍profile，offcputime如何生成火焰图进行可视化分析。\nbpftrace的安装 bpftrace 建议运行在Linux 4.9 kernel或更高版本。根据安装文档的说明，是因为kprobes、uprobes、tracepoints等主要特性是在4.x以上加入内核的：\n4.1 - kprobes 4.3 - uprobes 4.6 - stack traces, count and hist builtins (use PERCPU maps for accuracy and efficiency) 4.7 - tracepoints 4.9 - timers/profiling 可以运行scripts/check_kernel_features.sh脚本进行验证：\n1 2 $ ./scripts/check_kernel_features.sh All required features present! bpftrace对Linux的版本要求较高，以Ubuntu为例，19.04及以上才支持apt安装：\n1 sudo apt-get install -y libbpfcc-dev 18.04和18.10可以从源码build，但需要先build好BCC。\n安装依赖 1 2 3 sudo apt-get update sudo apt-get install -y bison cmake flex g++ git libelf-dev zlib1g-dev libfl-dev systemtap-sdt-dev binutils-dev sudo apt-get install -y llvm-7-dev llvm-7-runtime libclang-7-dev clang-7 编译和安装 1 2 3 4 5 git clone https://github.com/iovisor/bpftrace mkdir bpftrace/build; cd bpftrace/build; cmake -DCMAKE_BUILD_TYPE=Release .. make -j8 sudo make install make install完成后，bpftrace自带的工具安装在/usr/local/share/bpftrace/tools目录下，这些工具的说明文档可以在项目主页找到。\n我们同样试用查看Block I/O延迟直方图的工具：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -\u0026gt; % sudo bpftrace /usr/local/share/bpftrace/tools/biolatency.bt Attaching 4 probes... Tracing block device I/O... Hit Ctrl-C to end. ^C @usecs: [128, 256) 6 |@@@@@@@@@@ | [256, 512) 4 |@@@@@@ | [512, 1K) 8 |@@@@@@@@@@@@@ | [1K, 2K) 20 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ | [2K, 4K) 30 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@| [4K, 8K) 1 |@ | [8K, 16K) 3 |@@@@@ | [16K, 32K) 0 | | [32K, 64K) 2 |@@@ | 关于bpftrace脚本编写不在本文的讨论范围，感兴趣的可以参考reference_guide。\n火焰图 火焰图是Brendan Gregg发明的将stack traces可视化展示的方法。火焰图把时间和空间两个维度上的信息融合在一张图上，将频繁执行的代码路径以可视化的形式，非常直观的展现了出来。\n火焰图可以用于可视化来自任何profiler工具的记录的stack traces信息，除了用来CPU profiling，还适用于off-CPU，page faults等多种场景的分析。本文只讨论 on-CPU 和 off-CPU 火焰图的生成。\n要理解火焰图，先从理解Stack Trace开始。\nStack Trace Stack Trace是程序执行过程中，在特定时间点的函数调用列表。例如，func_a()调用func_b()，func_b()调用func_c()，此时的Stack Trace可写为：\n1 2 3 func_c func_b func_a Profiling Stack Traces 我们做CPU profiling时，会使用perf或bcc定时采样Stack Trace，这样会收集到非常多的Stack Trace。前面介绍了perf report会将Stack Trace样本汇总为调用树，并显示每个路径的百分比。火焰图是怎么展示的呢？\n考虑下面的示例，我们用perf定时采样收集了多个Stack Trace，然后将相同的Stack Trace归纳合并，统计出次数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func_e func_d func_b func_a 1 func_b func_a 2 func_c func_b func_a 7 可以看到，总共收集了10个样本，其中代码路径func_a-\u0026gt;func_b-\u0026gt;func_c有7次，该路径上的func_c在CPU上运行。 func_a-\u0026gt;func_b进行了两次采样，func_b在CPU上运行。func_a-\u0026gt;func_b-\u0026gt;func_d-\u0026gt;func_e一次采样，func_e在CPU上运行。\n火焰图 根据前面对Stack Trace的统计信息，可以绘制出如下的火焰图：\n火焰图具有以下特性：\n每个长方块代表了函数调用栈中的一个函数 Y 轴显示堆栈的深度（堆栈中的帧数）。调用栈越深，火焰就越高。顶层方块表示 CPU 上正在运行的函数，下面的函数即为它的祖先。 X 轴的宽度代表被采集的样本数量，越宽表示采集到的越多，即执行的时间长。需要注意的是，X轴从左到右不代表时间，而是所有的调用栈合并后，按字母顺序排列的。 拿到火焰图，寻找最宽的塔并首先了解它们。顶层的哪个函数占据的宽度最大，说明它可能存在性能问题。\n可以使用Brendan Gregg开发的开源项目FlameGraph生成交互式的SVG火焰图。该项目提供了脚本，可以将采集的样本归纳合并，统计出Stack Trace出现的频率，然后使用flamegraph.pl生成SVG火焰图。\n我们先把FlameGraph项目clone下来，后面会用到：\n1 git clone https://github.com/brendangregg/FlameGraph.git Java CPU Profiling 虽然有很多Java专用的profiler工具，但这些工具一般只能看到Java方法的执行，缺少了GC，JVM的CPU时间消耗，并且有些工具的Method tracing性能损耗比较大。\nperf和BCC profile的优点是它很高效，在内核上下文中对堆栈进行计数，并能完整显示用户态和内核态的CPU使用，能看到native libraries（例如libc），JVM（libjvm），Java方法和内核中花费的时间。\n但是，perf和BCC profile这种系统级的profiler不能很好地与Java配合使用，它们识别不了Java方法和stack traces。这是因为：\nJVM的JIT（just-in-time）没有给系统级profiler公开符号表 JVM还使用帧指针寄存器（frame pointer register，x86-64上的RBP）作为通用寄存器，打破了传统的堆栈遍历 为了能生成包含Java栈与Native栈的火焰图，目前有两种解决方式：\n使用JVMTI agent perf-map-agent，生成Java符号表，供perf和bcc读取（/tmp/perf-PID.map）。同时要加上-XX:+PreserveFramePointer JVM 参数，让perf可以遍历基于帧指针（frame pointer）的堆栈。 使用async-profiler，该项目将perf的堆栈追踪和JDK提供的AsyncGetCallTrace结合了起来，同样能够获得mixed-mode火焰图。同时，此方法不需要启用帧指针，所以不用加上-XX:+PreserveFramePointer参数。 下面我们就分别演示这两种方式。\nperf-map-agent perf期望能从/tmp/perf-\u0026lt;pid\u0026gt;.map中获得在未知内存区域执行的代码的符号表。perf-map-agent可以为JIT编译的方法生成/tmp/perf-\u0026lt;pid\u0026gt;.map文件，以满足perf的要求。\n首先下载并编译perf-map-agent：\n1 2 3 4 git clone https://github.com/jvm-profiling-tools/perf-map-agent.git cd perf-map-agent cmake . make 配合perf使用 perf-map-agent提供了perf-java-flames脚本，可以一步生成火焰图。\nperf-java-flames接收perf record命令参数，它会调用perf进行采样，然后使用FlameGraph生成火焰图，一步完成，非常方便。\n注意，记得要给被profiling的Java进程加上-XX:+PreserveFramePointer JVM 参数。\n设置必要的环境变量：\n1 2 export FLAMEGRAPH_DIR=[FlameGraph 所在的目录] export PERF_RECORD_SECONDS=[采样时间] ./bin/perf-java-flames [PID] -F 99 -a -g -p [PID] 对指定进程（-p PID），在所有CPU（-a）上进行call stacks（-g）采样，采样频率为99 Hertz （-F 99），持续时间为PERF_RECORD_SECONDS秒。命令运行完成后，会在当前目录生成名为flamegraph-pid.svg的火焰图。\n./bin/perf-java-flames [PID] -F 99 -g -a -e context-switches -p [PID] 对指定进程的上下文切换（-e context-switches）进行采样，并生成火焰图。\n当然也可以只为perf生成Java符号表，然后直接使用perf采样 1 2 3 4 5 6 ./bin/create-java-perf-map.sh [PID]; sudo perf record -F 99 -p [PID] -a -g -- sleep 15 ./bin/create-java-perf-map.sh [PID]; sudo perf record -g -a -e context-switches -p [PID] sleep 15 # 查看报告 sudo perf report --stdio 配合bcc profile使用 FlameGraph项目提供了jmaps脚本，它会调用perf-map-agent为当前运行的所有Java进程生成符号表。\n首先为jmaps脚本设置好JAVA_HOME和perf-map-agent的正确位置：\n1 2 JAVA_HOME=${JAVA_HOME:-/usr/lib/jvm/java-8-oracle} AGENT_HOME=${AGENT_HOME:-/usr/lib/jvm/perf-map-agent} # from https://github.com/jvm-profiling-tools/perf-map-agent 运行jmaps，可以看到它会为当前所有的Java进程生成符号表：\n1 2 3 4 $ sudo ./jmaps Fetching maps for all java processes... Mapping PID 30711 (user adp): wc(1): 3486 10896 214413 /tmp/perf-30711.map 我们在做任何profiling之前，都需要调用jmaps，保持符号表是最新的。\nCPU Profiling火焰图 1 2 3 4 5 # Profiling sudo ./jmaps ; sudo /usr/share/bcc/tools/profile -dF 99 -afp [PID] 10 \u0026gt; out.profile01.txt # 生成火焰图 ./flamegraph.pl --color=java --hash \u0026lt;out.profile01.txt \u0026gt; flamegraph.svg off-CPU火焰图 1 2 3 4 5 # Profiling sudo ./jmaps ; sudo /usr/share/bcc/tools/offcputime -fp [PID] 10 \u0026gt; out.offcpu01.txt # 生成火焰图 ./flamegraph.pl --color=java --bgcolor=blue --hash --countname=us --width=1024 --title=\u0026#34;Off-CPU Time Flame Graph\u0026#34; \u0026lt; out.offcpu01.txt \u0026gt; out.offcpu01.svg off-CPU，并过滤指定的进程状态 Linux的进程状态有：\n状态 描述 TASK_RUNNING 意味着进程处于可运行状态。这并不意味着已经实际分配了CPU。进程可能会一直等到调度器选中它。该状态确保进程可以立即运行，而无需等待外部事件。 TASK_INTERRUPTIBLE 可中断的等待状态，主要为恢复时间无法预测的长时间等待。例如等待来自用户的输入。 TASK_UNINTERRUPTIBLE 不可中断的等待状态。用于因内核指示而停用的睡眠进程。它们不能由外部信号唤醒，只能由内核亲自唤醒。例如磁盘输入输出等待。 TASK_STOPPED 响应暂停信号而运行中断的状态。直到恢复前都不会被调度 TASK_ZOMBIE 僵尸状态，子进程已经终止，但父进程尚未执行wait()，因此该进程的资源没有被系统释放。 在状态TASK_RUNNING（0）会发生非自愿上下文切换，而我们通常感兴趣的阻塞事件是TASK_INTERRUPTIBLE（1）或TASK_UNINTERRUPTIBLE（2），offcputime可以用--state过滤指定的进程状态：\n1 2 3 4 5 # Profiling sudo ./jmaps ; sudo /usr/share/bcc/tools/offcputime -K --state 2 -f 30 \u0026gt; out.offcpu01.txt # 生成火焰图 ./flamegraph.pl --color=io --countname=ms \u0026lt; out.offcpu01.txt \u0026gt; out.offcpu01.svg async-profiler async-profiler将perf的堆栈追踪和JDK提供的AsyncGetCallTrace结合了起来，做到同时采样Java栈与Native栈，因此也就可以同时分析Java代码和Native代码中存在的性能热点。\nAsyncGetCallTrace是JDK内部提供的一个函数，它的原型如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 typedef struct { jint lineno; // BCI in the source file jmethodID method_id; // method executed in this frame } ASGCT_CallFrame; typedef struct { JNIEnv *env_id //Env where trace was recorded jint num_frames; // number of frames in this trace ASGCT_CallFrame *frames; } ASGCT_CallTrace; void AsyncGetCallTrace(ASGCT_CallTrace *trace, // pre-allocated trace to fill jint depth, // max number of frames to walk up the stack void* ucontext) // signal context 可以看出，该函数直接通过ucontext就能获取到完整的Java调用栈。\nasync-profiler的使用 下载并解压好async-profiler安装包。\n从Linux 4.6开始，从non-root进程使用perf捕获内核的call stacks，需要设置如下两个内核参数：\n1 2 # echo 1 \u0026gt; /proc/sys/kernel/perf_event_paranoid # echo 0 \u0026gt; /proc/sys/kernel/kptr_restrict async-profiler的使用非常简单，一步就能生成火焰图。另外，也不需要为被profiling的Java进程设置-XX:+PreserveFramePointer参数。\n1 ./profiler.sh -d 30 -f /tmp/flamegraph.svg [PID] 总结 为Java生成CPU profiling火焰图，基本的流程都是：\n使用工具采集样本 使用FlameGraph项目提供的脚本，将采集的样本归纳合并，统计出Stack Trace出现的频率 最后使用flamegraph.pl利用上一步的输出，绘制SVG火焰图 为了能够生成Java stacks和native stacks完整的火焰图，解决perf和bcc profile不能识别Java符号和Java stack traces的问题，目前有以下两种方式：\nperf-map-agent 加上 perf或bcc profile async-profiler（内部会使用到perf） 如果只是对Java进程做on-CPU分析，async-profiler更加方便好用。如果需要更全面的了解Java进程的运行情况，例如分析系统锁的开销，阻塞的 I/O 操作，以及进程调度器（scheduler）的工作，那么还是需要使用功能更强大的perf和bcc。\n参考资料 perf Examples Linux Extended BPF (eBPF) Tracing Tools BPF Performance Tools (book) Off-CPU Analysis Flame Graphs ","date":"2020-03-23T11:01:58+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8%E7%81%AB%E7%84%B0%E5%9B%BE%E8%BF%9B%E8%A1%8Cjava%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","title":"使用火焰图进行Java性能分析"},{"content":" Previous Next \u0026nbsp; \u0026nbsp; Page: / ","date":"2020-02-07T10:44:10+08:00","permalink":"https://mazhen.tech/p/consensus-and-distributed-transactions/","title":"Consensus and Distributed Transactions"},{"content":"在上一篇\u0026lt;Tomcat系统架构\u0026gt;中提到，Tomcat的网络通信层支持多种 I/O 模型。本文将介绍NioEndpoint，它是直接使用NIO实现了 I/O 多路复用。\nNioEndpoint的处理流程 NioEndpoint的处理流程如下：\nAcceptor实现了Runnable接口，运行在一个独立的线程中。Acceptor的run方法在循环中调用ServerSocketChannel.accept()，将返回的SocketChannel包装成NioSocketWrapper，然后将NioSocketWrapper注册进Poller。\nPoller同样实现了Runnable接口，运行在一个独立的线程中。Poller的核心任务是检测I/O事件，它在无限循环中调用Selector.select()，会得到准备就绪的NioSocketWrapper列表，为每个NioSocketWrapper生成一个SocketProcessor任务，然后把任务扔进线程池Executor去处理。\nExecutor是可配置的线程池，负责运行SocketProcessor任务。SocketProcessor实现了Runnable接口，在run方法中会调用ConnectionHandler.process(NioSocketWrapper, SocketEvent)处理当前任务关联的NioSocketWrapper。\nConnectionHandler内部使用一个ConcurrentHashMap建立了NioSocketWrapper和Processor之间的映射。从上一篇\u0026lt;Tomcat系统架构\u0026gt;的介绍我们知道，Processor负责应用层协议的解析，那么我们需要为每个NioSocketWrapper创建并关联一个Processor。\n为什么要建立NioSocketWrapper和Processor之间的关联呢？因为Processor在从NioSocketWrapper中读取字节流进行协议解析时，数据可能并不完整，这时需要释放工作线程，当Poller再次触发I/O读取事件时，可以根据NioSocketWrapper找回关联的Processor，继续进行未完成的协议解析工作。\nProcessor解析的结果是生成Tomcat的Request对象，然后调用Adapter.service(request, response)方法。Adapter的职责是将Tomcat的Request对象转换为标准的ServletRequest后，传递给Servlet引擎，最终会调用到用户编写的Servlet.service(ServletRequest, ServletResponse)。\nNioEndpoint的线程模型 我们注意到，在Tomcat 9的实现中，Acceptor和Poller都只有一个线程，并且不可配置。Poller检测到的I/O事件会被扔进Executor线程池中处理，最终Servlet.service也是在Executor中执行。这是一种常见的NIO线程模型，将I/O事件的检测和处理分开在不同的线程。\n但这种处理方式也有缺点。当Selector检测到数据就绪事件时，运行Selector线程的CPU已经在CPU cache中缓存了数据。这时切换到另外一个线程去读，这个读取线程很可能运行在另一个CPU核，此前缓存在CPU cache中的数据就没用了。同时这样频繁的线程切换也增加了系统内核的开销。\n同样是基于NIO，Jetty使用了不同的线程模型：线程自己产生的I/O事件，由当前线程处理，\u0026ldquo;Eat What You Kill\u0026rdquo;，同时，Jetty可能会新建一个新线程继续检测和处理I/O事件。\n这篇博客详细的介绍了Jetty的 \u0026ldquo;Eat What You Kill\u0026rdquo; 策略。Jetty也支持类似Tomcat的ProduceExecuteConsume策略，即I/O事件的产出和消费用不同的线程处理。\nExecuteProduceConsume策略，也就是 \u0026ldquo;Eat What You Kill\u0026rdquo;，I/O事件的生产者自己消费任务。\nJetty对比了这两种策略，使用ExecuteProduceConsume能达到更高的吞吐量。\n其实，Netty也使用了和 \u0026ldquo;Eat What You Kill\u0026rdquo; 类似的线程模型。\nChannel注册到EventLoop，一个EventLoop能够服务多个Channel。EventLoop仅在一个线程上运行，因此所有I/O事件均由同一线程处理。\nblocking write的实现 当通过Response向客户端返回数据时，最终会调用NioSocketWrapper.write(boolean block, ByteBuffer from)或NioSocketWrapper.write(boolean block, byte[] buf, int off, int len)，将数据写入socket。\n我们注意到write方法的第一个参数block，它决定了write是使用blocking还是non-blocking方式。比较奇怪，虽然是NioEndpoint，但write动作也不全是non-blocking。\n一般NIO框架在处理write时都是non-blocking方式，先尝试SocketChannel.write(ByteBuffer)，如果buffer.remaining() \u0026gt; 0，将剩余数据以某种方式缓存，然后把SelectionKey.OP_WRITE添加到SelectionKey的interest set，等待被Selector触发时再次尝试写出，直到buffer中没有剩余数据。\n那是什么因素决定了NioSocketWrapper.write是blocking还是non-blocking呢？\n我们看一下Http11OutputBuffer.isBlocking的实现：\n1 2 3 4 5 6 7 /** * Is standard Servlet blocking IO being used for output? * @return \u0026lt;code\u0026gt;true\u0026lt;/code\u0026gt; if this is blocking IO */ protected final boolean isBlocking() { return response.getWriteListener() == null; } 如果response.getWriteListener()不为null，说明我们注册了WriteListener接收write事件的通知，这时我们肯定是在使用异步Servlet。\n也就是说，当我们使用异步Servlet时，才会使用NioSocketWrapper.write的non-blocking方式，普通的Servlet都是使用blocking方式的write。\nNioEndpoint在实现non-blocking的write时和一般的NIO框架类似，那它是如何实现blocking方式的write呢？\nTomcat的NIO connector有一个配置参数selectorPool.shared。selectorPool.shared的缺省值为true，这时会创建一个运行在独立线程中BlockPoller。调用者在发起blocking write时，会将SocketChannel注册到这个BlockPoller中，然后await在一个CountDownLatch上。当BlockPoller检测到准备就绪的SocketChannel，会通过关联的CountDownLatch唤醒被阻塞的调用者。这时调用者尝试往SocketChannel中写入，如果buffer中还有剩余数据，那么会再把SocketChannel注册回BlockPoller，并继续await，重复前面的过程，直到数据完全写出，最后调用者从blocking的write方法返回。\n当设置selectorPool.shared为false时，NioEndpoint会为每个发起blocking write的线程创建一个Selector，执行和上面类似的过程。当然NioEndpoint会使用NioSelectorPool来缓存Selector，并不是每次都创建一个新的Selector。NioSelectorPool中缓存的Selector的最大数量由selectorPool.maxSelectors参数控制。\n至此，相信你对NioEndpoint的内部实现已经有了整体的了解。\n","date":"2019-11-23T17:29:34+08:00","permalink":"https://mazhen.tech/p/tomcat%E7%9A%84nioendpoint%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/","title":"Tomcat的NioEndpoint实现分析"},{"content":"Tomcat系统架构图 从架构图可以看出，顶层组件Server代表一个Tomcat Server实例，一个Server中有一个或者多个Service，每个Service有多个Connector，以及一个Engine。\nConnector和Engine是Tomcat最核心的两个组件。\nConnector负责处理网络通信，以及应用层协议(HTTP，AJP)的解析，生成标准的ServletRequest和ServletResponse对象，然后传递给Engine处理。每个Connector监听不同的网络端口。\nEngine代表整个Servlet引擎，可以包含多个Host，表示它可以管理多个虚拟站点。Host代表的是一个虚拟主机，而一个虚拟主机下可以部署多个Web应用程序，Context表示一个Web应用程序。Wrapper表示一个Servlet，一个Web应用程序中可能会有多个Servlet。\n从Tomcat的配置文件server.xml也能看出Tomcat的系统架构设计。\n1 2 3 4 5 6 7 8 9 10 \u0026lt;Server\u0026gt; \u0026lt;Service\u0026gt; \u0026lt;Connector /\u0026gt; \u0026lt;Connector /\u0026gt; \u0026lt;Engine\u0026gt; \u0026lt;Host\u0026gt; \u0026lt;/Host\u0026gt; \u0026lt;/Engine\u0026gt; \u0026lt;/Service\u0026gt; \u0026lt;/Server\u0026gt; Connector 我们再仔细看一下Connector的内部实现。\nEndpoint 负责网络通信 Processor 实现应用层协议(HTTP，AJP)解析 Adapter 将Tomcat的Request/Response转换为标准的ServletRequest/ServletResponse Tomcat的网络通信层支持多种 I/O 模型：\nNIO：使用Java NIO实现 NIO.2：异步I/O，使用JDK NIO.2实现 APR：使用了Apache Portable Runtime (APR)实现 Tomcat实现支持了多种应用层协议：\nHTTP/1.1 HTTP/2 AJP：二进制协议，Web Server和Tomcat之间的通信协议 Processor解析网络字节流生成Tomcat的Request对象后，会调用Adapter.service(request, response)方法。Adapter是Servlet引擎的入口，Adapter负责将Tomcat的Request对象转换为标准的ServletRequest，然后再调用Servlet引擎的service方法。\nProtocolHandler Tomcat允许一个Engine对接多个Connector，每个Connector可以使用不同的 I/O 模型，实现不同的应用层协议解析。Connector屏蔽了 I/O 模型和协议的区别，传递给Engine的是标准的ServletRequest/ServletResponse对象。\n由于 I/O 模型和应用层协议解析可以自由组合，Tomcat使用ProtocolHandler实现这种组合。各种组合都有相应的具体实现类。比如：Http11NioProtocol 和 AjpNio2Protocol。\n关于NioEndpoint和Nio2Endpoint组件的内部实现，会在后续文章进行分析。\n","date":"2019-11-21T17:22:56+08:00","permalink":"https://mazhen.tech/p/tomcat%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/","title":"Tomcat系统架构简介"},{"content":"程序员的工作需要长期坐在电脑前，可能还会经常熬夜，作息时间不规律，所以码农是身体素质比较差的一群人。因此程序员健身锻炼的首要目标，不是为了有好看的身材，而是让你精力充沛，面对高强度工作游刃有余，同时还有精力去享受生活，这才是最关键的。\n心肺系统的重要性 这一切的基础是你必须要有一个好的心血管系统。心脏和全身的血管组成了心血管系统。通过持续不间断地跳动，心脏把富含氧气的血液不断输送到人体各个部位，并且将各个部位产生的废物和有害物质带到相应的排泄器官排出体外，这才维持起人体的各项机能。\n心血管系统就好比汽车的发动机，没了发动机，其他零件再好也没有用，这个发动机是最重要的。\n如果通过运动，让心肌得到有效的锻炼，那受益的是整个心血管系统。至于体重的減轻、体型的变化，这些都是随时而来的副产品。\n自测心肺功能水平 那么如何锻炼才能有效的增强心血管系统呢？\n首先我们要知道自己当前心血管系统的水平如何。一个专业的指标叫做最大摄氧量(VO2 max)。这个指标可以看出你现在心血管和心肺功能的水平。简单来说，最大摄氧量就是你在运动中能获取的最大氧气量。这个指标越高，说明你的心血管系统、心肺功能越好。\n对一个正常成年人，男性这个指标达到40，女性达到36才算是及格。普通人54以上可以算是优秀，而职业长跑运动员，最大摄氧量指标能达到88以上。\n如何测量自己的最大摄氧量呢？一般专业的心率表都会有这个指标。例如Apple watch可以通过你的体能训练，估算出最大摄氧量。\niOS系统自带的“健康”App里能查看到这个指标。\n合适强度的运动改善心肺功能 了解了自己当前的心肺功能水平后，如何提高改善你的心肺功能呢？需要选择合适的运动强度，不能一上来就强度过大。如果平时缺乏锻炼，心肺功能不达标，高强度的训练是比较危险的。\n建议使用“卡氏公式”计算一下你合适的运动心率区间。这个心率区间是和你的年龄，以及早上起来的静态心率状况有关。\n适合心肺功能训练的卡氏公式\n1 2 3 心肺训练心率 = (220 - 年龄 − 静态心率)×(55% ~ 65%) + 静态心率 如果是刚开始健身，比较推荐你在跑步机上进行走路。因为跑步机的速度是恒定的，把跑步机调成上坡的时候，你会发现很容易达到你想要达到的心率，只要在这个心率范围之内，就是你最合适的运动区间。随着你的心肺能力不断提高，你会逐渐提高坡度和速度。\n如何有效减脂 健身锻炼除了能充沛精力，另外一个大家关心的问题是，如何通过运动有效的减脂呢？\n实际上，饮食才是最有效的控制体重的方法。对于想减脂的人来说，最好的办法就是控制好你糖和脂肪的摄入，然后多吃一些蛋白质类食物，这样你会有足够的饱腹感。\n此外，可以做一些低强度运动，身体会消耗更多的脂肪。\n为什么低强度的运动能消耗脂肪呢？因为脂肪的消耗是需要氧气参与，运动过程中必须有充足的氧气才能消耗脂肪，因此这个运动强度应该比心肺训练的强度更低。\n减脂的运动强度就是卡氏公式的35%到55%，在这个强度运动是消耗脂肪最多的。\n适合减脂训练的卡氏公式\n1 2 3 减脂训练心率 = (220 - 年龄 − 静态心率)×(35% ~ 55%) + 静态心率 最后，男性的健康体脂率应该是15%到20%，女性的健康体脂率是20%到25%。推荐买一个体脂秤测试跟踪自己的体脂率变化。\n","date":"2019-11-11T10:36:27+08:00","permalink":"https://mazhen.tech/p/%E7%BB%99%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E5%81%A5%E8%BA%AB%E9%94%BB%E7%82%BC%E6%8C%87%E5%8D%97/","title":"给程序员的健身锻炼指南"},{"content":"Outline 回顾HTTP的发展简史，理解HTTP在设计上的关键转变，以及每次转变的动机\nHTTP简史 HTTP/1.1的主要特性和问题 HTTP/2 的核心概念、主要特性 HTTP/2 的升级与发现 HTTP/2 的问题及展望 HTTP简史 HTTP(HyperText Transfer Protocol，超文本传输协议)是互联网上最普遍采用的一种应用协议 由欧洲核子研究委员会CERN的英国工程师Tim Berners-Lee在1991年发明 Tim Berners-Lee也是WWW的发明者 HTTP简史 HTTP/0.9：只有一行的协议 请求只有一行，包括GET方法和要请求的文档的路径 响应是一个超文本文档，没有首部，也没有其他元数据，只有HTML 服务器与客户端之间的连接在每次请求之后都会关闭 HTTP/0.9的设计目标传递超文本文档 HTTP简史 HTTP/0.9演示 1 2 3 4 5 6 7 8 9 10 11 $\u0026gt; telnet apache.org 80 Trying 95.216.24.32... Connected to apache.org. Escape character is \u0026#39;^]\u0026#39;. GET /foundation/ \u0026lt;!DOCTYPE html\u0026gt; ... Connection closed by foreign host. HTTP简史 1996年HTTP工作组发布了RFC 1945，这就是HTTP/1.0 提供请求和响应的各种元数据 不局限于超文本的传输，响应可以是任何类型：HTML文件、图片、音频等 支持内容协商、内容编码、字符集、认证、缓存等 从超文本到超媒体传输 HTTP简史 HTTP/1.0演示 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $\u0026gt; telnet apache.org 80 Trying 95.216.24.32... Connected to apache.org. GET /foundation/ HTTP/1.0 Accept: */* HTTP/1.1 200 OK Server: Apache/2.4.18 (Ubuntu) Content-Length: 46012 Connection: close Content-Type: text/html \u0026lt;!DOCTYPE html\u0026gt; ... Connection closed by foreign host. HTTP简史 1997年1月定义HTTP/1.1标准的RFC 2068发布 1999年6月RFC 2616发布，取代了RFC 2068 性能优化 持久连接 除非明确告知，默认使用持久连接 分块编码传输 请求管道，支持并行请求处理（应用的非常有限） 增强的缓存机制 HTTP简史 HTTP/1.1演示 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt;$ telnet www.baidu.com 80 Trying 14.215.177.38... Connected to www.a.shifen.com. GET /s?wd=http2 HTTP/1.1 Accept: text/html,application/xhtml+xml,application/xml Accept-Language: zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7 Connection: keep-alive Host: www.baidu.com HTTP/1.1 200 OK Connection: Keep-Alive Content-Type: text/html;charset=utf-8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Date: Sun, 06 Oct 2019 12:49:28 GMT Server: BWS/1.1 Transfer-Encoding: chunked ffa \u0026lt;!DOCTYPE html\u0026gt; ... 1be7 ... 0 GET /img/bd_logo1.png HTTP/1.1 Accept: text/html,application/xhtml+xml,application/xml Accept-Language: zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7 Connection: close Host: www.baidu.com HTTP/1.1 200 OK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Content-Length: 7877 Content-Type: image/png Date: Sun, 06 Oct 2019 13:05:06 GMT Etag: \u0026#34;1ec5-502264e2ae4c0\u0026#34; Expires: Wed, 03 Oct 2029 13:05:06 GMT Last-Modified: Wed, 03 Sep 2014 10:00:27 GMT Server: Apache Set-Cookie: BAIDUID=0D01C3C9C00A6019C16F79CAEB1EFE91:FG=1 Connection: close . . . . . . Connection closed by foreign host. HTTP简史 Google在2009年发布了实验性协议SPDY，主要目标是解决HTTP/1.1的性能限制 Google工程师在09年11月分享了实验结果 A 2x Faster Web So far we have only tested SPDY in lab conditions. The initial results are very encouraging: when we download the top 25 websites over simulated home network connections, we see a significant improvement in performance - pages loaded up to 55% faster.\n2012年，SPDY得到Chrome、Firefox和Opera的支持 HTTP-WG(HTTP Working Group)开始在SPDY的基础上制定官方标准 HTTP简史 2015年正式发布HTTP/2 主要目标：改进传输性能，降低延迟，提高吞吐量 保持原有的高层协议语义不变 根据W3Techs的报告，截止2019年11月，全球已经有 42.1% 的网站开启了HTTP/2 HTTP简史 Google在2012年设计开发了QUIC协议，让HTTP不再基于TCP 2018年底，HTTP/3标准发布 HTTP/3协议业务逻辑变化不大，可以简单理解为 HTTP/2 + QUIC HTTP/1.1 持久连接 HTTP/1.1 持久连接 HTTP/1.1 持久连接 非持久HTTP连接的固定时间成本 至少两次网络往返： 握手、请求和响应 服务处理速度越快，固定延迟的影响就越大 持久连接避免TCP连接时的三次握手，消除TCP的慢启动 HTTP/1.1 管道 多次请求必须满足先进先出(FIFO)的顺序 HTTP/1.1 管道 尽早发送请求，不被每次响应阻塞 HTTP/1.1 管道 HTTP/1.1的局限性 只能严格串行地返回响应，不允许一个连接上的多个响应交错到达 管道的问题 并行处理请求时，服务器必须缓冲管道中的响应，占用服务器资源 由于失败可能导致重复处理，非幂等的方法不能pipeline化 由于中间代理的兼容性，可能会破坏管道 管道的应用非常有限 HTTP/1.1 的协议开销 每个HTTP请求都会携带500~800字节的header 如果使用了cookie，每个HTTP请求会增加几千字节的协议开销 HTTP header以纯文本形式发送，不会进行任何压缩 某些时候HTTP header开销会超过实际传输的数据一个数量级 例如访问RESTful API时返回JSON格式的响应 HTTP/1.1性能优化建议 由于HTTP/1.1不支持多路复用 浏览器支持每个主机打开多个连接（例如Chrome是6个） 应用使用多域名，将资源分散到多个子域名 浏览器连接限制针对的是主机名，不是IP地址 缺点 消耗客户端和服务器资源 域名分区增加了额外的DNS查询 避免不了TCP的慢启动 HTTP/1.1性能优化建议 使用多域名分区 HTTP/1.1性能优化建议 减少请求次数 把多个JavaScript或CSS组合为一个文件 把多张图片组合为一个更大的复合的图片 inlining内联，将图片嵌入到CSS或者HTML文件中，减少网络请求次数 增加应用的复杂度，导致缓存、更新等问题，只是权宜之计\nHTTP/2 的目标 性能优化 支持请求与响应的多路复用 支持请求优先级和流量控制 支持服务器端推送 压缩HTTP header降低协议开销 HTTP的语义不变 HTTP方法、header、状态码、URI HTTP/2 二进制分帧层 引入新的二进制分帧数据层 将传输的信息分割为消息和帧，并采用二进制格式的编码 HTTP/2 的核心概念 流(Stream) 已建立的连接上的双向字节流 该字节流可以携带一个或多个消息 消息(Message) 与请求/响应消息对应的一系列完整的数据帧 帧(Frame) 通信的最小单位 每个帧包含帧首部，标识出当前帧所属的流 HTTP/2 的核心概念 HTTP/2 的核心概念 所有HTTP/2通信都在一个TCP连接上完成 流是连接中的一个虚拟信道，可以承载双向的消息 一个连接可以承载任意数量的流，每个流都有一个唯一的整数标识符(1、2\u0026hellip;N) 消息是指逻辑上的HTTP消息，比如请求、响应等 消息由一或多个帧组成，这些帧可以交错发送，然后根据每个帧首部的流标识符重新组装 HTTP/2请求与响应的多路复用 HTTP/1.x中，如果客户端想发送多个并行的请求，那么必须使用多个TCP连接 HTTP/2中，客户端可以使用多个流发送请求，同时HTTP消息被分解为互不依赖的帧，交错传输，最后在另一端重新组装 HTTP/2 帧格式 详细说明请参考HTTP/2规范 HTTP/2 帧类型 客户端通过HEADERS帧来发起新的流 服务器通过PUSH_PROMISE帧来发起推送流 帧类型 类型编码 用途 DATA 0x0 传输HTTP消息体 HEADERS 0x1 传输HTTP头部 PRIORITY 0x2 指定流的优先级 RST_STREAM 0x3 通知流的非正常 SETTINGS 0x4 修改连接或者流的配置 PUSH_PROMISE 0x5 服务端推送资源时的请求帧 PING 0x6 心跳检测，计算RTT往返时间 GOAWAY 0x7 优雅的终止连接，或者通知错误 WINDOW_UPDATE 0x8 针对流或者连接，实现流量控制 CONTINUATION 0x9 传递较大HTTP头部时的持续帧 HTTP/2 请求优先级 HTTP/2允许每个流关联一个31bit的优先值 0 最高优先级 2^31 -1 最低优先级 浏览器会基于资源的类型、在页面中的位置等因素，决定请求的优先次序 服务器可以根据流的优先级，控制资源分配，优先将高优先级的帧发送给客户端 HTTP/2没有规定具体的优先级算法 HTTP/2 流量控制 流量控制有方向性，即接收方可能根据自己的情况为每个流，乃至整个连接设置任意窗口大小 连接建立后，客户端与服务器交换SETTINGS帧，设置 双向的流量控制窗口大小 流量控制窗口大小通过WINDOW_UPDATE帧更新 HTTP/2流量控制和TCP流量控制的机制相同，但TCP流量控制不能对同一个连接内的多个流实施差异化策略 HTTP/2 服务器端推送 服务器可以对一个客户端请求发送多个响应 服务器通过发送PUSH_PROMISE帧来发起推送流 客户端可以使用HTTP header向服务器发送信号，列出它希望推送的资源 服务器可以智能分析客户端的需求，自动推送关键资源 HTTP header压缩 HTTP/2使用HPACK压缩格式压缩请求/响应头 通过静态霍夫曼码对发送的header字段进行编码，减小了它们的传输大小 客户端和服务器使用索引表来维护和更新header字段。对于相同的数据，不再重复发送 HTTP header压缩 HTTP/2 vs HTTP/1.1 https://http2.akamai.com/demo\nHTTP/2的升级与发现 HTTP/1.x还将长期存在，客户端和服务器必须同时支持1.x和2.0 客户端和服务器在开始交换数据前，必须发现和协商使用哪个版本的协议进行通信 HTTP/2定义了两种协商机制 通过安全连接TLS和ALPN进行协商 基于TCP连接的协商机制 HTTP/2的升级与发现 HTTP/2标准不要求必须基于TLS，但浏览器要求必须基于TLS Web上存在大量的代理和中间设备：缓存服务器、安全网关、加速器等等 如果任何中间设备不支持，连接都不会成功 建立TLS信道，端到端加密传输，绕过中间代理，实现可靠的部署 新协议一般都要依赖于建立TLS信道，例如WebSocket、SPDY h2和h2c升级协商机制 基于TLS运行的HTTP/2被称为h2 直接在TCP之上运行的HTTP/2被称为h2c h2c演示环境 客户端测试工具 curl (\u0026gt; 7.46.0) 服务器端 Tomcat 9.x 1 2 3 4 5 6 \u0026lt;Connector port=\u0026#34;8080\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34; connectionTimeout=\u0026#34;20000\u0026#34; redirectPort=\u0026#34;8443\u0026#34; \u0026gt; \u0026lt;UpgradeProtocol className=\u0026#34;org.apache.coyote.http2.Http2Protocol\u0026#34;/\u0026gt; \u0026lt;/Connector\u0026gt; h2c协议升级 curl http://localhost:8080 --http2 -v 1 2 3 4 5 6 7 8 9 10 11 \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8080 \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; Connection: Upgrade, HTTP2-Settings \u0026gt; Upgrade: h2c \u0026gt; HTTP2-Settings: AAMAAABkAARAAAAAAAIAAAAA \u0026lt; HTTP/1.1 101 \u0026lt; Connection: Upgrade \u0026lt; Upgrade: h2c HTTP/2连接建立 HTTP/2连接建立 Magic帧 ASCII 编码，12字节 何时发送? 接收到服务器发送来的 101 Switching Protocols后 TLS 握手成功后 Preface 内容 HTTP/2连接建立 交换settings帧(client -\u0026gt; server) HTTP/2连接建立 交换settings帧(server -\u0026gt; client) HTTP/2连接建立 settings ACK 帧 (client \u0026lt;-\u0026gt; server) TLS协议的设计目标 保密性 完整性 身份验证 TLS发展史 1994年，NetScape 设计了SSL协议(Secure Sockets Layer) 1.0，未正式发布 1995年，NetScape 发布 SSL 2.0 1996年，发布SSL 3.0 1999年，IETF标准化了SSL协议，更名为TLS(Transport Layer Security)，发布TLS 1.0 2006年4月，IETF 工作组发布了TLS 1.1 2008年8月，IETF 工作组发布了TLS 1.2 2018年8月，TLS 1.3正式发布 TLS 1.2 握手过程 验证身份 达成安全套件共识 传递密钥 加密通讯 非对称加密只在建立TLS信道时使用，之后的通信使用握手时生成的共享密钥加密 TLS 安全密码套件 密钥交换算法 双方在完全没有对方任何预先信息，通过不安全信道创建密钥 1976年，Diffie–Hellman key exchange，简称 DH 基于椭圆曲线(Elliptic Curve)升级DH协议，ECDHE 身份验证算法 非对称加密算法，Public Key Infrastructure(PKI) 对称加密算法、强度、工作模式 工作模式：将明文分成多个等长的Block模块，对每个模块分别加解密 hash签名算法 TLS1.3的握手优化 An Overview of TLS 1.3 – Faster and More Secure 测试TLS的支持情况 https://www.ssllabs.com/ssltest/index.html Application-Layer Protocol Negotiation 基于TLS运行的HTTP/2使用ALPN扩展做协议协商\n客户端在ClientHello消息中增加ProtocolNameList字段，包含自己支持的应用协议 服务器检查ProtocolNameList字段，在ServerHello消息中以ProtocolName字段返回选中的协议 在TLS握手的同时协商应用协议，省掉了HTTP的Upgrade机制所需的额外往返时间\nALPN h2演示环境 客户端：浏览器 服务器端：Tomcat 9.x Tomcat提供了三种不同的TLS实现 Java运行时提供的JSSE实现 使用了OpenSSL的JSSE实现 APR实现，默认情况下使用OpenSSL引擎 Tomcat三种TLS实现 JSSE 非常慢 ALPN是因为HTTP/2才在2014年出现，JDK8不支持ALPN OpenSSL实现 只使用了OpenSSL，没有使用其他本地代码(native socket, poller等) 可以配合 NIO 和 NIO2 APR 大量的native code TLS同样使用了OpenSSL TLS实现的性能对比 OpenSSL性能比纯Java实现好很多；使用TLS可以不再需要APR Linux上NIO.2是通过epoll来模拟实现的EPollPort.java 使用JSSE 生成private key和自签名证书 keytool -genkey -alias tomcat -keyalg RSA 配置server.xml 1 2 3 4 5 6 7 8 9 10 11 \u0026lt;Connector protocol=\u0026#34;org.apache.coyote.http11.Http11NioProtocol\u0026#34; port=\u0026#34;8443\u0026#34; maxThreads=\u0026#34;200\u0026#34; sslImplementationName= \u0026#34;org.apache.tomcat.util.net.jsse.JSSEImplementation\u0026#34; scheme=\u0026#34;https\u0026#34; secure=\u0026#34;true\u0026#34; SSLEnabled=\u0026#34;true\u0026#34; keystoreFile=\u0026#34;${user.home}/.keystore\u0026#34; keystorePass=\u0026#34;changeit\u0026#34; clientAuth=\u0026#34;false\u0026#34; sslProtocol=\u0026#34;TLS\u0026#34;\u0026gt; \u0026lt;UpgradeProtocol className=\u0026#34;org.apache.coyote.http2.Http2Protocol\u0026#34; /\u0026gt; \u0026lt;/Connector\u0026gt; 使用JSSE JDK8不支持ALPN 1 2 3 4 5 严重 [main] org.apache.coyote.http11.AbstractHttp11Protocol.configureUpgradeProtocol The upgrade handler [org.apache.coyote.http2.Http2Protocol] for [h2] only supports upgrade via ALPN but has been configured for the [\u0026#34;https-jsse-nio-8443\u0026#34;] connector that does not support ALPN. JDK11 1 2 3 4 信息 [main] org.apache.coyote.http11.AbstractHttp11Protocol.configureUpgradeProtocol The [\u0026#34;https-jsse-nio-8443\u0026#34;] connector has been configured to support negotiation to [h2] via ALPN 使用OpenSSL 安装tomcat-native brew install tomcat-native 配置$CATALINA_HOME/bin/setenv.sh 1 CATALINA_OPTS=\u0026#34;$CATALINA_OPTS -Djava.library.path=/usr/local/opt/tomcat-native/lib\u0026#34; 配置server.xml 1 2 3 4 5 6 \u0026lt;Connector protocol=\u0026#34;org.apache.coyote.http11.Http11NioProtocol\u0026#34; sslImplementationName= \u0026#34;org.apache.tomcat.util.net.openssl.OpenSSLImplementation\u0026#34; ... \u0026gt; \u0026lt;/Connector\u0026gt; 使用OpenSSL JDK8 \u0026amp; JDK11 1 2 3 4 5 6 7 8 9 10 信息 [main] org.apache.coyote.http11.AbstractHttp11Protocol.configureUpgradeProtocol The [\u0026#34;https-openssl-nio-8443\u0026#34;] connector has been configured to support negotiation to [h2] via ALPN ... 信息 [main] org.apache.coyote.AbstractProtocol.start 开始协议处理句柄 [\u0026#34;https-openssl-nio-8443\u0026#34;] ALPN协议协商 ClientHello ALPN协议协商 ServerHello 使用Chrome开发者工具观察 HTTP/2的问题 HTTP/2消除了HTTP协议的队首阻塞现象，但TCP层面上仍然存在队首阻塞 HTTP/2多请求复用一个TCP连接，丢包可能会block住所有的HTTP请求 HTTP/2的问题 TCP及TCP+TLS建立连接需要多次round trips QUIC Quick UDP Internet Connections 由Goolge开发，并已经在Google部署使用 QUIC QUIC: next generation multiplexed transport over UDP\n参考资料 High Performance Browser Networking HTTP的前世今生 HTTP/3 的过去、现在和未来 HTTP/2协议 ","date":"2019-10-07T09:59:29+08:00","permalink":"https://mazhen.tech/p/http/2-%E7%AE%80%E4%BB%8B/","title":"HTTP/2 简介"},{"content":"安装helm客户端 在macOS上安装很简单：\n1 brew install kubernetes-helm 其他平台请参考Installing Helm\n配置RBAC 定义rbac-config.yaml文件，创建tiller账号，并和cluster-admin绑定：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system 执行命令：\n1 2 3 $ kubectl create -f rbac-config.yaml serviceaccount \u0026#34;tiller\u0026#34; created clusterrolebinding \u0026#34;tiller\u0026#34; created 安装Tiller镜像 在强国环境内，需要参考kubernetes-for-china，将helm服务端部分Tiller的镜像下载到集群节点上。\n初始化helm 执行初始化命令，注意指定上一步创建的ServiceAccount：\n1 helm init --service-account tiller --history-max 200 命令执行成功，会在集群中安装helm的服务端部分Tiller。可以使用kubectl get pods -n kube-system命令查看：\n1 2 3 4 5 $kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE ... tiller-deploy-7fbf5fc745-lxzxl 1/1 Running 0 179m Quickstart 增加Chart Repository（可选） 查看helm的Chart Repository：\n1 2 3 4 5 $ helm repo list NAME URL stable https://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts 如果你所处的网络环境无法访问缺省的Chart Repository，可以更换为其他repo，例如微软提供的 helm 仓库的镜像：\n1 2 3 4 5 $ helm repo add stable http://mirror.azure.cn/kubernetes/charts/ \u0026#34;stable\u0026#34; has been added to your repositories $ helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/ \u0026#34;incubator\u0026#34; has been added to your repositories 所有可用chart列表： 1 2 helm repo update helm search 搜索tomcat chart： 1 helm search tomcat 查看stable/tomcat的详细信息 1 helm inspect stable/tomcat stable/tomcat使用 sidecar 方式部署web应用，通过参数image.webarchive.repository指定war的镜像，不指定会部署缺省的sample应用。\n安装tomcat： 如果是在私有化集群部署，设置service.type为NodePort：\n1 helm install --name my-web --set service.type=NodePort stable/tomcat 测试安装效果 1 2 3 4 5 6 export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services my-web-tomcat) export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo http://$NODE_IP:$NODE_PORT # 访问sample应用 curl http://$NODE_IP:$NODE_PORT/sample/ 列表和删除 1 2 helm list helm del --purge my-web ","date":"2019-06-11T09:36:01+08:00","permalink":"https://mazhen.tech/p/helm%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","title":"Helm的安装和使用"},{"content":"Kubernetes 相关的docker镜像存放在Google的镜像仓库 k8s.gcr.io，在国内网络环境内无法访问。有人已经将这些镜像同步到了阿里云，你可以在阿里云容器镜像服务中搜索到它们。几乎所有的k8s镜像都已经同步到了阿里云。阿里云容器服务团队甚至还有一个开源项目sync-repo，专门做Docker Registry之间的同步。但是，如果你不放心别人同步的镜像，或者最新版本的镜像还没人同步过来，你可以按照本文将介绍的步骤，自己将gcr.io上的docker镜像搬到阿里云。\n安装配置shadowsocks客户端 先简单介绍下Shadowsocks协议，详细的工作原理可以参考这篇博客：\n当我们启动shadowsocks client时，实际上是启动了一个 ss-local 进程，左侧绿色的 Socks5 Client 可以是浏览器，也可以是Telegram等本地应用，它们和ss-local之间是使用 socks 协议进行通信。也就是说，浏览器像连接普通 socks 代理一样，连接到ss-local进程。ss-local 会将收到的请求，转发给ss-server，由ss-server完成实际的访问，并将结果通过ss-local返回给浏览器。ss-server部署在国内网络之外，和ss-local之间是加密传输，这样就实现了跨越长城。其实防火长城已经能够识别Shadowsocks协议，但发现我们是在努力学习先进技术，就先放我们过关。\n好了，我们现在首先要做的是在本机安装配置shadowsocks客户端。推荐使用shadowsocks-libev，纯C实现的shadowsocks协议，已经在很多操作系统的官方repository中 ，安装非常方便。\nmacOS 1 brew install shadowsocks-libev Ubuntu 1 sudo apt install shadowsocks-libev 接下来填写shadowsocks client配置文件，JSON格式，简单易懂：\n1 2 3 4 5 6 7 8 { \u0026#34;server\u0026#34;:\u0026#34;ss服务器IP\u0026#34;, \u0026#34;server_port\u0026#34;:443, // ss服务器port \u0026#34;local_port\u0026#34;:1080, // 本地监听端口 \u0026#34;password\u0026#34;:\u0026#34;xxxx\u0026#34;, // ss服务器密码 \u0026#34;timeout\u0026#34;:600, \u0026#34;method\u0026#34;:\u0026#34;aes-256-cfb\u0026#34; // ss服务器加密方法 } 至于shadowsocks服务器端，可以租用国内网络外的云主机自己搭建，也可以购买现成的机场服务，本文就不讨论了。\n然后启动shadowsocks client：\n1 nohup ss-local -c ss-client.conf \u0026amp; 安装配置HTTP代理 shadowsocks client创建的是socks5代理，不过一些程序无法使用 socks5 ，它们需要通过 http_proxy 和 https_proxy 环境变量，使用 HTTP 代理。polipo 可以帮助我们将 socks5 代理转换为 HTTP 代理。\nmacOS下安装polipo 1 brew install polipo Ubuntu下安装polipo 1 2 3 4 5 sudo apt install polipo # 建议停掉polipo服务，需要的时候自己启动 sudo systemctl stop polipo.service sudo systemctl disable polipo.service 启动HTTP代理\n1 sudo polipo socksParentProxy=127.0.0.1:1080 proxyPort=1087 socksParentProxy配置为localhost和ss-local监听端口，proxyPort是启动的HTTP代理端口。\n我们可以在命令行终端测试HTTP代理的效果：\n1 2 3 $ export http_proxy=http://localhost:1087 $ export https_proxy=http://localhost:1087 $ curl https://www.google.com 应该可以正常访问到Google。\n设置Docker HTTP代理 如果是在macOS上使用Docker Desctop，可以在Preference中的Proxies设置上一步启动的HTTP代理：\n如果是Linux平台，请参考Docker的官方文档进行设置。\n在阿里云创建容器镜像的命名空间 为了将镜像同步到阿里云，首先需要在阿里云的容器镜像服务控制台创建镜像的命名空间。\n建议将仓库类型设置为“公开”，这样其他人也能搜索、下载到镜像。\n从gcr.io下载镜像 在本机从gcr.io下载镜像，我们以镜像pause:3.1为例：\n1 docker pull k8s.gcr.io/pause:3.1 给镜像标记新的tag 根据前面在阿里云创建的命名空间，给镜像标记新的tag：\n1 docker tag k8s.gcr.io/pause:3.1 registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 mz-k8s是在前面创建的命名空间。 查看tag结果：\n1 2 3 4 5 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/pause 3.1 da86e6ba6ca1 17 months ago 742kB registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause 3.1 da86e6ba6ca1 17 months ago 742kB 通过IMAGE ID可以看出，两个镜像为同一个。\n将镜像上传到阿里云 登录阿里云镜像仓库：\n1 $ docker login --username=(阿里云账号) registry.cn-shenzhen.aliyuncs.com 根据提示输入password，登录成功后，显示Login Succeeded。\n上传镜像：\n1 docker push registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 从阿里云下载镜像 现在可以在其他机器上从阿里云下载pause:3.1镜像，这时候已经不需要科学上网了：\n1 $ docker pull registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 给镜像打上原来的tag，这样kubeadm等工具就可以使用本地仓库中的pause:3.1镜像了：\n1 $ docker tag registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 k8s.gcr.io/pause:3.1 至此，我们跨越长城，将一个docker镜像从gcr.io搬到了Aliyun。\n如果是需要批量、定时的从gcr.io同步镜像，建议考虑使用阿里开源的sync-repo。\n","date":"2019-06-06T09:53:24+08:00","permalink":"https://mazhen.tech/p/%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E5%B0%86%E8%B0%B7%E6%AD%8Ck8s%E9%95%9C%E5%83%8F%E5%90%8C%E6%AD%A5%E5%88%B0%E9%98%BF%E9%87%8C%E4%BA%91/","title":"自己动手将谷歌k8s镜像同步到阿里云"},{"content":"用户在访问Kubernetes集群的API server时，访问请求需要经过身份验证、授权和准入控制这三个阶段的检查，才能真正到达API服务，如下图所示：\nKubernetes中的用户有两种类型：service accounts 和 normal users。service accounts 由 Kubernetes管理，它是Pod中的进程用于访问API服务的account，为Pod中的进程提供了一种身份标识。normal users是由外部系统管理，在Kubernetes中并没有对应的 user 对象，它为人类用户使用kubectl之类的工具访问API服务时提供身份标识。所有用户，不管是使用 kubectl、客户端lib、还是直接发起REST请求访问API server，都需要经过上述三个步骤的检查。\n本文将介绍Kubernetes集群的身份验证，即Kubernetes如何确认来访者的身份。\nKubernetes支持多种方式的身份验证：客户端证书，Password， Plain Tokens，JWT(JSON Web Token)，HTTP basic auth等。你可以同时启用多种认证，一般建议至少使用两种：\n为验证normal users身份的客户端证书方式 为验证Service accounts身份的 JWT Tokens方式 使用客户端证书进行身份验证 理解数字证书 非对称加密算法是证书的基础。数字签名、数字证书等一系列概念有点绕，但只要记住：公钥用来加密，私钥用来签名 就可以了。\n怎么理解呢？公钥可以随意分发，谁都可以持有，如果你用私钥加密，任何持有对应公钥的人都可以解密，这样做和没加密一样，没什么意义。因此，我们需要用公钥加密，只有持有私钥的那个人才能解密。私钥之所以称为私钥，一定会私密保存，不会向其他人泄漏。同时，用私钥加密虽然没有意义，但如果别人用公钥解开了私钥加密的信息，就能够证明信息是由私钥持有者发出的，验证了信息发送者的身份，这就是数字签名。\n每个人制作好自己的公钥和私钥，然后把公钥发布出去。两个人如果都有对方的公钥，就可以用对方的公钥给对方发送加密信息，同时附上用私钥加密的信息摘要作为数字签名，证明消息发送者的身份。\n通过加密防止了窃听风险，通过数字签名防止了冒充风险，数字签名内的消息摘要防止了篡改风险，一起看似很完美。\n等等，这里有个很重要的问题被忽略了：如何安全的将公钥发布出去？如果双方希望安全通信，最好当面交换公钥，以免被别人冒充，并且要保护好自己的电脑，避免公钥被别有用心的人替换。现实中不可能这样分发公钥，效率太低，几乎无法大规模实行。于是出现了CA(certificate authority)，为公钥做认证。CA用自己的私钥，对申请用户的公钥和一些身份信息加密，生成\u0026quot;数字证书\u0026quot;（Digital Certificate）。你现在可以用任何方式将内含公钥的数字证书发布出去，例如有客户发起请求，希望以HTTPS的方式访问你的WEB服务，你可以在第一次回复客户的响应中带上数字证书。客户拿到你的数字证书，用CA的公钥解开数字证书，安全的获得你的公钥。有了CA为你的数字证书背书，客户可以确定你的身份，不是有人在冒充你。\n那么CA的公钥如何安全的分发呢？首先，证书的签发是“链”式结构，给你签发证书的CA，它的证书可能还是由上一级CA机构签发的，这样一直往上追溯，最终会到某个“根证书”。如果“根证书”是被我们信任的，那么整条“链”上的证书都可信。\n其次，操作系统都内置了“受信任的根证书”。我们拿到某个证书，如果它的根证书在系统的“受信任的根证书”列表中，那么这个证书就是可信的。例如知乎的证书：\n可以看到，它的根证书是DigiCert Global Root CA，在操作系统的“受信任的根证书”列表中能找到它：\n根证书是通过预装的方式完成的分发，因此安装来源不明的操作系统有风险，可能潜伏了非法的根证书。一旦被植入了非法的根证书，一整套的安全体系瞬间土崩瓦解。同时，不能随意向系统中添加可信任的根证书，你很难验证根证书的真伪，它已经是root，没人能为它做背书了。12306网站早期的根证书就不在操作系统的“受信任根证书”列表中，需要用户手工安装，在网上引起轩然大波。最终12306在17年底的时候换成了Digicert的证书。\n简单总结一下基于非对称加密算法的公钥/私钥体系，公钥用来加密，私钥用来签名，引入CA保证公钥的安全分发。你可以找CA签发数字证书，那么你的客户就可以根据本地“受信任的根证书”验证你的数字证书，从而确认你的身份，然后用证书内包含的公钥给你发加密的信息。同样，你也可以要求对方的数字证书，以便确认对方的身份，并给他回加密的信息。\n理解了数字证书的基本原理，我们再看看Kubernetes中如何使用客户端证书进行身份验证。\n数字证书在Kubernetes中的应用 Kubernetes各组件之间的通信都是基于TLS，实现服务的加密访问，同时支持基于证书的双向认证。\n我们在搭建私有Kubernetes集群时，一般是自建root CA，因为参与认证的所有集群节点，包括远程访问集群的客户端桌面都完全由自己控制，我们可以安全的将根证书分发到所有节点。有了CA，我们再用CA的私钥/公钥为各个组件签发所需的证书。\nCA的创建，以及一系列客户端、服务端证书的签发，实际上是建立了Kubernetes集群的PKI(Public key infrastructure)。\nKubernetes中的组件比较多，所以需要的证书会非常多，这篇文档做了介绍。我按证书的用途归类总结一下：\nCA证书 Kubernetes 一般用途 etcd 集群根证书 aggregation 相关功能 服务端证书 API server etcd kubelet 访问API server时进行身份验证的客户端证书 kubelet　-\u0026gt; API server controller-manager -\u0026gt; API server kube-scheduler -\u0026gt; API server admin用户　-\u0026gt; API server API server 访问其他组件时进行身份验证的客户端证书 API server -\u0026gt; etcd API server -\u0026gt; kubelet API server -\u0026gt; aggregated API server etcd 相关功能 etcd 集群中节点互相通信使用的客户端证书 如果etcd是以Pod方式运行，针对etcd的 Liveness 需要的客户端证书 Service accounts 私钥/公钥对，用于生成Service accounts身份验证的 JWT Tokens 最后一个不是证书，不过也在Kubernetes PKI的管理范围。关于Service accounts 私钥/公钥对的作用，后面会讲到。\n理论上CA根证书可以只使用一个，不过为了安全和方便管理，官方强调在不同的上下文最好使用不同的CA：\nWarning: Do not reuse a CA that is used in a different context unless you understand the risks and the mechanisms to protect the CA’s usage.\n可以看出，API server是核心组件，其他组件、包括admin用户对它的访问都需要TLS双向认证，所以会有API server的服务端证书和各个组件的客户端证书。API server作为客户端需要访问etcd、kubelet和aggregated API server，所以也会有相应的服务端、客户端证书。\n当我们使用kubeadm安装Kubernetes时，kubeadm会为我们生成上述的一系列私钥和证书，放在/etc/kubernetes/目录下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # tree --dirsfirst /etc/kubernetes/ /etc/kubernetes/ ├── manifests 组件的配置文件，以Pod方式运行在集群中 │ ├── etcd.yaml │ ├── kube-apiserver.yaml │ ├── kube-controller-manager.yaml │ └── kube-scheduler.yaml ├── pki │ ├── etcd │ │ ├── ca.crt etcd 集群CA证书 │ │ ├── ca.key etcd 集群CA私钥 │ │ ├── healthcheck-client.crt Liveness 健康检查使用的客户端证书 │ │ ├── healthcheck-client.key │ │ ├── peer.crt etcd节点间通信使用的客户端证书 │ │ ├── peer.key │ │ ├── server.crt etcd服务端证书 │ │ └── server.key │ ├── apiserver.crt API Server 服务端证书 │ ├── apiserver.key │ ├── apiserver-etcd-client.crt API server -\u0026gt; etcd │ ├── apiserver-etcd-client.key │ ├── apiserver-kubelet-client.crt API server -\u0026gt; kubelet │ ├── apiserver-kubelet-client.key │ ├── ca.crt CA证书 │ ├── ca.key CA私钥 │ ├── front-proxy-ca.crt aggregation 相关功能CA证书 │ ├── front-proxy-ca.key aggregation 相关功能CA私钥 │ ├── front-proxy-client.crt API server -\u0026gt; aggregated API server │ ├── front-proxy-client.key │ ├── sa.key Service accounts 私钥 │ └── sa.pub Service accounts 公钥 ├── admin.conf admin -\u0026gt; API server ├── controller-manager.conf controller-manager -\u0026gt; API server ├── kubelet.conf kubelet　-\u0026gt; API server └── scheduler.conf kube-scheduler -\u0026gt; API server 注意，最后四个*.conf是kubeconfig file，内容包含了集群、用户、namespace等信息，还有用来认证的CA证书、客户端证书和私钥。例如admin.conf就是kubectl访问集群用到的kubeconfig file，缺省情况下kubectl会使用$HOME/.kube/config，你也可以通过KUBECONFIG环境变量，或kubectl 的 --kubeconfig 参数进行设置。\nkubelet运行在每个工作节点，无法提前预知 node 的 IP 信息，所以 kubelet 一般不会明确指定服务端证书, 而是只指定 CA 根证书, 让 kubelet 根据本机信息自动生成服务端证书，保存到配置参数指定的\u0026ndash;cert-dir目录中。cert-dir的缺省值是/var/lib/kubelet/pki。\n1 2 3 4 5 6 7 # tree /var/lib/kubelet/pki /var/lib/kubelet/pki ├── kubelet-client-2019-04-28-10-48-13.pem ├── kubelet-client-current.pem -\u0026gt; /var/lib/kubelet/pki/kubelet-client-2019-04-28-10-48-13.pem ├── kubelet.crt kubelet服务端证书 └── kubelet.key kubelet服务端私钥 另外，API server有很多认证相关的启动参数，参数名称让人容易混淆，有人还专门提了issue，这个回答根据用途对这些参数进行了分类，说明的非常清晰。\nAPI server 如何用客户端证书进行身份验证 前面提到，当用户使用kubectl访问API server时，需要以某种方式进行身份验证，最常用的方式就是使用客户端证书。Kubernetes是没有 user 这种 API 对象，kubectl的用户身份信息就包含在客户端证书中。API server验证了客户端证书，也就可以从证书中获得用户名和所属的group。\n我们以/etc/kubernetes/admin.conf 为例，看看客户端证书中提供了那些信息。\n先查看admin.conf文件的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ kubectl --kubeconfig /etc/kubernetes/admin.conf config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://172.18.100.90:6443 name: test contexts: - context: cluster: test user: kubernetes-admin name: kubernetes-admin@test current-context: kubernetes-admin@test kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 这个文件提供了API server的地址，以及身份验证用到的证书：\ncertificate-authority-data ：CA证书 client-certificate-data ：客户端证书 client-key-data： 客户端私钥 客户端证书是以base64编码的方式保存在client-certificate-data字段中，我们将证书提取出来：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # cat /etc/kubernetes/admin.conf | grep client-certificate-data | cut -d \u0026#34; \u0026#34; -f 6 | base64 -d \u0026gt; admin.crt # cat admin.crt -----BEGIN CERTIFICATE----- MIIC8jCCAdqgAwIBAgIIaBuxevPYGaswDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE AxMKa3ViZXJuZXRlczAeFw0xOTA0MjgwMjQwMDBaFw0yMDA0MjcwMjQwMDNaMDQx FzAVBgNVBAoTDnN5c3RlbTptYXN0ZXJzMRkwFwYDVQQDExBrdWJlcm5ldGVzLWFk bWluMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvRTIQ4YEMh0mUWKP 17pLdUocgZMLVCK6tYmj0DJIihRk+wKvNzYSStfxsug9nnEqVVzmbW5/UxER776H 844y/1NGk/8LsDIkFGspf3cEmQ8OE8TlLNW7h9gWIGymLQ/K1qhYfNOPDYoJXPix eUWTJgn0+neJNbJ3JoJk2WRlDFwbE0uXgYYczuDcJablSdbb8Oc+E4qJ1U7u9YMN Bo/JBY68wYtdjXHl6Mg28aCioVZrs5eZWkNzNpXMVjQwFdZAdWbnS3OJGN1b6IrV gWk9PMoCE2TtFv5NdlHSYFtEAaBEwfl3/D3rGHKb4ZH/fgKWsepy8ffxxibM6pND pLnmAwIDAQABoycwJTAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0lBAwwCgYIKwYBBQUH AwIwDQYJKoZIhvcNAQELBQADggEBALPYorv2mlXyu6jpX/6gE1kTvpPGK4vylfSY 9jl4PtQZgRaXvmVsUKpyIdtVhdMZp9EFaYNC4AYkqaVEOoAbU96SYhdO/6h7Rn8T 0Ae+f1Vwt+8GxErEN3xp4noHfXM0eSEuFLPXt43BBJInYRyx1J0urAjYtNCvc9wX uQFVmNKsqgmjvHQsRkvKcb8HEzcaD1TqqnTpq3usGjNggVZFTChB58R909yGPEXL n7VsilmN86gom3fgqwCn2C00iKcuzCOwYN2T+Mi8KI2DraDDoVeRMSaYQNUfKNIX Ngeod/C4piq+OAdyrPPFEINdLi404EYHyod0CgiD6uhoX5W06O4= -----END CERTIFICATE----- 使用openssl查看证书内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # openssl x509 -in ./admin.crt -text Certificate: Data: Version: 3 (0x2) Serial Number: 7501784745950845355 (0x681bb17af3d819ab) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=kubernetes Validity Not Before: Apr 28 02:40:00 2019 GMT Not After : Apr 27 02:40:03 2020 GMT Subject: O=system:masters, CN=kubernetes-admin Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) ...... 注意这一行：\n1 Subject: O=system:masters, CN=kubernetes-admin API server会将证书中CN(Common Name)作为用户名，O(Organization)作为用户所属的group。API server从这个证书得到的信息是：admin用户所属的group是system:masters。至此，身份验证阶段完成。\n下一步是授权检查，也就是检查用户有没有权限执行这个操作。这是另外一个话题，本文不做详细讨论，只是简单介绍一下。根据官方文档，Kubernetes提供了缺省的 ClusterRole - group 绑定关系，已经将system:masters group 和 角色 cluster-admin绑定到了一起：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # kubectl get clusterrolebindings cluster-admin -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2019-04-28T02:40:17Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: cluster-admin resourceVersion: \u0026#34;98\u0026#34; selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin uid: f58a87a7-695e-11e9-91ca-005056ac1c1c roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:masters 这个绑定关系的意思是，属于system:mastersgroup的用户，都拥有cluster-admin角色包含的权限。我们再看看角色cluster-admin的具体权限信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # kubectl get clusterrole cluster-admin -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2019-04-28T02:40:17Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: cluster-admin resourceVersion: \u0026#34;44\u0026#34; selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-admin uid: f5523c21-695e-11e9-91ca-005056ac1c1c rules: - apiGroups: - \u0026#39;*\u0026#39; resources: - \u0026#39;*\u0026#39; verbs: - \u0026#39;*\u0026#39; - nonResourceURLs: - \u0026#39;*\u0026#39; verbs: - \u0026#39;*\u0026#39; 从rules列表可以看出，cluster-admin这个角色对所有资源都有无限制的操作权限。因此，使用了这个kubeconfig file的kubectl的请求就有了操控和管理整个集群的权限。\n使用JWT Tokens进行身份验证 运行在Pod中的进程需要访问API server时，同样需要进行身份验证和授权检查。如何让Pod具有用户身份呢？通过给Pod指定service account来实现。service account是Kubernetes内置的一种 “服务账户”，它为Pod中的进程提供了一种身份标识。如果Pod没有显式的指定service account，系统会自动为其关联default service account。\n我们自己创建service account对象非常简单：\n1 2 3 4 5 6 7 8 //serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: nginx-example # kubectl apply -f serviceaccount.yaml serviceaccount/nginx-example created 查看刚刚创建的service account：\n1 2 3 4 5 6 7 8 9 10 11 # kubectl describe serviceaccounts nginx-example Name: nginx-example Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;ServiceAccount\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;nginx-example\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;}} Image pull secrets: \u0026lt;none\u0026gt; Mountable secrets: nginx-example-token-r2cv6 Tokens: nginx-example-token-r2cv6 Events: \u0026lt;none\u0026gt; 当service account对象创建成功，controller-manager会发现这个新对象，然后为它生成token。token实际上是secret对象，内部包含了用来身份验证的token。service account对象的Tokens列引用的就是controller-manager为它创建的token。\n我们来看看token的内容：\n1 2 3 4 5 6 7 8 9 10 11 # kubectl get secrets nginx-example-token-r2cv6 -o yaml apiVersion: v1 data: ca.crt: (APISERVER\u0026#39;S CA BASE64 ENCODED) namespace: ZGVmYXVsdA== token: (BEARER TOKEN BASE64 ENCODED) kind: Secret metadata: ...... type: kubernetes.io/service-account-token 可以看到，这个secret对象的type是service-account-token，包含了三部分数据：\nca.crt： API Server的CA证书，用于Pod中的进程访问API Server时对服务端证书进行校验 namespace： secret所在namespace，使用了base64编码 token：JWT Tokens JWT Tokens 是 controller-manager 用 service account私钥sa.key签发的，其中包含了用户的身份信息，API Server可以用sa.pub验证token，拿到用户身份信息，从而完成身份验证。\n如果是使用kubeadm安装的Kubernetes，我们可以在/etc/kubernetes/manifests/目录中的配置文件确认sa.key和sa.pub的作用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # cat /etc/kubernetes/manifests/kube-controller-manager.yaml ... spec: containers: - command: - kube-controller-manager ... - --service-account-private-key-file=/etc/kubernetes/pki/sa.key ... # cat /etc/kubernetes/manifests/kube-apiserver.yaml ... spec: containers: - command: - kube-apiserver ... - --service-account-key-file=/etc/kubernetes/pki/sa.pub ... controller-manager用私钥sa.key签名，API Server用公钥sa.pub验签。\n运行在Pod中的进程在向API server发起HTTP请求时，HTTP header中会携带token，API server从header中拿到token，进行身份验证：\n1 Authorization: Bearer [token] JWT Tokens的是由用.分割的三部分组成：\nHeader Payload Signature 因此，一个JWT Tokens看起来是这样的：\n1 xxxxxxx.yyyyyyyy.zzzzzzz Header和Payload都是base64编码的JSON，以上面nginx-example关联的token为例，看看Header和Payload的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ kubectl describe secrets nginx-example-token-r2cv6 | grep token: | cut -d \u0026#34; \u0026#34; -f 7 eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im5naW54LWV4YW1wbGUtdG9rZW4tcjJjdjYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibmdpbngtZXhhbXBsZSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBmZWRlOWM1LTc2YjUtMTFlOS05MWNhLTAwNTA1NmFjMWMxYyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0Om5naW54LWV4YW1wbGUifQ.VOjVQBr5PKg1WyZYtIW0Fos5fxFN4cYE3Mz9p1eWbQP6rQRQGDEiGX-LBuM6ECI9cpSL-F4nYQAL9vmIlA4vbAgS4OFgC4nwu8SzLu2FVeE7RDpguvsdAsj-4T_LxEGX1RPljGTpvlt8HRjTnp9K8W4dy7PyJQEB5XvCf-IVNAs3zESgmuJ7wJwO7mXQe5WdeqhI5vXjcZiXP97oH0VRYT1vTKVP-GooC5YfaNhU7rHoJ0gmR10xNqZjwKGsHKkq5maC5BOrXFLlHRqVRwm9-hRn-ZLgAoCwujCIpLvPaFUR8HaatzX4GQ_HWev2soJnk1qcav0smxfjC-fu540vZA $ kubectl describe secrets nginx-example-token-r2cv6 | grep token: | cut -d \u0026#34; \u0026#34; -f 7 | cut -d \u0026#34;.\u0026#34; -f 1 | base64 -d | python -mjson.tool { \u0026#34;alg\u0026#34;: \u0026#34;RS256\u0026#34;, \u0026#34;kid\u0026#34;: \u0026#34;\u0026#34; } $ kubectl describe secrets nginx-example-token-r2cv6 | grep token: | cut -d \u0026#34; \u0026#34; -f 7 | cut -d \u0026#34;.\u0026#34; -f 2 | base64 -d | python -mjson.tool { \u0026#34;iss\u0026#34;: \u0026#34;kubernetes/serviceaccount\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/secret.name\u0026#34;: \u0026#34;nginx-example-token-r2cv6\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.name\u0026#34;: \u0026#34;nginx-example\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.uid\u0026#34;: \u0026#34;0fede9c5-76b5-11e9-91ca-005056ac1c1c\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;system:serviceaccount:default:nginx-example\u0026#34; } Header中的alg指明了签名用到的加密算法，Payload 中包含了用户的身份信息，可以知道这个service account属于的namespace为default，名称为nginx-example。\n第三部分Signature的构造方式如下，如果加密算法选择了PKCS SHA：\n1 2 3 4 PKCSSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), secret) controller-manager用sa.key签名，API Server用公钥sa.pub验签，进行身份验证。\n如果先深入了解JWT(JSON Web Token)，建议阅读这篇文档\u0026lt;JWT: The Complete Guide to JSON Web Tokens\u0026gt;\nPod中的进程如何获得这个token呢？Kubernetes在创建Pod时，会将service account token映射到Pod的/var/run/secrets/kubernetes.io/serviceaccount 目录下。我们通过一个例子演示一下。\n创建Pod：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // simple.yaml apiVersion: v1 kind: Pod metadata: name: firstpod spec: serviceAccountName: nginx-example containers: - image: nginx name: stan $ kubectl apply -f simple.yaml pod/firstpod created 查看Pod内/var/run/secrets/kubernetes.io/serviceaccount目录的内容：\n1 2 3 4 $ kubectl exec firstpod -- ls /var/run/secrets/kubernetes.io/serviceaccount ca.crt namespace token 查看Pod内文件/var/run/secrets/kubernetes.io/serviceaccount/token的内容：\n1 2 3 4 5 6 7 8 9 $ kubectl exec firstpod -- cat /var/run/secrets/kubernetes.io/serviceaccount/token | cut -d \u0026#34;.\u0026#34; -f 2 | base64 -d | python -mjson.tool { \u0026#34;iss\u0026#34;: \u0026#34;kubernetes/serviceaccount\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/secret.name\u0026#34;: \u0026#34;nginx-example-token-r2cv6\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.name\u0026#34;: \u0026#34;nginx-example\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.uid\u0026#34;: \u0026#34;0fede9c5-76b5-11e9-91ca-005056ac1c1c\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;system:serviceaccount:default:nginx-example\u0026#34; } 可以看到，映射进Pod中的token，正是我们在配置中通过serviceAccountName指定的nginx-example。Pod中的进程可以通过访问文件/var/run/secrets/kubernetes.io/serviceaccount/token拿到token。\n如何为service account授权？通过定义service account和role的绑定完成。本文简单演示一下，详细的说明参加官方文档。\n创建role：\n1 2 3 4 5 6 7 8 9 10 11 12 // example-role.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] $ kubectl apply -f example-role.yaml role.rbac.authorization.k8s.io/example-role created role可以理解为一组权限的集合，例如上面创建的example-role对default Namesapce内的Pods有get、watch和list操作权限。\n下一步就是将service account和role进行绑定：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //example-rolebinding.yaml kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: default subjects: - kind: ServiceAccount name: nginx-example namespace: default roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io $ kubectl apply -f example-rolebinding.yaml rolebinding.rbac.authorization.k8s.io/example-rolebinding created 通过绑定的创建，service account就拥有了role定义的权限。\n总结 用户对API server的访问需要通过身份验证、授权和准入控制这三个阶段的检查。\n一般集群外部用户访问API Server使用客户端证书进行身份验证。Kubernetes各组件之间的通信都使用了TLS加密传输，同时支持基于证书的双向认证。因此Kubernetes的安装过程涉及很多证书的创建，本文分类介绍了这些证书的作用。\n集群内Pod中的进程访问API server时，使用service account关联的token进行身份验证。\n每个Pod都会关联一个service account，没有明确指定时使用default。当我们创建service account对象，controller-manager会为这个service account生成Secret，内部包含了用来身份验证的JWT Tokens。Kubernetes会将token文件mount到Pod的/var/run/secrets/kubernetes.io/serviceaccount/token，Pod内的进程在向API server发起的HTTP时，就可以在请求头中携带这个token。\n","date":"2019-05-19T09:46:53+08:00","permalink":"https://mazhen.tech/p/kubernetes%E9%9B%86%E7%BE%A4%E7%9A%84%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81/","title":"Kubernetes集群的身份验证"},{"content":"界面概览 快捷键 描述 cmd + shift + e 文件资源管理器 cmd + shift + f 跨文件搜索 ctrl + shift + g 源代码管理 cmd + shift + d 启动和调试 cmd + shift + x 扩展管理 cmd + shift + p 查找并运行所有命令 cmd + j 打开、关闭panel 命令行的使用 命令 描述 code $path 新窗口中打开这个文件或文件夹 code -r $path 窗口复用打开文件 code -r -g $file:lineno 打开文件，跳转到指定行 code -r -d $file1 $file2 比较两个文件 ls code - 光标移动 快捷键 描述 option + 左/右方向键 针对单词的光标移动 cmd + 左/右方向键 移动到行首、行尾 cmd + shift + \\ 在花括号之间跳转 cmd + 上/下方向键 移动到文档的第一行、最后一行 文本选择 shift + 光标移动\n删除操作 可以先选择，再删除\n快捷键 描述 cmd + fn + del 删除到行尾 cmd + del 删除到行首 option + del 向前删除单词 option + fn + del 向后删除单词 代码行编辑 快捷键 描述 cmd + shift + k 删除行 cmd + x 剪切行 cmd + enter 在当前行下一行新开始一行 cmd + shift + enter 在当前行上一行新开始一行 option + 上/下方向键 将当前行上下移动 option + shift + 上/下方向键 将当前行上下复制 cmd + / 将一行代码注释 option + shift + a 注释整块代码 option + shift + f 代码格式化 cmd+k cmd+f 选中代码格式化 ctrl + t 光标前后字符调换位置 cmd+shift+p transform to up/low case 转换大小写 ctrl + j 合并代码行 cmd + u 撤销光标移动 创建多个光标 使用鼠标 option + 鼠标左键\n使用键盘 快捷键 描述 cmd + option + 上/下方向键 创建多个光标 cmd + d 选中相同单词，并创建多个光标 option + shift+ i 在选择的多行后创建光标 文件跳转 快捷键 描述 ctrl + tab 文件标签之间跳转 cmd + p 打开文件列表 行跳转 快捷键 描述 ctrl + g 跳转到指定行 符号跳转 快捷键 描述 cmd + shift + o 当前文件所有符号列表 @: 符号列表@后输入冒号，符号分类排列 cmd + t 在多个文件进行符号跳转 cmd + F12 跳转到函数的实现位置 shift + F12 函数引用列表 ctrl + - 跳回上一次光标所在位置 ctrl + shift + - 跳回下一次光标所在位置 代码自动补全 快捷键 描述 ctrl+ space 调出建议列表 cmd + shift + space 调出参数预览窗口 cmd + . 快速修复建议列表 F2 函数名重构 代码折叠 快捷键 描述 cmd+ option + [ 最内层折叠 cmd + option + ] 最内层展开 cmd+k cmd+0 全部折叠 cmd+k cmd+j 全部展开 搜索 快捷键 描述 cmd + f 搜索 cmd + g 搜索，光标在编辑器内跳转 cmd + option + f 查找替换 cmd + shift + f 多文件搜索 编辑器操作 快捷键 描述 cmd + \\ 拆分编辑器 option + cmd + 左/右方向键 编辑器间切换 cmd + num 在拆分的编辑器窗口跳转 Cmd +/- 缩放整个工作区 cmd + shift + p reset zoom 重置缩放 专注模式 快捷键 描述 cmd + b 打开或者关闭整个视图 cmd + j 打开或者关闭面板 cmd+shift+p Toggle Zen Mode 切换禅模式 cmd+shift+p Toggle Centered Layout 切换剧中布局 命令面板 快捷键 描述 cmd + shift + p 命令面板 命令面板的第一个符合对应着不同的功能：\n? 列出所有可用功能 \u0026gt; 用于显示所有的命令 @ 用于显示和跳转文件中的 “符号”（Symbols） @: 可以把符号们按类别归类 # 用于显示和跳转工作区中的 “符号”（Symbols）。 : 用于跳转到当前文件中的某一行。 edt 显示所有已经打开的文件 edt active 显示当前活动组中的文件 ext 插件的管理 ext install 搜索和安装插件。 task 任务 debug 调试功能 term创建和管理终端实例 view 打开各个 UI 组件 窗口管理 快捷键 描述 ctrl + w 窗口切换 ctrl + r 切换文件夹 ctrl+r cmd+enter 新建窗口打开文件夹 集成终端 快捷键 描述 ctrl + ` 切换集成终端 ctrl + shift + ` 新建集成终端 cmd+shift+p Run Active File In Active Terminal 在集成终端中运行当前脚本 cmd+shift+p Run Selected Text In Active Terminal 在集成终端中运行所选文本 任务管理 快捷键 描述 cmd+shift+p run task 自动检测当前项目中可运行的任务 cmd+shift+p Configure Task 配置任务 Cmd + Shift + b 运行默认的生成任务（build task） 鼠标操作 文本选择 双击鼠标，选中单词 三击鼠标，选中一行 四击鼠标，选中整个文档 单击行号，选中行 文本编辑 选中后可以拖动文本到指定区域 拖动过程中按option，变成复制文本到指定区域 在悬停窗口上按下cmd，提示函数的实现 ","date":"2019-05-16T10:56:00+08:00","permalink":"https://mazhen.tech/p/vs-code-%E5%BF%AB%E6%8D%B7%E9%94%AE/","title":"vs code 快捷键"},{"content":"飞腾芯片 + 银河麒麟OS是目前国产自主可控市场上的主流基础平台。飞腾芯片是aarch64架构，是支持64位的ARM芯片。银河麒麟是基于Ubuntu的发行版。因此可以认为飞腾芯片 + 银河麒麟OS相当于 ARM64 + Ubuntu。\n本文介绍在飞腾平台上编译安装nginx的步骤。\n下载nginx源码 从http://nginx.org/en/download.html下载当前稳定版本的源码，例如\n1 wget http://nginx.org/download/nginx-1.14.2.tar.gz 解压nginx源码：\n1 tar -zxvf nginx-1.14.2.tar.gz nginx配置文件的语法高亮 将nginx源码目录下contrib/vim/的所有内容，copy到用户的$HOME/.vim目录，可以实现nginx配置文件在vim中的语法高亮。\n1 2 mkdir ~/.vim cp -r contrib/vim/* ~/.vim/ 再使用vim打开nginx.conf，可以看到配置文件已经可以语法高亮。\n编译前的配置 查看编译配置支持的参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 $ ./configure --help | more --help print this message --prefix=PATH set installation prefix --sbin-path=PATH set nginx binary pathname --modules-path=PATH set modules path --conf-path=PATH set nginx.conf pathname --error-log-path=PATH set error log pathname --pid-path=PATH set nginx.pid pathname --lock-path=PATH set nginx.lock pathname --user=USER set non-privileged user for worker processes --group=GROUP set non-privileged group for worker processes --build=NAME set build name --builddir=DIR set build directory --with-select_module enable select module --without-select_module disable select module --with-poll_module enable poll module --without-poll_module disable poll module ...... --with-libatomic force libatomic_ops library usage --with-libatomic=DIR set path to libatomic_ops library sources --with-openssl=DIR set path to OpenSSL library sources --with-openssl-opt=OPTIONS set additional build options for OpenSSL --with-debug enable debug logging --with开头的模块缺省不包括在编译结果中，如果想使用需要在编译配置时显示的指定。--without开头的模块则相反，如果不想包含在编译结果中需要显示设定。\n例如我们可以这样进行编译前设置：\n1 ./configure --prefix=/home/adp/nginx --with-http_ssl_module 设置了nginx的安装目录，需要http_ssl模块。\n如果报错缺少OpenSSL，需要先安装libssl。在/etc/apt/sources.list.d目录下增加支持ARM64的apt源，例如国内的清华，创建tsinghua.list，内容如下：\n1 2 3 4 5 6 7 8 deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial main multiverse restricted universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-security main multiverse restricted universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-updates main multiverse restricted universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-backports main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-security main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-updates main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-backports main multiverse restricted universe 执行命令安装OpenSSL：\n1 2 $ sudo apt-get update $ sudo apt-get install libssl-dev configure命令执行完后，会生成中间文件，放在目录objs下。其中最重要的是ngx_modules.c文件，它决定了最终那些模块会编译进nginx。\n执行编译 在nginx目录下执行make编译：\n1 $ make 编译成功的nginx二进制文件在objs目录下。如果是做nginx的升级，可以直接将这个二进制文件copy到nginx的安装目录中。\n安装 在nginx目录下执行make install进行安装：\n1 $ make install 安装完成后，我们到--prefix指定的目录中查看安装结果：\n1 2 3 4 5 6 7 $ tree -L 1 /home/adp/nginx nginx/ ├── conf ├── html ├── logs └── sbin 验证安装结果 编辑nginx/conf/nginx.conf文件，设置监听端口为8080：\n1 2 3 4 5 6 7 http { ... server { listen 8080; server_name localhost; ... 启动nginx\n1 ./sbin/nginx 访问默认首页：\n1 2 3 4 5 6 7 8 9 10 11 $ curl -I http://localhost:8080 HTTP/1.1 200 OK Server: nginx/1.14.2 Date: Tue, 02 Apr 2019 08:38:02 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Tue, 02 Apr 2019 08:30:04 GMT Connection: keep-alive ETag: \u0026#34;5ca31d8c-264\u0026#34; Accept-Ranges: bytes 其他常用命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 查看帮助 $ ./sbin/nginx -? # 重新加载配置 $ ./sbin/nginx -s reload # 立即停止服务 $ ./sbin/nginx -s stop # 优雅停止服务 $ ./sbin/nginx -s quit # 测试配置文件是否有语法错误 $ ./sbin/nginx -t/-T # 打印nginx版本、编译信息 $ ./sbin/nginx -v/-V ","date":"2019-04-02T09:45:05+08:00","permalink":"https://mazhen.tech/p/%E5%9C%A8%E5%9B%BD%E4%BA%A7%E9%A3%9E%E8%85%BE%E5%B9%B3%E5%8F%B0%E4%B8%8A%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85nginx/","title":"在国产飞腾平台上编译安装nginx"},{"content":"kubectl会使用$HOME/.kube目录下的config文件作为缺省的配置文件。我们可以使用kubectl config view查看配置信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://172.18.100.90:6443 name: cluster-1 contexts: - context: cluster: cluster-1 user: cluster-1-admin name: cluster-1-admin@cluster-1 current-context: cluster-1-admin@cluster-1 kind: Config preferences: {} users: - name: cluster-1-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 可以看到，配置文件主要包含了clusters，users和contexts三部分信息。context是访问一个kubernetes集群所需要的参数集合。每个context有三个参数：\ncluster：要访问的集群信息 namespace：用户工作的namespace，缺省值为default user：连接集群的认证用户 缺省情况下，kubectl会使用current-context指定的context作为当前的工作集群环境。不难想象，切换context就可以切换到不同的kubernetes集群。\n在不了解context的概念之前，想访问不同的集群，每次都要把集群对应的config文件copy到$HOME/.kube目录下，同时要记得使用kubectl cluster-info确认当前访问的集群：\n1 2 3 4 5 6 $kubectl cluster-info Kubernetes master is running at https://172.18.100.90:6443 KubeDNS is running at https://172.18.100.90:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. 在看了这篇文档后，才知道kubectl可以切换context来管理多个集群。如果你有多个集群的config文件，可以在系统环境变量KUBECONFIG中指定每个config文件的路径，例如：\n1 export KUBECONFIG=/home/mazhen/kube-config/config-cluster-1:/home/mazhen/kube-config/config-cluster-1 再使用kubectl config view查看集群配置时，kubectl会自动合并多个config的信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://172.20.51.11:6443 name: cluster-2 - cluster: certificate-authority-data: DATA+OMITTED server: https://172.18.100.90:6443 name: cluster-1 contexts: - context: cluster: cluster-2 user: cluster-2-admin name: cluster-2-admin@cluster-2 - context: cluster: cluster-1 user: cluster-1-admin name: cluster-1-admin@cluster-1 current-context: cluster-1-admin@cluster-1 kind: Config preferences: {} users: - name: cluster-2-admin user: client-certificate-data: REDACTED client-key-data: REDACTED - name: cluster-1-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 可以看到，配置中包含了两个集群，两个用户，以及两个context。我们可以使用kubectl config get-contexts查看配置中所有的context：\n1 2 3 4 5 $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE cluster-2-admin@cluster-2 cluster-2 cluster-2-admin * cluster-1-admin@cluster-1 cluster-1 cluster-1-admin 星号*标识了当前的工作集群。如果想访问另一个集群，使用kubectl config use-context进行切换：\n1 2 3 $ kubectl config use-context cluster-2-admin@cluster-2 Switched to context \u0026#34;cluster-2-admin@cluster-2\u0026#34;. 我们可以再次确认切换的结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * cluster-2-admin@cluster-2 cluster-2 cluster-2-admin cluster-1-admin@cluster-1 cluster-1 cluster-1-admin $ kubectl cluster-info Kubernetes master is running at https://172.20.51.11:6443 KubeDNS is running at https://172.20.51.11:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://172.20.51.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. 看吧，kubectl切换context管理多集群是多么的方便。\n","date":"2019-03-29T09:43:28+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8kubectl%E7%AE%A1%E7%90%86%E5%A4%9A%E9%9B%86%E7%BE%A4/","title":"使用kubectl管理多集群"},{"content":"先使用visudo 查看当前的配置，这个命令编辑的是/etc/sudoers文件。可以直接在这个文件中为用户设置sudo权限：\n1 2 3 # User privilege specification root ALL=(ALL:ALL) ALL adp ALL=(ALL) ALL 也可以看看哪个group有root权限，然后将用户加入这个group。例如下面的配置，admin组有root权限：\n1 2 # Members of the admin group may gain root privileges %admin ALL=(ALL) ALL 可以将用户加入admin组，自然就有了sudo权限：\n1 usermod -a -G admin [user] 如果提示admin不存在，可以先创建这个组，再将用户加入这个group：\n1 2 groupadd admin usermod -a -G admin [user] 如果不想编辑/etc/sudoers，可以在/etc/sudoers.d/目录下，为需要sudo权限的用户创建独立的文件，在文件中分别为用户授权，格式和/etc/sudoers一样：\n1 adp ALL=(ALL) ALL 修改文件权限：\n1 chmod 440 adp 这样做的好处每个用户都有独立的配置文件，是方便管理。\n最后，建议将/sbin 和 /usr/sbin 加入到用户路径。\n1 PATH=$PATH:/usr/sbin:/sbin ","date":"2019-03-28T09:41:19+08:00","permalink":"https://mazhen.tech/p/%E5%A6%82%E4%BD%95%E8%AE%A9%E7%94%A8%E6%88%B7%E6%8B%A5%E6%9C%89sudo%E6%9D%83%E9%99%90/","title":"如何让用户拥有sudo权限"},{"content":"为了方便开发者体验Kubernetes，社区提供了可以在本地部署的Minikube。由于在国内网络环境内，无法顺利的安装使用Minikube，我们可以从阿里云的镜像地址来获取所需Docker镜像和配置。\n安装VirtualBox sudo apt-get install virtualbox\n安装 Minikube 1 curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v0.35.0/minikube-linux-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ 启动Minikube 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ minikube start --registry-mirror=https://registry.docker-cn.com 😄 minikube v0.35.0 on linux (amd64) 🔥 Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... 📶 \u0026#34;minikube\u0026#34; IP address is 192.168.99.100 🐳 Configuring Docker as the container runtime ... ✨ Preparing Kubernetes environment ... 🚜 Pulling images required by Kubernetes v1.13.4 ... 🚀 Launching Kubernetes v1.13.4 using kubeadm ... ⌛ Waiting for pods: apiserver proxy etcd scheduler controller addon-manager dns 🔑 Configuring cluster permissions ... 🤔 Verifying component health ..... 💗 kubectl is now configured to use \u0026#34;minikube\u0026#34; 🏄 Done! Thank you for using minikube! 检查状态 1 2 3 4 5 6 $ minikube status host: Running kubelet: Running apiserver: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100 kubernetes已经成功运行，可以使用kubectl访问集群：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-89cc84847-2k67h 1/1 Running 0 18m coredns-89cc84847-95zsj 1/1 Running 0 18m etcd-minikube 1/1 Running 0 18m kube-addon-manager-minikube 1/1 Running 0 19m kube-apiserver-minikube 1/1 Running 0 18m kube-controller-manager-minikube 1/1 Running 0 18m kube-proxy-f66hz 1/1 Running 0 18m kube-scheduler-minikube 1/1 Running 0 18m kubernetes-dashboard-7d8d567b4d-h82vx 1/1 Running 0 18m storage-provisioner 1/1 Running 0 18m 停止Minikube 1 2 3 4 $ minikube stop ✋ Stopping \u0026#34;minikube\u0026#34; in virtualbox ... 🛑 \u0026#34;minikube\u0026#34; stopped. 删除本地集群 1 2 3 4 $ minikube delete 🔥 Deleting \u0026#34;minikube\u0026#34; from virtualbox ... 💔 The \u0026#34;minikube\u0026#34; cluster has been deleted. ","date":"2019-03-25T09:39:12+08:00","permalink":"https://mazhen.tech/p/%E5%9C%A8ubuntu%E4%B8%8A%E5%AE%89%E8%A3%85minikube/","title":"在Ubuntu上安装Minikube"},{"content":"刚接触Kubernetes时很容易被它繁多的概念（POD，Service，Deployment \u0026hellip;)以及比较复杂的部署架构搞晕，本文希望能通过一个简单的例子，讲解Kubernetes最基本的工作原理。\nKubernetes本质上是为用户提供了一个容器编排工具，可以管理和调度用户提交的作业。用户在 YAML 配置文件中描述应用所需的环境配置、参数等信息，以及应用期待平台提供的服务（负载均衡，水平扩展等），然后将 YAML 提交，Kubernetes会按照用户的要求，在集群上将应用运行起来。在遇到异常情况，或用户的主动调整时，Kubernetes 将始终保持应用实际的运行状态，符合用户的期待状态。\nKubernetes 是由 Master 和 Node 两种节点组成。Master由3个独立的组件组成：\n负责 API 服务的 kube-apiserver 负责容器编排的 kube-controller-manager 负责调度的 kube-scheduler Kubernetes 集群的所有状态信息都存储在 etcd，其他组件对 etcd 的访问，必须通过 kube-apiserver。\nKubelet 运行在所有节点上，它通过容器运行时（例如Docker），让应用真正的在节点上运行起来。\n下面通过一个简单的例子，描述 Kubernetes 的各个组件，是如何协作完成工作的。\n用户将 YAML 提交给 kube-apiserver，YAML 经过校验后转换为 API 对象，存储在 etcd 中。\nkube-controller-manager 是负责编排的组件，当它发现有新提交的应用，会根据配置的要求生成对应的 Pod 对象。Pod 是 Kubernetes 调度管理的最小单元，可以简单的认为，Pod 就是一个虚拟机，其中运行着关系紧密的进程，共同组成用户的应用。例如Web应用进程和日志收集agent，可以包含在一个Pod中。Pod 对象也存储在 etcd 中。本例子中用户定义 replicas 为2，也就是用户期待有两个 Pod 实例。\n其实kube-controller-manager 内部一直在做循环检查，只要发现有应用没有对应的 Pod，或者 Pod 的数量不满足用户的期望，它都会进行适当的调整，创建或删除Pod 对象。\nkube-scheduler 负责 Pod 的调度。kube-scheduler 发现有新的 Pod 出现，它会按照调度算法，为每个 Pod 寻找一个最合适的节点（Node）。kube-scheduler 对一个 Pod 的调度成功，实际上就是在 Pod 对象上记录了调度结果的节点名称。注意，Pod 调度成功，只是在 Pod 上标记了节点的名字，Pod 是否真正在节点上运行，就不是kube-scheduler的责任了。\nKubelet 运行在所有节点上，它会订阅所有 Pod 对象的变化，当发现一个 Pod 与 Node 绑定，也就是这个 Pod 上标记了Node的名字，而这个被绑定的 Node 就是它自己，Kubelet 就会在这个节点将 Pod 启动。\n至此，用户提交的应用在Kubernetes集群中就运行起来了。\n同时，上述的过程一直在循环往复。例如，用户更新了 YAML，将 replicas 改为3，并将更新后的 YAML 再次提交。kube-controller-manager会发现实际运行的 Pod 数量与用户的期望不符，它会生成一个新的 Pod 对象。紧接着 kube-scheduler 发现一个没有绑定节点的 Pod，它会按照调度算法为这个Pod寻找一个最佳节点完成绑定。最后，某个Kubelet 发现新绑定节点的 Pod 应该在本节点上运行，它会通过接口调用Docker完成 Pod 的启动。\n上面就是 Kubernetes 基本工作流程的简单描述，希望对你理解它的工作原理有所帮助。\n","date":"2019-02-24T17:30:11+08:00","permalink":"https://mazhen.tech/p/kubernetes%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/","title":"Kubernetes工作原理概述"},{"content":"Kubernetes在2017年赢得了容器编排之战，使得基于容器+Kubernetes来构建PaaS平台成为了云计算的主流方式。在人们把关注的目光都聚焦在Kubernetes上时，容器技术领域在2018年也发生了很多创新，包括amazon最近开源的轻量级虚拟机管理器 Firecracker，Google在今年5月份开源的基于用户态操作系统内核的 gVisor 容器，还有更早开源的虚拟化容器项目 KataContainers，可谓百花齐放。一般的开发者可能认为容器就等于Docker，没想到容器领域还在发生着这么多创新。我在了解这些项目时，发现如果没有一些背景知识，很难get到它们的创新点。我试着通过这篇文章进行一次背景知识的梳理。让我们先从最基本的问题开始：操作系统是怎么工作的？\n操作系统的工作方式 我们不去讨论操作系统的标准定义，而是想想操作系统的本质是什么，它是怎么为应用提供服务的呢？\n其实操作系统很“懒”，它不是一直运行着主动干活，而是躺在那儿等着被中断“唤醒”，然后根据中断它的事件类型做下一步处理：是不是有键盘敲击、网络数据包到达，还是时间片到了该考虑进程切换，或者是有应用向内核发出了服务请求。内核处理完唤醒它的事件，会将控制权返还给应用程序，然后等着再次被中断“唤醒”。内核就是这样由中断驱动，控制着CPU、内存等硬件资源，为应用程序提供服务。\n”唤醒“操作系统内核的事件主要分为三类：\n中断：来自硬件设备的处理请求； 异常：当前正在执行的进程，由于非法指令或者其他原因导致执行失败而产生的异常事情处理请求，典型的如缺页异常； 系统调用：应用程序主动向操作系统内核发出的服务请求。系统调用的本质其实也是中断，相对于硬件设备的中断，这种中断被称为软中断。 CPU的特权等级 内核代码常驻在内存中，被每个进程映射到它的逻辑地址空间：\n进程逻辑地址空间的最大长度与实际可用的物理内存数量无关，由CPU的字长决定。进程的逻辑地址空间划分为两个部分，分别称为内核空间和用户空间。用户空间是彼此独立的，而逻辑地址空间顶部的内核空间是被所有进程共享。\n从每个进程的角度来看，地址空间中只有自身一个进程，它不会感知到其他进程的存在。由于内核空间被所有进程共享，为了防止进程修改彼此的数据而造成相互干扰，用户进程不能直接操作或读取内核空间中的数据。同时由于内核管理着所有硬件资源，也不能让用户进程直接执行内核空间中的代码。操作系统应该如何做到这种限制呢？\n实际上，操作系统的实现依赖 CPU 提供的功能。现代的CPU体系架构都提供几种特权级别，每个特权级别有各种限制。各级别可以看作是环，内环能够访问更多的功能，外环则较少，被称为protection rings：\nIntel 的 CPU 提供了4种特权级别， Linux 只使用了 Ring0 和 Ring3 两个级别。Ring 0 拥有最多的特权，它可以直接和CPU、内存等物理硬件交互。 Ring 0 被称为内核态，操作系统内核正是运行在Ring 0。Ring 3被称为用户态，应用程序运行在用户态。\n系统调用 在用户态禁止直接访问内核态，也就是说不同通过普通的函数调用方式调用内核代码，而必须使用系统调用陷入（trap）内核，完成从用户态到内核态的切换。内核首先检查进程是否允许执行想要的操作，然后代表进程执行所需的操作，完成后再返回到用户态。\n除了代表用户程序执行代码之外，内核还可以由硬件中断激活，然后在中断上下文中运行。另外除了普通进程，系统中还有内核线程在运行。内核线程不与任何特定的用户空间进程相关联。\nCPU 在任何时间点上的活动必然为下列三者之一 ：\n运行于用户空间，执行应用程序 运行于内核空间，处于进程上下文，即代表某个特定的进程执行 运行于内核空间，处于中断上下文，与任何进程无关，处理某个特定的中断 优化系统调用 从上面的讨论可以看出，由于系统调用会经过用户态到内核态的切换，开销要比普通函数调用大很多，因此在进行系统级编程时，减少用户态与内核态之间的切换是一个很重要的优化方法。\n例如同样是实现 Overlay网络，使用 VXLAN 完全在内核态完成封装和解封装，要比把数据包从内核态通过虚拟设备TUN传入用户态再进行处理要高效很多。\n对应的也可以从另外一个方向进行优化：使用 DPDK 跳过内核网络协议栈，数据从网卡直接到达用户态，在用户态处理数据包，也就是说网络协议栈完全运行在用户态，同样避免了用户态和内核态的切换。像腾讯开源的 F-Stack就是一个基于DPDK运行在用户空间的TCP/IP协议栈。\n了解了操作系统内核的基本工作方式，我们再看下一个话题：虚拟化。\n虚拟化技术 为了更高效灵活的使用硬件资源，同时能够实现服务间的安全隔离，我们需要虚拟化技术。运行虚拟化软件（Hypervisor或者叫VMM，virtual machine monitor）的物理设施我们称之为Host，安装在Hypervisor之上的虚拟机称为Guest。\n根据Hypervisor在系统中的位置，可以将它归类为type-1或type-2型。如果hypervisor直接运行在硬件之上，它通常被认为是Type-1型。如果hypervisor作为一个单独的层运行在操作系统之上，它将被认为是Type 2型。\nType-1型Hypervisor的概念图：\nType-2型Hypervisor的概念图：\nKVM \u0026amp; QEMU 实际上Type-1和Type-2并没有严格的区分，像最常见的虚拟化软件 KVM（Kernel-based Virtual Machine）是一个Linux内核模块，加载KVM后Linux内核就转换成了Type-1 hypervisor。同时，Linux还是一个通用的操作系统，也可以认为KVM是运行在Linux之上的Type-2 hypervisor。\n为了在Host上创建出虚拟机，仅仅有KVM是不够的。对于 I/O 的仿真，KVM 还需要 QEMU的配合。QEMU 是一个运行在用户空间程序，它可以仿真处理器和一系列的物理设备：磁盘、网络、VGA、PCI、USB、串口/并口等等，基于QEMU可以构造出一个完整的虚拟PC。\n值得注意的是，QEMU 有两种运行模式：仿真模式和虚拟化模式。在仿真模式下，QEMU可以在一个Intel的Host上运行ARM或MIPS虚拟机。这是怎么做到的呢？实际上，QEMU 通过 TCG（Tiny Code Generator）技术进行了二进制代码转换，可以认为这是一种高级语言的VM，就像JVM。例如可以将运行在ARM上的二进制转换为一种中间字节码，然后让它运行在Host的Intel CPU上。很明显，这种二进制代码转换有着巨大的性能开销。\n相对应的，QEMU的另一种是虚拟化模式，它借助KVM完成处理器的虚拟化。由于和CPU的体系结构紧密关联，虚拟化模式能够带来更好的性能，限制是Guest必须使用和Host一样的CPU体系机构。这就是我们最常用到的虚拟化技术栈：KVM/QEMU\nKVM 和 QEMU 有两种交互方式：通过设备文件/dev/kvm 和通过内存映射页面。QEMU 和 KVM之间的大块数据传递会使用内存映射页面。/dev/kvm是KVM暴露的主要API，它支持一系列ioctl接口，QEMU 使用这些接口和KVM交互。/dev/kvm API分为三个层次：\nSystem Level: 用于KVM全局状态的维护，例如创建 VM； VM Level: 用于处理和特定VM相关工作的 API，vCPU 就是通过这个级别的API创建出来的； vCPU Level: 这是最细粒度的API，用于和特定vCPU的交互。QEMU会为每个vCPU分配一个专门的线程。 CPU 虚拟化技术 VT-x KVM和QEMU配合完美，但和CPU的特权级别在一起就遇到了麻烦。我们知道，hypervisor需要管理宿主机的CPU、内存、I/O设备等资源，因此它需要运行在ring 0级别才能执行这些高特权操作。然而运行在VM中的操作系统希望得到访问所有资源的权限，它并不知道自己运行在虚拟机中。因为同一时间只有一个内核可以运行在ring 0，Guest OS不得不被“挤”到了ring 1，这一级别不能满足内核的需求。怎么解决？\n虽然我们可以使用软件的方式进行模拟，让hypervisor拦截应用发往ring 0的系统调用，再转发给Guest OS，但这么做会产生额外的性能损耗，而且方案复杂难以维护。Intel和AMD认识到了虚拟化的重要性，各自独立创建了X86架构的扩展指令集，分别称为 VT-x and AMD-V，从CPU层面支持虚拟化。\n以Intel CPU为例，VT-x不仅增加了虚拟化相关的指令集，还将CPU的指令划分会两种模式：root 和 non-root。hypervisor运行在 root 模式，而VM运行在non-root模式。指令在non-root模式的运行速度和root模式几乎一样，除了不能执行一些涉及CPU全局状态切换的指令。\nVMX（Virtual Machine Extensions）是增加到VT-x中的指令集，主要有四个指令：\nVMXON：在这个指令执行之前，CPU还没有root 和 non-root的概念。VMXON执行后，CPU进入虚拟化模式。 VMXOFF：VMXON的相反操作，执行VMXOFF退出虚拟化模式。 VMLAUNCH：创建一个VM实例，然后进入non-root模式。 VMRESUME：进入non-root模式，恢复前面退出的VM实例。当VM试图执行一个在non-root禁止的指令，CPU立即切换到root模式，类似前面介绍的系统调用trap方式，这就是VM的退出。 这样，VT-x/KVM/QEMU 构成了今天应用最广泛的虚拟化技术栈。\n有了上面的铺垫，终于要谈到容器技术了。\n容器的本质 虽然虚拟化技术在灵活高效的使用硬件资源方面前进了一大步，但人们还觉得远远不够。特别是在机器使用量巨大的互联网公司。因为虚拟机一旦创建，为它分配的资源就相对固定，缺乏弹性，很难再提高机器的利用率。而且创建、销毁虚拟机也是相对“重”的操作。这时候容器技术出现了。我们知道，容器依赖的底层技术，Linux Namesapce和Cgroups都是最早由Google开发，提交进Linux内核的。\n容器的本质就是一个进程，只不过对它进行了Linux Namesapce隔离，让它“看”不到外面的世界，用Cgroups限制了它能使用的资源，同时利用系统调用pivot_root或chroot切换了进程的根目录，把容器镜像挂载为根文件系统rootfs。rootfs中不仅有要运行的应用程序，还包含了应用的所有依赖库，以及操作系统的目录和文件。rootfs打包了应用运行的完整环境，这样就保证了在开发、测试、线上等多个场景的一致性。\n从上图可以看出，容器和虚拟机的最大区别就是，每个虚拟机都有独立的操作系统内核Guest OS，而容器只是一种特殊的进程，它们共享同一个操作系统内核。\n看清了容器的本质，很多问题就容易理解。例如我们执行 docker exec 命令能够进入运行中的容器，好像登录进独立的虚拟机一样。实际上这只不过是利用系统调用setns，让当前进程进入到容器进程的Namesapce中，它就能“看到”容器内部的情况了。\n由于容器就是进程，它的创建、销毁非常轻量，对资源的使用控制更加灵活，因此让Kubernetes这种容器编排和资源调度工具可以大显身手，通过合理的搭配，极大的提高了整个集群的资源利用率。\n虚拟化容器技术 前面提到，运行在一个宿主机上的所有容器共享同一个操作系统内核，这种隔离级别存在着很大的潜在安全风险。因此在公有云的多租户场景下，还是需要先用虚拟机进行租户强隔离，然后用户在虚拟机上再使用容器+Kubernetes部署应用。\n然而在Serverless的场景下，传统的先建虚拟机再创建容器的方式，在灵活性、执行效率方面难以满足需求。随着Serverless、FaaS（Function-as-a-Service）的兴起，各公有云厂商都将安全性容器作为了创新焦点。\n一个很自然能想到的方案，是结合虚拟机的强隔离安全性+容器的轻量灵活性，这就是虚拟化容器项目 KataContainers。\nOpenStack在2017年底发布的 KataContainers 项目，最初是由 Intel ClearContainer 和 Hyper runV 两个项目合并而产生的。在Kubernetes场景下，一个Pod对应于Kata Containers启动的一个轻量化虚拟机，Pod中的容器，就是运行在这个轻量级虚拟机里的进程。每个Pod都运行在独立的操作系统内核上，从而达到安全隔离的目的。\n可以看出，KataContainers 依赖 KVM/QEMU技术栈。\namazon最近开源的Firecracker也是为了实现在 functions-based services 场景下，多租户安全隔离的容器。\nFirecracker同样依赖KVM，然后它没有用到QEMU，因为Firecracker本身就是QEMU的替代实现。Firecracker是一个比QEMU更轻量级的VMM，它只仿真了4个设备：virtio-net，virtio-block，serial console和一个按钮的键盘，仅仅用来停止microVM。理论上，KataContainers可以用Firecracker换掉它现在使用的QEMU，从而将 Firecracker整合进Kubernetes生态圈。\n其实Google早就没有使用QEMU，而且对KVM进行了深度定制。我们可以从这篇介绍看出端倪：7 ways we harden our KVM hypervisor at Google Cloud: security in plaintext\nNon-QEMU implementation: Google does not use QEMU, the user-space virtual machine monitor and hardware emulation. Instead, we wrote our own user-space virtual machine monitor that has the following security advantages over QEMU\n\u0026hellip;\ngVisor Google 开源的gVisor为了实现安全容器另辟蹊径，它用 Go 实现了一个运行在用户态的操作系统内核，作为容器运行的Guest Kernel，每个容器都依赖独立的操作系统内核，实现了容器间安全隔离的目的。\n虽然 gVisor 今年才开源，但它已经在Google App Engine 和 Google Cloud Functions运行了多年。\ngVisor作为运行应用的安全沙箱，扮演着Virtual kernel的角色。同时gVisor 包含了一个兼容Open Container Initiative (OCI) 的运行时runsc，因此可以用它替换掉 Docker 的 runc，整合进Kubernetes生态圈，为Kubernetes带来另一种安全容器的实现方案。\nKata Containers和gVisor的本质都是为容器提供一个独立的操作系统内核，避免容器共享宿主机的内核而产生安全风险。KataContainers使用了传统的虚拟化技术，gVisor则自己实现了一个运行在用户态、极小的内核。gVisor比KataContainers更加轻量级，根据这个分享的介绍，gVisor目前只实现了211个Linux系统调用，启动时间150ms，内存占用15MB。\ngVisor实现原理，简单来说是模拟内核的行为，使用某种方式拦截应用发起的系统调用，经过gVisor的安全控制，代替容器进程向宿主机发起可控的系统调用。目前gVisor实现了两种拦截方式：\n基于Ptrace 机制的拦截 使用 KVM 来进行系统调用拦截。 因为gVisor基于拦截系统调用的实现原理，它并不适合系统调用密集的应用。\n最后，对于像我这样没有读过Linux内核代码的后端程序员，gVisor是一个很好的窥探内核内部实现的窗口，又激起了我研究内核的兴趣。Twitter上看到有人和我有类似的看法：\n希望下次能分享gVisor深入研究系列。保持好奇心，Stay hungry. Stay foolish.\n","date":"2018-12-16T22:03:21+08:00","permalink":"https://mazhen.tech/p/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E6%BC%AB%E8%B0%88/","title":"容器技术创新漫谈"},{"content":"容器的跨主机通信主要有两种方式：封包模式和路由模式。上一篇文章演示了使用VXLAN协议的封包模式，这篇将介绍另一种方式，利用三层网络的路由转发实现容器的跨主机通信。\n路由模式概述 宿主机将它负责的容器IP网段，以某种方式告诉其他节点，然后每个节点根据收到的\u0026lt;宿主机-容器IP网段\u0026gt;映射关系，配置本机路由表。\n这样对于容器间跨节点的IP包，就可以根据本机路由表获得到达目的容器的网关地址，即目的容器所在的宿主机地址。接着在把IP包封装成二层网络数据帧时，将目的MAC地址设置为网关的MAC地址，IP包就可以通过二层网络送达目的容器所在的宿主机。\n至于用什么方式将\u0026lt;宿主机-容器IP网段\u0026gt;映射关系发布出去，不同的项目采用了不同的实现方案。\nFlannel是将这些信息集中存储在etcd中，每个节点从etcd自动获取数据，更新宿主机路由表。\nCalico则使用BGP（Border Gateway Protocol）协议交换共享路由信息，每个宿主机都是运行在它之上的容器的边界网关。\n如果宿主机之间跨了网段怎么办？宿主机之间的二层网络不通，虽然知道目的容器所在的宿主机，但没办法将目的MAC地址设置为那台宿主机的MAC地址。\nCalico有两种解决方案：\nIPIP 模式，在跨网段的宿主机之间建立“隧道” 让宿主机之间的路由器“学习”到容器路由规则，每个路由器都知道某个容器IP网段是哪个宿主机负责的，容器间的IP包就能正常路由了。 动手实验 路由模式的实验比较简单，关键在于宿主机上路由规则的配置。为了简化实验，这些路由规则都是我们手工配置，而且两个节点之间二层网络互通，没有跨网段。\n参照Docker跨主机Overlay网络动手实验，创建“容器”，veth pairs，bridge，设置IP，激活虚拟设备。\n然后在node-1上增加路由规则：\n1 sudo ip route add 172.18.20.0/24 via 192.168.31.192 在node-2上增加路由规则：\n1 sudo ip route add 172.18.10.0/24 via 192.168.31.183 当docker1访问docker2时，IP包会从veth到达br0，然后根据node-1上刚设置的路由规则，访问172.18.20.0/24网段的网关地址为node-2，这样，IP包就能路由到node-2了。\n同时，node-2的路由表中包含这样的一条规则：\n1 2 3 4 $ ip route ... 172.18.20.0/24 dev br0 proto kernel scope link src 172.18.20.1 到达node-2的IP包，会根据这条规则路由到网桥br0，最终到达docker-2。反过来从docker2访问docker1的过程也是类似。\n总结 两种容器跨主机的通信方案我们都实验了一下，现在做个简单总结对比：\n封包模式对基础设施要求低，三层网络通就可以了。但封包、解包带来的性能损耗较大。 路由模式性能好，但要求二层网络连通，或者在跨网段的情况下，要求路由器能配合“学习”路由规则。 至此，容器网络的三篇系列完成：\nDocker单机网络模型动手实验 Docker跨主机Overlay网络动手实验 Docker跨主机通信路由模式动手实验（本篇） ","date":"2018-11-11T22:00:22+08:00","permalink":"https://mazhen.tech/p/docker%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E4%BF%A1%E8%B7%AF%E7%94%B1%E6%A8%A1%E5%BC%8F%E5%8A%A8%E6%89%8B%E5%AE%9E%E9%AA%8C/","title":"Docker跨主机通信路由模式动手实验"},{"content":"上一篇文章我演示了docker bridge网络模型的实验，这次我将展示如何利用Overlay 网络实现跨主机容器的通信。\n两个容器docker1和docker2分别位于节点Node-1和Node-2，如何实现容器的跨主机通信呢？一般来说有两种实现方式：\n封包模式：利用Overlay网络协议在节点间建立“隧道”，容器之间的网络包被封装在外层的网络协议包中进行传输。 路由模式：容器间的网络包传输全部用三层网络的路由转发来实现。 本文主要介绍封包模式。Overlay网络主要有两种方式，一种是使用UDP在用户态封装，一种是利用VXLAN 在内核态封装。由于减少了用户态到内核态的切换，封包解包逻辑都在内核态进行，VXLAN 的性能更好，成为了容器网络的主流方案。\n关于路由模式，会在下一篇文章介绍。\nVXLAN VXLAN（Virtual Extensible LAN）是一种网络虚拟化技术，它将链路层的以太网包封装到UDP包中进行传输。VXLAN最初是由VMware、Cisco开发，主要解决云环境下多租户的二层网络隔离。我们常听到公有云厂商宣称支持VPC（virtual private cloud），实际底层就是使用VXLAN实现的。\nVXLAN packet的结构：\n我们可以看到，最内部是原始的二层网络包，外面加上一个VXLAN header，其中最重要的是VNI（VXLAN network identifier)字段，它用来唯一标识一个VXLAN。也就是说，使用不同的VNI来区分不同的虚拟二层网络。VNI有24位，基本够公用云厂商使用了。要知道原先用来网络隔离的虚拟局域网VLAN只支持4096个虚拟网络。\n在 VXLAN header外面封装了正常的UDP包。VXLAN在UDP之上，实现了一个虚拟的二层网络，连接在这个虚拟二层网络上的主机，就像连接在普通的局域网上一样，可以互相通信。\n介绍完背景知识，我们可以开始动手实验了。\n实现方案一 参照Flannel的实现方案：\n配置内核参数，允许IP forwarding 分别在Node-1、Node-2上执行：\n1 sudo sysctl net.ipv4.conf.all.forwarding=1 创建“容器” 在Node-1上执行：\n1 sudo ip netns add docker1 在Node-2上执行：\n1 sudo ip netns add docker2 为什么创建个Namesapce就说是“容器”？请参考上一篇文章。\n创建Veth pairs 分别在Node-1、Node-2上执行：\n1 sudo ip link add veth0 type veth peer name veth1 将Veth的一端放入“容器” 在Node-1上执行：\n1 sudo ip link set veth0 netns docker1 在Node-2上执行：\n1 sudo ip link set veth0 netns docker2 创建bridge 分别在Node-1、Node-2上创建bridge br0：\n1 sudo brctl addbr br0 将Veth的另一端接入bridge 分别在Node-1、Node-2上执行：\n1 sudo brctl addif br0 veth1 为\u0026quot;容器“内的网卡分配IP地址，并激活上线 在Node-1上执行：\n1 2 sudo ip netns exec docker1 ip addr add 172.18.10.2/24 dev veth0 sudo ip netns exec docker1 ip link set veth0 up 在Node-2上执行：\n1 2 sudo ip netns exec docker2 ip addr add 172.18.20.2/24 dev veth0 sudo ip netns exec docker2 ip link set veth0 up Veth另一端的网卡激活上线 分别在Node-1、Node-2上执行：\n1 sudo ip link set veth1 up 为bridge分配IP地址，激活上线 在Node-1上执行：\n1 2 sudo ip addr add 172.18.10.1/24 dev br0 sudo ip link set br0 up 在Node-2上执行：\n1 2 sudo ip addr add 172.18.20.1/24 dev br0 sudo ip link set br0 up 将bridge设置为“容器”的缺省网关 在Node-1上执行：\n1 sudo ip netns exec docker1 route add default gw 172.18.10.1 veth0 在Node-2上执行：\n1 sudo ip netns exec docker2 route add default gw 172.18.20.1 veth0 创建VXLAN虚拟网卡 VXLAN需要在宿主机上创建一个虚拟网络设备对 VXLAN 的包进行封装和解封装，实现这个功能的设备称为 VTEP（VXLAN Tunnel Endpoint）。宿主机之间通过VTEP建立“隧道”，在其中传输虚拟二层网络包。\n在Node-1创建vxlan100：\n1 2 3 4 5 6 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.183 \\ dev enp0s3 \\ dstport 4789 \\ nolearning 为vxlan100分配IP地址，然后激活：\n1 2 sudo ip addr add 172.18.10.0/32 dev vxlan100 sudo ip link set vxlan100 up 为了让Node-1上访问172.18.20.0/24网段的数据包能进入“隧道”，我们需要增加如下的路由规则：\n1 sudo ip route add 172.18.20.0/24 dev vxlan100 在Node-2上执行相应的命令：\n1 2 3 4 5 6 7 8 9 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.192 \\ dev enp0s3 \\ dstport 4789 \\ nolearning sudo ip addr add 172.18.20.0/32 dev vxlan100 sudo ip link set vxlan100 up sudo ip route add 172.18.10.0/24 dev vxlan100 scope global 手动更新ARP和FDB 虚拟设备vxlan100会用ARP和FDB (forwarding database) 数据库中记录的信息，填充网络协议包，建立节点间转发虚拟网络数据包的“隧道”。\n我们知道，在二层网络上传输IP包，需要先根据目的IP地址查询到目的MAC地址，这就是ARP（Address Resolution Protocol）协议的作用。我们应该可以通过ARP查询到其他节点上容器IP地址对应的MAC地址，然后填充在VXLAN内层的网络包中。\nFDB是记录网桥设备转发数据包的规则。虚拟网络数据包根据上面定义的路由规则，从br0进入了本机的vxlan100“隧道”入口，应该可以在FDB中查询到“隧道”出口的MAC地址应该如何到达，这样，两个VTEP就能完成”隧道“的建立。\nvxlan为了建立节点间的“隧道”，需要一种机制，能让一个节点的加入、退出信息通知到其他节点，可以采用multicast的方式进行节点的自动发现，也有很多Unicast的方案，这篇文章\u0026lt;VXLAN \u0026amp; Linux\u0026gt;有很详细的介绍。总之就是要找到一种方式，能够更新每个节点的ARP和FDB数据库。\n如果是使用Flannel，它在节点启动的时候会采用某种机制自动更新其他节点的ARP和FDB数据库。现在我们的实验只能在两个节点上手动更新ARP和FDB。\n首先在两个节点上查询到设备vxlan100的MAC地址，例如在我当前的环境：\nNode-1上vxlan100的MAC地址是3a:8d:b8:69:10:3e Node-2上vxlan100的MAC地址是0e:e6:e6:5d:c2:da\n然后在Node-1上增加ARP和FDB的记录：\n1 2 sudo ip neighbor add 172.18.20.2 lladdr 0e:e6:e6:5d:c2:da dev vxlan100 sudo bridge fdb append 0e:e6:e6:5d:c2:da dev vxlan100 dst 192.168.31.192 我们可以确认下执行结果：\nARP中已经记录了Node-2上容器IP对应的MAC地址。再看看FDB的情况：\n根据最后一条新增规则，我们可以知道如何到达Node-2上“隧道”的出口vxlan100。“隧道”两端是使用UDP进行传输，即容器间通讯的二层网络包是靠UDP在宿主机之间通信。\n类似的，在Node-2上执行下面的命令：\n1 2 sudo ip neighbor add 172.18.10.2 lladdr 3a:8d:b8:69:10:3e dev vxlan100 sudo bridge fdb append 3a:8d:b8:69:10:3e dev vxlan100 dst 192.168.31.183 测试容器的跨节点通信 现在，容器docker1和docker1之间就可以相互访问了。\n我们从docker1访问docker2，在Node-1上执行：\n1 sudo ip netns exec docker1 ping -c 3 172.18.20.2 同样可以从docker2访问docker1，在Node-2上执行：\n1 sudo ip netns exec docker2 ping -c 3 172.18.10.2 在测试过程中如果需要troubleshooting，可以使用tcpdump在veth1、br0、vxlan100等虚拟设备上抓包，确认网络包是按照预定路线在转发：\n1 sudo tcpdump -i vxlan100 -n 测试环境恢复 在两个节点上删除我们创建的虚拟设备：\n1 2 3 4 sudo ip link set br0 down sudo brctl delbr br0 sudo ip link del veth1 sudo ip link del vxlan100 实现方案二 Docker原生的overlay driver底层也是使用VXLAN技术，但实现方案和Flannel略有不同：\n我们可以看到，vxlan100被“插”在了虚拟交换机br0上，虚拟网络数据包从br0到vxlan100不是通过本机路由，而是vxlan100根据FDB直接进行了转发。\n执行的命令略有差异，我不再赘述过程，直接提供了命令，大家自己实验吧：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 # 在Node-1上执行 sudo sysctl net.ipv4.conf.all.forwarding=1 sudo ip netns add docker1 sudo ip link add veth0 type veth peer name veth1 sudo ip link set veth0 netns docker1 sudo brctl addbr br0 sudo brctl addif br0 veth1 sudo ip netns exec docker1 ip addr add 172.18.10.2/24 dev veth0 sudo ip netns exec docker1 ip link set veth0 up sudo ip link set veth1 up sudo ip link set br0 up sudo ip netns exec docker1 route add default veth0 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.183 \\ dev enp0s5 \\ dstport 4789 \\ nolearning \\ proxy sudo ip link set vxlan100 up sudo brctl addif br0 vxlan100 sudo ip neigh add 172.18.20.2 lladdr [docker2的MAC地址] dev vxlan100 sudo bridge fdb append [docker2的MAC地址] dev vxlan100 dst 192.168.31.192 # 在Node-2上执行 sudo sysctl net.ipv4.conf.all.forwarding=1 sudo ip netns add docker2 sudo ip link add veth0 type veth peer name veth1 sudo ip link set veth0 netns docker2 sudo brctl addbr br0 sudo brctl addif br0 veth1 sudo ip netns exec docker2 ip addr add 172.18.20.2/24 dev veth0 sudo ip netns exec docker2 ip link set veth0 up sudo ip link set veth1 up sudo ip link set br0 up sudo ip netns exec docker2 route add default veth0 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.192 \\ dev enp0s5 \\ dstport 4789 \\ nolearning \\ proxy sudo ip link set vxlan100 up sudo brctl addif br0 vxlan100 sudo ip neigh add 172.18.10.2 lladdr [docker1的MAC地址] dev vxlan100 sudo bridge fdb append [docker1的MAC地址] dev vxlan100 dst 192.168.31.183 相信通过亲自动手实验，容器网络对你来说不再神秘。希望本文对你理解容器网络有所帮助。\n下一篇我将动手实验容器跨主机通信的路由模式。\n","date":"2018-11-07T21:52:36+08:00","permalink":"https://mazhen.tech/p/docker%E8%B7%A8%E4%B8%BB%E6%9C%BAoverlay%E7%BD%91%E7%BB%9C%E5%8A%A8%E6%89%8B%E5%AE%9E%E9%AA%8C/","title":"Docker跨主机Overlay网络动手实验"},{"content":"容器的本质 容器的本质就是一个进程，只不过对它进行了Linux Namesapce隔离，让它看不到外面的世界，用Cgroups限制了它能使用的资源，同时利用系统调用pivot_root或chroot切换了进程的根目录，把容器镜像挂载为根文件系统rootfs。rootfs中不仅有要运行的应用程序，还包含了应用的所有依赖库，以及操作系统的目录和文件。rootfs打包了应用运行的完整环境，这样就保证了在开发、测试、线上等多个场景的一致性。\n从上图可以看出，容器和虚拟机的最大区别就是，每个虚拟机都有独立的操作系统内核Guest OS，而容器只是一种特殊的进程，它们共享同一个操作系统内核。\n看清了容器的本质，很多问题就容易理解。例如我们执行 docker exec 命令能够进入运行中的容器，好像登录进独立的虚拟机一样。实际上这只不过是利用系统调用setns，让当前进程进入到容器进程的Namesapce中，它就能“看到”容器内部的情况了。\n关于容器涉及的基础技术，左耳朵耗子多年前写的系列文章仍然很有参考价值：\nDOCKER基础技术：LINUX NAMESPACE（上） DOCKER基础技术：LINUX NAMESPACE（下） DOCKER基础技术：LINUX CGROUP DOCKER基础技术：AUFS DOCKER基础技术：DEVICEMAPPER 容器网络 如何让容器之间互相连接保持网络通畅，Docker有多种网络模型。对于单机上运行的多个容器，可以使用缺省的bridge网络驱动。而容器的跨主机通信，一种常用的方式是利用Overlay 网络，基于物理网络的虚拟化网络来实现。\n本文会在单机上实验展示bridge网络模型，揭示其背后的实现原理。下一篇文章会演示容器如何利用Overlay 网络进行跨主机通信。\n我们按照下图创建网络拓扑，让容器之间网络互通，从容器内部可以访问外部资源，同时，容器内可以暴露服务让外部访问。\n在开始动手实验之前，先简单介绍一下bridge网络模型会用到的Linux虚拟化网络技术。\nVeth Pairs Veth是成对出现的两张虚拟网卡，从一端发送的数据包，总会在另一端接收到。利用Veth的特性，我们可以将一端的虚拟网卡\u0026quot;放入\u0026quot;容器内，另一端接入虚拟交换机。这样，接入同一个虚拟交换机的容器之间就实现了网络互通。\nLinux Bridge 交换机是工作在数据链路层的网络设备，它转发的是二层网络包。最简单的转发策略是将到达交换机输入端口的报文，广播到所有的输出端口。当然更好的策略是在转发过程中进行学习，记录交换机端口和MAC地址的映射关系，这样在下次转发时就能够根据报文中的MAC地址，发送到对应的输出端口。\n我们可以认为Linux bridge就是虚拟交换机，连接在同一个bridge上的容器组成局域网，不同的bridge之间网络是隔离的。 docker network create [NETWORK NAME]实际上就是创建出虚拟交换机。\niptables 容器需要能够访问外部世界，同时也可以暴露服务让外界访问，这时就要用到iptables。另外，不同bridge之间的隔离也会用到iptables。\n我们说的iptables包含了用户态的配置工具(/sbin/iptables)和内核netfilter模块，通过使用iptables命令对内核的netfilter模块做规则配置。\nnetfilter允许在网络数据包处理流程的各个阶段插入hook函数，可以根据预先定义的规则对数据包进行修改、过滤或传送。\n从上图可以看出，网络包的处理流程有五个关键节点：\nPREROUTING：数据包进入路由表之前 INPUT：通过路由表后目的地为本机 FORWARDING：通过路由表后，目的地不为本机 OUTPUT：由本机产生，向外转发 POSTROUTIONG：发送到网卡接口之前 iptables 提供了四种内置的表 raw → mangle → nat → filter，优先级从高到低：\nraw 用于配置数据包，raw中的数据包不会被系统跟踪。不常用。 mangle 用于对特定数据包的修改。不常用。 nat: 用于网络地址转换（NAT）功能（端口映射，地址映射等）。 filter：一般的过滤功能，默认表。 每个表可以设置在多个指定的节点，例如filter表可以设置在INPUT、FORWARDING、OUTPUT等节点。同一个节点中的多个表串联成链。\niptables 是按照表的维度来管理规则，表中包含多个链，链中包含规则列表。例如我们使用sudo iptables -t filter -L 查看filter表：\n可以看到，filter表中包含三个链，每个链中定义了多条规则。由于filter是缺省表，上面的命令可以简化为：sudo iptables -L，即不通过-t指定表时，操作的就是filter表。\n在容器化网络场景，我们经常用到的是在nat表中设置SNAT和DNAT。源地址转换是发生在数据包离开机器被发送之前，因此SNAT只能设置在POSTROUTIONG阶段。DNAT是对目标地址的转换，需要在路由选择前完成，因此可以设置在PREROUTING和OUTPUT阶段。\n动手实验 有了前面的背景知识，我们就可以开始动手实验了。因为涉及到很多系统级设置，建议在一个“干净”的虚拟机内折腾，以免干扰到工作环境。我使用的实验环境是Ubuntu 18.04.1 LTS，不需要安装docker，我们使用系统命令模拟出容器网络环境。\n场景一：容器间的网络互通 创建“容器” 从前面的背景知识了解到，容器的本质是 Namespace + Cgroups + rootfs。因此本实验我们可以仅仅创建出Namespace网络隔离环境来模拟容器行为：\n1 2 sudo ip netns add docker0 sudo ip netns add docker1 查看创建出的网络Namesapce：\n1 2 3 $ ls -l /var/run/netns -r--r--r-- 1 root root 0 Nov 11 03:52 docker0 -r--r--r-- 1 root root 0 Nov 11 03:52 docker1 创建Veth pairs 1 2 sudo ip link add veth0 type veth peer name veth1 sudo ip link add veth2 type veth peer name veth3 查看创建出的Veth pairs：\n1 2 3 4 5 6 7 8 9 10 $ip addr show ... 3: veth1@veth0: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 3e:fe:2b:90:3e:b7 brd ff:ff:ff:ff:ff:ff 4: veth0@veth1: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 6a:a3:02:07:f4:92 brd ff:ff:ff:ff:ff:ff 5: veth3@veth2: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 76:14:e5:0e:26:98 brd ff:ff:ff:ff:ff:ff 6: veth2@veth3: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 6a:0a:84:0f:a7:f7 brd ff:ff:ff:ff:ff:ff 将Veth的一端放入“容器” 设置Veth一端的虚拟网卡的Namespace，相当于将这张网卡放入“容器”内：\n1 2 sudo ip link set veth0 netns docker0 sudo ip link set veth2 netns docker1 查看“容器” docker0 内的网卡：\n1 2 3 4 5 $ sudo ip netns exec docker0 ip addr show 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 4: veth0@if3: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 6a:a3:02:07:f4:92 brd ff:ff:ff:ff:ff:ff link-netnsid 0 ip netns exec docker0 ...的意思是在网络Namesapce docker0的限制下执行后面跟着的命令，相当于在“容器”内执行命令。\n可以看到，veth0已经放入了“容器”docker0内。同样使用命令sudo ip netns exec docker1 ip addr show查看“容器”docker1内的网卡。\n同时，在宿主机上查看网卡ip addr，发现veth0和veth2已经消失，确实是放入“容器”内了。\n创建bridge 安装bridge管理工具brctl\n1 sudo apt-get install bridge-utils 创建bridge br0：\n1 sudo brctl addbr br0 将Veth的另一端接入bridge 1 2 sudo brctl addif br0 veth1 sudo brctl addif br0 veth3 查看接入效果：\n1 sudo brctl show 两个网卡veth1和veth3已经“插”在bridge上。\n为\u0026quot;容器“内的网卡分配IP地址，并激活上线 docker0容器：\n1 2 sudo ip netns exec docker0 ip addr add 172.18.0.2/24 dev veth0 sudo ip netns exec docker0 ip link set veth0 up docker1容器：\n1 2 sudo ip netns exec docker1 ip addr add 172.18.0.3/24 dev veth2 sudo ip netns exec docker1 ip link set veth2 up Veth另一端的网卡激活上线 1 2 sudo ip link set veth1 up sudo ip link set veth3 up 为bridge分配IP地址，激活上线 1 2 sudo ip addr add 172.18.0.1/24 dev br0 sudo ip link set br0 up “容器”间的互通测试 我们可以先设置监听br0：\n1 sudo tcpdump -i br0 -n 从容器docker0 ping 容器docker1：\n1 sudo ip netns exec docker0 ping -c 3 172.18.0.3 br0上监控到的网络流量：\n1 2 3 4 5 6 7 8 9 10 05:53:10.859956 ARP, Request who-has 172.18.0.3 tell 172.18.0.2, length 28 05:53:10.859973 ARP, Reply 172.18.0.3 is-at 06:f4:01:c2:dd:6e, length 28 05:53:10.860030 IP 172.18.0.2 \u0026gt; 172.18.0.3: ICMP echo request, id 1310, seq 1, length 64 05:53:10.860050 IP 172.18.0.3 \u0026gt; 172.18.0.2: ICMP echo reply, id 1310, seq 1, length 64 05:53:11.878348 IP 172.18.0.2 \u0026gt; 172.18.0.3: ICMP echo request, id 1310, seq 2, length 64 05:53:11.878365 IP 172.18.0.3 \u0026gt; 172.18.0.2: ICMP echo reply, id 1310, seq 2, length 64 05:53:12.901334 IP 172.18.0.2 \u0026gt; 172.18.0.3: ICMP echo request, id 1310, seq 3, length 64 05:53:12.901350 IP 172.18.0.3 \u0026gt; 172.18.0.2: ICMP echo reply, id 1310, seq 3, length 64 05:53:16.006471 ARP, Request who-has 172.18.0.2 tell 172.18.0.3, length 28 05:53:16.006498 ARP, Reply 172.18.0.2 is-at c2:23:fe:ac:f5:4e, length 28 可以看到，先是172.18.0.2发起的ARP请求，询问172.18.0.3的MAC地址，然后是ICMP的请求和响应，最后是172.18.0.3的ARP请求。因为接在同一个bridge br0上，所以是二层互通的局域网。\n同样，从容器docker1 ping 容器docker0也是通的：\n1 sudo ip netns exec docker1 ping -c 3 172.18.0.2 场景二：从宿主机访问“容器”内网络 在“容器”docker0内启动服务，监听80端口：\n1 sudo ip netns exec docker0 nc -lp 80 在宿主机上执行telnet，可以连接到docker0的80端口：\n1 telnet 172.18.0.2 80 场景三：从“容器”内访问外网 配置内核参数，允许IP forwarding 1 sudo sysctl net.ipv4.conf.all.forwarding=1 配置iptables FORWARD规则 首先确认iptables FORWARD的缺省策略：\n1 sudo iptables -L 如果缺省策略是DROP，需要设置为ACCEPT：\n1 sudo iptables -P FORWARD ACCEPT 缺省策略的含义是，在数据包没有匹配到规则时执行的缺省动作。\n将bridge设置为“容器”的缺省网关 1 2 sudo ip netns exec docker0 route add default gw 172.18.0.1 veth0 sudo ip netns exec docker1 route add default gw 172.18.0.1 veth2 查看“容器”内的路由表：\n1 2 3 4 5 6 $sudo ip netns exec docker0 route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.18.0.1 0.0.0.0 UG 0 0 0 veth0 172.18.0.0 0.0.0.0 255.255.255.0 U 0 0 0 veth0 可以看出，“容器”内的缺省Gateway是bridge的IP地址，非172.18.0.0/24网段的数据包会路由给bridge。\n配置iptables的SNAT规则 容器的IP地址外部并不认识，如果它要访问外网，需要在数据包离开前将源地址替换为宿主机的IP，这样外部主机才能用宿主机的IP作为目的地址发回响应。\n另外一个需要注意的问题，内核netfilter会追踪记录连接，我们在增加了SNAT规则时，系统会自动增加一个隐式的反向规则，这样返回的包会自动将宿主机的IP替换为容器IP。\n1 sudo iptables -t nat -A POSTROUTING -s 172.18.0.0/24 ! -o br0 -j MASQUERADE 上面的命令的含义是：在nat表的POSTROUTING链增加规则，当数据包的源地址为172.18.0.0/24网段，出口设备不是br0时，就执行MASQUERADE动作。\nMASQUERADE也是一种源地址转换动作，它会动态选择宿主机的一个IP做源地址转换，而SNAT动作必须在命令中指定固定的IP地址。\n从“容器”内访问外部地址 1 2 sudo ip netns exec docker0 ping -c 3 123.125.115.110 sudo ip netns exec docker1 ping -c 3 123.125.115.110 我们确认在“容器”内是可以访问外部网络的。\n场景四：从外部访问“容器”内暴露的服务 配置iptables的DNAT规则 当外部通过宿主机的IP和端口访问容器内启动的服务时，在数据包进入PREROUTING阶段就要进行目的地址转换，将宿主机IP转换为容器IP。同样，系统会为我们自动增加一个隐式的反向规则，数据包在离开宿主机时自动做反向转换。\n1 sudo iptables -t nat -A PREROUTING ! -i br0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.18.0.2:80 上面命令的含义是：在nat表的PREROUTING链增加规则，当输入设备不是br0，目的端口为80时，做目的地址转换，将宿主机IP替换为容器IP。\n从远程访问“容器”内暴露的服务 在“容器”docker0内启动服务：\n1 sudo ip netns exec docker0 nc -lp 80 在和宿主机同一个局域网的远程主机访问宿主机IP:80\n1 telnet 192.168.31.183 80 确认可以访问到容器内启动的服务。\n测试环境恢复 删除虚拟网络设备\n1 2 3 4 sudo ip link set br0 down sudo brctl delbr br0 sudo ip link del veth1 sudo ip link del veth3 iptablers和Namesapce的配置在机器重启后被清除。\n总结 本文我们在介绍了veth、Linux bridge、iptables等概念后，亲自动手模拟出了docker bridge网络模型，并测试了几种场景的网络互通。实际上docker network 就是使用了上述技术，帮我们创建和维护网络。通过动手实验，相信你对docker bridge网络理解的更加深入。\n下一篇我将动手实验容器如何利用Overlay 网络进行跨主机通信。\n","date":"2018-10-26T21:40:03+08:00","permalink":"https://mazhen.tech/p/docker%E5%8D%95%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%8A%A8%E6%89%8B%E5%AE%9E%E9%AA%8C/","title":"Docker单机网络模型动手实验"},{"content":"当TiDB 源码阅读系列更新到第六篇《Select 语句概览》时，我发现需要一些关系数据库的基础知识才能更好的理解，例如逻辑查询计划优化其实就是：使用代数定律对查询语句的代数表达式树做等价转换，使改进后的代数表达式树预期可以生成更有效的物理查询计划。有了这些基础知识，看代码才能做到知其然知其所以然。本文希望通过梳理关系数据库背后的知识，为读懂 TiDB 查询处理器部分的源码扫清障碍。\n极简数据库发展史 数据库的应用及其广泛，已经成为信息系统的核心技术和重要的基础设施。简单说数据库需要做两件事：存储数据，以及随后在你需要的时候能访问读取数据。\n最早的数据库是基于文件系统，虽然它满足了长期存储数据的需求，但没有提供对文件的查询语言，读取访问非常不便利。于是人们在文件系统上引入一层抽象：数据模型。数据模型是对现实世界数据特征的抽象，能比较真实地模拟现实世界，容易为人所理解，也便于在计算机上实现。\n最早出现的是层次模型（Hierarchical Model），数据被组织为一棵树，类似于今天文档数据库使用的JSON的结构。层次模型很适合处理one-to-many关系，但要表现many-to-many关系则非常困难，一般也不支持join。使用层次模型最著名的数据库是 IBM 的Information Management System (IMS)，它最初是为了解决阿波罗飞船登月计划的需求，协调分散在全球制造的200万个阿波罗飞船零部件的生产进度。\n随后出现了不同的方案解决层次模型的限制，其中最突出的两个模型是网络模型（Network Model）和关系数据模型，最终关系数据模型胜出。\n今天最著名和使用最广泛的数据模型是由 Edgar Codd 博士提出的关系数据模型，他在1970年发布的论文《A Relational Model of Data for Large Shared Data Banks》，奠定了关系数据库的理论基础。ACM在1983年把这篇论文列为从1958年以来的四分之一世纪中具有里程碑式意义的最重要的25篇研究论文之一。到了80年代中期，基于关系数据模型的关系数据库已经成为人们存储、查询结构化数据的首选工具。\n到了2010年，NoSQL兴起，试图颠覆关系数据模型的统治地位。随着互联网的爆发式发展，数据库领域又一次发生了摇摆，伴随着互联网的特殊需求，一批有着新鲜血液的 NoSQL 数据库涌现了出来，层次模型又重新站在了大家面前。NoSQL为了应对海量数据存储和高并发访问，决定放弃关系数据模型和事务等关系数据数据库的关键特性。自从 NoSQL 概念横空出世，关系数据库似乎成了低效、高成本、速度慢的数据处理模式的代名词。然而，NoSQL在解决问题的同时也给使用者带来了很多困扰， 最终一致让应用开发者要面对各种复杂的场景。\n数据库技术的发展是螺旋式上升，Google发布的Spanner和F1两篇论文，让人们看到了关系数据模型 和 NoSQL 融合的可能性。以 TiDB 为代表的 NewSQL 数据库，让人们重新享受关系模型、强一致性事务等对使用者友好的特性，同时也具备了 NoSQL 的水平扩展能力。\n关系数据模型 和 关系代数 数据模型是对现实世界事物的抽象，而关系数据模型将一切事物都抽象为关系，并通过集合运算的方式规定了关系之间的运算过程，模型相对的比较简单，数据证明严谨，因此很快被大家广泛接受。\n这一节我将介绍关系数据库的数学基础：关系数据模型和关系代数。\n关系数据模型 关系模型为人们提供了一种描述数据的方法：一个称为关系（relation）的二维表。现实世界的实体以及实体间的各种联系都可以用关系来表示。我们通过例子来了解关系模型的重要术语：\n雇员表\nemp_no name birth_date gender hire_date 1 汤唯 1990-06-08 女 2015-08-01 2 刘亦菲 1994-09-10 女 2017-05-06 3 刘德华 1986-04-18 男 2008-09-01 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 关系（Relation） ：一个关系对应通常说的一张二维表 元组（Tuple） ： 表中的一行即为一个元组 属性（Attribute） ：表中的一列即为一个属性，给每一个属性起一个名称即属性名 键（Key）：表中的某个属性组，它可以唯一确定一个元组 域（Domain） : 是一组具有相同数据类型的值的集合。属性的取值范围来自某个域。例如性别的域是（男，女）。 关系模式（schema）：对关系的描述，先给出一个关系名，其后是用圆括号扩起来的所有属性，例如：employees（emp_no, name, birth_date, gender, hire_date） 关系代数 一门代数是由一些操作符和操作数组成。例如算术代数的加、减、乘、除是操作符，变量x和常量8是操作数。任何一门代数都允许把操作符作用在操作数上构造出表达式（expression），例如算术表达式 (x+y)*3。\n关系代数 也是一门代数，它的操作数是关系，操作运算符有两类：集合运算符和专门的关系运算符。\n关系代数可以认为是一种抽象的查询语言，利用对关系的运算来表达查询，运算对象是关系，运算结果也是关系。因此，关系代数的表达式也被称为查询（query）。\n传统的集合运算 三个最常见的集合操作是：并（union）、交（intersection）、差（difference）。\nR ∪ S，表示关系R和S的并，得到的结果关系的元素来自R或者S，或R和S中都出现过。 R ∩ S，表示关系R和S的交，同时在R和S中存在的元素集合。 R - S，表示关系R和S的差，它是由在R中出现但不在S中出现的元素构成的集合。 另外一个集合操作是笛卡尔积（Cartesian Product）。\n关系R和S的笛卡尔积是一个有序对的集合，有序对的一个元素是关系R中的任何一个元组，第二个元素是关系S中的任何一个元组表示为 R × S。 关系R\nA B 1 2 3 4 关系S\nB C D 2 5 6 4 7 8 9 10 11 R × S的结果\nA R.B S.B C D 1 2 2 5 6 1 2 4 7 8 1 2 9 10 11 3 4 2 5 6 3 4 4 7 8 3 4 9 10 11 专门的关系运算 选择（selection)，当选择操作符应用到关系R上时，产生一个关系R的元组的子集合。结果关系元组必须满足某个涉及R中属性的条件C，表示为 σC( R )\n投影 （projection），用来从一个关系生成一个新的关系，这个关系只包含原来关系R中的部分列。表达式 πA1,A2,\u0026hellip;,An ( R ) 的值是这样一个关系，它只包含关系R属性A1,A2,...An所代表的列。\nθ连接，关系R和关系S满足条件C的θ连接可以用这样的符号来表示： R ⋈C S\nθ连接的结果这样构造：\n先得到R和S的笛卡尔积 在得到的关系中寻找满足条件C的元组 关系R\nA B C 1 2 3 6 7 8 9 7 8 关系S\nB C D 2 3 4 2 3 5 7 8 10 R ⋈ A\u0026lt;D AND R.B ≠ S.B S 的结果是：\nA R.B R.C S.B S.C D 1 2 3 7 8 10 有两类常用的连接运算：\n等值连接（equijoin）：比较运算符为 = 的连接运算称为等值连接。例如： R ⋈ R.A = S.B S 是从关系R与S的笛卡尔积中选取A、B属性值相等的那些元组。 自然连接（Natural join）：自然连接是一种特殊的等值连接，两个关系中进行比较的分量必须是相同的属性组，并在结果中把重复的属性列去掉。关系R和S的自然连接表示为 R ⋈ S 关系R\nA B 1 2 3 4 关系S\nB C D 2 5 6 4 7 8 9 10 11 R ⋈ S\nA B C D 1 2 5 6 3 4 7 8 两个关系R和S在做自然连接时，如果一个元组不能和另外关系中的任何一个元组配对的话，这个元组被称为悬浮元组（Dangling tuple）。上面的例子中，关系S的第三个元组就是悬浮元组。\n如果把悬浮元组也保存在结果关系中，而在其他属性上填空值(Null)，就叫做外连接（Outer Join）。\n左外连接(LEFT OUTER JOIN或LEFT JOIN)：只保留左边关系R中的悬浮元组 右外连接(RIGHT OUTER JOIN或RIGHT JOIN)：只保留右边关系S中的悬浮元组 关系代数的扩展操作符 消除重复操作符（duplicated-elimination operator）用 δ(R) 来返回一个没有重复元组的关系R 聚集操作符 （aggregation operator）用来汇总或者聚集关系某一列中出现的值，有 SUM，AVG，MIN，MAX，COUNT 等 分组操作（grouping）根据元组在一个或多个属性上的值把关系的元组拆成“组”。这样聚集操作就可以对分组的各个列进行计算。分组操作符 γ 是组合了分组和聚合操作的一个算子。例如表达式： γ gender, COUNT(emp_no)-\u0026gt;count(employees) 代表把性别（gender）作为分组属性，然后对每一个分组进行COUNT(emp_no)的操作。 排序算子（sorting operator）如果关系R的模式是 R(A,B,C)，那么 τC( R ) 就把R中的元组按照属性C的值排序。 关系代数小结 上面的知识有些枯燥，但非常容易理解，因为我们经常使用关系数据库，已经接受了这些概念。掌握了一些关系代数的知识，在阅读TiDB源码时，当看到selection、projection 这些术语就能一下想到它们对应的关系代数运算符。\n这里只介绍了关系代数最基本的概念，如果想完整学习，建议参考斯坦福大学大学的课程CS145: A First Course in Database Systems，对应的教材有中文版《数据库系统基础教程》。\n其实我们在查询时提交给数据库的就是关系代数表达式，它是关系运算符的组合，数据库会根据一些代数定律对最初的表达式做等价变换，得出一个最优的等价表达式（equivalent expression），即可以更快的被计算出结果的表达式。这个过程就是逻辑查询计划优化，后面我会简单的介绍相关概念。\nSQL 的诞生 SQL(Structured Query Language) 结构化查询语言，是关系数据库的标准语言。\n在1970年Codd博士提出了关系模型之后，由于关系代数或者关系都太数学了，难以被普通用户接受。IBM在研制关系数据库管理系统原型System R的过程中，决定摈弃数学语言，以自然语言为方向，结果诞生了结构化英语查询语言（Structured English Query Language，SEQUEL），后来更名为SQL。System R 因此获得1988年度ACM“软件系统奖”。\nSQL是声明式查询语言，你只需要指定想要获得什么样的数据，而无须了解如何实现这个目标。SQL具体是如何执行的，取决于数据库系统的查询处理器，它来决定哪些索引和哪些连接方法可以使用，以及以什么样的顺序执行查询的各个部分。SQL隐藏了数据库引擎的实现细节，因此用户可以在不修改查询语句的情况下，享受到数据库性能优化带来的好处。\n下面我们来看看数据库的查询处理器。\n关系数据库的查询处理器 SQL是在很高层次上表达查询，那么数据库的查询处理器必须提供查询被如何执行的大量细节。下面我从概念上介绍查询处理器的处理流程，实际的数据库实现要复杂的多，特别是像 TiDB 这样的分布式数据库。如果想比较系统的了解数据库的实现技术，同样推荐斯坦福大学计算机科学专业的课程 CS245: Database System Implementation。上面提到的CS145是CS245的预修课。国内很少有讲数据库内部实现的书，这门课的教材值得阅读。当然最好的学习方法是理论联系实践，多去读 TiDB 的源代码:)\n一般查询处理可以简单的划分为以下几个步骤：\n对SQL进行语法分析，将查询语句转换成抽象语法树。 把抽象语法树转换成关系代数表达式树，这就是初始的逻辑查询计划。 使用关系代数中的多个代数定律改进初始的代数表达式树。利用一些代数定律，可以把代数表达式树转换成一个等价的表达式树，后者预期可以生成更有效的物理查询计划。这一步进行了查询重写，可以称为逻辑查询计划优化。 为逻辑查询计划的每一个操作符选择实现算法，并确定这些操作符的执行顺序，逻辑查询计划被转化为物理查询计划。物理查询计划指明了要执行的操作，操作的执行顺序，执行每步所用的算法，获取数据的方式，以及数据从一个操作传递给另一个操作的方式。 查询示例 本文准备以一个简单的例子来介绍查询处理的流程，下面是查询涉及的两张表：\nemployees\nemp_no name birth_date gender hire_date 1 汤唯 1990-06-08 女 2015-08-01 2 刘亦菲 1994-09-10 女 2017-05-06 3 刘德华 1986-04-18 男 2008-09-01 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; salaries\nemp_no salary last_modified 1 8000 2018-04-01 2 6000 2018-04-01 3 15000 2018-04-01 \u0026hellip; \u0026hellip; \u0026hellip; 想要获取工资大于7000的员工姓名列表，SQL语句可以这么写：\n1 2 3 4 SELECT name FROM employees, salaries WHERE employees.emp_no = salaries.emp_no AND salary \u0026gt; 7000; SQL 语法分析 SQL Parser的功能是把SQL语句按照SQL语法规则进行解析，将文本转换成抽象语法树（AST）。具体的实现可以参考这篇文章《TiDB SQL Parser 的实现》。 示例SQL解析完成后得到下面的语法树：\n生成逻辑查询计划 现在将上一步生成的语法树转换成关系代数表达式树，也就是逻辑查询计划。对于示例SQL的抽象语法树转换过程如下：\n\u0026lt;FromList\u0026gt; 中涉及的关系employees和salaries做笛卡尔积运算 选择（selection）运算 σC，其中C被替换成\u0026lt;Condition\u0026gt;表达式，即employees.emp_no = salaries.emp_no AND salary \u0026gt; 7000 投影（projection） πL，其中L是\u0026lt;SelList\u0026gt;中的属性列表，对于这个查询只有一个属性name 我们得到下面的关系代数表达式树：\n逻辑查询计划的改进 当我们把查询语句转换成关系代数表达式时，得到了一个初始的逻辑查询计划。现在我们可以使用关系代数中的多个代数定律改进逻辑查询计划。\n这里仅仅列出一小部分这样的代数定律，它们可以将一个表达式树转换成一个等价的表达式树。\n交换律和结合律 R × S = S × R; (R X S) × T = R × (S × T)\nR ⋈ S = S ⋈ R; (R ⋈ S) ⋈ T = R ⋈ (S ⋈ T)\nR ∪ S = S ∪ R; (R ∪ S) ∪ T = R ∪ (S ∪ T)\nR ∩ S = S ∩ R; (R ∩ S) ∩ T = R ∩ (S ∩ T)\n涉及选择的定律 σC1 AND C2 = σC1(σC2( R ))\nσC1 OR C2 = (σC1( R ) ∪ (σC2( R ) )\nσC1(σC2( R )) = σC2(σC1( R ))\n下推选择 在表达式中下推选择是查询优化器最强有力的工具。\nσC( R × S ) = σC( R ) × S\nσC( R ⋈ S) = σC( R ) ⋈ S\nσC( R ⋈D S) = σC( R ) ⋈D S\n涉及连接和笛卡尔积的定律 R ⋈C S = σC( R × S)\n涉及消除重复的定律 δ(R×S) = δ( R ) × δ(S)\nδ(R⋈CS) = δ( R ) ⋈C δ(S)\n另外还有涉及投影的定律、涉及分组和聚集的定律。这部分有些理论化，可以参考这篇《TiDB 源码阅读系列文章（七）基于规则的优化》看看 TiDB 具体是怎么做的。\n对于本例使用到的定律比较简单。先将选择的两部分分解为 σemployees.emp_no = salaries.emp_no 和 σsalary \u0026gt; 7000，后者可以在树中下推。第一个条件涉及笛卡尔积两边的属性，可以把上面提到的定律 R ⋈C S = σC( R × S)\n从右向左使用，把笛卡尔积转换成连接。使用了两个定律后，得到优化后的逻辑查询计划如下图：\n物理查询计划的生成 这一步我们需要把逻辑查询计划转换成物理查询计划。通常由逻辑计划可以得到多个物理计划，我们需要对每个物理计划进行评估，选择具有最小估计代价的物理查询计划。\n基于代价的物理计划选择 在从逻辑计划构建物理计划的时候，因为可能得到多个物理计划，我们需要通过估计物理计划执行的代价来确定最优选择。\n如何计算这个代价呢？我们可以用物理计划每一步的任务执行时发生的磁盘I/O数、网络吞吐量、占用的内存空间大小等近似估算。\n这些资源的访问和占用，又是由什么决定的呢？可能包括的决定因素有：\n参与运算任务执行的数据大小 数据的分布位置（连续的磁盘空间、离散的磁盘空间、网络节点等） 关系中属性的不同值的数目 属性值的最大值、最小值、以及值的分布情况 数据库会收集统计这些信息，用来估算具体任务的代价。\n逻辑查询计划在转换成物理计划的时候，每一步的转换都会面临多种情况的选择，最容易想到的是使用穷举法，估算每一种情况的代价，最后确定最优的物理计划。但使用穷举法的话，很可能估算本身的代价变得非常大，实践中可以采用动态规划（dynamic programming）等算法。\n枚举物理查询计划 以上一步输出的逻辑查询计划为例，看看在枚举物理查询计划时需要做出哪些选择。\n首先，逻辑查询计划的每个结点转换成什么样的物理运算符会遇到多种选择。我们从逻辑查询计划树自底往上来看：\n逻辑计划的叶子结点 逻辑查询计划树的叶子结点被一个扫描运算符替代。这些运算符一般包括：\nTableScan( R )：以任意顺序读人所有元组 SortScan(R, L)：按照顺序读入R的元组，并以列L中的属性进行排列 IndexScan(R, C)：C是一个带有比较运算符的条件，例如 A = 100，A是R的一个属性，如果A上建立的索引，可以通过索引来访问R的元组。如果比较运算符不是等值比较，则索引必须是一个支持范围查询的索引，例如B+ Tree IndexScan(R, A)：这里A是R的一个属性，关系R通过A上的索引被检索。 如果R的数据在磁盘上不是占用连续的存储空间，该运算符可能比TableScan更有效。 逻辑计划的σ选择（selection)结点 σ选择结点一般有两种选择：\n可以简单的用物理过滤运算符Filter( C )替代 σC( R ) 如果C是一个带有比较运算符的条件，例如 A = 100，并且属性A上有索引，可以把比较运算合并到扫描运算符：IndexScan(R, A = 100)。 对于本例，salary 一般都不会建立索引，因此可以把σ( salary \u0026gt; 7000 ) 替换为 Filter( salary \u0026gt; 7000 )\n逻辑计划的连接结点 常见的等值连接满足结合律和交换律，因此连接可以得到很多候选的物理执行计划。其中最关键的问题是确定连接顺序。\n当两个关系连接，只有两种选择，一般我们应该将估计值较小的关系放在前面。当连接有2个以上关系时，可能的连接树的数量会迅速增加，例如4个关系的连接将会有4!=24种连接方式。这一部分很复杂，就不在本文讨论了。\n除了连接顺序，还需要确定具体使用的连接算法。常见的算法有：\nnested loops 基于排序的join（sort merge join） hash join 基于索引的join 逻辑计划的投影结点 投影结点的任务比较明确，输出包含指定列的关系。\n除了将逻辑查询计划的结点转换成物理运算符，在选择物理计划时还要考虑数据如何在运算符之间流动（中间结果保存到磁盘或流水线方式），物理运算符的执行顺序等。这些细节本文就不再讨论。\n最后，假定我们在所有选择的组合中，确定了其中一个作为最优的物理查询计划，然后就可以把它交给查询执行器真正的执行了：\n写在最后 本文把关系数据库查询处理涉及的基础知识进行了梳理，希望对你理解 TiDB 的代码能有所帮助。\n数据库是一个非常迷人的领域，它有很强的理论基础，同时又涉及大量的工程实践，可以说是最复杂的系统软件之一。我相信能开发数据库是很多程序员的梦想。梦想还是要有的，让我们一起努力吧！\n","date":"2018-07-01T17:04:31+08:00","permalink":"https://mazhen.tech/p/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%89%AB%E7%9B%B2/","title":"关系数据库查询处理基础知识扫盲"},{"content":"为什么XA事务建议用SERIALIZABLE隔离级别 在MySQL最新的官方文档中，关于XA Transactions的介绍有这么一段描述：\nAs with nondistributed transactions, SERIALIZABLE may be preferred if your applications are sensitive to read phenomena. REPEATABLE READ may not be sufficient for distributed transactions.\n这段话表达的意思是，对于分布式XA事务， REPEATABLE READ 隔离级别是不够的。\nMySQL旧版本的文档，对于这个问题表达的更加直接：\nHowever, for a distributed transaction, you must use the SERIALIZABLE isolation level to achieve ACID properties. It is enough to use REPEATABLE READ for a nondistributed transaction, but not for a distributed transaction.\n怎么理解呢？举个简单的例子：假设MySQL使用的是REPEATABLE READ 隔离级别，XA事务 T1 修改的数据涉及两个节点 A 和 B，当事务 T1 在 A 上完成commit，而在 B 上还没commit之前，也就是说这时事务 T1 并没有真正结束，另一个XA事务 T2 已经可以访问到 T1 在 A 上提交后数据，这不是出现脏读了吗？\n那么使用SERIALIZABLE就能保证吗？还是看例子：事务 T1 修改节点 A 上的数据 a -\u0026gt; a'，修改 B 上的数据 b -\u0026gt; b'，在提交阶段，可能被其他事务 T2 读取到了 a'， 因为使用了SERIALIZABLE隔离级别， MySQL 会对所有读加锁，那么 T2 在 B 上读取 b 时会被一直阻塞，直到 T1 在 B 上完成commit，这时 T2 在 B 读取到的就是 b'。 也就是说，SERIALIZABLE隔离级别保证了读到 a' 的事务，不会读到 b ，而是读到 b'，确保了事务ACID的要求。\n更加详细的描述可以参考鹅厂 TDSQL XA 事务隔离级别的奥秘，他们的结论是：\n如果某个并发事务调度机制可以让具有依赖关系的事务构成一个有向无环图(DAG)，那么这个调度就是可串行化的调度。由于每个后端DB都在使用serializable隔离级别，所以每个后端DB上面并发执行的事务分支构成的依赖关系图一定是DAG。\n只要所有连接都是用serializable隔离级别，那么TDSQL XA执行的事务仍然可以达到可串行化隔离级别。\nSERIALIZABLE性能差，有更好的实现方式吗 如果分布式事务想实现read-committed以上的隔离级别，又不想使用SERIALIZABLE，有什么更好的方式吗？\n当然有，想想看TiDB是怎么做的，底层TiKV是一个整体，有全局的MVCC，所以能够做到分布式事务的Snapshot隔离级别。\nPostgreSQL社区中，有Postgres-XC和Postgres-XL的方案，采用的并发机制是全局MVCC 和本地写锁。 Postgres-XC 维持了全局活跃事务列表，从而提供全局MVCC。\n虽然MySQL也实现了MVCC，但它没有将底层K/V带有时间戳的版本信息暴露出来。也就是说，多个MySQL实例组成的集群没有全局的MVCC，无法得到全局一致性快照，自然就很难做到分布式的Snapshot隔离级别。腾讯的这篇文章也分析了这么做比较困难：\n由于MySQL innodb使用MVCC做select（除了serializable和for update/lock in share mode子句），还需要将这个全局事务id给予innodb做事务id，同时，还需要TDSQL XA集群的多个set的innodb 共享各自的本地事务状态给所有其他innodb（这也是PGXL 所做的），任何一个innodb的本地事务的启动，prepare，commit，abort都需要通知给所有其他innodb实例。只有这样做，集群中的每个innodb实例才能够建立全局完全有一致的、当前集群中正在处理的所有事务的状态，以便做多版本并发控制。 这本身都会造成极大的性能开销，并且导致set之间的严重依赖，降低系统可靠性。这些都是我们要极力避免的。\n结论 根据上面的分析，如果使用MySQL 的 XA分布式事务，最安全的方式还是按照官方建议，使用SERIALIZABLE隔离级别。\n如果想基于MySQL做改造，实现全局MVCC，从而实现分布式事务的Snapshot隔离级别，目前还没有看到MySQL社区有这类项目，相信实现难度比较大。\n","date":"2018-06-05T14:44:59+08:00","permalink":"https://mazhen.tech/p/%E5%85%B3%E4%BA%8Emysql-xa%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","title":"关于MySQL XA事务的隔离级别"},{"content":"现在新出现的MySQL中间件除了基本的数据sharding功能，都增加了很多高级特性，我觉得有三个特性比较重要：\n分布式事务的支持 数据的强一致复制，提高了数据的安全性和可用性 支持跨shard join 通过对这些特性的支持，MySQL中间件具备了一些newSQL数据库的能力，不再是个纯粹的中间件，让用户更容易使用。我调研了最近开源的青云RadonDB，希望了解下这方面最新的进展。\n先简单看下RadonDB的整体架构：存储/计算分离。存储节点是MySQL，3个一组用raft实现数据的强一致性，不再是异步/半同步复制，数据的安全性、可用性级别更高。上层是SQL节点，负责客户端连接、SQL语句的解析和执行、分布式事务协调、数据sharding逻辑等。右下脚计算节点的作用，后面会解释。\n在知乎上看到\u0026lt;如何评价青云开源的分布式数据库 radondb\u0026gt;，RandoDB被吐槽的很厉害。我们从这些吐槽可以了解产品宣传之外的一些信息，知道做这种中间件不是那么容易。大家对RandoDB的几个关键特性的实现方式都不太满意。让我们逐一看看。\n分布式事务的实现 对分布式事务的实现大家吐槽的最厉害：\n官方宣传用XA实现了Snapshot Isolation，然而众所周知XA是无法实现SI的。所谓的事务其实只支持单条SQL，BEGIN / SET AUTOCOMMIT=0 都不支持。\n单语句事务，就是不能 begin 开启事务。\n为了达到 SI 隔离级别，在执行用户 SQL 时，会加上一个 commitLock，防止其他事务提交。这决定了加锁必须时间很短，比如一条SQL，如果你从start transaction开始加锁，那其他事务全都无法提交了，系统事实上已经不可用。\n所谓分布式事物快照隔离级别是 radondb 层 query 和 commit 语句串行化实现的。这个应该是串行化隔离级别了。而且是和冲突没关系的串行化，就是说根本不管两个事物之间有没有冲突数据。性能自行脑补。\n没有 XA log 的 XA 事务原子性实现都是耍流氓。\n为什么XA是无法实现SI？ 我的理解是，单个MySQL实例虽然实现了MVCC，但它没有将底层K/V带有时间戳的版本信息暴露出来。也就是说，多个MySQL实例组成的集群没有全局的MVCC，每个实例内部的MVCC是独立的，无法得到全局一致性快照。XA事务跨越了多个节点，所以没办法实现Snapshot隔离级别。可以对比下TiDB的实现，底层TiKV是一个整体，有全局的MVCC，所以能在上层支持分布式事务的Snapshot隔离级别。\nRandoDB的实现能work，但相当于在Proxy层将所有事务串行化，即使两个事务之间没有数据冲突。而且只有单语句事务。\n对于XA log，开发者的解释是：\nproxy xa log只针对xa commit出错，目前通过分析log然后人工介入，这里没有再记log的必要\n我觉得这么做很不严谨。2PC协议有一个问题，一旦事务参与者投票，它必须等待coordinator给出指示，提交或放弃。如果这时coordinator挂了，事务参与者除了等待什么也做不了。事务处于未决状态，事务的最终结果记录在coordinator的事务日志中，只能等它recovery。因此，现在很多改进的做法是用Paxos/raft保证事务日志的高可用，coordinator挂了可以快速切换。即使不用raft，找一个地方可靠持久的保存事务日志是非常必要的。\n使用Raft保证强一致性 现在很多项目都会使用Paxos/Raft来改进MySQL的复制机制，实现数据的强一致性。如果主、备间任何时刻都完全一致，那么任何时刻都可以安全的进行主备切换。如果无法保证主、备间的强一致性，那么当有持续不断的更新时，主备切换就无法保证强一致性，需要在切换时暂停主库的写入，服务会有短暂的中断。\n腾讯的PhxSQL就是建立在Paxos的一致性和MySQL的binlog流水基础上，通过Paxos保证强一致和高可用的MySQL集群。关于PhxSQL的设计理念可以参见：\n谈谈PhxSQL的设计和实现哲学（上） 谈谈PhxSQL的设计和实现哲学（下） 采用类似做法的还有阿里云的MySQL金融版。另外，MySQL官方也从5.7开始推出了Group Replication机制，内部使用Paxos实现了多个副本的一致。\nRadonDB的实现机制和PhxSQL不太一样。它在一组MySQL集群内的复制还是通过Semi-Sync机制（Semi-Sync设置为无限大，不退化为异步复制），保证有一个slave和master完全一致。主备切换时会选择这个slave为主，然后结合MySQL的 Multi-threaded replication 和 GTID机制 进行快速回放，让主备重新一致。Raft用在哪里了？在 RadonDB 只使用 Raft 进行选主，当主挂掉之后，使用 Raft 选出新的主。Raft选主的逻辑是选出一个拥有最多的committed log的成员作为主，那么对于RadonDB来说，哪个MySQL的GTID最大就选哪个。\n我自己还没有使用Raft的经验，不确定RadonDB的实现机制是否合理。但利用Semi-Sync模拟同步复制的方案，我觉得有一个地方不妥。当和主库保持强同步的备库有问题时，这组MySQL整体就不可用，因为它需要至少一个备库和主库完全一致，这就因为单点降低了整个集群的可用性。如果是用Raft做数据复制，就不会有这种单点影响全局可用性的问题。\n另外，RandoDB被吐槽 Raft 的实现业余、不严谨：\n打开用来做HA的Xenon模块，一看又是作业级的肯写代码不肯写测试的raft练手实现。raft测试用例一共1500行go代码 刚才数了下，自己的raft库光election相关的单元测试用例就数千行代码了。做生产环境用的系统不是练手写作业，需要一个go的raft库，既然都不肯写完备的测试了，那就老老实实用etcd或者hashicorp的raft。自己私下撸一个raft库练手，和给自己全职项目选一个可靠的raft实现，两者真的不矛盾。最滑稽，只做选主干嘛自己撸一个raft实现？\njoin等复杂查询的实现 严格说RandoDB是不支持join的。它的做法是让计算节点通过binglog复制了全量数据，SQL节点会把join等复杂查处路由到计算节点执行。\n“计算节点”使用tokudb存储引擎存储全量数据，为了支持复杂查询。。。如果我一个分布式系统的数据总量有20T、100T，也用单个“计算节点”存储全量数据？而且这个数据同步过程是异步的，显然没法用在OLTP场景。\n通过一些实用的方式支持了Join，这种做法可以work，但RandoDB离它宣称的数据库还差很远，缺少全局的执行计划优化。\n总体来说，RandoDB的理想很宏大，用实用的方案解决了一些问题，但要成为真正成熟的数据库产品还差的比较远。RadonDB 的核心代码1万行左右。加上其它类库引入，Radon代码11万+， Xenon代码5万行+ 。\n最后，看到有人推荐腾讯的TDSQL，也顺便了解了一下。从资料看TDSQL很不错，可惜不是开源产品。除了水平扩张、安全增强、自动化运维以外，它具备了我们上面提到的数据库中间件的高级特性：\n支持分布式事务XA 全局事务的隔离级别最高可以达到serializable级别 分布式事务对业务透明，兼容单机事务语法 允许事务中多条语句分别发给多个分片 支持autocommit下单条语句写访问多个分片 默认采用强同步复制，即主从节点数据完全一致 复杂查询方面 允许以流式处理方式运行group by、order by 支持两个Shard使用shardkey（分表键）做等值连接，以及使用shardkey的子查询 支持了部分受限的复杂查询，对于数据库中间件来说已经算比较强大了。关于TDSQL的分布式事务，可以通过这两篇进行更多的了解：\n一文教你迅速解决分布式事务 XA 一致性问题 鹅厂 TDSQL XA 事务隔离级别的奥秘 如果我们做MySQL中间件，可以瞄准TDSQL，对于分布式事务、数据强一致性，以及复杂查询、跨shard join 等特性都要考虑支持。\n","date":"2018-06-03T14:43:12+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8Eradondb%E7%9C%8B%E6%96%B0%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E7%89%B9%E6%80%A7/","title":"从RadonDB看新型数据库中间件的特性"},{"content":"TiDB源码解读系列的《Insert语句概览》讲解了Insert执行的整体流程，并在最后用一幅图描述了整个流程：\n我按照自己的理解对这幅图扩展了一下，在原先数据结构转换流程的基础上，补充了代码的调用流程，个人感觉更加全面，希望对你阅读代码也有帮助。\n","date":"2018-05-15T14:38:05+08:00","permalink":"https://mazhen.tech/p/tidb-insert-%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE/","title":"TiDB Insert 执行流程图"},{"content":" Previous Next \u0026nbsp; \u0026nbsp; Page: / ","date":"2018-05-11T17:01:04+08:00","permalink":"https://mazhen.tech/p/cursor%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0%E6%80%BB%E7%BB%93/","title":"Cursor功能实现总结"},{"content":"PingCAP发布了TiDB的源码阅读系列文章，让我们可以比较系统的去学习了解TiDB的内部实现。最近的一篇《SQL 的一生》，从整体上讲解了一条SQL语句的处理流程，从网络上接收数据，MySQL协议解析和转换，SQL语法解析，查询计划的制定和优化，查询计划执行，到最后返回结果。\n其中，SQL Parser的功能是把SQL语句按照SQL语法规则进行解析，将文本转换成抽象语法树（AST），这部分功能需要些背景知识才能比较容易理解，我尝试做下相关知识的介绍，希望能对读懂这部分代码有点帮助。\nTiDB是使用goyacc根据预定义的SQL语法规则文件parser.y生成SQL语法解析器。我们可以在TiDB的Makefile文件中看到这个过程，先build goyacc工具，然后使用goyacc根据parser.y生成解析器parser.go：\n1 2 3 4 5 6 goyacc: $(GOBUILD) -o bin/goyacc parser/goyacc/main.go parser: goyacc bin/goyacc -o /dev/null parser/parser.y bin/goyacc -o parser/parser.go parser/parser.y 2\u0026gt;\u0026amp;1 ... goyacc是yacc的Golang版，所以要想看懂语法规则定义文件parser.y，了解解析器是如何工作的，先要对Lex \u0026amp; Yacc有些了解。\nLex \u0026amp; Yacc 介绍 Lex \u0026amp; Yacc 是用来生成词法分析器和语法分析器的工具，它们的出现简化了编译器的编写。Lex \u0026amp; Yacc 分别是由贝尔实验室的Mike Lesk 和 Stephen C. Johnson在1975年发布。对于Java程序员来说，更熟悉的是ANTLR，ANTLR 4 提供了 Listener+Visitor 组合接口， 不需要在语法定义中嵌入actions，使应用代码和语法定义解耦。Spark的SQL解析就是使用了ANTLR。Lex \u0026amp; Yacc 相对显得有些古老，实现的不是那么优雅，不过我们也不需要非常深入的学习，只要能看懂语法定义文件，了解生成的解析器是如何工作的就够了。我们可以从一个简单的例子开始：\n上图描述了使用Lex \u0026amp; Yacc构建编译器的流程。Lex根据用户定义的patterns生成词法分析器。词法分析器读取源代码，根据patterns将源代码转换成tokens输出。Yacc根据用户定义的语法规则生成语法分析器。语法分析器以词法分析器输出的tokens作为输入，根据语法规则创建出语法树。最后对语法树遍历生成输出结果，结果可以是产生机器代码，或者是边遍历 AST 边解释执行。\n从上面的流程可以看出，用户需要分别为Lex提供patterns的定义，为 Yacc 提供语法规则文件，Lex \u0026amp; Yacc 根据用户提供的输入文件，生成符合他们需求的词法分析器和语法分析器。这两种配置都是文本文件，并且结构相同：\n1 2 3 4 5 ... definitions ... %% ... rules ... %% ... subroutines ... 文件内容由 %% 分割成三部分，我们重点关注中间规则定义部分。对于上面的例子，Lex 的输入文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ... %% /* 变量 */ [a-z] { yylval = *yytext - \u0026#39;a\u0026#39;; return VARIABLE; } /* 整数 */ [0-9]+ { yylval = atoi(yytext); return INTEGER; } /* 操作符 */ [-+()=/*\\n] { return *yytext; } /* 跳过空格 */ [ \\t] ; /* 其他格式报错 */ . yyerror(\u0026#34;invalid character\u0026#34;); %% ... 上面只列出了规则定义部分，可以看出该规则使用正则表达式定义了变量、整数和操作符等几种token。例如整数token的定义如下：\n1 2 3 4 [0-9]+ { yylval = atoi(yytext); return INTEGER; } 当输入字符串匹配这个正则表达式，大括号内的动作会被执行：将整数值存储在变量 yylval 中，并返回 token 类型 INTEGER 给 Yacc。\n再来看看 Yacc 语法规则定义文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 %token INTEGER VARIABLE %left \u0026#39;+\u0026#39; \u0026#39;-\u0026#39; %left \u0026#39;*\u0026#39; \u0026#39;/\u0026#39; ... %% program: program statement \u0026#39;\\n\u0026#39; | ; statement: expr { printf(\u0026#34;%d\\n\u0026#34;, $1); } | VARIABLE \u0026#39;=\u0026#39; expr { sym[$1] = $3; } ; expr: INTEGER | VARIABLE { $$ = sym[$1]; } | expr \u0026#39;+\u0026#39; expr { $$ = $1 + $3; } | expr \u0026#39;-\u0026#39; expr { $$ = $1 - $3; } | expr \u0026#39;*\u0026#39; expr { $$ = $1 * $3; } | expr \u0026#39;/\u0026#39; expr { $$ = $1 / $3; } | \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; { $$ = $2; } ; %% ... 第一部分定义了 token 类型和运算符的结合性。四种运算符都是左结合，同一行的运算符优先级相同，不同行的运算符，后定义的行具有更高的优先级。\n语法规则使用了BNF定义。BNF 可以用来表达上下文无关（context-free）语言，大部分的现代编程语言都可以使用 BNF 表示。上面的规则定义了三个产生式。产生式冒号左边的项（例如 statement）被称为非终结符， INTEGER 和 VARIABLE 被称为终结符,它们是由 Lex 返回的 token 。终结符只能出现在产生式的右侧。可以使用产生式定义的语法生成表达式：\n1 2 3 4 5 expr -\u0026gt; expr * expr -\u0026gt; expr * INTEGER -\u0026gt; expr + expr * INTEGER -\u0026gt; expr + INTEGER * INTEGER -\u0026gt; INTEGER + INTEGER * INTEGER 解析表达式是生成表达式的逆向操作，我们需要归约表达式到一个非终结符。Yacc 生成的语法分析器使用自底向上的归约（shift-reduce）方式进行语法解析，同时使用堆栈保存中间状态。还是看例子，表达式x + y * z的解析过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 1 . x + y * z 2 x . + y * z 3 expr . + y * z 4 expr + . y * z 5 expr + y . * z 6 expr + expr . * z 7 expr + expr * . z 8 expr + expr * z . 9 expr + expr * expr . 10 expr + expr . 11 expr . 12 statement . 13 program . 点（.）表示当前的读取位置，随着 . 从左向右移动，我们将读取的token压入堆栈，当发现堆栈中的内容匹配了某个产生式的右侧，则将匹配的项从堆栈中弹出，将该产生式左侧的非终结符压入堆栈。这个过程持续进行，直到读取完所有的tokens，并且只有启始非终结符（本例为 program）保留在堆栈中。\n产生式右侧的大括号中定义了该规则关联的动作，例如：\n1 expr: expr \u0026#39;*\u0026#39; expr { $$ = $1 * $3; } 我们将堆栈中匹配该产生式右侧的项替换为产生式左侧的非终结符，本例中我们弹出 expr '*' expr，然后把 expr 压回堆栈。 我们可以使用 $position 的形式访问堆栈中的项，$1引用的是第一项，$2引用的是第二项，以此类推。$$ 代表的是归约操作执行后的堆栈顶。本例的动作是将三项从堆栈中弹出，两个表达式相加，结果再压回堆栈顶。\n上面例子中语法规则关联的动作，在完成语法解析的同时，也完成了表达式求值。一般我们希望语法解析的结果是一棵抽象语法树（AST），可以这么定义语法规则关联的动作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ... %% ... expr: INTEGER { $$ = con($1); } | VARIABLE { $$ = id($1); } | expr \u0026#39;+\u0026#39; expr { $$ = opr(\u0026#39;+\u0026#39;, 2, $1, $3); } | expr \u0026#39;-\u0026#39; expr { $$ = opr(\u0026#39;-\u0026#39;, 2, $1, $3); } | expr \u0026#39;*\u0026#39; expr { $$ = opr(\u0026#39;*\u0026#39;, 2, $1, $3); } | expr \u0026#39;/\u0026#39; expr { $$ = opr(\u0026#39;/\u0026#39;, 2, $1, $3); } | \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; { $$ = $2; } ; %% nodeType *con(int value) { ... } nodeType *id(int i) { ... } nodeType *opr(int oper, int nops, ...) { ... } 上面是一个语法规则定义的片段，我们可以看到，每个规则关联的动作不再是求值，而是调用相应的函数，该函数会返回抽象语法树的节点类型 nodeType，然后将这个节点压回堆栈，解析完成时，我们就得到了一颗由 nodeType 构成的抽象语法树。对这个语法树进行遍历访问，可以生成机器代码，也可以解释执行。\n至此，我们大致了解了Lex \u0026amp; Yacc的原理。其实还有非常多的细节，例如如何消除语法的歧义，但我们的目的是读懂TiDB的代码，掌握这些概念已经够用了。\ngoyacc 简介 goyacc 是golang版的 Yacc。和 Yacc的功能一样，goyacc 根据输入的语法规则文件，生成该语法规则的go语言版解析器。goyacc 生成的解析器 yyParse 要求词法分析器符合下面的接口：\n1 2 3 4 type yyLexer interface { Lex(lval *yySymType) int Error(e string) } 或者\n1 2 3 4 5 type yyLexerEx interface { yyLexer // Hook for recording a reduction. Reduced(rule, state int, lval *yySymType) (stop bool) // Client should copy *lval. } TiDB没有使用类似 Lex 的工具生成词法分析器，而是纯手工打造，词法分析器对应的代码是 parser/lexer.go， 它实现了 goyacc 要求的接口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ... // Scanner implements the yyLexer interface. type Scanner struct { r reader buf bytes.Buffer errs []error stmtStartPos int // For scanning such kind of comment: /*! MySQL-specific code */ or /*+ optimizer hint */ specialComment specialCommentScanner sqlMode mysql.SQLMode } // Lex returns a token and store the token value in v. // Scanner satisfies yyLexer interface. // 0 and invalid are special token id this function would return: // return 0 tells parser that scanner meets EOF, // return invalid tells parser that scanner meets illegal character. func (s *Scanner) Lex(v *yySymType) int { tok, pos, lit := s.scan() v.offset = pos.Offset v.ident = lit ... } // Errors returns the errors during a scan. func (s *Scanner) Errors() []error { return s.errs } 另外lexer 使用了字典树技术进行 token 识别，具体的实现代码在parser/misc.go\nTiDB SQL Parser的实现 终于到了正题。有了上面的背景知识，对TiDB 的 SQL Parser 模块会相对容易理解一些。先看SQL语法规则文件parser.y，goyacc 就是根据这个文件生成SQL语法解析器的。\nparser.y 有6500多行，第一次打开可能会被吓到，其实这个文件仍然符合我们上面介绍过的结构：\n1 2 3 4 5 ... definitions ... %% ... rules ... %% ... subroutines ... parser.y 第三部分 subroutines 是空白没有内容的， 所以我们只需要关注第一部分 definitions 和第二部分 rules。\n第一部分主要是定义token的类型、优先级、结合性等。注意 union 这个联合体结构体：\n1 2 3 4 5 6 7 %union { offset int // offset item interface{} ident string expr ast.ExprNode statement ast.StmtNode } 该联合体结构体定义了在语法解析过程中被压入堆栈的项的属性和类型。\n压入堆栈的项可能是终结符，也就是 token，它的类型可以是item 或 ident；\n这个项也可能是非终结符，即产生式的左侧，它的类型可以是 expr 、 statement 、 item 或 ident。\ngoyacc 根据这个 union 在解析器里生成对应的 struct 是：\n1 2 3 4 5 6 7 8 type yySymType struct { yys int offset int // offset item interface{} ident string expr ast.ExprNode statement ast.StmtNode } 在语法解析过程中，非终结符会被构造成抽象语法树（AST）的节点 ast.ExprNode 或 ast.StmtNode。抽象语法树相关的数据结构都定义在 ast 包中，它们大都实现了 ast.Node 接口：\n1 2 3 4 5 6 7 // Node is the basic element of the AST. // Interfaces embed Node should have \u0026#39;Node\u0026#39; name suffix. type Node interface { Accept(v Visitor) (node Node, ok bool) Text() string SetText(text string) } 这个接口有一个 Accept 方法，接受 Visitor 参数，后续对 AST 的处理，主要依赖这个 Accept 方法，以 Visitor 模式遍历所有的节点以及对 AST 做结构转换。\n1 2 3 4 5 // Visitor visits a Node. type Visitor interface { Enter(n Node) (node Node, skipChildren bool) Leave(n Node) (node Node, ok bool) } 例如 plan.preprocess 是对 AST 做预处理，包括合法性检查以及名字绑定。\nunion 后面是对 token 和 非终结符 按照类型分别定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 /* 这部分的token是 ident类型 */ %token \u0026lt;ident\u0026gt; ... add \u0026#34;ADD\u0026#34; all \u0026#34;ALL\u0026#34; alter \u0026#34;ALTER\u0026#34; analyze \u0026#34;ANALYZE\u0026#34; and \u0026#34;AND\u0026#34; as \u0026#34;AS\u0026#34; asc \u0026#34;ASC\u0026#34; between \u0026#34;BETWEEN\u0026#34; bigIntType \u0026#34;BIGINT\u0026#34; ... /* 这部分的token是 item 类型 */ %token \u0026lt;item\u0026gt; /*yy:token \u0026#34;1.%d\u0026#34; */ floatLit \u0026#34;floating-point literal\u0026#34; /*yy:token \u0026#34;1.%d\u0026#34; */ decLit \u0026#34;decimal literal\u0026#34; /*yy:token \u0026#34;%d\u0026#34; */ intLit \u0026#34;integer literal\u0026#34; /*yy:token \u0026#34;%x\u0026#34; */ hexLit \u0026#34;hexadecimal literal\u0026#34; /*yy:token \u0026#34;%b\u0026#34; */ bitLit \u0026#34;bit literal\u0026#34; andnot \u0026#34;\u0026amp;^\u0026#34; assignmentEq \u0026#34;:=\u0026#34; eq \u0026#34;=\u0026#34; ge \u0026#34;\u0026gt;=\u0026#34; ... /* 非终结符按照类型分别定义 */ %type \u0026lt;expr\u0026gt; Expression \u0026#34;expression\u0026#34; BoolPri \u0026#34;boolean primary expression\u0026#34; ExprOrDefault \u0026#34;expression or default\u0026#34; PredicateExpr \u0026#34;Predicate expression factor\u0026#34; SetExpr \u0026#34;Set variable statement value\u0026#39;s expression\u0026#34; ... %type \u0026lt;statement\u0026gt; AdminStmt \u0026#34;Check table statement or show ddl statement\u0026#34; AlterTableStmt \u0026#34;Alter table statement\u0026#34; AlterUserStmt \u0026#34;Alter user statement\u0026#34; AnalyzeTableStmt \u0026#34;Analyze table statement\u0026#34; BeginTransactionStmt \u0026#34;BEGIN TRANSACTION statement\u0026#34; BinlogStmt \u0026#34;Binlog base64 statement\u0026#34; ... %type \u0026lt;item\u0026gt; AlterTableOptionListOpt \u0026#34;alter table option list opt\u0026#34; AlterTableSpec \u0026#34;Alter table specification\u0026#34; AlterTableSpecList \u0026#34;Alter table specification list\u0026#34; AnyOrAll \u0026#34;Any or All for subquery\u0026#34; Assignment \u0026#34;assignment\u0026#34; ... %type \u0026lt;ident\u0026gt; KeyOrIndex \u0026#34;{KEY|INDEX}\u0026#34; ColumnKeywordOpt \u0026#34;Column keyword or empty\u0026#34; PrimaryOpt \u0026#34;Optional primary keyword\u0026#34; NowSym \u0026#34;CURRENT_TIMESTAMP/LOCALTIME/LOCALTIMESTAMP\u0026#34; NowSymFunc \u0026#34;CURRENT_TIMESTAMP/LOCALTIME/LOCALTIMESTAMP/NOW\u0026#34; ... 第一部分的最后是对优先级和结合性的定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 ... %precedence sqlCache sqlNoCache %precedence lowerThanIntervalKeyword %precedence interval %precedence lowerThanStringLitToken %precedence stringLit ... %right assignmentEq %left pipes or pipesAsOr %left xor %left andand and %left between ... parser.y文件的第二部分是SQL语法的产生式和每个规则对应的 aciton 。SQL语法非常复杂，parser.y 的大部分内容都是产生式的定义。\nSQL 语法可以参照MySQL参考手册的SQL Statement Syntax 部分，例如 SELECT 语法的定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 SELECT [ALL | DISTINCT | DISTINCTROW ] [HIGH_PRIORITY] [STRAIGHT_JOIN] [SQL_SMALL_RESULT] [SQL_BIG_RESULT] [SQL_BUFFER_RESULT] [SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS] select_expr [, select_expr ...] [FROM table_references [PARTITION partition_list] [WHERE where_condition] [GROUP BY {col_name | expr | position} [ASC | DESC], ... [WITH ROLLUP]] [HAVING where_condition] [ORDER BY {col_name | expr | position} [ASC | DESC], ...] [LIMIT {[offset,] row_count | row_count OFFSET offset}] [PROCEDURE procedure_name(argument_list)] [INTO OUTFILE \u0026#39;file_name\u0026#39; [CHARACTER SET charset_name] export_options | INTO DUMPFILE \u0026#39;file_name\u0026#39; | INTO var_name [, var_name]] [FOR UPDATE | LOCK IN SHARE MODE]] 我们可以在 parser.y 中找到 SELECT 语句的产生式：\n1 2 3 4 5 6 7 8 9 SelectStmt: \u0026#34;SELECT\u0026#34; SelectStmtOpts SelectStmtFieldList OrderByOptional SelectStmtLimit SelectLockOpt { ... } | \u0026#34;SELECT\u0026#34; SelectStmtOpts SelectStmtFieldList FromDual WhereClauseOptional SelectStmtLimit SelectLockOpt { ... } | \u0026#34;SELECT\u0026#34; SelectStmtOpts SelectStmtFieldList \u0026#34;FROM\u0026#34; TableRefsClause WhereClauseOptional SelectStmtGroup HavingClause OrderByOptional SelectStmtLimit SelectLockOpt { ... } 产生式 SelectStmt 和 SELECT 语法是对应的。\n我省略了大括号中的 action ，这部分代码会构建出 AST 的 ast.SelectStmt 节点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 type SelectStmt struct { dmlNode resultSetNode // SelectStmtOpts wraps around select hints and switches. *SelectStmtOpts // Distinct represents whether the select has distinct option. Distinct bool // From is the from clause of the query. From *TableRefsClause // Where is the where clause in select statement. Where ExprNode // Fields is the select expression list. Fields *FieldList // GroupBy is the group by expression list. GroupBy *GroupByClause // Having is the having condition. Having *HavingClause // OrderBy is the ordering expression list. OrderBy *OrderByClause // Limit is the limit clause. Limit *Limit // LockTp is the lock type LockTp SelectLockType // TableHints represents the level Optimizer Hint TableHints []*TableOptimizerHint } 可以看出，ast.SelectStmt 结构体内包含的内容和 SELECT 语法也是一一对应的。\n其他的产生式也都是根据对应的 SQL 语法来编写的。从 parser.y 的注释看到，这个文件最初是用工具从 BNF 转化生成的，从头手写这个规则文件，工作量会非常大。\n完成了语法规则文件 parser.y 的定义，就可以使用 goyacc 生成语法解析器：\n1 bin/goyacc -o parser/parser.go parser/parser.y 2\u0026gt;\u0026amp;1 TiDB对 lexer 和 parser.go 进行了封装，对外提供 parser.yy_parser 进行SQL语句的解析：\n1 2 3 4 // Parse parses a query string to raw ast.StmtNode. func (parser *Parser) Parse(sql, charset, collation string) ([]ast.StmtNode, error) { ... } 最后，我写了一个简单的例子，使用TiDB的 SQL Parser 进行SQL语法解析，构建出 AST，然后利用 visitor 遍历 AST ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/pingcap/tidb/parser\u0026#34; \u0026#34;github.com/pingcap/tidb/ast\u0026#34; ) type visitor struct{} func (v *visitor) Enter(in ast.Node) (out ast.Node, skipChildren bool) { fmt.Printf(\u0026#34;%T\\n\u0026#34;, in) return in, false } func (v *visitor) Leave(in ast.Node) (out ast.Node, ok bool) { return in, true } func main() { sql := \u0026#34;SELECT /*+ TIDB_SMJ(employees) */ emp_no, first_name, last_name \u0026#34; + \u0026#34;FROM employees USE INDEX (last_name) \u0026#34; + \u0026#34;where last_name=\u0026#39;Aamodt\u0026#39; and gender=\u0026#39;F\u0026#39; and birth_date \u0026gt; \u0026#39;1960-01-01\u0026#39;\u0026#34; sqlParser := parser.New() stmtNodes, err := sqlParser.Parse(sql, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) if err != nil { fmt.Printf(\u0026#34;parse error:\\n%v\\n%s\u0026#34;, err, sql) return } for _, stmtNode := range stmtNodes { v := visitor{} stmtNode.Accept(\u0026amp;v) } } 我实现的 visitor 什么也没干，只是输出了节点的类型。 这段代码的运行结果如下，依次输出遍历过程中遇到的节点类型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 *ast.SelectStmt *ast.TableOptimizerHint *ast.TableRefsClause *ast.Join *ast.TableSource *ast.TableName *ast.BinaryOperationExpr *ast.BinaryOperationExpr *ast.BinaryOperationExpr *ast.ColumnNameExpr *ast.ColumnName *ast.ValueExpr *ast.BinaryOperationExpr *ast.ColumnNameExpr *ast.ColumnName *ast.ValueExpr *ast.BinaryOperationExpr *ast.ColumnNameExpr *ast.ColumnName *ast.ValueExpr *ast.FieldList *ast.SelectField *ast.ColumnNameExpr *ast.ColumnName *ast.SelectField *ast.ColumnNameExpr *ast.ColumnName *ast.SelectField *ast.ColumnNameExpr *ast.ColumnName 了解了TiDB SQL Parser 的实现，我们就有可能实现TiDB当前不支持的语法，例如添加内置函数，也为我们学习查询计划以及优化打下了基础。希望这篇文章对你能有所帮助。\n","date":"2018-05-09T14:34:58+08:00","permalink":"https://mazhen.tech/p/tidb-sql-parser-%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"TiDB SQL Parser 的实现"},{"content":"配合这篇《基于代价的优化》 阅读。\nCBO的整体思路是：从逻辑查询计划树，自上而下枚举每个逻辑运算符可能的物理算子，从所有可能的执行路径中选择一条评估代价最小的作为物理查询计划。\n一个逻辑运算符受两个因素的影响，导致生成多个候选的物理执行计划：\n逻辑运算符可能有多种候选的物理算子供选择，如下表： 有些物理算子会根据参与运算的属性、属性的顺序等因素，生成多种物理执行计划，例如Join的物理算子会根据参与连接的表的顺序，生成多种可能的执行计划。 CBO核心流程的代码在plan/optimizer.go中的physicalOptimize：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func physicalOptimize(logic LogicalPlan) (PhysicalPlan, error) { logic.preparePossibleProperties() _, err := logic.deriveStats() if err != nil { return nil, errors.Trace(err) } t, err := logic.findBestTask(\u0026amp;requiredProp{taskTp: rootTaskType, expectedCnt: math.MaxFloat64}) if err != nil { return nil, errors.Trace(err) } p := t.plan() p.ResolveIndices() return p, nil } 三行关键的代码：\nlogic.preparePossibleProperties()：裁剪参与运算的属性，从而尽可能早的裁减掉成物理计划搜索路径上的分支 logic.deriveStats()：为每个逻辑计划节点生成统计信息，为评估物理计划的代价做准备 logic.findBestTask：生成执行代价最小的task findBestTask的核心逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 for _, pp := range p.self.exhaustPhysicalPlans(prop) { // find best child tasks firstly. childTasks = childTasks[:0] for i, child := range p.children { childTask, err := child.findBestTask(pp.getChildReqProps(i)) if err != nil { return nil, errors.Trace(err) } childTasks = append(childTasks, childTask) } // combine best child tasks with parent physical plan. curTask := pp.attach2Task(childTasks...) // get the most efficient one. if curTask.cost() \u0026lt; bestTask.cost() { bestTask = curTask } } 首先枚举可能的物理执行计划p.self.exhaustPhysicalPlans，然后遍历每种候选计划，找到代价最小的task。这是个递归的过程，当前节点的代价是由所有子节点的代价组成的，所以在遍历的过程中，又会调用child.findBestTask(pp.getChildReqProps(i))找到子节点的最佳task。\n如何评估物理执行计划的代价呢？根据参与运算的关系（表）的统计信息进行评估。代价评估相关逻辑涉及的代码：\n计算关系的统计信息：plan/stats.go 计算task的代价：plan/task.go中的attach2Task系列方法。 ","date":"2018-05-08T14:40:17+08:00","permalink":"https://mazhen.tech/p/%E5%9F%BA%E4%BA%8E%E4%BB%A3%E4%BB%B7%E4%BC%98%E5%8C%96cbo%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81%E5%AF%BC%E8%AF%BB/","title":"基于代价优化（CBO）实现代码导读"},{"content":"Go的错误处理机制很简洁，使用errors.New(text)创建 error，方法的调用者一般按照如下模式处理：\n1 2 3 if err != nil { return err } 这样做最大的问题是error中没有保存方法调用栈等上下文信息，只能靠创建时传递的string参数来区分error，很难定位错误发生的具体位置。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import ( \u0026#34;fmt\u0026#34; \u0026#34;errors\u0026#34; ) func f1() error { return f2() } func f2() error { return f3() } func f3() error { return errors.New(\u0026#34;std error\u0026#34;) } func main() { if err := f1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) } } 执行的输出为：\n1 std error 在实际的程序中调用关系复杂，仅凭错误信息很难定位错误源头。TiDB 使用了juju/errors来记录调用栈：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import ( \u0026#34;github.com/juju/errors\u0026#34; \u0026#34;fmt\u0026#34; ) func jf1() error { err := jf2() if err != nil { return errors.Trace(err) } return nil } func jf2() error { err := jf3() if err != nil { return errors.Trace(err) } return nil } func jf3() error { return errors.New(\u0026#34;juju error\u0026#34;) } func main() { if err := jf1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) } } 这段代码的输出为：\n1 2 3 github.com/mz1999/error/main.go:25: juju error github.com/mz1999/error/main.go:19: github.com/mz1999/error/main.go:11: 可以看到，如果想记录调用栈，每次都需要调用errors.Trace。这样做比较繁琐，而且每次trace时内部都会调用runtime.Caller，性能不佳。TiDB已经调研了新的第三方包pkg/errors准备替换掉juju/errors。\n使用pkg/errors会简单很多，和标准库的errors一致，但可以记录调用栈信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/pkg/errors\u0026#34; ) func pf1() error { return pf2() } func pf2() error { return pf3() } func pf3() error { return errors.New(\u0026#34;pkg error\u0026#34;) } func main() { if err := pf1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) } } 这段代码的输出为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 pkg error main.pf3 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:17 main.pf2 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:13 main.pf1 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:9 main.main /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:21 runtime.main /usr/local/go/src/runtime/proc.go:198 runtime.goexit /usr/local/go/src/runtime/asm_amd64.s:2361 这样看，使用pkg/errors替代标准库的errors就可以满足我们的需求。\n另外，pkg/errors的作者还给了一些最佳实践的建议：\n在你自己的代码中，在错误的发生点使用errors.New 或 errors.Errorf ： 1 2 3 4 5 6 func parseArgs(args []string) error { if len(args) \u0026lt; 3 { return errors.Errorf(\u0026#34;not enough arguments, expected at least 3, got %d\u0026#34;, len(args)) } // ... } 如果你接收到一个error，一般简单的直接返回： 1 2 3 if err != nil { return err } 如果你是调用第三方的包或标准库时接收到error，使用 errors.Wrap or errors.Wrapf 包装这个error，它会记录在这个点的调用栈： 1 2 3 4 f, err := os.Open(path) if err != nil { return errors.Wrapf(err, \u0026#34;failed to open %q\u0026#34;, path) } Always return errors to their caller rather than logging them throughout your program.\n在程序的top level，或者是worker goroutine，使用 %+v 输出error的详细信息。\n1 2 3 4 5 6 7 func main() { err := app.Run() if err != nil { fmt.Printf(\u0026#34;FATAL: %+v\\n\u0026#34;, err) os.Exit(1) } } 如果需要抛出包含MySQL错误码的内部错误，可以使用errors.Wrap包装，附带上报错位置的调用栈信息： 1 2 3 func pf3() error { return errors.Wrap(mysql.NewErr(mysql.ErrCantCreateTable, \u0026#34;tablename\u0026#34;, 500), \u0026#34;\u0026#34;) } 这样，我们既拿到了完整调用栈，又可以使用errors.Cause获取MySQL的错误码等信息：\n1 2 3 4 5 6 7 8 9 10 11 12 if err := pf1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) var sqlError *mysql.SQLError if m, ok := errors.Cause(err).(*mysql.SQLError); ok { sqlError = m } else { sqlError = mysql.NewErrf(mysql.ErrUnknown, \u0026#34;%s\u0026#34;, err.Error()) } fmt.Printf(\u0026#34;\\nMySQL error code: %d, state: %s\u0026#34;, sqlError.Code, sqlError.State) } ","date":"2018-04-06T14:23:40+08:00","permalink":"https://mazhen.tech/p/golang-error%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/","title":"Golang error处理实践"},{"content":"Go中的引用类型不是指针，而是对指针的包装，在它的内部通过指针引用底层数据结构。每一种引用类型也包含一些其他的field，用来管理底层的数据结构。\n看一个例子比较直观：\n1 2 s := []string{\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;} fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) 简单解释一下这段代码。先初始化一个slice，然后使用unsafe.Pointer(\u0026amp;s)把slice的指针转换为通用指针Pointer。Pointer是可以代表任何数据类型的指针。最后把Pointer强制转换为*reflect.SliceHeader。SliceHeader代表的是slice运行时数据结构，定义如下：\n1 2 3 4 5 type SliceHeader struct { Data uintptr Len int Cap int } 可以看到，SliceHeader内部有一个用来指向底层数组的指针Data，另外还有两个属性Len和Cap用来保存slice的内部状态。\n上面的代码运行结果如下：\n\u0026amp;reflect.SliceHeader{Data:0xc420078180, Len:3, Cap:3}\nslice可以自动扩容，当底层数组容量不够时，会自动创建一个新的数组替换。让我们做个实验：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 s := []string{\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;} fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Println() s = append(s, \u0026#34;d\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Println() s = append(s, \u0026#34;e\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Println() 运行结果如下：\n对于初始化容量为3的slice，在向这个slice append 新元素时，底层会创建一个容量翻倍的新数组，并将原先的内容复制过来，再将新元素append到最后。我们可以看到这个slice内部保存底层数组的指针在第一次append后，指向了新的地址。当再向它append新元素时，由于底层数组还有空间，内部指针保持不变，只是更新Len属性为5。\n在Go中进行函数调用时，参数都是按值传递的。对于引用类型也是按值传递，会复制引用本身，但不会复制引用指向的底层数据结构。还是看代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func foo(s []string) { fmt.Println(\u0026#34;======= func foo =======\u0026#34;) s = append(s, \u0026#34;f\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Printf(\u0026#34;\u0026amp;s: %p \\n\u0026#34;, \u0026amp;s) fmt.Println(\u0026#34;========================\\n\u0026#34;) } func main() { ...... s = append(s, \u0026#34;e\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Printf(\u0026#34;\u0026amp;s: %p \\n\u0026#34;, \u0026amp;s) fmt.Println() foo(s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Printf(\u0026#34;\u0026amp;s: %p \\n\u0026#34;, \u0026amp;s) } 运行结果为：\n在函数调用时，传递给函数的slice进行了复制，函数的参数是一个新的slice，但slice内部指针指向的底层数组还是同一个。\n完整的示例代码在https://play.golang.org/p/qwwSuskLfCa，可以在Playground中直接运行。\nGo语言的引用类型有slice, map, channel, interface和function。技术上，string也是引用类型：\n1 2 3 4 type StringHeader struct { Data uintptr Len int } 有时候为了性能优化，可以利用[]byte和string头部结构的“部分相同”，以非安全的指针类型转换来实现类型变更，避免底层数组的复制。例如 TiDB 中就使用了这个技巧：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // String converts slice to string without copy. // Use at your own risk. func String(b []byte) (s string) { if len(b) == 0 { return \u0026#34;\u0026#34; } pbytes := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;b)) pstring := (*reflect.StringHeader)(unsafe.Pointer(\u0026amp;s)) pstring.Data = pbytes.Data pstring.Len = pbytes.Len return } // Slice converts string to slice without copy. // Use at your own risk. func Slice(s string) (b []byte) { pbytes := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;b)) pstring := (*reflect.StringHeader)(unsafe.Pointer(\u0026amp;s)) pbytes.Data = pstring.Data pbytes.Len = pstring.Len pbytes.Cap = pstring.Len return } 上面两个函数实现了[]byte和string的互相转换，不需要底层数组的copy。\n","date":"2018-03-05T14:20:18+08:00","permalink":"https://mazhen.tech/p/go%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%95%E7%94%A8%E7%B1%BB%E5%9E%8B/","title":"Go语言的引用类型"},{"content":"TiDB提供了docker compose的部署方式，可以很方便的在单机上搭建一个TiDB集群作为开发测试环境。如果修改了TiDB源码，可以使用这样方式，先在本机部署集群做一些验证。\n首先本机要安装docker和docker compose，建议参考官方文档Install Docker 和 Install Docker Compose\n下载tidb-docker-compose项目\n1 git clone https://github.com/pingcap/tidb-docker-compose.git 使用docker compose启动TiDB集群 1 cd tidb-docker-compose \u0026amp;\u0026amp; sudo docker-compose up -d 就这么简单，集群启动成功了。使用docker ps查看：\n可以看到，已经启动了三个tikv实例，一个tidb实例，三个pd实例，还有监控和tidb-vision。\n监控的访问地址是 http://localhost:3000，用户名/密码：admin/admin。\ntidb-vision 的访问地址是 http://localhost:8010\n使用MySQL客户端访问TiDB 如果本机有MySQL客户端，可以直接连接：\n1 mysql -h 127.0.0.1 -P 4000 -u root 如果本机没有MySQL客户端，可以使用docker启动一个MySQL容器，然后登录到容器内，再使用MySQL客户端连接TiDB集群。这种方式比较环保，只要有docker环境就行。先查看TiDB集群的docker网络：\n然后启动MySQL容器，注意要加入TiDB集群的docker网络：\n1 sudo docker run --network=tidbdockercompose_default --rm -it mysql /bin/bash 因为和TiDB集群在同一个docker网络，在MySQL容器内，可以使用tidb名称访问到TiDB：\n1 mysql -h tidb -P 4000 -u root 停止集群 1 sudo docker-compose down 如果自己build了TiDB版本想在本机run集群，文档写的很清楚，告诉你镜像应该放在什么位置。\nHave fun!\n","date":"2018-02-09T14:32:20+08:00","permalink":"https://mazhen.tech/p/%E5%88%A9%E7%94%A8docker-compose%E5%9C%A8%E5%8D%95%E6%9C%BA%E4%B8%8A%E7%8E%A9%E8%BD%ACtidb/","title":"利用docker compose在单机上玩转TiDB"},{"content":"翻了一下TiDB的文档，对TiDB有了个大概的了解。简单说，TiDB的实现架构是：底层是分布式KV引擎TiKV，上层是SQL引擎TiDB Servers。一般传统数据库也是这么分层实现的，只不过TiKV实现了一个分布式、强一致、支持事务的K/V，不像数据库是单机版K/V。在TiKV之上实现SQL引擎就简化了很多，因此TiDB Servers是无状态的。\n简化的抽象架构分层：\nTiDB官方文档里的架构图：\n可以看出，TiDB的基础工作和最突出的创新在TiKV，理论上有了这个KV，可以把单机版的SQl引擎实现方式搬过来，就有了一个可扩展的分布式数据库。\n那就看看TiKV的架构：用RocksDB作为单机存储引擎，然后上层用Raft实现了一个分布式、强一致性的K/V。有了这个很强大的分布式K/V，在上面实现了MVCC层，就是对每个Key加了version，然后基于MVCC层最终实现了分布式事务。\nRocksDB内部用的是LSM-Tree，写入性能肯定比MySQL的B+ tree好。读取性能看实现的优化情况了，不过RocksDB是Facebook做的，应该没啥问题。\nRaft的实现和测试用例是从Etcd完全拷贝过来的，可以认为Raft的实现也是稳定的。 作者的原话：\n我们做了一件比较疯狂的事情，就是我们把 Etcd 的 Raft 状态机的每一行代码，line by line 的翻译成了 Rust。而我们第一个转的就是所有 Etcd 本身的测试用例。我们写一模一样的 test ，保证这个东西我们 port 的过程是没有问题的。\n分布式事务参照的是Percolator。Percolator和Spanner差不多，只不过Spanner引入了专有硬件原子钟，而Percolator依靠单点的授时服务器。两者都是对两阶段提交协议的改进。我们搞过J2EE，对两阶段提交协议应该比较熟悉，2PC的问题是：一旦事务参与者投票，它必须等待coordinator给出指示：提交或放弃。如果这时coordinator挂了，事务参与者除了等待什么也做不了。事务处于未决状态，事务的最终结果记录在coordinator的事务日志中，只能等它recovery（HeuristicCommitException、HeuristicMixedException、HeuristicRollbackException等异常就是遇到了这种情况，只好资源自己做了决定）。这么看在本质上，2PC为了达到一致性，实际上是退化到由coordinator单节点来实现atomic commit. Spanner引入了trueTime api，底下存储是MVCC，每行数据都带一个时间戳做version，TrueTime API就是打时间戳的，用时间戳标识事务顺序，解决2PC依赖单点coordinator的问题。而依赖单点的授时服务器的问题，他们是这样解释的：\n因为 TSO 的逻辑极其简单，只需要保证对于每一个请求返回单调递增的 id 即可，通过一些简单的优化手段（比如 pipeline）性能可以达到每秒生成百万 id 以上，同时 TSO 本身的高可用方案也非常好做，所以整个 Percolator 模型的分布式程度很高。\nTiDB的事务隔离级别实现了Read committed和Repeatable read，没有实现最严格的Serializable。不过串行化的隔离级别在现实中很少使用，性能会很差。oracle 11g也没有实现它。oracle实现的是snapshot isolation，实际上比串行化的保证要弱。TiDB和oracle都用是MVCC保证了Repeatable read，简单说就是每个事务都读取一个一致性的snapshot，这个snapshot肯定就是完整状态。所以叫做snapshot isolation。按照TiDB的文档，TiDB 实现的 snapshot 隔离级别，该隔离级别不会出现幻读，但是会出现写偏斜。\n写偏斜是什么，举个简单的例子：两个事务都先分别查询在线值班的医生总数，发现还有两个在线的医生，然后各自更新不同的记录，分别让不同的医生下线。事务提交后，两个医生都下线了，没有一个医生在线值班，出现错误的业务场景。这种异常情况是两个事务分别更新不同的记录。引起写倾斜的的模式：先查询很多列看是否满足某种条件，然后依赖查询结果写入数据并提交。解决的方法有：真正的串行化隔离级别，或者显示的锁定事务依赖的行。\n从文档看，TiDB利用了成熟的开源项目，自己实现了分布式事务、分布式存储和SQL引擎，整体方案诱人，至于软件成熟程度，还需要经过实际的使用测试。\n","date":"2018-02-09T14:28:29+08:00","permalink":"https://mazhen.tech/p/tidb%E5%88%9D%E6%8E%A2/","title":"TiDB初探"},{"content":"Zion项目我们采用Feature Branch Workflow，即每个特性在branch中开发，master始终保持稳定。特性开发完成，需提交pull request，接受其他成员的code review，同时可以在PR中围绕该特性进行讨论，PR记录了开发过程的细节。\n由于是内部项目，我们没有使用fork机制，代码都维护在Github上的一个仓库：apusic/zion。在看具体的流程前，先有一个全局视图：\n基本工作流程 从远程clone respository 1 git clone https://github.com/apusic/zion.git 创建特性分支 首先让本地的master处于最新状态：\n1 2 3 git fetch git checkout master git rebase origin/master 创建分支\n1 git checkout -b myfeature 在分支上进行开发 1 2 3 git status # View the state of the repo git add # Stage a file git commit # Commit a file 进行一个功能特性开发时，可以多次提交到本地仓库Repository，不必每次commit都push到远程仓库Remote。\n开发过程中，保持分支和最新代码同步 1 2 3 # While on your myfeature branch. git fetch git rebase origin/master 关于rebase的详细说明，请参考 The \u0026ldquo;Git Branching - Rebasing\u0026rdquo; chapter from the Pro Git book.\n后面会单独介绍rebase冲突的处理。\n将分支发布到中心仓库 所有改动都提交后，执行：\n1 git push -f origin myfeature 创建pull request 访问项目主页，点击Compare \u0026amp; pull request创建pull request\ncode review \u0026amp; discussions 可以要求一个或两个项目成员进行review，也可以围绕该特性进行讨论。\nMerge pull requests 根据项目的配置，pull requests在merge进master之前要满足一些条件，例如至少两个成员review，通过集成测试等。\n所有的检查都通过后，这个pull request就可以merge了。详细的操作参见 Merging a pull request\n这里有三个merge选型：Merge pull request，Squash and merge，Rebase and merge，关于它们的区别请参考GitHub的帮助。 一般建议选择Squash and merge。\n至此，一个特性就开发完成了。\n删除分支 已经完成merged的 pull requests 关联的分支可以在GitHub删除，详细的操作步骤参见GitHub文档 Deleting and restoring branches in a pull request\n冲突处理 上面的流程中，在保持分支和最新代码同步时，最有可能产生冲突。\nrebase提示冲突，会列出冲突文件，执行下列步骤：\n手工解决冲突 git add \u0026lt;some-file\u0026gt; 将发生冲突的文件放回index区 git rebase --continue 继续进行rebase 提示rebase成功 在Merge pull requests过程中也可能产生冲突，可以在GitHub的界面上解决冲突，详细的操作轻参考Addressing merge conflicts。\n如果冲突较多，建议先在客户端执行rebase，按照上面的步骤解决完冲突，再进行Merge pull requests。\n","date":"2018-01-15T14:38:05+08:00","permalink":"https://mazhen.tech/p/git-feature-branch-workflow/","title":"Git Feature Branch Workflow"},{"content":"Wireshark是排查网络问题最常用的工具，它已经内置支持了上百种通用协议，同时它的扩展性也很好，对于自定义的应用层网络协议，你可以使用c或者lua编写协议解析插件，这样你就可以在Wireshark中观察到协议的内容而不是二进制流，为排查问题带来一定的便利性。\n最近在排查一个HSF超时的问题，顺便花了些时间为Wireshark写了一个HSF2协议解析插件，目前支持HSF2的request、response和heart beat协议，支持将多个packet还原为上层PDU。暂不支持HSF原先的TB Remoting协议。先看效果。\n首先在Packet List区域已经能识别HSF2协议：\nHSF的请求和响应\nHSF的心跳协议\n点击某个数据包，可以在Packet details区域查看详细的协议内容： HSF请求\n可以看到很多协议的重要信息，包括序列化方式，超时时间，服务名称、方法及参数\nHSF响应\nHeartBeat请求\n心跳协议比较简单，响应就不看了。\n插件是使用lua开发的，安装比较简单，以OS X平台为例：\n将协议解析脚本copy到/Applications/Wireshark.app/Contents/Resources/share/wireshark/ 目录\n编辑init.lua文件，设置disable_lua = false，确保lua支持打开\n在init.lua文件末尾增加\n1 dofile(\u0026#34;hsf2.lua\u0026#34;) 再次启动Wireshark，会对12200端口的数据流使用脚本解析，已经可以识别HSF协议了。\n备注\n附上hsf2.lua，边翻HSF代码边写的，写完眼已经花了，错误难免，欢迎试用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 -- declare the protocol hsf2_proto = Proto(\u0026#34;hsf2\u0026#34;, \u0026#34;Taobao HSF2 Protocol\u0026#34;) -- declare the value strings local vs_id = { [12] = \u0026#34;HSF2 Heart Beat\u0026#34;, [13] = \u0026#34;HSF2 TB Remoting\u0026#34;, [14] = \u0026#34;HSF2 HSF Remoting\u0026#34; } local vs_version = { [1] = \u0026#34;HSF2\u0026#34; } local vs_op = { [0] = \u0026#34;request\u0026#34;, [1] = \u0026#34;response\u0026#34; } local vs_codectype = { [1] = \u0026#34;HESSIAN_CODEC\u0026#34;, [2] = \u0026#34;JAVA_CODEC\u0026#34;, [3] = \u0026#34;TOP_CODEC\u0026#34;, [4] = \u0026#34;HESSIAN2_CODEC\u0026#34;, [5] = \u0026#34;KRYO_CODEC\u0026#34;, [6] = \u0026#34;JSON_CODEC\u0026#34;, [7] = \u0026#34;CUSTOMIZED_CODEC\u0026#34;, } local vs_responsestatus = { [20] = \u0026#34;OK\u0026#34;, [30] = \u0026#34;client timeout\u0026#34;, [31] = \u0026#34;server timeout\u0026#34;, [40] = \u0026#34;bad request\u0026#34;, [50] = \u0026#34;bad response\u0026#34;, [60] = \u0026#34;service not found\u0026#34;, [70] = \u0026#34;service error\u0026#34;, [80] = \u0026#34;server error\u0026#34;, [90] = \u0026#34;client error\u0026#34;, [91] = \u0026#34;Unknow error\u0026#34;, [81] = \u0026#34;Thread pool is busy\u0026#34;, [82] = \u0026#34;Communication error\u0026#34;, [88] = \u0026#34;server will close soon\u0026#34;, [10] = \u0026#34;server send coders\u0026#34;, [83] = \u0026#34;Unkown code\u0026#34; } -- declare the fields local f_id = ProtoField.uint8(\u0026#34;hsf2.id\u0026#34;, \u0026#34;Identification\u0026#34;, base.Dec, vs_id) local f_version = ProtoField.uint8(\u0026#34;hsf2.version\u0026#34;, \u0026#34;version\u0026#34;, base.Dec, vs_version) local f_op = ProtoField.uint8(\u0026#34;hsf2.op\u0026#34;, \u0026#34;operation\u0026#34;, base.DEC, vs_op) local f_codectype = ProtoField.uint8(\u0026#34;hsf2.codectype\u0026#34;, \u0026#34;codectype\u0026#34;, base.DEC, vs_codectype) local f_reserved = ProtoField.uint8(\u0026#34;hsf2.reserved\u0026#34;, \u0026#34;reserved\u0026#34;, base.DEC) local f_req_id = ProtoField.uint64(\u0026#34;hsf2.req_id\u0026#34;, \u0026#34;RequestID\u0026#34;, base.DEC) local f_timeout = ProtoField.uint32(\u0026#34;hsf2.timeout\u0026#34;, \u0026#34;timeout\u0026#34;, base.DEC) local f_service_name_len = ProtoField.uint32(\u0026#34;hsf2.service_name_len\u0026#34;, \u0026#34;Service Name length\u0026#34;, base.DEC) local f_method_name_len = ProtoField.uint32(\u0026#34;hsf2.method_name_len\u0026#34;, \u0026#34;Method Name length\u0026#34;, base.DEC) local f_arg_count = ProtoField.uint32(\u0026#34;hsf2.arg.count\u0026#34;, \u0026#34;Argument Count\u0026#34;, base.DEC) local f_arg_type_len = ProtoField.uint32(\u0026#34;hsf2.arg.type.len\u0026#34;, \u0026#34;Argument Type length\u0026#34;, base.DEC) local f_arg_obj_len = ProtoField.uint32(\u0026#34;hsf2.arg.obj.len\u0026#34;, \u0026#34;Argument Object length\u0026#34;, base.DEC) local f_req_prop_len = ProtoField.uint32(\u0026#34;hsf2.req.prop.len\u0026#34;, \u0026#34;Request Prop Length\u0026#34;, base.DEC) local f_service_name = ProtoField.string(\u0026#34;hsf2.service.name\u0026#34;, \u0026#34;Service Name\u0026#34;) local f_method_name = ProtoField.string(\u0026#34;hsf2.method.name\u0026#34;, \u0026#34;Method Name\u0026#34;) local f_arg_type = ProtoField.string(\u0026#34;hsf2.arg.type\u0026#34;, \u0026#34;Argument Type\u0026#34;) local f_arg_obj = ProtoField.bytes(\u0026#34;hsf2.arg.obj\u0026#34;, \u0026#34;Argument Object\u0026#34;) local f_req_prop = ProtoField.bytes(\u0026#34;hsf2.req.prop\u0026#34;, \u0026#34;Request Prop\u0026#34;) local f_response_status = ProtoField.uint32(\u0026#34;hsf2.response.status\u0026#34;, \u0026#34;Response Status\u0026#34;, base.DEC, vs_responsestatus) local f_response_body_len = ProtoField.uint32(\u0026#34;hsf2.response.body.len\u0026#34;, \u0026#34;Response Body Length\u0026#34;, base.DEC) local f_response_body = ProtoField.bytes(\u0026#34;hsf2.response.body\u0026#34;, \u0026#34;Response Body\u0026#34;, base.DEC) hsf2_proto.fields = { f_id, f_version, f_op, f_codectype, f_reserved, f_req_id, f_timeout, f_service_name_len, f_method_name_len, f_arg_count, f_arg_type_len, f_arg_obj_len, f_req_prop_len, f_service_name, f_method_name, f_arg_type, f_arg_obj, f_req_prop, f_response_status, f_response_body_len, f_response_body } function get_pdu_length(buffer) local offset = 0 local id = buffer(offset, 1):uint() offset = offset + 1 -- heart beat if id == 12 then return 18 end -- TB REMOTING if id == 13 then -- TODO return 18 end -- HSF REMOTING if id == 14 then local version = buffer(offset, 1):uint() offset = offset + 1 local op = buffer(offset, 1):uint() offset = offset + 1 -- request if op == 0 then local service_name_len = buffer(19, 4):uint() local method_name_len = buffer(23,4):uint() local arg_count = buffer(27,4):uint() offset = 27 + 4 local arg_content_len = 0 for i = 1, arg_count do arg_content_len = arg_content_len + buffer(offset,4):uint() offset = offset + 4 end for i = 1, arg_count do arg_content_len = arg_content_len + buffer(offset,4):uint() offset = offset + 4 end local req_prop_len = buffer(offset,4):uint() local len = 30 + arg_count*4*2 + 5 + service_name_len + method_name_len + arg_content_len + req_prop_len return len end -- response if op == 1 then local body_len = buffer(16, 4):uint() return 20 + body_len end end end -- create the dissection function function hsf2_proto.dissector(buffer, pinfo, tree) -- check the protocol -- TODO support TB Remoting local check_proto = buffer(0, 1):uint() if check_proto \u0026lt; 12 or check_proto \u0026gt; 14 or check_proto == 13 then return end -- Set the protocol column pinfo.cols[\u0026#39;protocol\u0026#39;] = \u0026#34;HSF2\u0026#34; -- Reassembling packets into one PDU local pdu_len = get_pdu_length(buffer) if pdu_len \u0026gt; buffer:len() then pinfo.desegment_len = pdu_len - buffer:len() pinfo.desegment_offset = 0 return end -- create the HSF2 protocol tree item local t_hsf2 = tree:add(hsf2_proto, buffer()) local offset = 0 local id = buffer(offset, 1):uint() offset = offset + 1 t_hsf2:add(f_id, id) -- heart beat if id == 12 then local op = buffer(offset, 1):uint() offset = offset + 1 t_hsf2:add(f_op, op) -- Set the info column to the name of the function local info = vs_id[id]..\u0026#34;:\u0026#34;..vs_op[op] pinfo.cols[\u0026#39;info\u0026#39;] = info t_hsf2:add(f_version, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_req_id, buffer(offset, 8)) offset = offset + 8 t_hsf2:add(f_timeout, buffer(offset, 4)) end -- TB REMOTING if id == 13 then -- TODO end -- HSF REMOTING if id == 14 then t_hsf2:add(f_version, buffer(offset, 1)) offset = offset + 1 local op = buffer(offset, 1):uint() offset = offset + 1 t_hsf2:add(f_op, op) -- Set the info column to the name of the function local info = vs_id[id]..\u0026#34;:\u0026#34;..vs_op[op] pinfo.cols[\u0026#39;info\u0026#39;] = info -- request if op == 0 then t_hsf2:add(f_codectype, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_req_id, buffer(offset, 8)) offset = offset + 8 t_hsf2:add(f_timeout, buffer(offset, 4)) offset = offset + 4 local service_name_len = buffer(offset, 4):uint() t_hsf2:add(f_service_name_len, service_name_len) offset = offset + 4 local method_name_len = buffer(offset,4):uint() t_hsf2:add(f_method_name_len, method_name_len) offset = offset + 4 local arg_count = buffer(offset,4):uint() t_hsf2:add(f_arg_count, arg_count) offset = offset + 4 local arg_type_len_array = {} for i = 1, arg_count do arg_type_len_array[i] = buffer(offset, 4):uint(); offset = offset + 4 t_hsf2:add(f_arg_type_len, arg_type_len_array[i]) end local arg_obj_len_array = {} for i = 1, arg_count do arg_obj_len_array[i] = buffer(offset, 4):uint(); offset = offset + 4 t_hsf2:add(f_arg_obj_len, arg_obj_len_array[i]) end local prop_len = buffer(offset, 4):uint(); offset = offset + 4 t_hsf2:add(f_req_prop_len, prop_len) t_hsf2:add(f_service_name, buffer(offset, service_name_len)) offset = offset + service_name_len t_hsf2:add(f_method_name, buffer(offset, method_name_len)) offset = offset + method_name_len for i = 1, #arg_type_len_array do t_hsf2:add(f_arg_type, buffer(offset, arg_type_len_array[i])) offset = offset + arg_type_len_array[i] end for i = 1, #arg_obj_len_array do t_hsf2:add(f_arg_obj, buffer(offset, arg_obj_len_array[i])) offset = offset + arg_obj_len_array[i] end if prop_len \u0026gt; 0 then t_hsf2:add(f_req_prop, buffer(offset, prop_len)) end end -- response if op == 1 then t_hsf2:add(f_response_status, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_codectype, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_req_id, buffer(offset, 8)) offset = offset + 8 local body_len = buffer(offset, 4):uint() t_hsf2:add(f_response_body_len, body_len) offset = offset + 4 t_hsf2:add(f_response_body, buffer(offset, body_len)) end end end -- load the tcp port table tcp_table = DissectorTable.get(\u0026#34;tcp.port\u0026#34;) -- register the protocol to port 12200 tcp_table:add(12200, hsf2_proto) ","date":"2014-12-21T14:15:17+08:00","permalink":"https://mazhen.tech/p/%E4%B8%BAwireshark%E7%BC%96%E5%86%99hsf2%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E6%8F%92%E4%BB%B6/","title":"为Wireshark编写HSF2协议解析插件"},{"content":"DNS（Domain Name System）简单说就是一个名称到IP地址的映射，使用容易记住的域名代替IP地址。基本原理就不讲了，网上的文章很多。\n了解了基本原理，你就可以使用dig(Domain Information Groper）命令进行探索。当对淘宝的几个域名进行dig时，你发现事情并不像想像的那么简单。\n为了使大家的输出一致，我在dig命令中显示的指定了DNS服务器（@222.172.200.68 云南电信DNS）。\ndig @222.172.200.68 login.alibaba-inc.com dig @222.172.200.68 guang.taobao.com dig @222.172.200.68 item.taobao.com 你会发现对于域名的查询，都不是直接返回IP地址这么简单，而是经过了神奇的CNAME。一般文档在介绍CNAME时只是说可以给一个域名指定别名（alias），其实这是DNS运维非常重要的手段，使得DNS配置具有一定的灵活性和可扩展性。结合上面三个域名的解析说一下。先给一张高大上的图，是按照我自己的理解画的，不一定完全正确:)\n图上分了三层，最上层是常规的DNS解析过程，用户通过local DNS做递归查询，最终定位到taobao.com权威DNS服务器。\n中间层可以称为GSLB（Global Server Load Balancing），作用是提供域名的智能解析，根据一定的策略返回结果。淘系目前有三套GSLB：\nF5 GTM：F5的硬件设备，基本已经被淘汰，全部替换为自研软件。GTM功能强大，但对用户而言是黑盒，性能一般价格昂贵。早期淘宝CDN智能调度就是基于F5 GTM做的。 ADNS：阿里自研权威DNS，替换GTM。ADNS很牛逼，可惜资料太少。 Pharos：阿里CDN的大脑，实现CDN流量精确，稳定，安全的调度。 taobao.com权威DNS服务器会根据不用的域名，CNAME到不同的GSLB做智能调度。CNAME的作用有点类似请求分发，taobao.com权威DNS服务器将域名解析请求转交给下一层域名服务器处理。\n最下层是应用层，提供真正的服务。\n现在再看看这三个域名的解析。\nlogin.alibaba-inc.com 被转交给了GTM做智能解析，GTM通过返回不同机房的VIP做流量调度，用户的请求最终经过LVS到达我们的应用。\nguang.taobao.com的解析过程和login.alibaba-inc.com类似，只不过智能调度换成了ADNS。\nitem.taobao.com有点小复杂。我们都知道CDN是做静态资源加速的，像这样的静态资源域名img04.taobaocdn.com会由Pharos解析调度，为用户返回就近的CDN节点。但什么时候动态内容也经过CDN代理了？这就是高大上的统一接入层。简单说下过程：item.taobao.com通过Pharos的智能调度，返回给用户就近的CDN节点。当用户的请求到达CDN节点时，这个节点会为动态内容的域名选择合适的后端服务，相当于每次都做回源处理。这个CDN节点可以理解为用户请求的代理。CDN在选择后端服务时，会执行单元化、小淘宝等逻辑，将请求发送到正确的机房。请求到达机房后，先进入统一接入层，注意这里的后端应用不需要申请VIP，IP地址列表保存在VIPServer中。统一接入层从VIPServer中拿到后端应用的IP地址列表，进行请求分发。VIPServer的作用类似HSF的ConfigServer，可以大大减少应用VIP的数量。以后做单元化部署的域名都会接入统一接入层，将单元化的逻辑上推到了CDN节点。\n貌似是讲清楚了，不过这个过程已经做了很大的简化，因为我也仅仅是了解个大概。作为业务开发重点关注的是最下面的Java应用，转岗到技术保障后，才发现有机会可以从全局了解网站架构，接触到网络、DNS、CDN、LVS\u0026amp;VIP等等基础设施。\n","date":"2014-09-10T11:04:27+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8E%E5%BC%80%E5%8F%91%E8%A7%92%E5%BA%A6%E7%9C%8Bdns/","title":"从开发角度看DNS"},{"content":"在Linux上做网络应用的性能优化时，一般都会对TCP相关的内核参数进行调节，特别是和缓冲、队列有关的参数。网上搜到的文章会告诉你需要修改哪些参数，但我们经常是知其然而不知其所以然，每次照抄过来后，可能很快就忘记或混淆了它们的含义。本文尝试总结TCP队列缓冲相关的内核参数，从协议栈的角度梳理它们，希望可以更容易的理解和记忆。注意，本文内容均来源于参考文档，没有去读相关的内核源码做验证，不能保证内容严谨正确。作为Java程序员没读过内核源码是硬伤。\n下面我以server端为视角，从连接建立、数据包接收和数据包发送这3条路径对参数进行归类梳理。\n一、连接建立 简单看下连接的建立过程，客户端向server发送SYN包，server回复SYN＋ACK，同时将这个处于SYN_RECV状态的连接保存到半连接队列。客户端返回ACK包完成三次握手，server将ESTABLISHED状态的连接移入accept队列，等待应用调用accept()。\n可以看到建立连接涉及两个队列：\n半连接队列，保存SYN_RECV状态的连接。队列长度由net.ipv4.tcp_max_syn_backlog设置\naccept队列，保存ESTABLISHED状态的连接。队列长度为min(net.core.somaxconn, backlog)。其中backlog是我们创建ServerSocket(int port,int backlog)时指定的参数，最终会传递给listen方法：\n1 2 #include \u0026lt;sys/socket.h\u0026gt; int listen(int sockfd, int backlog); 如果我们设置的backlog大于net.core.somaxconn，accept队列的长度将被设置为net.core.somaxconn\n另外，为了应对SYN flooding（即客户端只发送SYN包发起握手而不回应ACK完成连接建立，填满server端的半连接队列，让它无法处理正常的握手请求），Linux实现了一种称为SYN cookie的机制，通过net.ipv4.tcp_syncookies控制，设置为1表示开启。简单说SYN cookie就是将连接信息编码在ISN(initial sequence number)中返回给客户端，这时server不需要将半连接保存在队列中，而是利用客户端随后发来的ACK带回的ISN还原连接信息，以完成连接的建立，避免了半连接队列被攻击SYN包填满。对于一去不复返的客户端握手，不理它就是了。\n二、数据包的接收 先看看接收数据包经过的路径：\n数据包的接收，从下往上经过了三层：网卡驱动、系统内核空间，最后到用户态空间的应用。Linux内核使用sk_buff(socket kernel buffers)数据结构描述一个数据包。当一个新的数据包到达，NIC（network interface controller）调用DMA engine，通过Ring Buffer将数据包放置到内核内存区。Ring Buffer的大小固定，它不包含实际的数据包，而是包含了指向sk_buff的描述符。当Ring Buffer满的时候，新来的数据包将给丢弃。一旦数据包被成功接收，NIC发起中断，由内核的中断处理程序将数据包传递给IP层。经过IP层的处理，数据包被放入队列等待TCP层处理。每个数据包经过TCP层一系列复杂的步骤，更新TCP状态机，最终到达recv Buffer，等待被应用接收处理。有一点需要注意，数据包到达recv Buffer，TCP就会回ACK确认，既TCP的ACK表示数据包已经被操作系统内核收到，但并不确保应用层一定收到数据（例如这个时候系统crash），因此一般建议应用协议层也要设计自己的ACK确认机制。\n上面就是一个相当简化的数据包接收流程，让我们逐层看看队列缓冲有关的参数。\n网卡Bonding模式 当主机有1个以上的网卡时，Linux会将多个网卡绑定为一个虚拟的bonded网络接口，对TCP/IP而言只存在一个bonded网卡。多网卡绑定一方面能够提高网络吞吐量，另一方面也可以增强网络高可用。Linux支持7种Bonding模式：\n- `Mode 0 (balance-rr)` Round-robin策略，这个模式具备负载均衡和容错能力 - `Mode 1 (active-backup)` 主备策略，在绑定中只有一个网卡被激活，其他处于备份状态 - `Mode 2 (balance-xor)` XOR策略，通过源MAC地址与目的MAC地址做异或操作选择slave网卡 - `Mode 3 (broadcast)` 广播，在所有的网卡上传送所有的报文 - `Mode 4 (802.3ad)` IEEE 802.3ad 动态链路聚合。创建共享相同的速率和双工模式的聚合组 - `Mode 5 (balance-tlb)` Adaptive transmit load balancing - `Mode 6 (balance-alb)` Adaptive load balancing 详细的说明参考内核文档Linux Ethernet Bonding Driver HOWTO。我们可以通过cat /proc/net/bonding/bond0查看本机的Bonding模式：\n一般很少需要开发去设置网卡Bonding模式，自己实验的话可以参考这篇文档\n网卡多队列及中断绑定 随着网络的带宽的不断提升，单核CPU已经不能满足网卡的需求，这时通过多队列网卡驱动的支持，可以将每个队列通过中断绑定到不同的CPU核上，充分利用多核提升数据包的处理能力。\n首先查看网卡是否支持多队列，使用lspci -vvv命令，找到Ethernet controller项：\n如果有MSI-X， Enable+ 并且Count \u0026gt; 1，则该网卡是多队列网卡。\n然后查看是否打开了网卡多队列。使用命令cat /proc/interrupts，如果看到eth0-TxRx-0表明多队列支持已经打开：\n最后确认每个队列是否绑定到不同的CPU。cat /proc/interrupts查询到每个队列的中断号，对应的文件/proc/irq/${IRQ_NUM}/smp_affinity为中断号IRQ_NUM绑定的CPU核的情况。以十六进制表示，每一位代表一个CPU核：\n``` （00000001）代表CPU0 （00000010）代表CPU1 （00000011）代表CPU0和CPU1 ``` 如果绑定的不均衡，可以手工设置，例如：\n``` echo \u0026quot;1\u0026quot; \u0026gt; /proc/irq/99/smp_affinity echo \u0026quot;2\u0026quot; \u0026gt; /proc/irq/100/smp_affinity echo \u0026quot;4\u0026quot; \u0026gt; /proc/irq/101/smp_affinity echo \u0026quot;8\u0026quot; \u0026gt; /proc/irq/102/smp_affinity echo \u0026quot;10\u0026quot; \u0026gt; /proc/irq/103/smp_affinity echo \u0026quot;20\u0026quot; \u0026gt; /proc/irq/104/smp_affinity echo \u0026quot;40\u0026quot; \u0026gt; /proc/irq/105/smp_affinity echo \u0026quot;80\u0026quot; \u0026gt; /proc/irq/106/smp_affinity ``` Ring Buffer\nRing Buffer位于NIC和IP层之间，是一个典型的FIFO（先进先出）环形队列。Ring Buffer没有包含数据本身，而是包含了指向sk_buff（socket kernel buffers）的描述符。\n可以使用ethtool -g eth0查看当前Ring Buffer的设置：\n上面的例子接收队列为4096，传输队列为256。可以通过ifconfig观察接收和传输队列的运行状况：\nRX errors：收包总的错误数\nRX dropped: 表示数据包已经进入了Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃。\nRX overruns: overruns意味着数据包没到Ring Buffer就被网卡物理层给丢弃了，而CPU无法及时的处理中断是造成Ring Buffer满的原因之一，例如中断分配的不均匀。\n当dropped数量持续增加，建议增大Ring Buffer，使用ethtool -G进行设置。\nInput Packet Queue(数据包接收队列) 当接收数据包的速率大于内核TCP处理包的速率，数据包将会缓冲在TCP层之前的队列中。接收队列的长度由参数net.core.netdev_max_backlog设置。\nrecv Buffer recv buffer是调节TCP性能的关键参数。BDP(Bandwidth-delay product，带宽延迟积) 是网络的带宽和与RTT(round trip time)的乘积，BDP的含义是任意时刻处于在途未确认的最大数据量。RTT使用ping命令可以很容易的得到。为了达到最大的吞吐量，recv Buffer的设置应该大于BDP，即recv Buffer \u0026gt;= bandwidth * RTT。假设带宽是100Mbps，RTT是100ms，那么BDP的计算如下：\n1 BDP = 100Mbps * 100ms = (100 / 8) * (100 / 1000) = 1.25MB Linux在2.6.17以后增加了recv Buffer自动调节机制，recv buffer的实际大小会自动在最小值和最大值之间浮动，以期找到性能和资源的平衡点，因此大多数情况下不建议将recv buffer手工设置成固定值。\n当net.ipv4.tcp_moderate_rcvbuf设置为1时，自动调节机制生效，每个TCP连接的recv Buffer由下面的3元数组指定：\n1 net.ipv4.tcp_rmem = \u0026lt;MIN\u0026gt; \u0026lt;DEFAULT\u0026gt; \u0026lt;MAX\u0026gt; 最初recv buffer被设置为，同时这个缺省值会覆盖net.core.rmem_default的设置。随后recv buffer根据实际情况在最大值和最小值之间动态调节。在缓冲的动态调优机制开启的情况下，我们将net.ipv4.tcp_rmem的最大值设置为BDP。\n当net.ipv4.tcp_moderate_rcvbuf被设置为0，或者设置了socket选项SO_RCVBUF，缓冲的动态调节机制被关闭。recv buffer的缺省值由net.core.rmem_default设置，但如果设置了net.ipv4.tcp_rmem，缺省值则被\u0026lt;DEFAULT\u0026gt;覆盖。可以通过系统调用setsockopt()设置recv buffer的最大值为net.core.rmem_max。在缓冲动态调节机制关闭的情况下，建议把缓冲的缺省值设置为BDP。\n注意这里还有一个细节，缓冲除了保存接收的数据本身，还需要一部分空间保存socket数据结构等额外信息。因此上面讨论的recv buffer最佳值仅仅等于BDP是不够的，还需要考虑保存socket等额外信息的开销。Linux根据参数net.ipv4.tcp_adv_win_scale计算额外开销的大小：\nBuffer / 2tcp_adv_win_scale\n如果net.ipv4.tcp_adv_win_scale的值为1，则二分之一的缓冲空间用来做额外开销，如果为2的话，则四分之一缓冲空间用来做额外开销。因此recv buffer的最佳值应该设置为：\nBDP / (1 – 1 / 2tcp_adv_win_scale)\n三、数据包的发送 发送数据包经过的路径：\n和接收数据的路径相反，数据包的发送从上往下也经过了三层：用户态空间的应用、系统内核空间、最后到网卡驱动。应用先将数据写入TCP send buffer，TCP层将send buffer中的数据构建成数据包转交给IP层。IP层会将待发送的数据包放入队列QDisc(queueing discipline)。数据包成功放入QDisc后，指向数据包的描述符sk_buff被放入Ring Buffer输出队列，随后网卡驱动调用DMA engine将数据发送到网络链路上。\n同样我们逐层来梳理队列缓冲有关的参数。\nsend Buffer 同recv Buffer类似，和send Buffer有关的参数如下：\n1 2 3 net.ipv4.tcp_wmem = \u0026lt;MIN\u0026gt; \u0026lt;DEFAULT\u0026gt; \u0026lt;MAX\u0026gt; net.core.wmem_default net.core.wmem_max 发送端缓冲的自动调节机制很早就已经实现，并且是无条件开启，没有参数去设置。如果指定了tcp_wmem，则net.core.wmem_default被tcp_wmem的覆盖。send Buffer在tcp_wmem的最小值和最大值之间自动调节。如果调用setsockopt()设置了socket选项SO_SNDBUF，将关闭发送端缓冲的自动调节机制，tcp_wmem将被忽略，SO_SNDBUF的最大值由net.core.wmem_max限制。\nQDisc QDisc（queueing discipline ）位于IP层和网卡的ring buffer之间。我们已经知道，ring buffer是一个简单的FIFO队列，这种设计使网卡的驱动层保持简单和快速。而QDisc实现了流量管理的高级功能，包括流量分类，优先级和流量整形（rate-shaping）。可以使用tc命令配置QDisc。\nQDisc的队列长度由txqueuelen设置，和接收数据包的队列长度由内核参数net.core.netdev_max_backlog控制所不同，txqueuelen是和网卡关联，可以用ifconfig命令查看当前的大小：\n使用ifconfig调整txqueuelen的大小：\n1 ifconfig eth0 txqueuelen 2000 Ring Buffer 和数据包的接收一样，发送数据包也要经过Ring Buffer，使用ethtool -g eth0查看：\n其中TX项是Ring Buffer的传输队列，也就是发送队列的长度。设置也是使用命令ethtool -G。\nTCP Segmentation和Checksum Offloading 操作系统可以把一些TCP/IP的功能转交给网卡去完成，特别是Segmentation(分片)和checksum的计算，这样可以节省CPU资源，并且由硬件代替OS执行这些操作会带来性能的提升。\n一般以太网的MTU（Maximum Transmission Unit）为1500 bytes，假设应用要发送数据包的大小为7300bytes，MTU1500字节 － IP头部20字节 － TCP头部20字节＝有效负载为1460字节，因此7300字节需要拆分成5个segment：\nSegmentation(分片)操作可以由操作系统移交给网卡完成，虽然最终线路上仍然是传输5个包，但这样节省了CPU资源并带来性能的提升：\n可以使用ethtool -k eth0查看网卡当前的offloading情况：\n上面这个例子checksum和tcp segmentation的offloading都是打开的。如果想设置网卡的offloading开关，可以使用ethtool -K(注意K是大写)命令，例如下面的命令关闭了tcp segmentation offload：\n1 sudo ethtool -K eth0 tso off 网卡多队列和网卡Bonding模式 在数据包的接收过程中已经介绍过了。\n至此，终于梳理完毕。整理TCP队列相关参数的起因是最近在排查一个网络超时问题，原因还没有找到，产生的“副作用”就是这篇文档。再想深入解决这个问题可能需要做TCP协议代码的profile，需要继续学习，希望不久的将来就可以再写文档和大家分享了。\n参考文档\nQueueing in the Linux Network Stack TCP Implementation in Linux: A Brief Tutorial Impact of Bandwidth Delay Product on TCP Throughput Java程序员也应该知道的系统知识系列之网卡 ","date":"2014-08-16T11:11:24+08:00","permalink":"https://mazhen.tech/p/linux-tcp%E9%98%9F%E5%88%97%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E7%9A%84%E6%80%BB%E7%BB%93/","title":"Linux TCP队列相关参数的总结"},{"content":"Java中通过Socket.setSoLinger设置SO_LINGER选项，有三种组合形式：\nSocket.setSoLinger(false, linger) 设置为false，这时linger值被忽略。摘自unix network programming：\nThe default action of close with a TCP socket is to mark the socket as closed and return to the process immediately. The socket descriptor is on longer usable by the process: it can\u0026rsquo;t be used as an argument to read or write. TCP will try to send any data that is already queued to be sent to the other end, and after this occurs, the normal TCP connection termination sequence takes place.\n如果设置为false，socket主动调用close时会立即返回，操作系统会将残留在缓冲区中的数据发送到对端，并按照正常流程关闭(交换FIN-ACK），最后连接进入TIME_WAIT状态。\n我们可以写个演示程序，客户端发送较大的数据包后，立刻调用close，而server端将Receive Buffer设置的很小。close会立即返回，客户端的Java进程结束，但是当我们用tcpdump/Wireshark抓包会发现，操作系统正在帮你发送数据，内核缓冲区中的数据发送完毕后，发送FIN包关闭连接。\nSocket.setSoLinger(true, 0) TCP discards any data still remaining in the socket send buffer and sends an RST to the peer, not the normal four-packet connection termination sequence.\n主动调用close的一方也是立刻返回，但是这时TCP会丢弃发送缓冲中的数据，而且不是按照正常流程关闭连接（不发送FIN包），直接发送RST，对端会收到java.net.SocketException: Connection reset异常。同样使用tcpdump抓包可以很容易观察到。\n另外有些人会用这种方式解决主动关闭放方有大量TIME_WAIT状态连接的问题，因为发送完RST后，连接立即销毁，不会停留在TIME_WAIT状态。一般不建议这么做，除非你有合适的理由：\nIf the a client of your server application misbehaves (times out, returns invalid data, etc.) an abortive close makes sense to avoid being stuck in CLOSE_WAIT or ending up in the TIME_WAIT state. If you must restart your server application which currently has thousands of client connections you might consider setting this socket option to avoid thousands of server sockets in TIME_WAIT (when calling close() from the server end) as this might prevent the server from getting available ports for new client connections after being restarted.\nOn page 202 in the aforementioned book it specifically says: \u0026ldquo;There are certain circumstances which warrant using this feature to send an abortive close. One example is an RS-232 terminal server, which might hang forever in CLOSE_WAIT trying to deliver data to a stuck terminal port, but would properly reset the stuck port if it got an RST to discard the pending data.\u0026rdquo;\nSocket.setSoLinger(true, linger \u0026gt; 0)\nif there is any data still remaining in the socket send buffer, the process will sleep when calling close() until either all the data is sent and acknowledged by the peer or the configured linger timer expires. if the linger time expires before the remaining data is sent and acknowledged, close returns EWOULDBLOCK and any remaining data in the send buffer is discarded.\n如果SO_LINGER选项生效，并且超时设置大于零，调用close的线程被阻塞，TCP会发送缓冲区中的残留数据，这时有两种可能的情况：\n数据发送完毕，收到对方的ACK，然后进行连接的正常关闭（交换FIN-ACK） 超时，未发送完成的数据被丢弃，连接发送RST进行非正常关闭 类似的我们也可以构造demo观察这种场景。客户端发送较大的数据包，server端将Receive Buffer设置的很小。设置linger为1，调用close时等待1秒。注意SO_LINGER的单位为秒，好多人被坑过。假设close后1秒内缓冲区中的数据发送不完，使用tcpdump/Wireshark可以观察到客户端发送RST包，服务端收到java.net.SocketException: Connection reset异常。\n最后，在使用NIO时，最好不设置SO_LINGER，以后会再写一篇文章分析。\n","date":"2014-08-10T11:08:57+08:00","permalink":"https://mazhen.tech/p/tcp-so_linger-%E9%80%89%E9%A1%B9%E5%AF%B9socket.close%E7%9A%84%E5%BD%B1%E5%93%8D/","title":"TCP `SO_LINGER` 选项对Socket.close的影响"},{"content":"早上毕玄转给我一个问题，vsearch在上海机房部署的应用，在应用关闭后，端口释放的时间要比杭州机房的时间长。\nTCP的基本知识，主动关闭连接的一方会处于TIME_WAIT状态，并停留两倍的MSL（Maximum segment lifetime）时长。\n那就检查一下MSL的设置。网上有很多文章说，可以通过设置net.ipv4.tcp_fin_timeout来控制MSL。其实这有点误导人。查看Linux kernel的文档 ，发现tcp_fin_timeout是指停留在FIN_WAIT_2状态的时间：\ntcp_fin_timeout - INTEGER The length of time an orphaned (no longer referenced by any application) connection will remain in the FIN_WAIT_2 state before it is aborted at the local end. While a perfectly valid \u0026ldquo;receive only\u0026rdquo; state for an un-orphaned connection, an orphaned connection in FIN_WAIT_2 state could otherwise wait forever for the remote to close its end of the connection. Default: 60 seconds\n幸好这个问题原先在内部请教过：\nsysctl调节不了，只能调节复用和回收。 以前改小是改下面文件，重新编译内核的。 grep -i timewait_len /usr/src/kernels/2.6.32-220.el6.x86_64/include/net/tcp.h define TCP_TIMEWAIT_LEN (60HZ) / how long to wait to destroy TIME-WAIT define TCP_FIN_TIMEOUT TCP_TIMEWAIT_LEN\n而阿里内核支持修改TIME_WAIT时间：\nnet.ipv4.tcp_tw_timeout 然后找了两台机器做对比，用sysctl命令查看。杭州机房的机器：\nsudo sysctl -a | grep net.ipv4.tcp_tw_timeout net.ipv4.tcp_tw_timeout = 3 上海机房的机器：\n$sudo sysctl -a | grep net.ipv4.tcp_tw_timeout net.ipv4.tcp_tw_timeout = 60 原因很明显，上海机器的设置为60S。\n","date":"2014-07-01T11:00:10+08:00","permalink":"https://mazhen.tech/p/%E5%BA%94%E7%94%A8%E5%85%B3%E9%97%AD%E5%90%8E%E5%8D%A0%E7%94%A8%E7%AB%AF%E5%8F%A3%E6%97%B6%E9%97%B4%E8%BF%87%E9%95%BF%E7%9A%84%E9%97%AE%E9%A2%98/","title":"应用关闭后占用端口时间过长的问题"},{"content":"遇到性能问题怎么分析定位？这个问题太难回答了，各种底层环境、依赖系统、业务场景，怎么可能有统一的答案。于是产生了各种分析性能问题的“流派”。两个典型的 ANTI-METHODOLOGIES：\nblame-someone-else 使用此方法的人遵循下列步骤：\n找到一个不是他负责的系统或环境 假定问题和这个组件有关 将问题转交个负责这个组件的团队 如果证明是错误的，重复步骤1 路灯法 没有系统的方法论，只是使用自己擅长的工具去观察，而不管问题到底出现在哪儿。就像丢了钥匙的人去路灯下寻找，仅仅是因为路灯下比较亮。这种行为被称为路灯效应。\n相信很多同学已经脑补出上述的两个场景，他们的行为模式让人抓狂。于是有聪明人总结出了《The USE Method》。USE是Utilization，Saturation 和 Errors的缩写，简单说USE是一套分析系统性能问题的方法论，具体表现为一个checklist，分析过程就是对照checklist一项项检查，希望能快速定位瓶颈资源或错误。\n初看这个方法感觉有点太简单了吧，这也能称为方法论？不过这确实体现出了老外的做事风格，任何事情都会去做定量分析，力求逻辑完整。而我们往往讳莫高深的一笑，只可意会不可言传。\n简单介绍下USE，详细内容推荐看这篇《The USE Method》。USE的一句话总结：\nFor every resource, check utilization, saturation, and errors.\n术语解释\nresource：CPU，内存，磁盘，网络等一切物理设备资源 utilization：资源利用率。例如CPU的资源利用率90% saturation：当资源繁忙时仍能接收新的任务，这些额外的任务一般都放入了等待队列。saturation就表现为队列的长度，例如CPU的平均运行队列为4（Linux上使用vmstat命令获得）。 errors：系统的错误报告数，例如TCP监听队列overflowed次数。 列出系统中的所有资源，然后逐项检查利用率、等待队列和错误数，就这么简单！下表是一个范例：\nresource type metric CPU utilization CPU utilization (either per-CPU or a system-wide average) CPU saturation run-queue length Memory capacity utilization available free memory (system-wide) Memory capacity saturation anonymous paging or thread swapping Network interface utilization RX/TX throughput / max bandwidth Storage Storage device I/O utilization device busy percent Storage device I/O saturation wait queue length Storage device I/O errors device errors (\u0026ldquo;soft\u0026rdquo;, \u0026ldquo;hard\u0026rdquo;, \u0026hellip;) 对于资源测量数据的解读，作者给了一些建议，例如：资源利用率100%肯定表示该资源是系统瓶颈，70%以上的利用率就要引起足够的重视，一般IO设备利用率高于70%，响应时间将大幅上升。资源等待队列大于0意味着可能存在问题。资源的任何错误计数，都值得仔细调查，特别是当性能变差时，错误计数在上升。\n要使用这个方法，你还需要一份完整的资源列表，一般的系统资源包括：\nCPUs: sockets, cores, hardware threads (virtual CPUs) Memory: capacity Network interfaces Storage devices: I/O, capacity Controllers: storage, network cards Interconnects: CPUs, memory, I/O 作者很厚道的按照每种操作系统给出了checklist，重点关注《USE Method: Linux Performance Checklist》，不仅列出了资源，而且告诉你如何进行测量。例如CPU运行队列的测量：\nsystem-wide: vmstat 1, \u0026ldquo;r\u0026rdquo; \u0026gt; CPU count [2]; sar -q, \u0026ldquo;runq-sz\u0026rdquo; \u0026gt; CPU count; dstat -p, \u0026ldquo;run\u0026rdquo; \u0026gt; CPU count; per-process: /proc/PID/schedstat 2nd field (sched_info.run_delay); perf sched latency (shows \u0026ldquo;Average\u0026rdquo; and \u0026ldquo;Maximum\u0026rdquo; delay per-schedule); dynamic tracing, eg, SystemTap schedtimes.stp \u0026ldquo;queued(us)\u0026rdquo;\n根据作者的实践经验，使用USE方法解决了80%的性能问题，只付出了5%的努力，当考虑了所有的资源，你不太可能忽视任何问题。简单有效！\n","date":"2014-06-07T10:56:00+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8use-method%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98/","title":"使用USE Method分析系统性能问题"},{"content":"/proc是一个伪文件系统，可以像访问普通文件系统一样访问系统内部的数据结构，获取当前运行的进程、统计和硬件等各种信息。例如可以使用cat /proc/cpuinfo获取CPU信息。\n/proc/sys/下的文件和子目录比较特别，它们对应的是系统内核参数，更改文件内容就意味着修改了相应的内核参数，可以简单的使用echo命令来完成修改：\n1 echo 1 \u0026gt; /proc/sys/net/ipv4/tcp_syncookies 上面这个命令启用了TCP SYN Cookie保护。使用echo修改内核参数很方便，但是系统重启后这些修改都会消失，而且不方便配置参数的集中管理。/sbin/sysctl命令就是用来查看和修改内核参数的工具。sysctl -a会列出所有内核参数当前的配置信息，比遍历目录/proc/sys/方便多了。sysctl -w修改单个参数的配置，例如：\n1 sysctl -w net.ipv4.tcp_syncookies=1 和上面echo命令的效果一样。需要注意的是，要把目录分隔符斜杠/替换为点.，并省略proc.sys部分。\n通过sysctl -w修改，还是没有解决重启后修改失效的问题。更常用的方式是，把需要修改的配置集中放在/etc/sysctl.conf文件中，使用sysctl -p重新加载配置使其生效。在系统启动阶段，init程序会运行/etc/rc.d/rc.sysinit脚本，其中包含了执行sysctl命令，并使用了/etc/sysctl.conf中的配置信息。因此放在/etc/sysctl.conf中的系统参数设置在重启后也同样生效，同时也便于集中管理修改过了哪些内核参数。\n最后，哪里有比较完整的内核参数说明文档？我觉得kernel.org的文档比较全。例如我们常会遇到的网络内核参数，net.core 和 net.ipv4 。TCP相关的参数，也可以通过man文档了解。\n","date":"2014-05-30T20:21:08+08:00","permalink":"https://mazhen.tech/p/linux%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%B9%E6%B3%95/","title":"Linux内核参数的配置方法"}]