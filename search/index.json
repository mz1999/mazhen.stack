[{"content":" 最近在处理一个 EJB 调用的问题，和底层的 CORBA 通信有关，都是很古老的技术名词。\n二十多年前我刚参加工作的时候，EJB 带着神秘和时髦的色彩横空出世，可后来没几年就被 Spring Framework 祛魅，很少有人再使用 EJB 开发应用。\nCORBA 则更加古老，估计现在很多程序员都没听说过，更别说开发过 CORBA 组件了。实际上 CORBA 是最早的分布式服务规范，早在 1991 年就发布了 1.0。可以说后来的 EJB，Web Services，甚至微服务，service mesh 都有 CORBA 的影子。\nCORBA 定义了 IDL（Interface Definition Language），用它来描述对象的接口、方法、参数和返回类型等信息，根据 IDL 可以生成各种语言的实现，不同语言编写的对象可以进行交互。 CORBA 定义了一系列服务，如Naming Service，Transaction Service，Security Service等，作为分布式系统的基础服务。事务、安全等服务会随着远程调用进行传播。 CORBA 的 ORB（Object Request Broker） 负责分布式系统中对象之间的通信。用户可以像调用本地对象一样调用远程对象上的方法，ORB 会处理网络通信和远程调用的细节。ORB 之间通过 IIOP（Internet Inter-ORB Protocol）协议进行通信。 就这样，IDL、一系列服务，再加上ORB，构成了 CORBA 的完整体系。其实 CORBA 的理念很好，面向对象，跨语言跨平台，服务传播和网络通信对用户透明。\nCORBA 作为一套成熟的工业规范，后来者自然会想办法吸收和兼容。1998年发布的 JDK 1.2，内置了 Java IDL ，以及全面兼容 ORB 规范的 Java ORB 实现。这时 Java 已经准备在企业端开发领域大展拳脚，JDK 内置了对 CORBA 的支持，为 J2EE （也就是后来的 Java EE，现在的 Jakarta EE）做好了准备。\nEJB 全面继承了 CORBA，Java Transaction Service (JTS) 是 CORBA 事务服务 OTS 的 Java 映射，EJB 之间的远程调用走 RMI/IIOP 协议，事务、安全上下文会通过 IIOP 进行传播。理论上，部署在不同品牌应用服务器上的 EJB 之间可以互相调用，EJB 也可以和任何语言开发的 CORBA 对象进行交互，并且所有 EJB 和 CORBA 对象，可以运行在同一个事务、安全上下文中。\nEJB 的目标是做真正的中间件，连接不同厂商的 J2EE 应用服务器，连接不同语言开发、运行在不同平台上的 CORBA 对象，并且它们可以加入到同一个分布式事务中，受到同样的安全策略保护。\n理想很丰满，现实是 EJB 的理想从来没有被实现过。Rod Johnson 在总结了 J2EE 的优缺点后，干脆抛弃了 EJB(without EJB) ，开发了轻量级 Spring Framework。Spring 太成功了，以至于对很多人来说，Java 开发 ≈ 使用 Spring 进行开发。\n后来的 Web Services/SOA 又把 CORBA、EJB 的路重新走了一遍， 定义了和 IDL 类似的 WSDL，以及一系列的事务规范 WS-Transaction，WS-Coordination，WS-Atomic Transaction。然后开发者又觉得大公司定义的规范太复杂，才有了轻量级的 REST，微服务。\nJava 的 CORBA 实现在 JDK 9 中被标记为 deprecated， 并最终在2018年发布的 JDK 11中被正式移除。EJB 和 CORBA 都没有成功，Java 宣告和 CORBA 分手，一段历史结束。\n在2024年的今天，有着30多年历史的 CORBA 和20多年历史的 EJB 已经是遗留系统，不会再有大批聪明的年轻人愿意投入到这个技术领域。不过对于像我这样还在一线搬砖的大龄程序员，遗留系统也是一种选择。它们和自身的境况很像：激情已过，一天天的老去。我们互相扶持着，每天对它们进行修修补补，打着补丁，它们也回报勉强够养家的报酬，然后一起等待着被淘汰的一天。在 AI 革命，号称要取代码农的今天，竟然还能靠着 20 多年前学到的技能挣到工资，也算一个小小的奇迹吧。\n在翻阅 Java ORB 的源代码时，注意到了很多源文件上都标记了作者的名字，于是顺手在网上一搜，还真找到了作者的信息。\nHarold Carr，84年至94年在惠普实验室从事分布式 C++ 工作，94年加入 Sun，设计了 Sun 的 CORBA ORB，JAX-WS 2.0，负责过 GlassFish 中的 RMI-IIOP 负载平衡和故障切换。Sun 被收购后他一直留在 Oracle，目前仍在 Oracle 实验室从事技术工作。同时他组过乐队录过专辑，还出版过诗集。有意思的人，有意思的经历。\n回到开头的 EJB 问题，仍然没有头绪，继续在代码里找线索。生活充满了眼前的苟且，还能有诗和远方吗？\n","date":"2024-05-14T15:04:43+08:00","permalink":"https://mazhen.tech/p/%E9%81%97%E7%95%99%E7%B3%BB%E7%BB%9F/","title":"遗留系统"},{"content":"\n# perf 是什么 perf 是由 Linux 官方提供的系统性能分析工具 。我们通常说的 perf 实际上包含两部分：\nperf 命令，用户空间的应用程序 perf_events ，Linux 内核中的一个子系统 内核子系统 perf_events 提供了性能计数器（hardware performance counters）和性能事件的支持，它以事件驱动型的方式工作，通过收集特定事件（如 CPU 时钟周期，缓存未命中等）来跟踪和分析系统性能。perf_events是在 2009 年合并到 Linux 内核源代码中，成为内核一个新的子系统。\nperf 命令是一个用户空间工具，具备 profiling、tracing 和脚本编写等多种功能，是内核子系统 perf_events 的前端工具。通过perf 命令可以设置和操作内核子系统 perf_events，完成系统性能数据的收集和分析。\n虽然 perf 命令是一个用户空间的应用程序，但它却位于 Linux 内核源代码树中，在 tools/perf 目录下，它可能是唯一一个被包含在 Linux 内核源码中的复杂用户软件。\nperf 和 perf_events 最初支持硬件计数器（performance monitoring counters，PMC），后来逐步扩展到支持多种事件源，包括：tracepoints、kernel 软件事件、kprobes、uprobes 和 USDT（User-level statically-defined tracing）。\n下图显示了 perf 命令和 perf_events 的关系，以及 perf_events 支持的事件源。\n# perf 事件源 perf 支持来自硬件和软件方面的各种事件。硬件事件来自芯片组中的硬件性能计数器（hardware performance counters），而软件事件则由tracepoints、kprobe 和 uprobe 等调试设施提供。\n可以使用 perf 的子命令 list 列出当前可用的所有事件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ sudo perf list List of pre-defined events (to be used in -e or -M): branch-instructions OR branches [Hardware event] branch-misses [Hardware event] bus-cycles [Hardware event] cache-misses [Hardware event] cache-references [Hardware event] cpu-cycles OR cycles [Hardware event] [...] cgroup-switches [Software event] context-switches OR cs [Software event] cpu-clock [Software event] [...] L1-dcache-load-misses [Hardware cache event] L1-dcache-loads [Hardware cache event] [...] branch-instructions OR cpu/branch-instructions/ [Kernel PMU event] branch-misses OR cpu/branch-misses/ [Kernel PMU event] [...] sched:sched_process_exec [Tracepoint event] sched:sched_process_exit [Tracepoint event] sched:sched_process_fork [Tracepoint event] [...] 事件类型如下：\nHardware Event：CPU 性能计数器（performance monitoring counters） Software event：内核计数器事件 Hardware cache event：CPU Cache 事件 Kernel PMU event：Performance Monitoring Unit (PMU) 事件 Tracepoint event：包含了静态和动态代码追踪事件 Kernel tracepoints：在 kernel 中关键位置的静态追踪代码 kprobes：在内核中的任意位置动态地被插入追踪代码 uprobes：与kprobes类似，但用于用户空间。动态地在应用程序和库中的任意位置插入追踪代码 USDT：是 tracepoints 在用户空间的对应技术，是应用程序和库在它们的代码中提前加入的静态追踪代码 perf 的 “Tracepoint event” 事件源很容易引起混淆，因为除了内核的 tracepoints，基于 kprobe、uprobe 和 USDT 的跟踪事件也被标记为了“Tracepoint event”。默认情况下它们不会出现在 perf list 的输出中， 必须先初始化才会作为 \u0026ldquo;Tracepoint event\u0026rdquo; 中的事件。\n内核 tracepoints 是由 TRACE_EVENT 宏定义。TRACE_EVENT 自动生成静态追踪代码，定义并格式化其参数，并将跟踪事件放入 tracefs （/sys/kernel/debug/tracing）和 perf_event_open 接口。\n与其他性能分析工具相比，perf 特别适合 CPU 分析，它能对运行在 CPU 上代码调用栈（stack traces）进行采样，以确定程序在 CPU 上的运行情况，识别和优化代码中的热点。这种 CPU Profiling 能力是基于硬件计数器 (performance monitoring counters，PMC) 实现的，而 PMC 被内核子系统 perf_events 包装成了 Hardware Event，下面重点介绍。\n# Hardware Event CPU 和其他硬件设备通常提供用于观测性能数据的 PMC。简单来说，PMC 就是 CPU 上的可编程寄存器，可通过编程对特定硬件事件进行计数。通过 PMC 可以监控和计算 CPU 内部各种事件，比如 CPU 指令的执行效率、CPU caches 的命中率、分支预测的成功率等 micro-architectural 级的性能信息。利用这些数据分析性能，可以实现各种性能优化。\nperf 命令通过 perf_event_open(2) 系统调用访问 PMC，配置想要捕获的硬件事件。PMC 可以在两种模式下使用：\nCounting（计数模式），只计算和报告硬件事件的总数，开销几乎为零。 Sampling（采样模式），当发生一定数量的事件后，会触发一个中断，以便捕获系统的状态信息。可用于采集代码路径。 glibc 没有提供对系统调用 perf_event_open 的包装，perf 在 tools/perf/perf-sys.h 定义了自己的包装函数：\n1 2 3 4 5 6 7 8 static inline int sys_perf_event_open(struct perf_event_attr *attr, pid_t pid, int cpu, int group_fd, unsigned long flags) { return syscall(__NR_perf_event_open, attr, pid, cpu, group_fd, flags); } 第一个参数 perf_event_attr 结构体定义了想要监控的事件的类型和行为。例如想要统计 CPU 的总指令数，可以这样初始化 perf_event_attr 结构体：\n1 2 3 4 5 6 7 8 9 struct perf_event_attr pe; memset(\u0026amp;pe, 0, sizeof(pe)); pe.type = PERF_TYPE_HARDWARE; pe.size = sizeof(pe); pe.config = PERF_COUNT_HW_INSTRUCTIONS; pe.disabled = 1; pe.exclude_kernel = 1; pe.exclude_hv = 1; perf 在 perf_event.h 中定义了适用于各类处理器的通用事件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 enum perf_hw_id { PERF_COUNT_HW_CPU_CYCLES = 0, PERF_COUNT_HW_INSTRUCTIONS = 1, PERF_COUNT_HW_CACHE_REFERENCES = 2, ... } enum perf_hw_cache_id { PERF_COUNT_HW_CACHE_L1D = 0, PERF_COUNT_HW_CACHE_L1I = 1, PERF_COUNT_HW_CACHE_LL = 2, ... } ... 针对每种 CPU，需要将事件枚举类型映射为特定 CPU 的原始硬件事件描述符。例如对于 Intel x86 架构的 CPU，PERF_COUNT_HW_INSTRUCTIONS 事件映射为 0x00c0，在 arch/x86/events/intel/core.c 中定义：\n1 2 3 4 5 6 7 static u64 intel_perfmon_event_map[PERF_COUNT_HW_MAX] __read_mostly = { [PERF_COUNT_HW_CPU_CYCLES] = 0x003c, [PERF_COUNT_HW_INSTRUCTIONS] = 0x00c0, [PERF_COUNT_HW_CACHE_REFERENCES] = 0x4f2e, ... } 这些原始硬件事件描述符在相应的处理器软件开发人员手册中进行了说明。内核开发人员根据 CPU 厂商提供的软件开发人员手册，完成 PMC 事件和特定 CPU 代码的映射。\n例如 Intel x86 架构的 CPU可以参考 Intel® 64 and IA-32 Architectures Software Developer’s Manual 的第三卷第 20 章 “PERFORMANCE MONITORING”。Intel 还提供了一个网站 perfmon-events.intel.com，可以查询 CPU 的所有 PMC 事件。\n我们在使用 perf 时，可以直接指定特定 CPU 的原始事件代码：\n1 sudo perf stat -e r00c0 -e instructions -a sleep 1 对于 Intel CPU，instructions 事件的原始事件代码为 r00c0，两者可以等价使用。\n对于大部分通用事件，我们不需要记住这些原始事件代码，perf 都提供了可读的映射。但在某些情况下，例如最新 CPU 增加的事件还没有在 perf 中添加映射，或者某种 CPU 的特定事件不会通过可读的名称暴露出来，这时就只能通过指定原始硬件事件代码来监控事件。\n简单了解了 PMC 后，我们来看如何基于 PMC 事件进行 CPU Profiling。\n# CPU Profiling perf 是事件驱动的方式工作，通过 -e 参数指定想要收集的特定事件，例如：\n1 sudo perf stat -e LLC-loads,LLC-load-misses,LLC-stores,LLC-prefetches ls 我们在对整个系统的 CPU 进行30 秒的采样时，使用的命令如下：\n1 sudo perf record -F 99 -a -g -- sleep 30 这里并未明确指定事件（没有 -e 参数），perf 将默认使用以下预定义事件中第一个可用的：\ncycles:ppp cycles:pp cycles:p cycles cpu-clock 前四个事件都是 PMC 提供的 CPU cycles 事件，区别在于精确度不同，从最精确（ppp）到无精确设置（没有 p），最精确的事件优先被选择。cpu-clock 是基于软件的 CPU 频率采样，在没有硬件 cycles 事件可用时，会选择使用 cpu-clock 软件事件。\n那么什么是CPU cycles 事件，为什么对 cycles 事件进行采样可以分析 CPU 的性能？\n# cycles 事件 首先介绍一个关于 CPU 性能的重要概念，Clock Rate（时钟频率）。Clock（时钟）是驱动 CPU 的数字信号，CPU 以特定的时钟频率执行，例如 4 GHz的 CPU 每秒可执行40亿个 cycles（周期）。\nCPU cycles （周期）是 CPU 执行指令的时间单位，而时钟频率表示 CPU 每秒执行的 CPU 周期数。每个 CPU 指令执行可能需要一个或多个 CPU 周期。 通常情况下，更高的时钟频率意味着更快的CPU，因为它允许 CPU 在单位时间内执行更多的指令。\n每经过一个 CPU 周期都会触发一个 cycles 事件。可以认为，cycles 事件是均匀的分布在程序的执行期间。这样，以固定频率去采样的 cycles 事件，也是均匀的分布在程序的执行期间。我们在采样 cycles 事件时，记录 CPU 正在干什么，持续一段时间收集到多个采样后，我们就能基于这些信息分析程序的行为，多次出现的同样动作，可以认为是程序的热点，成为下一步分析重点关注的方面。\n因为 cycles 事件的均匀分布，通过以固定频率采样 cycles 事件获得的信息，我们就能进行 CPU 性能分析。那么如何指定采样频率呢？\n# 设置采样频率 在使用 perf record 记录 PMC 事件时，会使用一个默认的采样频率，不是每个事件都会被记录。例如记录 cycles 事件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ perf record -vve cycles -a sleep 1 Using CPUID GenuineIntel-6-45-1 DEBUGINFOD_URLS= nr_cblocks: 0 affinity: SYS mmap flush: 1 comp level: 0 ------------------------------------------------------------ perf_event_attr: size 128 { sample_period, sample_freq } 4000 sample_type IP|TID|TIME|ID|CPU|PERIOD read_format ID|LOST disabled 1 inherit 1 freq 1 sample_id_all 1 exclude_guest 1 ... [ perf record: Captured and wrote 0.422 MB perf.data (297 samples) ] 加了 -vv 选项可以输出更详细的信息。从输出中可以看出，即使我们没有明确设置采样频率，采样频率已经启用（freq 1），并且采样频率为 4000 （{ sample_period, sample_freq } 4000），即每 CPU 每秒采集约 4000个事件。cycles 事件每秒中有几十亿次，默认采样频率的设置很合理，否则记录事件的开销过高。\n可以使用 -F 选项明确设置事件采样频率，例如：\n1 perf record -F 99 -e cycles -a sleep 1 -F 99 设置采样频率为 99 Hertz，即每秒进行 99 次采样。Brendan Gregg 在大量的例子中都使用了 99 Hertz 这个采样频率，至于为什么这样设置，他在文章 perf Examples 中给出了解释，大意是：选择 99 Hertz 而不是100 Hertz，是为了避免意外地与一些周期性活动同步，这会产生偏差的结果。也就是说，如果程序中有周期性的定时任务，例如每秒钟执行的任务，以 100 Hertz 频率进行采样，那么每次周期性任务运行时都会被采样，这样产生的结果“放大”了周期性任务的影响，偏离了程序正常的行为模式。\nperf record 命令还可以使用 -c 选项来设置采样事件的周期，这个周期代表了采样事件之间的间隔。例如：\n1 sudo perf record -c 1000 -e cycles -a sleep 1 在这个示例中，-c 选项设置采样周期为 1000，即每隔 1000 次事件进行一次采样。\n现在我们知道了如何以固定的频率对 cycles 事件进行采样，那么如何获知在采样时，CPU 正在干什么呢？\n# 背景知识 要知道 cycles 事件发生时 CPU 正在干什么，我们需要了解一些硬件知识，以及内核与硬件是如何配合工作的。先看看 CPU 是如何执行指令的。\n# CPU 执行指令 CPU 内部有多种不同功能的寄存器，涉及到指令执行的，有三个重要的寄存器：\nPC 寄存器（PC，Program Counter），存放下一条指令的内存地址 指令寄存器（IR，Instruction Register），存放当前正在执行的指令 状态寄存器（SR，Status Register），用于存储 CPU 当前的状态，如条件标志位、中断禁止位、处理器模式标志等 CPU 还有其他用于存储数据和内存地址的寄存器，根据存放内容命名，如整数寄存器、浮点数寄存器、向量寄存器和地址寄存器等。有些寄存器既可以存放数据，又可以存放地址，被称为通用寄存器（GR，General register）。\n程序执行时，CPU 根据 PC 寄存器中的地址从内存中读取指令到 IR 寄存器中执行，并根据指令长度自增，加载下一条指令。\n只要我们在采样时获取CPU 的 PC 寄存器和 IR 寄存器的内容，就能推断出 CPU 当时正在干什么。\n在 x86-64 架构中，Program Counter 的功能是由 RIP (Instruction Pointer Register) 寄存器实现的。\n在编译程序时，可以让编译器生成一个映射，将源代码行与生成的机器指令关联起来，这个映射通常存储在 DWARF 格式（Debugging With Attributed Record Formats）的调试信息中。同时编译时会生成符号表（Symbol Table），其中包含了程序中各种符号（如函数名、变量名）及其地址的映射。perf 借助调试信息和符号表（symbol table），可以将采样时寄存器中的指令地址转换为对应的函数名、源代码行号等信息。\n知道了 CPU 当时的“动作”还不够，我们还需要知道 CPU 是怎么做这个“动作”的，也就是代码的执行路径。下面介绍函数调用栈的相关概念。\n# 还原函数调用栈 函数是软件中的一个关键抽象概念，它让开发者将具有特定功能的代码打包，然后这个功能可以在程序的多个位置被调用。\n假设函数 P 调用函数 Q，然后 Q 执行并返回结果给 P，这个过程涉及到以下机制：\n传递控制：在进入函数 Q 时，PC 寄存器设置为 Q 的起始地址；在从 Q 返回时，PC 寄存器设置为 P 中调用 Q 后的下一条指令处。 传递数据：P 能够向 Q 提供一个或多个参数，Q 也能够将一个值返回给 P。 分配和释放内存：Q 需要在开始时为局部变量分配空间，然后在返回前释放该存储空间。 x86-64 平台上的程序使用堆栈（Stack）来实现函数调用。堆栈（Stack）的特性是后进先出（LIFO），函数调用正是利用了这一特性。调用某个函数就是在堆栈上为这个函数分配所需的内存空间，这部分空间被称为栈帧（stack frame），从函数返回，就是将这个函数的**栈帧（stack frame）从堆栈中弹出，释放空间。多个栈帧（stack frame）**组成 Call stack，体现出了函数的调用关系。\n注意：编译后的程序存储在代码段，是静态的；而 Call stack 是动态的，反应了程序运行时的状态。\n下面以示例程序为例，x86-64 平台上如何利用**堆栈（Stack）**实现函数调用的。\nmain函数有三个局部变量a、b和 res 存储在自己的 stack frame 中。当 main 调用 Calc 函数时，会先将参数 i和j 压入 Stack，然后将 PC 寄存器中的值也压入 Stack。我们知道，PC 寄存器存放的是下一条指令的地址，这时 PC 寄存器中的值是函数调用指令（call）后紧跟着的那条指令的地址。把 PC 寄存器压入 Stack，相当于保留了函数调用结束后要执行的指令地址，这样Calc完成后程序知道从哪里继续执行。\nrbp 是栈基址寄存器（register base pointer ），又叫栈帧指针（Frame Pointer），存放了当前 stack frame 的起始地址。rsp 是栈顶寄存器（register stack pointer），称为栈指针（Stack Pointer），随着入栈出栈动作而移动，始终指向栈顶元素。x86-64 的堆栈是从高地址向低地址增长的。\n在为 calc 新建 stack frame 时，会先将 rbp 寄存器压入 Stack。当前 rbp 寄存器中存放的是 main stack frame 的起始地址，将 rbp 寄存器压入 Stack，也就是将 main stack frame 的起始地址压入了 Stack。\n随后把 rsp 的值复制到 rbp，因为 rsp 始终会指向栈顶，把 rsp 的值复制到 rbp 就是让 rbp 寄存器指向当前位置，即 calc stack frame 的起始位置。\n注意 Calc 的参数和返回地址包含在 main 的 stack frame 中，因为它们保存了与 main 相关的状态。Calc 函数局部变量sum和result 被分配在自己的 stack frame 上。\n在 Calc 调用 Sum 时，重复上面的动作：参数和返回地址入栈，保存并更新 rbp 寄存器，为 Sum 的局部变量分配地址。\n在函数 Sum 执行完成之后，会将之前保存的 rbp 出栈，恢复到 rbp 中，也就是让 rbp 指向 Calc stack frame 的起始地址，将 Sum的 stack frame 弹出了 Stack，释放了 Sum 占用的空间。然后将返回地址出栈，更新到 PC 寄存器中。返回地址是函数调用指令（call）后的下一条指令，即 Calc 调用完 Sum 后紧跟着的下一条指令，把这个指令的地址恢复到 PC 寄存器中，实际上是将控制权返回给了 Calc ，让 Calc 剩余部分接着执行。\nCalc 执行完也会做同样的出栈动作，释放 stack frame ，将控制权返回给 main。\n这样，函数调用利用了**堆栈（Stack）**传递参数，存储返回信息，保存寄存器中的值，以及存储函数的局部变量，来实现函数调用。\n每个函数的活动记录对应一个栈帧（stack frame），多个栈帧（stack frame）叠加在一起构成调用栈（ Call stack）。Frame Pointer（通常是 rbp 寄存器）指向当前激活的函数的栈帧（stack frame）的起始处，这个起始处保存了调用它的函数的栈帧（stack frame）的起始地址。通过这种链接，我们就能以 Frame Pointer 为起点，追溯整个调用链，即从当前函数开始，逐级访问到每个调用者的栈帧（stack frame），从而重构出程序执行的路径。\n需要注意的是，出于空间和时间效率的考虑，程序都会优先使用通用寄存器来传递参数，只有在寄存器不够用的时候才会将多出的参数压入栈中。\nperf 正是利用 Frame Pointer，还原采样时的代码执行路径。\n在开启编译器优化的情况下，程序会将 rbp 寄存器作为通用寄存器重新使用，这时就不能再使用 Frame Pointer 还原函数调用栈。perf 还可以使用其他方法进行 stack walking：\n\u0026ndash;call-graph dwarf ：使用调试信息 \u0026ndash;call-graph lbr： 使用 Intel 的 last branch record (LBR) \u0026ndash;call-graph fp：使用 Frame Pointer ，缺省方法 本文就不详细讨论其他两种还原调用栈的方法，感兴趣的可以参考《BPF Performance Tools》。\n在 Linux 上，进程的执行分为了用户态和内核态，要知道完整的代码执行路径，就需要分别还原用户栈和内核栈。什么是用户态和内核态呢？\n# 用户态和内核态 操作系统需要能够限制对关键系统资源的访问，提供对资源不同级别的访问权限，这样可以保护系统免受错误和恶意行为的侵害。这种访问权限由 CPU 在硬件级别上实现，例如 x86 架构定义了特权级别，也称为保护环（protection rings），从 Ring 0 到 Ring 3 ，每个级别定义了可使用的指令集和可访问的资源，Ring 0 具有最高的特权级别。\nRing 0: 最高特权级别，用于操作系统的内核，可以直接访问所有的硬件和系统资源。 Ring 1 和 Ring 2: 这些中间层次的环通常用于特定的系统任务，如设备驱动程序，但在现代操作系统中，这些任务通常也在 Ring 0 执行。 Ring 3: 最低特权级别，用于普通的应用程序，这些应用程序不能直接执行影响系统稳定性或安全性的操作。 Linux 主要使用了 Ring 0 和 Ring 3，将能够访问关键资源的内核放在 Ring0，称为内核态（Kernel Mode），普通的应用程序将放在 Ring3，称为用户态（User Mode）。\n# 系统调用 如果用户态代码需要访问核心资源，它必须通过系统调用（system call ）。系统调用是进入内核的入口点，内核通过系统调用向程序提供一系列服务，如文件读写、进程创建、输入输出操作等。应用程序通过系统调用请求内核代为执行这些服务。\n调用系统调用看起来很像调用 C 函数。但实际上，在系统调用的执行过程中，会进行多个步骤。以 x86 平台的实现为例，包括以下几个关键环节：\n应用程序通过调用 C 库中的包装函数来发起系统调用。 包装函数负责将所有系统调用参数传递给内核。这些参数通常通过栈传递给包装函数，然后被复制到特定的 CPU 寄存器中。 为了让内核识别不同的系统调用，包装函数会把系统调用的编号复制到一个特定的 CPU 寄存器（%eax）中。 包装函数执行 trap 机器指令（ x86 架构 sysenter 指令），使 CPU 从用户模式切换到内核模式。 内核响应中断，把当前的寄存器值保存到内核栈数据结构 struct pt_regs 中，根据编号在一个表格中找到相应的系统调用服务程序，并执行它，然后返回结果。 完成操作后，内核将寄存器值恢复到原始状态，并将控制权返回给用户空间的应用程序，同时返回系统调用的结果。 # 用户栈和内核栈 可以看出在执行系统调用时，进程具有两个栈：用户栈（User Stack）和内核栈（Kernel Stack）。\n用户栈（User Stack） 保留了进入系统调用前的状态，用户栈在系统调用期间不会改变 内核栈（Kernel Stack） 是在系统调用期间使用，用于存储在内核态下执行的状态信息，包括寄存器的值和系统调用的参数。此外处理中断和异常时，也会使用内核栈。 用户栈和内核栈在什么什么位置？我们需要先了解虚拟地址空间的概念。\n# 进程虚拟地址空间 在现代操作系统上，用户程序都不能直接操作物理内存。操作系统会给进程分配虚拟内存空间，所有进程看到的这个地址都是一样的，里面的内存都是从 0 开始编号。\n程序里指令操作的都是虚拟地址。内核会维护一个虚拟内存到物理内存的映射表，将不同进程的虚拟地址和不同的物理地址映射起来。当程序要访问虚拟地址时，会通过映射表进行转换，找到对应的物理内存地址。不同进程相同的虚拟地址，会映射到不同的物理地址，不会发生冲突。这样每个进程的地址空间都是独立的，相互隔离互不影响。\n我们来看一下进程的虚拟地址空间布局。\n一个进程的虚拟地址空间分为两个部分，一部分是用户态地址空间，一部分是内核态地址空间。\n用户空间是应用程序执行的场所，每个进程都有自己独立的用户空间，其布局包含代码、全局变量、堆、栈和内存映射区域等多种部分。\n内核空间是内核代码运行的内存区域，它并非专属于某个单独的进程，所有进程通过系统调用进入到内核之后，看到的虚拟地址空间都是一样的。\n用户空间与内核空间的这种分离，确保了用户应用程序不能直接干扰内核，保证了系统的安全稳定性。\nLinux 的可执行文件是 ELF（Executable and Linkable Format）格式，执行时从硬盘加载到内存，ELF 文件中的代码段和数据段被直接映射到进程虚拟地址空间用户态的数据段和代码段。\n用户空间的**堆（heap）**是动态内存分配的区域，可以使用系统调用 sbrk 、mmap 和 glibc 提供的 malloc 函数进行堆内存的申请。内核会维护一个变量 brk 指向堆的顶部，sbrk 通过改变 brk 来维护堆的大小。malloc 内部也使用了系统调用 sbrk 或 mmap，但 glibc 会维护一个内存池，并不是每次使用 malloc 申请内存时都直接进行系统调用。\n内存映射区域为共享库及文件映射提供空间。可以使用系统调用 mmap 将创建文件映射提升 IO 效率。\n用户空间的堆栈（Stack） 是用户态函数执行的活跃记录，%rsp指向当前堆栈顶部。\n内核空间也有代码段和数据段，映射内核的代码段和数据段。\n当进程执行系统调用时，会从用户空间切换到内核空间，进程的当前状态，包括栈指针（rsp 寄存器）、程序计数器（rip，也就是 PC 寄存器）等，会被保存在内核数据结构 struct pt_regs 内，以便在系统调用完成后能够准确地恢复，继续执行因系统调用而暂停的用户空间操作。\n执行内核代码会使用内核栈（Kernel-Stack）。\n现在有了进程虚拟地址空间的全景图，我们再回头看看还原函数调用栈的问题。\n# 还原完整调用栈 在 Linux 系统中，我们可以说在任何给定的时刻，CPU 处于下面三种状态之一：\n在用户空间，执行某个进程里的用户级代码 在内核空间，以进程的身份运行，为特定的进程服务，也就是执行系统调用 在内核空间，处于处理中断的状态，此时不与任何进程相关联，运行在内核线程中，专注于处理中断事件 对于第一、第三种情况， perf 在采样事件触发时，只要通过 Frame Pointer（ rsp 寄存器）就可以还原用户栈或内核栈，并且已经是完整的调用栈。\n对于第二种情况，进程正好陷入内核执行系统调用，那么通过 Frame Pointer（ rsp 寄存器）可以还原内核代码的执行路径。然后再通过内核数据结构 struct pt_regs 内保存的寄存器状态，还原进入内核前用户空间的代码执行路径。这样我们就能获得采样事件触发时，包含了用户态和内核态的完整代码执行路径。\n通过 PC 寄存器、 rsp 寄存器，以及内核数据结构pt_regs，我们能知道 CPU 瞬时的“动作”，以及它是怎么做这个动作的（代码执行路径），那么还剩最后一个问题，我们怎么知道采样发生时刻 CPU 寄存器的内容呢？\n# 中断处理 内核是被动工作模式，它“躺”在内核空间不会主动工作，要么通过系统调用让它为用户进程服务，要么由时钟中断和各种外部设备中断事件驱动执行。\n中断是 Linux 的核心功能之一，它允许 CPU 响应外部或内部事件，如源自硬件设备的键盘、鼠标、网卡，或来自软件的异常。当这些事件，也就是中断事件发生时，CPU 会暂停当前的工作，转入内核的中断处理程序（Interrupt Handler）。当处理完成后，会从中断处理程序返回到原来被中断的代码处继续执行。\n中断可以分为**同步（Synchronous）中断和异步（Asynchronous）**中断两类：\n同步中断：由CPU当前执行的指令序列引起的。 异步中断：由外部事件（定时中断和 I/O 设备）引起，与CPU当前执行的指令序列无关。 在 x86 平台上，同步中断被称为异常（Exception），而异步中断被称为中断（Interrupt）。注意“中断”这个词根据上下文，可以仅指异步中断，也可以指包含了异常的两类中断的总称。\n在 x86 架构中，每个中断或异常都通过一个 0 到 255 范围内的数字来识别，这个数字是一个 8 位的无符号数，被称为“向量（vector）”。其中，异常和不可屏蔽中断的向量值是固定不变的（0～31），而可屏蔽中断的向量可以通过可编程中断控制器（Programmable Interrupt Controller，PIC）进行调整。\n每个 I/O 设备通常有一个单独的输出线路，用来发送中断请求（Interrupt ReQuest, IRQ）。所有 IRQ 线路都连接到 PIC，然后 PIC 又连接到 CPU 的 INTR 引脚。当某个 I/O 设备发生了需要 CPU 注意的事件，例如用户敲击键盘，数据到达网卡，该设备在相应的 IRQ 线路上发送信号，PIC 将这个 IRQ 信号转换成一个中断向量，然后在 CPU 的 INTR 引脚上发起一个中断请求，等待 CPU 处理。\n不可屏蔽中断通过 NMI 引脚接入 CPU。\n我们可以在 /proc/interrupts 查看到系统硬件设备 IRQ 线路及对应 CPU 的统计信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ cat /proc/interrupts CPU0 CPU1 CPU2 CPU3 8: 0 0 0 0 IR-IO-APIC 8-edge rtc0 9: 0 4 0 0 IR-IO-APIC 9-fasteoi acpi 18: 0 2 0 0 IR-IO-APIC 18-fasteoi i801_smbus 23: 35 0 0 0 IR-IO-APIC 23-fasteoi ehci_hcd:usb1 40: 0 0 0 0 DMAR-MSI 0-edge dmar0 41: 0 0 0 0 DMAR-MSI 1-edge dmar1 42: 0 0 0 0 IR-PCI-MSI-0000:00:1c.0 0-edge PCIe PME ... NMI: 67 124 61 60 Non-maskable interrupts LOC: 7386692 9261862 8162396 7051922 Local timer interrupts ... PMI: 67 124 61 60 Performance monitoring interrupts ... 注意第一列输出的是 IRQ 线路编号，需要通过 PIC 转换为 CPU 使用的中断向量，对于 Intel CPU，IRQn 转换为的中断向量是 n+32，因为 0～31 是固定给了异常使用。\n×86 系列的 CPU 能处理 20 种不同类型的异常，内核必须为每一种异常都提供一个专门的异常处理程序。我们可以在 Intel\u0026rsquo;s Software Development Manual: System Programming Guide 中找到对这些异常的完整描述。\nVector Mnemonic Description Type Error Code Source 0 #DE Divide Error Fault No DIV and IDIV instructions. 1 #DB Debug Exception Fault/ Trap No Instruction, data, and I/O breakpoints; single-step; and others. 2 - NMI Interrupt Interrupt No Nonmaskable external interrupt. 3 #BP Breakpoint Trap No INT3 instruction. 4 #OF Overflow Trap No INTO instruction. 5 #BR BOUND Range Exceeded Fault No BOUND instruction. 6 #UD Invalid Opcode (Undefined Opcode) Fault No UD instruction or reserved opcode. 7 #NM Device Not Available (No Math Coprocessor) Fault No Floating-point or WAIT/FWAIT instruction. 8 #DF Double Fault Abort Yes (zero) Any instruction that can generate an exception, an NMI, or an INTR. 9 Coprocessor Segment Overrun (reserved) Fault No Floating-point instruction. 10 #TS Invalid TSS Fault Yes Task switch or TSS access. 11 #NP Segment Not Present Fault Yes Loading segment registers or accessing system segments. 12 #SS Stack-Segment Fault Fault Yes Stack operations and SS register loads. 13 #GP General Protection Fault Yes Any memory reference and other protection checks. 14 #PF Page Fault Fault Yes Any memory reference. 15 — (Intel reserved. Do not use.) No 16 #MF x87 FPU Floating-Point Error (Math Fault) Fault No x87 FPU floating-point or WAIT/FWAIT instruction. 17 #AC Alignment Check Fault Yes (Zero) Any data reference in memory. 18 #MC Machine Check Abort No Error codes (if any) and source are model dependent. 19 #XM SIMD Floating-Point Exception Fault No SSE/SSE2/SSE3 floating-point instructions 20 #VE Virtualization Exception Fault No EPT violations 21 #CP Control Protection Exception Fault Yes RET, IRET, RSTORSSP, and SETSSBSY instructions can generate this exception. When CET indirect branch tracking is enabled, this exception can be generated due to a missing ENDBRANCH instruction at target of an indirect call or jump. 22-31 - Intel reserved. Do not use. 下图描述了中断处理的大致流程：\n异常是由 CPU 执行的指令产生，中断是由外部设备的紧急事件产生。\nCPU 在收到中断请求后，根据中断号在中断向量表中找到相应的**中断处理程序（Interrupt Handler）**入口，从而做出相应的处理。中断向量表将每个中断或异常与对应的中断处理程序关联了起来。\n从上图可以看出，中断向量表的 0～31固定为异常处理，32～127 和 129～238 项用来处理外部 I/O 设备的请求。 PIC 会将 IRQ 编号转换为中断向量。\n当系统收到中断请求时，如果不在内核态，先会从用户态切换到内核态，并在内核栈中保存当前的状态信息（主要是寄存器信息）。\n接着使用中断向量号，在中断向量表中查找对应的处理代码的入口地址。然后系统跳转到这个地址，执行相应的中断处理程序。\n处理完成后，系统通过中断返回机制恢复被中断任务的现场（内核态或用户态），并继续执行原来的代码。\n中断处理的一个关键步骤是，保留中断发生时的现场信息，perf 的 CPU 采样功能正是利用了这一点。\n我们使用 perf record 进行 CPU 分析时，会通过 -F 指定采样频率。当达到预设的阈值（如一定数量的指令执行或特定时间间隔），硬件性能计数器会触发一个 PMI （Performance monitoring interrupts）中断。\nPMI 是个什么类型的中断呢？一般 PMI 是由本地 APIC（ Advanced PIC）产生，而 APIC 接入 CPU 的 INTR 引脚，你可能觉得它是一个可屏蔽中断。但根据Intel\u0026rsquo;s Software Development Manual: System Programming Guide ，非屏蔽中断 (NMI) 可通过两种方式生成：\n外部硬件激活 CPU 的 NMI 引脚 CPU 通过系统总线或 APIC 串行总线接收一条包含 NMI 传递模式的消息 也就是说，APIC 可以生成 NMI 模式的中断消息，以调用 NMI 中断处理程序。\n由于 NMI 无法被忽略，它们常被一些系统用作硬件监控工具。如果出现某些特定情况，比如在预定的时间内没有触发中断，NMI 处理程序就会产生警告并提供关于该问题的调试信息。这种机制有助于发现并预防系统死锁。\n硬件性能计数器触发的 PMI 中断被设置为了 NMI类型。我们以 x86 平台为例，当 PMI 中断发生时，处理入口位置在 arch/x86/entry/entry_64.S：\n1 2 3 4 5 6 7 8 9 10 11 12 13 SYM_CODE_START(asm_exc_nmi) ... pushq\t5*8(%rdx)\t/* pt_regs-\u0026gt;ss */ pushq\t4*8(%rdx)\t/* pt_regs-\u0026gt;rsp */ pushq\t3*8(%rdx)\t/* pt_regs-\u0026gt;flags */ pushq\t2*8(%rdx)\t/* pt_regs-\u0026gt;cs */ pushq\t1*8(%rdx)\t/* pt_regs-\u0026gt;rip */ UNWIND_HINT_IRET_REGS pushq $-1\t/* pt_regs-\u0026gt;orig_ax */ ... movq\t%rsp, %rdi movq\t$-1, %rsi call\texc_nmi arch/x86/entry/entry_64.S是用汇编语言编写，负责系统调用、中断异常处理、任务切换和信号处理，专门针对 x86_64 架构进行了优化。\nasm_exc_nmi 函数是处理 NMI 的入口，从截取代码片段的注释可以看出， asm_exc_nmi 会使用 pushq 指令将当前的寄存器状态保存到内核栈上，这些包括程序计数器（rip，也就是 PC 寄存器）、代码段（cs）、标志（flags）、堆栈指针（rsp）和堆栈段（ss）。这一步非常关键，保留中断发生时的现场信息。最后使用**call exc_nmi** 指令调用 exc_nmi 函数。exc_nmi 会根据类型，调用预先注册的 NMI 处理函数。\n如果是 PMI 类型的中断，最终调用的处理函数是 perf_event_nmi_handler ，定义在 arch/x86/events/core.c中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 static int perf_event_nmi_handler(unsigned int cmd, struct pt_regs *regs) { u64 start_clock; u64 finish_clock; int ret; /* * All PMUs/events that share this PMI handler should make sure to * increment active_events for their events. */ if (!atomic_read(\u0026amp;active_events)) return NMI_DONE; start_clock = sched_clock(); ret = static_call(x86_pmu_handle_irq)(regs); finish_clock = sched_clock(); perf_sample_event_took(finish_clock - start_clock); return ret; } 可以看到，第二个参数 pt_regs 在前面介绍系统调用时已经出现过，是内核用来保存寄存器状态的结构体。这样在perf_event_nmi_handler中，我们就可以使用 pt_regs 中保存的寄存器状态，还原出 PMI 中断发生时，CPU 当时的动作，以及包含了用户态和内核态的完整代码执行路径。\n至此，我们了解了 perf 在进行 CPU Profiling 时涉及的全部技术机制。\n# 总结 现在再回顾我们使用 perf 进行 CPU Profiling 时的命令：\n1 $ sudo perf record -F 99 -a -g -- sleep 30 perf 是事件驱动的方式工作，这个命令没有指定 -e 参数，会收集什么事件呢？perf 会默认收集 cycles 相关事件，从最精确的 cycles:ppp 到无精确设置的 cycles，优先选择可用且精度高的事件。如果没有硬件 cycles 事件可用，退而选择 cpu-clock 软件事件。\n为什么采样 cycles 事件就能分析程序的 CPU 性能？因为每个 CPU 周期都会触发一个 cycles 事件，cycles 事件均匀的分布在程序的执行期间，以固定频率采样的 cycles 事件同样均匀分布，如果我们在采样 cycles 事件时，记录 CPU 正在干什么，持续一段时间收集到多个采样后，就能基于这些信息分析程序的行为，多次出现的同样动作，就可以认为是程序的热点。\n如果指定采样频率？-F 99 设置采样频率为 99 Hertz，即每秒进行 99 次采样。也可以使用 **-c 1000 ** 设置采样周期，即每隔 1000 次事件进行一次采样。\n如果知道采样时 CPU 正在做什么？通过CPU 的 PC（Program Counter）寄存器（x86-64 平台上对应的是 rip 寄存器）、指令寄存器等状态信息，能推断出 CPU 的瞬时动作。\n知道了 CPU 采样时的“动作”还不够，还需要知道 CPU 是怎么做这个“动作”的，也就是代码的执行路径。系统利用了堆栈（Stack）的后进先出（LIFO）实现了函数调用，每个函数在堆栈上分配的空间称为栈帧（stack frame），多个栈帧（stack frame）组成 Call stack，体现出了函数的调用关系。通过栈帧指针Frame Pointer（rbp 寄存器），可以追溯整个调用链，逐级访问到每个调用者的栈帧（stack frame），重构出程序执行的路径。这就是 -g 参数的作用：使用 Frame Pointer 还原调用栈。\n操作系统为了安全会限制用户进程对关键资源的访问，将系统分为了用户态和内核态，用户态的代码必须通过**系统调用（system call ）**访问核心资源。所以在执行系统调用时，进程具有两个栈：用户栈（User Stack）和内核栈（Kernel Stack）。为了还原包含了用户栈和内核栈在内完整的调用栈，我们探索了进程虚拟地址空间的布局，以及系统调用的实现：原来在系统调用时，会将进程用户态的执行状态（rsp、rip等寄存器）保存在内核数据结构 struct pt_regs 内，这样就能通过 Frame Pointer 和 pt_regs 分别还原内核栈和用户栈。\n怎么获取采样发生时刻 CPU 寄存器的内容呢？在特定的时间间隔到达时，也就是该采样的时刻，APIC 会触发 PMI 中断，CPU 在将控制权转给中断处理程序之前，将当前的寄存器状态保存到pt_regs，然后作为参数传递给 perf_event_nmi_handler。这样 perf 就拿到了采样发生时刻，CPU 寄存器的内容。\n最后，我们可以看一下 perf record 收集了什么样的数据。使用 perf script 命令可以打印收集在 perf.data 中的每个样本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 $ sudo perf script ... sshd 50588 430947.269426: 16854 cycles: ffffffff824b84f6 native_write_msr+0x6 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff82413dc5 intel_pmu_enable_all+0x15 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff82407abb x86_pmu_enable+0x1ab (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff8273d05a perf_ctx_enable+0x3a (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff8274646a __perf_event_task_sched_in+0x15a (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff82534bf9 finish_task_switch.isra.0+0x179 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff834a57bf __schedule+0x2bf (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff834a5b68 schedule+0x68 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff834ac3cb schedule_hrtimeout_range_clock+0x11b (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff834ac403 schedule_hrtimeout_range+0x13 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff8289de3a do_poll.constprop.0+0x22a (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff8289e136 do_sys_poll+0x166 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff8289e7cc __x64_sys_ppoll+0xbc (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff834931ac do_syscall_64+0x5c (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff836000eb entry_SYSCALL_64+0xab (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) 118e5f __ppoll+0x4f (inlined) 89a97 server_loop2.constprop.0+0x547 (/usr/sbin/sshd) 89a97 wait_until_can_do_something+0x547 (inlined) 89a97 server_loop2.constprop.0+0x547 (/usr/sbin/sshd) 2bcf7 do_authenticated2+0x1e7 (inlined) 2bcf7 do_authenticated+0x1e7 (/usr/sbin/sshd) 11b66 main+0x3616 (/usr/sbin/sshd) 29d8f __libc_start_call_main+0x7f (/usr/lib/x86_64-linux-gnu/libc.so.6) 29e3f __libc_start_main_impl+0x7f (inlined) 12844 _start+0x24 (/usr/sbin/sshd) ... 任意截取了其中一段输出，可以看到包含了用户态和内核态完整的调用栈。基于多个这样的代码执行路径，我们还能生成火焰图进一步进行分析。\n为了了解 perf record 的实现原理，我们在 Linux 内核进行了一段深入而刺激的旅程，感谢各位参与探险！\n我的博客即将同步至腾讯云开发者社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=1k39tbi20c30f\n","date":"2023-11-06T11:18:06+08:00","permalink":"https://mazhen.tech/p/%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2-perf-cpu-profiling-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","title":"深入探索 perf CPU Profiling 实现原理"},{"content":"perf 是 Linux 官方的性能分析工具，它具备 profiling、tracing 和脚本编写等多种功能，是内核 perf_events 子系统的前端工具。\nperf_events 也被称为 Performance Counters for Linux (PCL) ，是在 2009 年合并到 Linux内核主线源代码中，成为内核一个新的子系统。perf_events 最初支持 performance monitoring counters (PMC) ，后来逐步扩展支持基于多种事件源，包括：tracepoints、kprobes、uprobes 和 USDT。\n# 安装预编译二进制包 perf 包含在 linux-tools-common 中，首先安装该软件包：\n1 $ sudo apt install linux-tools-common 运行 perf 命令，可能会提示你安装另一个相关的软件包：\n1 2 3 4 5 6 7 8 9 10 $ perf WARNING: perf not found for kernel 6.2.0-35 You may need to install the following packages for this specific kernel: linux-tools-6.2.0-35-generic linux-cloud-tools-6.2.0-35-generic You may also want to install one of the following packages to keep up to date: linux-tools-generic linux-cloud-tools-generic 按照提示安装和内核版本相关的 package：\n1 $ sudo apt install linux-tools-6.2.0-35-generic # 安装 Debug Symbol perf 像其他调试工具一样，需要调试符号表信息（Debug symbols）。这些符号信息用于将内存地址转换为函数和变量名称。如果没有符号信息，你将看到代表被分析内存地址的十六进制数字。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 13.19% 0.00% sshd libc.so.6 [.] __libc_start_call_main | ---__libc_start_call_main | --12.91%--0x5641182e4b66 0x5641182fecf7 | |--9.50%--0x56411835c752 | | | --9.10%--0x5641183330dd | | | --8.80%--0x5641182ee730 | | | |--4.64%--0x564118303c43 通过 package 方式安装的软件，一般都会提供以 -dbgsym 或 -dbg 结尾的调试符号信息 package。在 Ubuntu 上，首先需要将-dbgsym 仓库添加到更新源中，执行下面的代码：\n1 2 3 4 echo \u0026#34;deb http://ddebs.ubuntu.com $(lsb_release -cs) main restricted universe multiverse deb http://ddebs.ubuntu.com $(lsb_release -cs)-updates main restricted universe multiverse deb http://ddebs.ubuntu.com $(lsb_release -cs)-proposed main restricted universe multiverse\u0026#34; | \\ sudo tee -a /etc/apt/sources.list.d/ddebs.list 然后从 Ubuntu 服务器导入调试符号 package 签名密钥：\n1 $ sudo apt install ubuntu-dbgsym-keyring 最后更新软件源，安装 glibc 和 openssh-server 的调试符号信息：\n1 2 3 $ sudo apt update $ sudo apt install libc6-dbg $ sudo apt install openssh-server-dbgsym 安装了调试符号信息后，再使用 perf report 就会将十六进制数字转换为对应的方法名：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 11.68% 0.00% sshd libc.so.6 [.] __libc_start_call_main | ---__libc_start_call_main main | --11.46%--do_authenticated do_authenticated2 (inlined) server_loop2.constprop.0 | |--8.30%--process_buffered_input_packets (inlined) | ssh_dispatch_run_fatal | ssh_dispatch_run (inlined) | | | --7.86%--server_input_channel_req (inlined) | | | --7.78%--session_input_channel_req (inlined) 我们还可以安装内核镜像和一些内置命令行工具的调试符号信息：\n1 2 $ sudo apt install coreutils-dbgsym $ sudo apt install linux-image-`uname -r`-dbgsym # Kernel Tracepoints Kernel tracepoint 是在内核源码中关键位置的埋点，允许开发人员监视内核中的各种事件和操作，例如系统调用、TCP事件、文件系统I/O、磁盘I/O等，以了解内核的行为，进行性能分析和故障诊断。\n使用 perf 可以对 tracepoints 进行统计、追踪和采样。例如可以使用下面的命令对上下文切换进行 1 秒钟的跟踪：\n1 $ sudo perf record -e sched:sched_switch -a -g sleep 1 在执行这个命令的时候可能遇到下面的错误：\n1 2 3 4 5 event syntax error: \u0026#39;sched:sched_switch\u0026#39; \\___ unsupported tracepoint libtraceevent is necessary for tracepoint support Run \u0026#39;perf list\u0026#39; for a list of valid events 错误信息说明不支持 sched:sched_switch 这个 tracepoint。我们运行 perf list 查看可用的 tracepoint：\n1 2 3 4 5 6 7 8 9 $ sudo perf list \u0026#39;sched:*\u0026#39; List of pre-defined events (to be used in -e or -M): ... sched:sched_stat_wait [Tracepoint event] sched:sched_stick_numa [Tracepoint event] sched:sched_swap_numa [Tracepoint event] sched:sched_switch [Tracepoint event] sched:sched_wait_task [Tracepoint event] ... 输出中明明包含了 sched:sched_switch，为什么 perf 不支持呢？\n使用 perf version --build-options 查看 perf 的 build 选项：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ perf version --build-options perf version 6.2.16 dwarf: [ on ] # HAVE_DWARF_SUPPORT dwarf_getlocations: [ on ] # HAVE_DWARF_GETLOCATIONS_SUPPORT glibc: [ on ] # HAVE_GLIBC_SUPPORT syscall_table: [ on ] # HAVE_SYSCALL_TABLE_SUPPORT libbfd: [ OFF ] # HAVE_LIBBFD_SUPPORT debuginfod: [ OFF ] # HAVE_DEBUGINFOD_SUPPORT libelf: [ on ] # HAVE_LIBELF_SUPPORT libnuma: [ on ] # HAVE_LIBNUMA_SUPPORT numa_num_possible_cpus: [ on ] # HAVE_LIBNUMA_SUPPORT libperl: [ OFF ] # HAVE_LIBPERL_SUPPORT libpython: [ OFF ] # HAVE_LIBPYTHON_SUPPORT libslang: [ on ] # HAVE_SLANG_SUPPORT libcrypto: [ on ] # HAVE_LIBCRYPTO_SUPPORT libunwind: [ on ] # HAVE_LIBUNWIND_SUPPORT libdw-dwarf-unwind: [ on ] # HAVE_DWARF_SUPPORT zlib: [ on ] # HAVE_ZLIB_SUPPORT lzma: [ on ] # HAVE_LZMA_SUPPORT get_cpuid: [ on ] # HAVE_AUXTRACE_SUPPORT bpf: [ on ] # HAVE_LIBBPF_SUPPORT aio: [ on ] # HAVE_AIO_SUPPORT zstd: [ OFF ] # HAVE_ZSTD_SUPPORT libpfm4: [ OFF ] # HAVE_LIBPFM libtraceevent: [ OFF ] # HAVE_LIBTRACEEVENT libtraceevent 提供了访问内核 tracepoint 事件的 API。注意到最后一行，说明 perf 在 build 时没有打开 libtraceevent的支持。因此我们安装的预编译二进制包不能进行 tracepoint 追踪。我们需要自己从源码构建 perf。\n# 从源码构建 perf # 源码下载 首先下载 perf 的源代码。perf 的源码位于 Linux 内核源码中的 tools/perf 目录下。perf 是一个复杂的用户空间应用程序，而它却位于Linux 内核源代码树中，可能是唯一一个被包含在 Linux 源代码中的复杂用户软件。\n为了下载和内核匹配的源码，先确定内核版本：\n1 2 $ uname -r 6.2.0-35-generic 然后去 https://www.kernel.org/pub/ 浏览并下载正确版本的源码。\n# 安装依赖 安装构建依赖：\n1 2 $ sudo apt-get install build-essential flex bison python3 python3-dev $ sudo apt-get install libelf-dev libnewt-dev libdw-dev libaudit-dev libiberty-dev libunwind-dev libcap-dev libzstd-dev libnuma-dev libssl-dev python3-dev python3-setuptools binutils-dev gcc-multilib liblzma-dev 我们需要支持 tracepoint，所以还要安装 libtraceevent package：\n1 $ sudo apt install libtraceevent-dev # 构建 解压下载的 Linux 源码，进入源码目录，运行下面的命令：\n1 $ PYTHON=python3 make -C tools/perf install 成功构建后 perf 被安装到了 $HOME/bin 目录。\n# 测试验证 卸载先前安装的预编译版本：\n1 $ sudo apt remove linux-tools-common 将 $HOME/bin 加入到环境变量 $PATH，确保我们构建的 perf 命令能被找到。注意一般我们都需要 sudo 执行 perf 命令，所以还要编辑 /etc/sudoers（必须使用 visudo）文件，在 Defaults\tsecure_path=\u0026quot;...\u0026quot; 中加入 perf 命令的路径。\n验证 perf 的构建选项：\n1 2 3 4 $ sudo perf version --build-options perf version 6.2.0 ... libtraceevent: [ on ] # HAVE_LIBTRACEEVENT 这次 libtraceevent 打开了，支持 tracepoint 的追踪。执行下面的命令追踪系统的上下文切换：\n1 $ sudo perf record -e sched:sched_switch -a --call-graph dwarf sleep 1 查看报告：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ sudo perf report # To display the perf.data header info, please use --header/--header-only options. # # # Total Lost Samples: 0 # # Samples: 499 of event \u0026#39;sched:sched_switch\u0026#39; # Event count (approx.): 499 # # Children Self Command Shared Object Symbol # ........ ........ ............... .......................... ..................................... # 48.70% 48.70% swapper [kernel.vmlinux] [k] __schedule | ---secondary_startup_64_no_verify | |--39.28%--start_secondary | cpu_startup_entry | do_idle | schedule_idle | __schedule | __schedule ... 另外一个例子，按类型统计整个系统的系统调用，持续 5 秒钟：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ sudo perf stat -e \u0026#39;syscalls:sys_enter_*\u0026#39; -a sleep 5 Performance counter stats for \u0026#39;system wide\u0026#39;: ... 25 syscalls:sys_enter_timerfd_settime 0 syscalls:sys_enter_timerfd_gettime 0 syscalls:sys_enter_signalfd4 0 syscalls:sys_enter_signalfd 0 syscalls:sys_enter_epoll_create1 0 syscalls:sys_enter_epoll_create 0 syscalls:sys_enter_epoll_ctl 37 syscalls:sys_enter_epoll_wait 80 syscalls:sys_enter_epoll_pwait 0 syscalls:sys_enter_epoll_pwait2 ... 5.004101037 seconds time elapsed 可以看到，我们自己 build 的 perf 可以对 tracepoint 进行追踪和统计。\n","date":"2023-10-26T09:30:20+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8E%E6%BA%90%E7%A0%81%E6%9E%84%E5%BB%BA-perf/","title":"从源码构建 perf"},{"content":" 题图使用 Microsoft Bing 图像创建器生成。\n对于冯·诺伊曼体系结构的计算机，CPU 要数据才能正常工作。如果没有可处理的数据，那么CPU的运算速度再快也没有用，它只能等待。\n在计算机和芯片发展的历史中，CPU 速度不断提高，但主内存的访问速度改进相对较慢，导致 CPU经常处于等待数据的状态，无法充分发挥其处理能力。为了解决这个问题，出现了 CPU 缓存。\n寄存器和 CPU 缓存共同构成 CPU 内部的高速缓冲存储体系。\n寄存器直接位于 CPU 内部，是距离 CPU 最近的存储单元。CPU 缓存分为多级，距离 CPU 最近的一级缓存(L1缓存)接在寄存器之后。\n在内存层次结构中，从和 CPU 的接近程度来看是：\n寄存器 \u0026gt; L1缓存 \u0026gt; L2缓存 \u0026gt; L3缓存 \u0026gt; 主内存\n寄存器访问速度最快，但容量最小。随着级别升高，访问速度下降但容量增大，以平衡访问速度和存储空间。\nBrown University 的 Fundamentals of Computer Systems课程有专门介绍计算机的存储层次结构：以大小和速度为标准，将不同类型的存储设备从最快但容量最小到最慢但容量最大进行排列。\n存储层次结构这样设计是基于不同存储设备的成本和性能特点。内存成本高但访问速度快，而硬盘成本低但访问速度慢。所以采用这种层次结构可以在平衡成本和性能的前提下更好地利用各种存储设备。\n在2010年，Google 的 Jeff Dean 在斯坦福大学发表了一次精彩的演讲 Designs, Lessons and Advice from Building Large Distributed Systems，在演讲中总结了计算机工程师应该了解的一些重要数字：\n后来有人做了一个非常好的交互式web UI，展示了这些数字随着时间的变化。\n这些数字对人的感觉不那么直观，它们之间的差异可以相差数个数量级，让我们很难真正理解这些差距有多大。于是 Brendan Gregg在他的书 Systems Performance中，以 3.3 GHz 的 CPU 寄存器访问开始，放大成日常生活的时间单位，直观感受各系统组件访问时间的数量级差异：\n如果一个 CPU cycle 是1秒，那么内存访问的延迟是6分钟，从旧金山到纽约（相当于深圳到乌鲁木齐）的网络延迟就是4年！\n","date":"2023-10-11T16:20:21+08:00","permalink":"https://mazhen.tech/p/latency-numbers-every-programmer-should-know/","title":"Latency Numbers Every Programmer Should Know"},{"content":" 题图使用 Microsoft Bing 图像创建器生成。\n# 计算机原理/体系结构 极客时间：深入浅出计算机组成原理\nComputer Systems: A Programmer\u0026rsquo;s Perspective 从程序员的角度学习计算机系统，了解计算机系统的各个方面，包括硬件、操作系统、编译器和网络。这本书涵盖了数据表示、C语言程序的机器级表示、处理器架构、程序优化、内存层次结构、链接、异常控制流（异常、中断、进程和Unix信号）、虚拟内存和内存管理、系统级I/O、基本的网络编程和并发编程等概念。这些概念由一系列有趣且实践性强的实验室作业支持。\nComputer Systems: A programmer\u0026rsquo;s Perspective 视频课\n编码 Code: The Hidden Language of Computer Hardware and Software\nComputer Science from the Bottom Up 采用“从下到上”的方法,从最基础的二进制、数据表示开始,逐步深入计算机内部工作原理,目的是帮助读者真正掌握计算机科学的基础知识。\n漫画计算机原理\n趣话计算机底层技术\n计算机底层的秘密\n穿越计算机的迷雾\n嵌入式C语言自我修养\n# 编程 征服C指针 彻底理解和掌握指针的各种用法和技巧 C专家编程 Sun公司编译器和OS核心开发团队成员，对C的历史、语言特性、声明、数组、指针、链接、运行时、内存等问题进行了细致的讲解和深入的分析 C from Scratch 一个学习 C 语言的从零开始的路线图，包括推荐的课程、项目和资源，以及进阶到 x86-64 汇编语言和操作系统内部的指导。 Online Compiler, Visual Debugger 独特的逐步可视化调试工具，强烈推荐！ 极客时间：深入 C 语言和程序运行原理 Linux/UNIX系统编程手册 The Linux Programming Interface: A Linux and UNIX System Programming Handbook UNIX环境高级编程 Advanced Programming in the UNIX Environment # Linux 极客时间：Linux 实战技能 100 讲 Efficient Linux at the Command Line 像黑客一样使用命令行 Linux是怎么工作的 Linux技术内幕 Linux Foundation 的认证考试 LFCA 和 LFCS Linux内核设计与实现 Linux Kernel Development 深入理解Linux网络 极客时间：Linux 内核技术实战课 极客时间：编程高手必学的内存知识 极客时间：容器实战高手课 Learning Modern Linux 交互式的 Linux 内核地图 Linux From Scratch step-by-step instructions for building your own customized Linux system entirely from source. # 网络 趣谈网络协议 极客时间：Web 协议详解与抓包实战 图解TCP/IP 图解HTTP 网络是怎样连接的 # 编译原理 程序是怎样跑起来的 程序员的自我修养：链接、装载与库 如何从对象文件中导入和执行代码 part1 part2 part3 # 数据结构和算法 极客时间：数据结构与算法之美 极客时间：算法面试通关 40 讲 极客时间：常用算法 25 讲 极客时间：算法训练营 Hello 算法 动画图解、一键运行的数据结构与算法教程 通过动画可视化数据结构和算法 # 综合 计算机自学指南 (GitHub仓库) YouTube视频课：Crash Course Computer Science Preview 计算机教育中缺失的一课 Developer Roadmaps 为开发者提供学习路线图和指南 Online Coding Classes – For Beginners 3000 小时的免费课程，涵盖了编程涉及到的方方面面 # 在线课程 educative 为开发者提供交互式在线课程，重点关注技术领域的知识与技能 edX 由麻省理工学院（MIT）和哈佛大学共同创立的在线教育平台 exercism 专注于通过有趣且具有挑战性的练习问题、支持建设性同行评审机制来促进积极参与和技能提升，从而培养对各种现代计算范式的熟练掌握。 # 技术面试 Leetcode 一个广受欢迎的在线编程题库 Cracking the coding interview book 一本深受程序员喜爱的面试指南书 Neetcode 另一个在线编程练习平台 编程面试大学 涵盖了算法、数据结构、面试准备和工作机会等主题，帮助你准备大公司的技术面试 interviewing.io 一个提供模拟技术面试的平台 Pramp 一个模拟面试平台 Meetapro 一个可以找到专业人士进行模拟面试的网站 # 交互式教程 Grep by example 如何使用命令行工具 grep 进行文本搜索的交互式指南 Learn Git Branching 一个交互式的在线教程，帮助用户学习并练习 Git 的基本使用方法 # 大语言模型 Learn Prompting 一个开源的、多元化社区构建的课程，旨在提供完整、公正的提示工程知识。 提示工程指南 介绍大语言模型（LLM）相关的论文研究、学习指南、模型、讲座、参考资料、大语言模型能力及其与其他工具的对接。 面向开发者的大模型手册 基于吴恩达大模型系列课程的翻译和复现项目，涵盖了从 Prompt Engineering 到 RAG 开发的全部流程，为国内开发者提供了学习和入门 LLM 相关项目的方式。 LLM 应用开发实践笔记 作者在学习基于大语言模型的应用开发过程中总结出来的经验和方法，包括理论学习和代码实践两部分。 动手学大模型应用开发 面向小白开发者的大模型应用开发教程，基于阿里云服务器，结合个人知识库助手项目，通过一个课程完成大模型开发的重点入门。 # iOS开发 iOS \u0026amp; Swift - The Complete iOS App Development Bootcamp The 100 Days of SwiftUI Stanford CS193p - Developing Apps for iOS iOS and SwiftUI for Beginners Meta iOS Developer Develop in Swift Tutorials 苹果官方教程 SwiftUI Tutorials 苹果官方教程 # 计算机科学史 信息简史 ","date":"2023-10-07T10:45:48+08:00","permalink":"https://mazhen.tech/p/incomplete-list-of-computer-science-learning-resources-for-college-students/","title":"Incomplete List of Computer Science Learning Resources for College Students"},{"content":"\n我在 Jakarta EE 中文技术大会上的分享 《Jakarta EE应用服务器的事务处理》\n","date":"2023-09-27T18:30:59+08:00","permalink":"https://mazhen.tech/p/jakarta-ee%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/","title":"Jakarta EE应用服务器的事务处理"},{"content":"asadmin is a command-line tool for GlassFish , which provides a series of subcommands. Using asadmin, you can complete all management tasks of GlassFish.\nThe subcommand start-domain of asadmin can start GlassFish. The following will describe the main process of GlassFish startup, starting from the execution of the asadmin command.\n# asadmin Execution Process The entry point of the asadmin command is org.glassfish.admin.cli.AsadminMain, which is included in the ${AS_INSTALL_LIB}/client/appserver-cli.jar package.\nThe main process of AsadminMain execution is as follows:\nSome key points:\nThe CLICommand.getCommand() is called to obtain the subcommand to start the GlassFish. All subcommands of asadmin inherit from com.sun.enterprise.admin.cli.CLICommand and are loaded from the following directories or Jars:\n${com.sun.aas.installRoot}/lib/asadmin\n${com.sun.aas.installRoot}/modules/admin-cli.jar\nAll subcommands are executed by calling CLICommand.execute(String... argv).\nThe implementation class of the subcommand to start GlassFish is StartDomainCommand, which internally calls GFLauncher.launch() to start the GlassFish.\nFinally, GFLauncher uses ProcessBuilder to start a new process, which is the main process of GlassFish. The entry point of this new process is com.sun.enterprise.glassfish.bootstrap.ASMain.\nIn addition, if the verbose or watchdog is set, the parent process asadmin will not exit and will wait until GlassFish runs to the end:\n1 2 3 4 // If verbose, hang around until the domain stops if (getInfo().isVerboseOrWatchdog()) { wait(glassFishProcess); } Next, we analyze the startup process of the GlassFish main process.\n# Main Process Startup Process The entry point of the GlassFish main process is the main method of com.sun.enterprise.glassfish.bootstrap.ASMain , and the main process of the startup process is as follows:\nThe startup process is complicated, but the main steps are clear:\nCreate GlassFishRuntime using RuntimeBuilder Create a GlassFish instance using GlassFishRuntime Start the Glassfish instance by calling GlassFish.start() # Creating GlassFishRuntime The main steps to create GlassFishRuntime include:\nCreate RuntimeBuilder Create and initialize the OSGi Framework Load OSGi bundles Start the OSGi Framework Start the BundleActivator in the bundles During the startup process of HK2Main, it searches for and registers HK2 modules. During the creation of GlassFishRuntime, OSGiGlassFishRuntimeBuilder will create and initialize the OSGi Framework, and then use the installBundles() method of BundleProvisioner to install all bundles of GlassFish to OSGi.\nWhere does BundleProvisioner find the bundles to load? The glassfish.osgi.auto.install property in the ${com.sun.aas.installRoot}/config/osgi.properties file defines the loading path of OSGi bundles. The discoverJars() method of BundleProvisioner will scan these paths and discover the Jar packages that need to be loaded.\nAfter completing the loading of bundles, OSGiGlassFishRuntimeBuilder will call Framework.start() to start the OSGi Framework. During the startup process of the OSGi Framework, the BundleActivator in the bundles will be started. Two important BundleActivators are:\nGlassFishMainActivator HK2Main During the startup process of GlassFishMainActivator , EmbeddedOSGiGlassFishRuntime will be registered in OSGi.\nHK2Main will create ModulesRegistry. ModulesRegistry is a key component of HK2. All modules in HK2 are registered here. In the OSGi environment, the specific implementation class of ModulesRegistry is OSGiModulesRegistryImpl, which will find and register the HK2 modules contained in the META-INF/hk2-locator directory of all bundle Jars.\nModulesRegistry and HK2Main will be registered as OSGi\u0026rsquo;s service.\n# Creating GlassFish Instance Create a GlassFish instance through GlassFishRuntime.newGlassFish(). This process mainly does two things:\nCreate HK2\u0026rsquo;s ServiceLocator Get ModuleStartup from ServiceLocator In EmbeddedOSGiGlassFishRuntime, use ModulesRegistry.newServiceLocator() to create ServiceLocator, and then get ModuleStartup from ServiceLocator. In the GlassFish startup scenario, the specific implementation of ModuleStartup is AppServerStartup.\nServiceLocator is the registry of HK2 services, which provides a series of methods to get HK2 service.\nThe relationship between HK2 Module and Service can be regarded as the relationship between container and content. Module (container) contains a group of Service (content) and is responsible for registering these Services in ServiceLocator. When a Module is initialized, all its Services will be registered in ServiceLocator, and then these Services can be found and used by other Services.\nFinally, create a GlassFish instance with AppServerStartup and ServiceLocator as the parameters of the constructor.\n# Starting GlassFish Instance Use GlassFish.start() to start the Glassfish instance. The most critical step is to call AppServerStartup.start(), which starts the HK2 service in stages. The service of HK2 can specify the startup level, the lower the level, the earlier the startup.\nAfter AppServerStartup.start() runs, all services start, and Glassfish completes startup and runs.\n","date":"2023-07-21T16:48:55+08:00","permalink":"https://mazhen.tech/p/glassfish-startup-process/","title":"GlassFish Startup Process"},{"content":"asadmin 是 GlassFish 的命令行工具，它提供了一系列子命令，使用 asadmin 可以让你完成 Glassfish 的所有管理任务。\n使用 asadmin 的子命令 start-domain 可以启动 GlassFish。下面将描述 GlassFish启动过程的主要流程。先从 asadmin 命令的执行开始。\n# asadmin 执行流程 asadmin 命令的入口是 org.glassfish.admin.cli.AsadminMain， 包含在 ${AS_INSTALL_LIB}/client/appserver-cli.jar包中。\nAsadminMain 执行的主要流程如下：\n其中的一些关键点：\n调用CLICommand.getCommand()获得启动服务器的子命令。asadmin 的所有子命令的都继承自com.sun.enterprise.admin.cli.CLICommand ，从下列目录或 Jar 中加载：\n${com.sun.aas.installRoot}/lib/asadmin\n${com.sun.aas.installRoot}/modules/admin-cli.jar\n所有子命令的执行都是调用CLICommand.execute(String... argv) 。\n启动 GlassFish 的子命令实现类为StartDomainCommand，内部调用 GFLauncher.launch()启动服务器。\n最终 GFLauncher 使用 ProcessBuilder 启动一个新的进程，就是 GlassFish 的主进程。这个新进程的入口是com.sun.enterprise.glassfish.bootstrap.ASMain。\n另外，如果设置了 verbose 或 watchdog 参数，作为父进程的asadmin 不会退出，一直等到 GlassFish 运行结束：\n1 2 3 4 // If verbose, hang around until the domain stops if (getInfo().isVerboseOrWatchdog()) { wait(glassFishProcess); } 下面分析 GlassFish 主进程的启动流程。\n# 主进程启动流程 GlassFish 主进程的入口是 com.sun.enterprise.glassfish.bootstrap.ASMain 的 main方法，启动过程的主要流程如下：\n启动过程比较复杂，但主要步骤很清晰：\n使用 RuntimeBuilder 创建 GlassFishRuntime 使用 GlassFishRuntime 创建 GlassFish 实例 调用 GlassFish.start() 启动 Glassfish 实例 # 创建 GlassFishRuntime 创建 GlassFishRuntime 的主要步骤包括：\n创建 RuntimeBuilder 创建并初始化 OSGi Framework 加载 OSGi bundles 启动 OSGi Framework 启动 bundles 中的 BundleActivator 在 HK2Main 的启动过程中查找并注册 HK2 modules 在创建 GlassFishRuntime的过程中，OSGiGlassFishRuntimeBuilder 会创建并初始化 OSGi Framework ，然后使用 BundleProvisioner 的 installBundles() 方法向 OSGi 安装 GlassFish 的所有 bundles。\nBundleProvisioner从哪里找到要加载的 bundles？config/osgi.properties 文件中的 glassfish.osgi.auto.install 属性定义了 OSGi bundles 的加载路径。BundleProvisioner.discoverJars() 方法会扫描这些路径，发现需要加载的 Jar 包。\n在完成 bundles 的加载后，OSGiGlassFishRuntimeBuilder会调用 Framework.start() 启动 OSGi Framework。 Framework 的启动过程中，bundles 中的 BundleActivator 会被启动。其中两个重要的 BundleActivator 是：\nGlassFishMainActivator HK2Main GlassFishMainActivator 启动过程中会向 OSGi 中注册 EmbeddedOSGiGlassFishRuntime。\nHK2Main 会创建 ModulesRegistry。ModulesRegistry 是 HK2 的关键组件，HK2 中的 modules 都注册在这里。在 OSGi 环境下，ModulesRegistry 的具体实现类是 OSGiModulesRegistryImpl，它会从所有 bundle Jar 的 META-INF/hk2-locator 目录中查找并注册该 bundle 包含的 HK2 modules。\nModulesRegistry 和 HK2Main 都会注册为 OSGi 的 service。\n# 创建 GlassFish 实例 通过GlassFishRuntime.newGlassFish() 创建出 GlassFish 实例，这个过程主要做了两件事：\n创建出 HK2 的 ServiceLocator 从 ServiceLocator 获取 ModuleStartup 在 EmbeddedOSGiGlassFishRuntime 中使用 ModulesRegistry.newServiceLocator() 创建出 ServiceLocator，然后从 ServiceLocator 获取 ModuleStartup。在 GlassFish 启动场景获取的是 ModuleStartup 的一个具体实现 AppServerStartup。\nServiceLocator 是 HK2 service 的注册表，它提供了一系列获取 HK2 service 的方法。\nHK2 Module 和 Service 的关系可以看作是容器和内容的关系。Module（容器）包含了一组 Service（内容），并且负责将这些 Service 注册到 ServiceLocator 中。当一个 Module 被初始化时，它的所有 Service 都会被注册到 ServiceLocator 中，然后这些 Service 就可以被其他 Service 查找和使用。\n最后将 AppServerStartup 和 ServiceLocator 作为构造函数的参数，创建出 GlassFish 实例。\n# 启动 Glassfish 实例 使用 GlassFish.start() 启动 Glassfish 实例。其中最关键的步骤是调用 AppServerStartup.start()，分级启动 HK2 的 service。HK2 的 service 可以指定启动级别，级别越低，越先启动。\nAppServerStartup.start() 运行完成，所有 service 启动，Glassfish 完成启动并运行。\n","date":"2023-07-21T15:32:57+08:00","permalink":"https://mazhen.tech/p/glassfish-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/","title":"Glassfish 启动流程"},{"content":" # 获得源代码 首先从 Github 获取 OpenJDK的源代码\n1 $ git clone https://github.com/openjdk/jdk.git # 安装必要的软件 Xcode App Store 中获取 Xcode Command Line Tools 通过 xcode-select --install 命令安装 GNU Autoconf 使用 brew install autoconf 命令安装 freetype 使用 brew install freetype 命令安装 boot JDK 构建 JDK 需要预先存在的JDK，这被称为“boot JDK”。 经验法则是，用于构建 JDK 主版本N的 boot JDK应该是主版本 N-1 的 JDK 建议使用 SDKMAN! 来安装维护 JDK 的多个版本 # 配置构建 通过运行 bash configure 命令来完成配置构建。这个脚本将检查你的系统，确保所有必要的依赖项都已经满足。如果一切顺利，该脚本将汇总build的配置、将使用的工具，以及 build 将使用的硬件资源：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Configuration summary: * Name: macosx-x86_64-server-release * Debug level: release * HS debug level: product * JVM variants: server * JVM features: server: \u0026#39;cds compiler1 compiler2 dtrace epsilongc g1gc jfr jni-check jvmci jvmti management parallelgc serialgc services shenandoahgc vm-structs zgc\u0026#39; * OpenJDK target: OS: macosx, CPU architecture: x86, address length: 64 * Version string: 22-internal-adhoc.mazhen.jdk (22-internal) * Source date: 1689128166 (2023-07-12T02:16:06Z) Tools summary: * Boot JDK: openjdk version \u0026#34;20.0.1\u0026#34; 2023-04-18 OpenJDK Runtime Environment Temurin-20.0.1+9 (build 20.0.1+9) OpenJDK 64-Bit Server VM Temurin-20.0.1+9 (build 20.0.1+9, mixed mode, sharing) (at /Users/mazhen/.sdkman/candidates/java/20.0.1-tem) * Toolchain: clang (clang/LLVM from Xcode 14.3.1) * Sysroot: /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX13.3.sdk * C Compiler: Version 14.0.3 (at /usr/bin/clang) * C++ Compiler: Version 14.0.3 (at /usr/bin/clang++) Build performance summary: * Build jobs: 12 * Memory limit: 16384 MB # 构建 OpenJDK 一旦配置完成，你就可以开始构建 JDK 了。\n1 $ make images 这个命令将开始构建过程，在完成后生成一个 JDK 的 image。\n# 验证构建 新构建的 JDK 在 ./build/*/images/jdk目录下，运行命令查看JDK版本\n1 2 3 4 $ ./build/macosx-x86_64-server-release/images/jdk/bin/java -version openjdk version \u0026#34;22-internal\u0026#34; 2024-03-19 OpenJDK Runtime Environment (build 22-internal-adhoc.mazhen.jdk) OpenJDK 64-Bit Server VM (build 22-internal-adhoc.mazhen.jdk, mixed mode, sharing) # 在VS code中调试 OpenJDK 首先在 VS code 中安装 C++ extension for VS Code。在 VS cod 中配置C++ 开发环境可以参考这篇文档 Using Clang in Visual Studio Code。\n使用 VS code 打开 OpenJDK的源代码，在恰当的位置设置好断点，点击右上角三角运行图标，选择“Debug C/C++ file”：\n然后在弹出列表中选择“(lldb) Launch“：\n第一次运行会弹出错误信息，我们选择打开 launch.json，创建新的 debugger 配置。点击右下角的 “add configuration\u0026hellip;“，在弹出的列表中选择 \u0026ldquo;C/C++： (lldb) Launch\u0026rdquo;\nVS code会自动添加缺省的配置，我们需要修改的是 program 和 args，设置为上面build好的 OpenJDK，以及准备运行的Java程序。\n1 2 3 4 5 6 \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/build/macosx-x86_64-server-release/images/jdk/bin/java\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-cp\u0026#34;, \u0026#34;/Users/mazhen/Documents/works/javaprojects/samples/playground/target/classes\u0026#34;, \u0026#34;tech.mazhen.test.Main\u0026#34; ], 保存文件 launch.json，然后重新开始调试。可以在断点处停止，但是不能定位源代码，报错如下：\n1 Could not load source \u0026#39;make/src/java.base/unix/native/libnio/ch/Net.c\u0026#39;: \u0026#39;SourceRequest\u0026#39; not supported.. 为了正确的找到源代码，需要在launch.json中配置 sourceFileMap，将源代码的编译时路径映射到本地源代码位置。完整的配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;(lldb) Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/build/macosx-x86_64-server-release/images/jdk/bin/java\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-cp\u0026#34;, \u0026#34;/Users/mazhen/Documents/works/javaprojects/samples/playground/target/classes\u0026#34;, \u0026#34;com.apusic.test.Main\u0026#34; ], \u0026#34;stopAtEntry\u0026#34;: false, \u0026#34;cwd\u0026#34;: \u0026#34;${fileDirname}\u0026#34;, \u0026#34;environment\u0026#34;: [], \u0026#34;externalConsole\u0026#34;: false, \u0026#34;MIMode\u0026#34;: \u0026#34;lldb\u0026#34;, \u0026#34;sourceFileMap\u0026#34;: { \u0026#34;make/\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34; }, } ] } 现在就可以在VS code 中正常调试OpenJDK的C++代码了。\n","date":"2023-07-11T17:26:22+08:00","permalink":"https://mazhen.tech/p/%E5%9C%A8macos%E4%B8%8A%E7%BC%96%E8%AF%91%E5%92%8C%E8%B0%83%E8%AF%95openjdk/","title":"在macOS上编译和调试OpenJDK"},{"content":"在计算机科学中，事务处理（transaction processing ）是将信息处理划分为独立的、不可分割的操作，称为事务（Transaction）。每个事务必须作为一个完整的执行单元，要么整个事务成功（提交），要么失败（中止，回滚），它永远不能只是部分完成。使用事务可以简化应用程序的错误处理，因为它不需要担心部分失败，系统（通常是数据库或某些现代文件系统）的完整性始终处于已知的、一致的状态。\n事务处理是一项关键技术，可以应用于多个问题领域——企业架构、电子商务解决方案、金融系统和许多其他领域。事务的一个很好的例子就是，资金从一家银行的账户转移到另一家银行的账户。资金转移涉及在一个账户上扣款，并在另一个账户上增加相同的金额。使用事务可以确保不会出现由于其中一项操作失败，而导致资金丢失或产生的不一致状态。\n# 事务处理简史 现代事务处理技术是在20世纪60年代开始的大型机计算背景下发展起来的，在许多方面，我们今天使用的技术是对这些模型的改进和调整。第一个事务处理系统（Transaction processing system）是著名的 SABRE 航空预订系统，由IBM和美国航空公司在20世纪50年代末和60年代初开发。以今天的标准来看，SABRE 相当粗糙： ACID 事务语义完全是由应用程序实现的。IBM 很快就意识到这种技术可以应用于其他行业，由此产生了CICS（Customer Information Control System）产品，它最初是完全用汇编语言编写，并没有使用现代意义上的数据库，而是依赖于扁平文件和其他低级数据结构，但 CICS 已经实现了基本的事务处理功能。\n1970年 Edgar F. Codd 发表了一篇名为《A Relational Model of Data for Large Shared Data Banks》的论文，首次提出了关系模型的概念，这一模型为后来的关系数据库管理系统（RDBMS）奠定了基础。随后关系数据库管理系统开始兴起，IBM 的 System R 项目是一个关键的里程碑，它是 SQL 的第一个实现，从此成为标准的关系数据查询语言。在 System R 项目中，事务处理技术被引入到关系数据库领域，为后来的数据库系统的发展奠定了基础。数据库管理系统成为了事务处理的核心组件，负责数据存储和管理、事务处理、并发控制和恢复等关键功能。\n随着计算机技术的发展和网络通信技术的普及，分布式计算逐渐成为企业应用的重要趋势。在分布式环境中实现事务处理面临着许多挑战，传统的单机事务处理系统无法满足需求，两阶段提交协议（2PC）等技术应运而生。为了管理这些分布式事务，提供更好的并发控制和容错能力，事务处理监视器（Transaction Processing Monitor 或 TP Monitor）被引入。\nTP Monitor 负责在分布式环境中管理和监控事务处理过程。它处理客户端请求、协调事务、确保数据一致性、管理资源访问以及处理故障恢复等。TP Monitor 是一个软件框架或应用程序执行环境，为应用程序提供了一个完整的运行时，允许应用以安全和事务性的方式访问后端系统（包括数据库）。TP Monitor 作为事务处理中间件，目标是让程序员更容易的编写和部署可靠、可扩展的事务应用程序，使程序员能够专注于业务逻辑，而不是底层的事务管理。\n在20世纪90年代初，X/Open 发布了X/Open DTP模型，为分布式事务处理提供了一个统一的框架和一组标准接口。遵循 X/Open DTP 模型的 TP Monitor 实现了该模型所定义的组件和接口，包括事务管理器（TM）、资源管理器（RM）和通信资源管理器（CRM）。一些广泛使用的遵循 X/Open DTP 模型的 TP Monitor 产品包括BEA 的 Tuxedo 和 Transarc 的 Encina 等。\nCORBA Object Transaction Service (OTS) 是一个定义在 CORBA（Common Object Request Broker Architecture） 规范中的分布式事务服务。OTS 将分布式事务处理模型（DTP）扩展到了对象领域，它提供了一种在分布式对象系统中进行事务处理的方法。OTS 定义了一组标准的接口和协议，允许 CORBA 对象参与分布式事务。\nJava EE 应用服务器是在 X/Open DTP 模型和 CORBA OTS 的基础上发展出来的事务处理监视器，TP Monitor 开始融入 Java EE应用服务器，提供更丰富的中间件服务和组件化的应用程序模型。TP Monitor 本质上是一个具有事务感知功能的应用服务器，事实上，Java EE 应用服务器中的许多功能都源于TP Monitor。同样地，许多现代的 TP Monitor 是带有事务服务核心的 Java EE 应用服务器。\n# 事务概念基础 本章我们简要地回顾一些事务处理的基本概念，它们塑造了中间件对事务的支持。\n# ACID 属性 事务提供的安全保障通常用缩写ACID来描述，它代表原子性（atomicity）、一致性（consistency）、隔离性（Isolation）和持久性（Durability）。事务是一个原子（Atomicity）工作单元，它将系统从一个一致（Consistency）的状态转换为另一个一致状态，执行时不受其他同时执行的事务的干扰（isolation），并且一旦提交，就不能因系统故障而撤消(Durability)。ACID 是1983年由 Theo Härder 和Andreas Reuter 在论文《 Principles of Transaction-Oriented Database Recovery》中首次提出。\n下面我们更详细地研究一下ACID属性，这将让我们更深入的理解事务。\n# Atomicity Atomicity 这个术语在不同的领域有着类似但又不相同的含义。例如在多线程编程领域，如果一个线程执行了一个原子操作，这意味着另一个线程不可能看到这个操作的半成品。系统只能处于操作前或操作后的状态，而不能介于两者之间。\n在 ACID 的背景下，Atomicity 不是关于并发性的，它并没有描述当多个进程试图同时访问相同的数据时会发生什么，因为关于并发访问的场景是在隔离性（Isolation）中描述的。\nACID 原子性描述的是，如果一个客户想要进行多次写入，但在处理部分写操作后出现故障的情况。这些故障可能是进程崩溃、网络连接中断、磁盘已满等。如果将这些写操作组合到一个事务中，由于故障无法完成事务提交，那么该事务将被中止，并且数据库必须撤消之前的任何写操作。\n在没有 Atomicity 保证的情况下，如果在进行多次修改的过程中发生错误，就很难知道哪些修改已经生效，哪些没有生效。Atomicity 简化了这个问题：如果事务被中止，应用程序可以确定它没有改变任何东西。\n所以 Atomicity 的本质是，在出错时中止事务，并丢弃该事务对数据的所有修改。\n# Consistency 事务应该确保系统从一个一致性状态转换到另一个一致性状态。在事务开始和结束时，系统的完整性约束必须得到满足。\n我们所说的一致性，有些是通过数据库的完整性约束来保证的，例如使用主键作为员工编号，那么由数据库来保证所有主键都是唯一的。\n但在很多情况下，一致性都是有具体业务含义的，应用程序定义了什么状态是有效或无效的，例如每个部门的支出必须小于或等于该部门的预算，这种一致性只能由应用程序来保证。\n原子性、隔离性和持久性是数据库的属性，而一致性是应用程序的属性。维护事务的一致性是应用程序和数据库的共同责任，应用程序是依靠数据库的原子性和隔离性来实现一致性。\n# Isolation 如果多个用户同时读写数据库相同的记录，就会遇到并发问题。Isolation 意味着同时执行的事务是相互隔离的，事务的执行不会受到其他并发事务的影响，每个事务都可以假装它是整个数据库中唯一运行的事务。\n# Durability 持久性是指一旦事务成功提交，它所写入的任何数据都不会丢失，即使出现硬件故障或数据库崩溃。改变系统持久状态的唯一方法是提交一个事务。\n对于单节点数据库，持久性通常意味着数据已写入硬盘或 SSD 等非易失性存储中。数据库一般都会使用WAL（write-ahead log）技术，在向持久化存储写入未提交的变更之前，先向日志中写入相应的事务日志记录，并确保事务日志记录在事务提交之前被持久化。当遇到故障重启系统时，数据库可以通过重新执行所有已提交事务的日志记录，撤消所有中止事务的日志记录，让数据库恢复到一致性状态。\n# 隔离级别 隔离性（Isolation）是事务 ACID 四个属性之一，它确保多个并发事务在操作数据库时，彼此之间不会互相干扰，从而保证数据的一致性。\n当一个事务读取被另一个事务同时修改的数据，或者两个事务试图同时修改相同的数据时，会出现并发性问题。数据库通过提供事务隔离向应用开发人员隐藏了并发性问题的复杂性。\n隔离级别（Isolation Level）定义了不同的隔离性强度，以在性能和数据一致性之间取得平衡。可串行化（serializable）的隔离意味着数据库保证事务具有与串行运行相同的效果，即事务一个接着一个的运行，没有任何并发。然而在实践中，可串行化（serializable）的隔离有一定的性能成本，因此使用较弱的隔离级别是很常见的。根据SQL标准，隔离级别分为四个等级，从低到高分别为：\n读未提交（Read Uncommitted）：这是最低的隔离级别。在这个级别下，一个事务可以看到其他事务尚未提交的数据。这意味着可能发生脏读（Dirty Read），即一个事务读取到了另一个尚未提交的事务所修改的数据。这个级别的优点是并发性能较高，但数据一致性较差。\n读已提交（Read Committed）：这个级别要求一个事务只能看到其他事务已经提交的数据。这意味着脏读不会发生，但仍然可能发生不可重复读（Non-repeatable Read），即在同一个事务中，多次读取同一数据可能得到不同的结果。这个级别在性能和数据一致性之间取得了一定的平衡。\n可重复读（Repeatable Read）：这个级别要求在同一个事务中，对同一数据的多次读取结果必须一致。这可以避免不可重复读的问题，但仍然可能发生幻读（Phantom Read），即在一个事务执行过程中，其他事务插入了满足查询条件的新数据。这个级别提供了较好的数据一致性保障，但并发性能受到一定影响。\n可串行化（Serializable）：这是最高的隔离级别。在这个级别下，事务被处理得就像是串行执行一样，完全避免了脏读、不可重复读和幻读问题。然而，这种级别的数据一致性保障是以牺牲并发性能为代价的，可能导致事务处理效率降低。\n较低的隔离级别增加了用户并发访问相同数据的能力，但也增加了用户可能遇到的并发问题（例如脏读或丢失更新）的数量。相反，较高的隔离级别减少了用户遇到的并发问题，但需要更多的系统资源，并增加了一个事务阻塞另一个事务的机会。\n在实践中，可串行化（serializable）隔离很少被使用，Oracle 数据库甚至没有实现它。在Oracle中，有一个叫做 \u0026ldquo;serializable \u0026ldquo;的隔离级别，但它实际上实现的是快照隔离（snapshot isolation）。\n快照隔离通过为每个事务提供一个数据快照来实现，在一个事务执行过程中，它只能看到和操作事务开始时的数据快照，而不会受到其他并发事务的影响。快照隔离的实现通常依赖于多版本并发控制（MVCC，Multi-Version Concurrency Control）技术。快照隔离能够解决脏读、不可重复读和幻读问题，但它会导致写偏斜（Write Skew），即两个或多个事务同时读取相同的数据，然后基于读取到的数据做出独立的修改，最终导致数据不一致的状态。因此，快照隔离比可串行化的保证要弱。 # 分布式事务 随着计算机网络的发展，分布式计算变得越来越普遍。这导致了分布式事务处理的需求，即在多个独立的数据库或资源管理器上执行的事务。分布式事务处理具有更高的复杂性，需要协调和管理跨越不同系统的事务。这样的事务处理通常需要遵循分布式事务处理的规范和算法，如两阶段提交协议。\n# 两阶段提交 为了使分布式事务的操作表现得像一个原子单元，参与的分布式资源必须根据事务的结果全部提交或全部放弃。两阶段提交（2PC，Two-Phase Commit）协议是一种用于分布式事务处理的原子性协议，它通过在所有事务参与者之间进行协调和同步，以确保分布式事务的原子性得以维护。\n两阶段提交协议引入了一个新的组件：协调器 coordinator，也被称为transaction manager。coordinator 通常和应用进程在同一个进程中，例如 Java EE应用服务器中 transaction manager，但它也可以是一个单独的程序或服务。\n两阶段提交协议包括两个阶段：提交请求阶段（Prepare Phase）和提交阶段（Commit Phase）。\n准备阶段（Prepare Phase）\n事务协调器（Transaction Coordinator）向所有事务参与者（Transaction Participants）发送准备（Prepare）消息，要求它们准备提交事务。 每个事务参与者在收到准备消息后，会执行本地事务操作（例如修改数据、写日志等），然后将其状态设置为“准备就绪”（Ready）。 如果事务参与者成功完成了本地操作并准备好提交事务，它会向事务协调者发送一个“同意”（Agree）消息。否则，它会发送一个“中止”（Abort）消息。 提交阶段（Commit Phase）\n当事务协调者收到所有事务参与者的响应后，会做出全局决策。如果所有参与者都发送了“同意”消息，协调者会决定提交事务。否则，协调者会决定中止事务。 事务协调者向所有事务参与者发送全局决策，即“提交”（Commit）或“中止”（Abort）消息。 事务参与者根据协调者的全局决策执行相应的操作。如果接收到“提交”消息，参与者会提交本地事务，并向协调者发送一个“已提交”（Committed）消息；如果接收到“中止”消息，参与者会回滚本地事务，并向协调者发送一个“已中止”（Aborted）消息。 两阶段提交协议的目标是确保分布式事务中的所有参与者要么都提交事务，要么都中止事务，从而满足原子性要求。\n两阶段提交协议也有一些局限性，例如性能开销、同步延迟和单点故障风险。\n# coordinator 故障 如果任何一个Prepare请求失败或超时，coordinator 将中止交易；如果任何一个 commit 或 abort 请求失败，coordinator 将无限期地重试它们。如果 coordinator 失败了会怎么样呢？\n如果 coordinator 在发送 Prepare 请求之前就失败了，参与者可以安全地中止事务。但是，一旦参与者收到Prepare 请求并投了 \u0026ldquo;yes\u0026rdquo;，参与者不能再单方面中止，必须等待 coordinator 的回复，以确定事务是被 commit 还是被 abort。如果 coordinator 在这时崩溃或发生网络故障，事务处于 in doubt 状态，参与者除了等待之外什么也做不了。\n# 单点故障风险 两阶段提交协议的问题是，一旦事务参与者完成投票，它必须等待 coordinator 给出指示，提交或放弃。如果这时coordinator 挂了，事务参与者除了等待什么也做不了，事务处于未决状态。coordinator 成为了整个系统的单点故障。\ncoordinator 在向参与者发送提交或中止请求之前，必须将事务的最终结果写入到磁盘上的事务日志中。当coordinator 从故障中恢复时，它通过事务日志来确定所有未决状态事务的处理。所以从本质上看，两阶段提交协议为了达到一致性，实际上是退化到由 coordinator 单节点来实现 atomic commit。\n因为两阶段提交协议会在等待 coordinator 恢复的过程中处于阻塞状态，所以它被称为阻塞原子提交（blocking atomic commit）协议。\n# coordinator 的故障恢复 如果 coordinator 发生故障，如何进行故障恢复呢？有三种解决方法：\n等待 coordinator 恢复，并接受在此期间系统将被阻塞的事实。 由人工选择一个新的 coordinator 节点，进行手动故障切换。 使用一个算法来自动选择一个新的 coordinator。 后两种解决方法的前提是，事务日志必须安全可靠的存储，不能因为 coordinator 的任何故障而被损坏。\n# 启发式决策 为了保证原子性，两阶段提交协议必须是阻塞的。这意味着，即使存在故障恢复机制，参与者也可能长时间内被阻塞，但一些应用可能无法容忍这种长时间的阻塞。更糟的情况是，如果事务日志丢失或损坏， 即使 coordinator 恢复了也不能决定事务的最终结果。 未决的事务不会自动解决，它们会驻留在数据库中，持有锁并阻塞其他事务。这时即使重启数据库也不能解决问题，因为数据库必须在重新启动时保留对未决事务的锁定，否则将可能违反两阶段提交的原子性保证。\n为了打破两阶段提交的阻塞性，事务参与者在没有 coordinator 的明确指示下，独立决定中止或提交一个未决事务，这就是启发式决策（heuristic decision）。\n启发式决策可能导致数据不一致，因为事务参与者在没有 coordinator 指示的情况下独立决定事务的命运。这可能导致某些参与者提交事务，而另一些参与者中止事务。事实上，启发式决策违反了两阶段提交协议的承诺，因此，做出启发式决策只是用于摆脱灾难性的情况，而不是常规使用。\nJTA 定义了几种与启发式决策有关异常。\njavax.transaction.HeuristicCommitException coordinator 要求事务参与者回滚，但事务参与者此前已经做出了提交的启发式决策。 javax.transaction.HeuristicRollbackException coordinator 要求事务参与者提交，但事务参与者此前已经做出了回滚的启发式决策。 javax.transaction.HeuristicMixedException 是最糟糕的启发式异常。抛出它表示事务的一部分已提交，而其他部分被回滚。当一些事务参与者进行启发式提交，而其他事务参与者进行启发式回滚时，coordinator会抛出此异常。 # 事务模式 事务模型（Transaction Models）是指在事务处理系统中使用的一组原则和方法，用于定义事务的结构、范围、行为和执行方式。不同的事务模型反映了不同的设计和实现方法，以满足特定的应用需求。以下是三种常见的事务模型。\n# 扁平事务模型 扁平事务模型（Flat Transaction Model）是最简单和最常见的事务模型，其中每个事务都是独立的，并且没有任何嵌套或链接关系。扁平事务模型规定在任何给定时间只有一个事务在其他事务中处于活动状态。\n我们是否可以在一个事务中同时开始另一个事务？有两种方法可以做到这一点：\n在第一个事务结束之前，我们可以禁止开始另一个事务。\n我们也可以暂停当前事务，并开始新事务，在新事务完成后，将恢复原始事务。\n扁平事务模型广泛应用于各种数据库系统和应用程序。应用服务器必须支持扁平事务模型。\n# 链式事务模型 在链式事务模型（Chained Transaction Model）中，多个事务可以相互链接，使得一个事务的结束与下一个事务的开始紧密相连。当一个事务提交或回滚时，立即启动另一个事务，而不需要显式地发出BEGIN TRANSACTION命令。链式事务模型有助于提高事务处理的效率，尤其是在需要频繁执行事务的应用场景中。\n# 嵌套事务模型 在嵌套事务模型（Nested Transaction Model）中，事务可以嵌套在其他事务之内，形成一个层次结构。这意味着一个事务可以包含一个或多个子事务，子事务又可以包含它们自己的子事务。一个嵌套的子事务可以单独提交或中止。因此，复杂的事务可以被分解成更容易管理的子事务。子交易可以提交或回滚，而不需要整个交易提交或回滚。\nJTA（Java Transaction API ）规范不要求支持嵌套事务模型。大多数 JTA 实现只支持扁平事务模型。\n# 分布式事务处理模型 分布式事务是一个涉及到由多个分布式应用程序执行的操作，以及可能涉及多个分布式数据库的事务。在分布式环境中保证事务遵守 ACID 原则是很困难的，需要协调和管理跨越不同系统的事务。对于复杂、异构的分布式系统来说，应用程序必须遵守同一个标准来协调事务工作，以进行分布式事务处理（DTP，distributed transaction processing ）。其中一个 DTP 标准是由 Open Group 开发的 X/Open DTP。Java EE 中的全局事务处理使用的就是 X/Open DTP 模型。在企业 Java 应用的世界中，X/Open DTP 是事务处理的基石。\n# X/Open DTP X/Open 是一家成立于1984年的非营利性质的技术联盟，其目标是制定开放系统标准，以便于实现操作系统、数据库、网络和分布式计算等领域的互操作性。1996 年，X/Open 与 Open Software Foundation合并，组成 The Open Group。\nX/Open 在1991年开发了一个分布式事务处理（DTP）模型，其中包括传统的 TP monitors 所提供的许多功能。大多数关系型数据库、消息队列都支持基于 X/Open DTP 的规范。该模型将一个交易处理系统分为几个部分：交易管理器、数据库或其他资源管理器以及交易通信管理器\nX/Open DTP 模型由事务管理器（TM）、资源管理器（RM）、通信资源管理器（CRM）和应用程序（AP）组成。X/Open DTP 标准规定了这些组件功能，以及组件之间的标准接口。\nX/Open 的资源管理器用于描述任何共享资源的管理进程，但它最常用于表示关系数据库。在 X/Open DTP模型下，应用程序和资源管理器之间的接口是对于不同的 RM 是不一样的，但是可以使用资源适配器作为接口，提供应用程序和各种资源管理器类进行通信的通用方法，例如 JDBC 可以被认为是资源适配器。\n事务管理器是 X/Open DTP 模型的核心，负责协调各分布式组件之间事务。资源管理器通过实现 XA 规范来参与分布式事务。XA 规范定义了事务管理器（TM）和资源管理器（RM）之间的双向接口。事务管理器实现了两阶段提交协议，确保所有的资源管理器都能同时提交完成事务，或在失败时回滚到原始状态。\n通信资源管理器 为连接分布式的事务管理器提供了一种标准方法，以便在不同事务域之间传播事务信息，实现更广泛的分布式事务。事务管理器和通信资源管理器之间的标准接口由 XA+接口 定义。通信资源管理器到应用程序的接口由三个不同的接口定义，即TxRPC、XATMI 和 CPI-C。\n# X/Open XA X/Open XA规范定义了事务管理器（Transaction Manager）与资源管理器（Resource Manager）之间的协作机制，以便在分布式环境中实现两阶段提交2PC协议。X/Open XA规范主要包括以下几个组成部分：\nXA接口： 这是一组标准的函数和数据结构，用于定义事务管理器和资源管理器之间的通信方式。XA接口包括一系列函数，如xa_open()、xa_close()、xa_start()、xa_end()、xa_prepare()、xa_commit()、xa_rollback()等，这些函数分别对应分布式事务处理过程中的不同阶段。\nXID（Transaction Identifier）：唯一的事务标识符，用于跟踪和管理分布式环境中的事务。XID 包括三个主要部分：全局事务ID（Global Transaction ID）、分支限定符（Branch Qualifier）和格式ID（Format ID）。全局事务ID用于唯一标识一个分布式事务，分支限定符用于标识事务中的不同资源管理器，而格式ID用于指定XID的表示格式。\n两阶段提交协议：X/Open XA规范采用两阶段提交协议来实现分布式事务处理。\n遵循X/Open XA规范的事务管理器和资源管理器可以跨平台、跨系统地协同工作，实现分布式事务处理的互操作性。\n# Java Transaction API (JTA) Java Transaction API (JTA) 是Java平台上的一个事务处理规范，它为 Java 应用程序提供了一组统一的事务处理接口。JTA 是 Java EE 规范的一部分，旨在简化分布式事务处理。JTA 遵循 X/Open DTP模型，将事务管理器和资源管理器的接口抽象为 Java 接口。\nJTA 规定了事务管理器和分布式事务系统中涉及的各方之间的 Java 接口：应用程序、资源管理器和应用服务器。\nJTA包由两部分组成：\n应用接口，由应用程序划定事务边界 事务管理器接口，由应用服务器控制事务边界的划分 上图显示了 JTA 的三个主要接口，包括 JTA TransactionManager、JTA UserTransaction 和 JTA XA XAResource。该图还显示了 JTA 与 Java事务服务（JTS）的关系。\nJTA 组件被定义在 javax.transaction和 javax.transaction.xa 两个包内。其中 javax.transaction.xa\n# JTA 事务管理接口 JTA 支持事务管理服务的标准接口，应用服务器主要通过 TransactionManager 和 Transaction 接口来访问这些服务。\n应用服务器使用 TransactionManager 接口来管理用户应用程序的事务。 TransactionManager 将事务与线程相关联。TransactionManager上的begin()、**commit()和rollback()方法被应用服务器调用，分别为当前线程开始、提交和回滚事务。TransactionManager还支持setRollbackOnly()**方法，指定对当前线程的事务只支持回滚。**setTransactionTimeout()**方法还以秒为单位定义事务超时，getStatus() 方法返回当前线程事务的静态常量 Status。\n调用 TransactionManager.getTransaction() 可以获得当前线程关联的事务对象 Transaction。通过调用 TransactionManager.suspend() 可以暂停当前事务并获得 Transaction 对象， TransactionManager.resume() 方法恢复当前事务。\nTransaction接口表示具体的事务实例。Transaction由TransactionManager创建，提供了一些与事务相关的方法，如commit()，rollback()和getStatus()等。可以使用 setRollbackOnly() 调用告诉Transaction对象仅允许回滚。enlistResource 方法用于将 XAResource 对象添加到事务上下文中，delistResource方法用于将 XAResource对象从事务上下文中移除。\nSynchronization接口用于在事务完成时接收回调通知。调用 Transaction.registerSynchronization() 可以将Synchronization注册到与当前线程关联的事务中。\nStatus 接口定义了一组静态常量，表示事务的状态。\n# JTA 应用接口 JTA 的应用接口是 UserTransaction ，被应用程序用来控制事务边界。\n**UserTransaction.begin()**方法可以被应用程序调用，开始一个与应用程序当前线程相关联的事务。\nUserTransaction.commit() 提交与当前线程关联的事务。UserTransaction.rollback() 回滚与当前线程关联的事务。通过调用UserTransaction.setRollbackOnly()，设置与当前线程相关的事务只能被回滚。\n通过调用UserTransaction.setTransactionTimeout()可以设置与事务相关的超时，超时的单位是秒。事务状态Status可以通过 UserTransaction.getStatus() 获得。\nEJB 可以依赖声明式和容器管理事务。但是如果希望 EJB 以编程方式管理自己的事务，就可以利用UserTransaction接口。Servlets 和 JSP 也可以利用 UserTransaction 接口来划分事务。UserTransaction 可以从JNDI查询中获得，或者直接从 EJB 容器环境中获得。\n# JTA 和 X/Open XA X/Open 制定的 XA 规范 定义了分布式资源管理器的接口，被 X/Open DTP 模型中的分布式事务管理器访问。JTA 使用 XAResource 和 Xid 接口封装 XA。TransactionManager 使用 XAResource 接口来管理资源间的分布式事务。\nXid 是分布式事务的标识符，可以从 Xid 获取标准的X/Open格式标识符、全局事务标识符字节和分支标识符。\nXAResource 接口是事务管理器和资源管理器之间标准 X/Open 接口的 Java 映射。资源管理器的资源适配器必须实现 XAResource 接口，使资源能够参与进分布式事务。一个资源管理器的例子是关系数据库，对应的资源适配器就是 JDBC 接口。\n**XAResource.start()**方法用于将分布式事务与资源关联。**XAResource.end()**将资源与事务分离。XAResource还提供了提交、准备提交、回滚、恢复和遗忘分布式事务的方法。事务超时也可以从XAResource中设置和获取。\n# Java Transaction Service (JTS) CORBA Object Transaction Service (OTS) 将分布式事务处理模型（DTP）扩展到了对象领域，它提供了一种在分布式对象系统中进行事务处理的方法。OTS 定义了一组标准的接口和协议，允许 CORBA 对象参与分布式事务。Java Transaction Service (JTS) 是 OTS 的 Java 映射， JTA 推荐使用 JTS 作为其底层事务系统的实现。\n从事务管理器的角度来看，JTA 接口是以 high-level 的形式出现，而 JTS 是事务管理器内部使用的 low-level 接口。应用服务器间的事务互操作性是通过底层使用 JTS 实现获得的。\n# CORBA 由 Object Management Group（OMG）定义的通用对象请求代理架构（Common Object Request Broker Architecture，CORBA）是一个由包括IBM、BEA和惠普在内的工业联盟制定的标准，它促进了可互操作应用程序的构建，这些应用程序基于分布式对象的概念。\nCORBA 使用一个标准的通信模型，在这个模型上，用不同的语言组合实现的客户和服务器，以及在不同的硬件和操作系统平台上运行的客户和服务器可以进行交互。CORBA 体系结构主要包含以下几个部分：\n对象请求代理（Object Request Broker，ORB），它使对象能够在分布式的异质环境中透明地发出和接收请求。这个组件是OMG参考模型的核心。 对象服务，一组支持使用和实现对象功能的服务集合。这些服务是构建分布式应用程序所必需的，例如Object Transaction Service (OTS)。 通用设施，应用程序可能需要的其他有用服务。 CORBA 比 Java EE 的出现早了十年，并且不受限于单一的实现语言。在 Java EE 出现之前，CORBA 是企业应用程序的标准开发平台。 EJB 采用的底层分布式对象通信协议是由 CORBA 定义的。EJB 使用 CORBA 通信协议将它们的服务暴露给客户，也可以使用 CORBA 通信协议与其他 EJB 和基于 CORBA 的服务器环境通信。一些 CORBA 服务，如 CORBA 命名服务、CORBA 事务和 CORBA 安全，被 Java EE 标准所接受，作为创建可互操作的 EJB 服务的手段。\n# ORB ORB是 CORBA 的核心组件，负责在客户端和服务端之间传递请求和响应。ORB的主要功能包括：\n为客户端提供透明访问：客户端可以像调用本地对象一样调用远程对象，而不用关心底层通信和数据交换的细节。 定位和激活服务对象：ORB负责在分布式系统中查找和激活服务对象，以便客户端能够与它们进行通信。 消息封装和解封装：ORB将客户端的请求封装为消息，并在服务端解封装，以便服务对象能够处理请求。响应也会经过类似的处理。 系统间通信：ORB处理不同系统间的通信，包括连接管理、错误处理和安全性。 跨平台和跨语言：通过 IDL，ORB 可以实现不同编程语言之间的对象互操作。 # GIOP 和 IIOP GIOP 是一种通用的协议，用于定义分布式系统中不同 ORB之间的通信。GIOP 指定了在 ORB 之间传递的消息格式和通信规则。IIOP 是一种基于 TCP/IP 协议的 GIOP 实现。\nGIOP 将 IDL 数据类型映射成二进制数据流，并通过网络发送。GIOP 使用通用数据表示（Common Data Representation ，CDR）语法来完成这一任务，以有效地在IDL数据类型和二进制数据流之间进行映射。\nIIOP 将 GIOP 消息数据映射到 TCP/IP 连接行为，以及对输入/输出流的读/写。当一个 CORBA 服务器对象要被分发时，ORB 通过Interoperable Object Reference (IOR) 使网络上唯一识别该对象的信息可用。IOR 包含 CORBA 服务器对象进程的 IP 地址和 TCP 端口。CORBA 客户端利用IOR 建立和CORBA 服务器的连接。\n# RMI/IIOP Java 远程方法调用（JAVA REMOTE METHOD INVOCATION，RMI）框架是Java的分布式对象通信框架。RMI允许客户端和服务器将对象作为方法参数和返回值通过值或引用来传递。如果在方法参数或返回类型中使用的类的类型对客户端或服务器都是未知的，它可以被动态加载。RMI还为分布式垃圾收集提供了一种方法，以清理不再被任何分布式客户端引用的任何分布式服务器对象。\nRMI 客户端与实现 Java 接口的对象对话，该接口与特定 RMI 服务器暴露的远程接口相对应。该接口实际上是由 RMI stub实现的，它接受来自 RMI 客户端的调用，并将其打包成可通过网络发送的序列化数据包。同样地，stub 将来自RMI服务器的序列化响应数据包解封为可由RMI客户端使用的Java对象。\nRemote Reference Layer 从RMI stub 获取序列化的数据，并处理建立在传输协议之上的 RMI 特定的通信协议。Remote Reference Layer 的职责包括解决RMI服务器的位置，启动连接，以及激活远程服务器。\nRMI目前支持两个网络传输协议。JRMP 是标准的 RMI 通信信息传递协议。CORBA的 IIOP 消息传输协议现在也可以通过 RMI/IIOP 标准扩展来实现。\nJRMP 是一个非标准的协议，不能实现与跨语言的 CORBA 对象的通信。与 JRMP 不同，RMI/IIOP 可以在不同平台和编程语言之间进行通信，因为它使用了 CORBA 的 IIOP 协议。RMI/IIOP 使用 IDL 来定义远程对象的接口，这样不同编程语言的客户端都可以调用远程对象。RMI/IIOP 使用 CORBA 的对象传输方式，而不是 Java 序列化，这样可以实现跨平台和跨编程语言的对象传输。\n# OTS OTS 定义了事务服务实现的接口。OTS 的接口基本上可以分为客户端可用的接口和服务器可用的接口。这些接口之间有一些重叠，因为在某些情况下需要同时提供给客户和服务器。\n简要地描述一下这些接口在OTS规范中的作用：\nCurrent 是应用开发者与事务实现的典型交互方式，允许事务的开始和结束。使用 Current 创建的事务会自动与调用的线程相关联。底层实现通常会使用 TransactionFactory 来创建top-level 事务。OTS 规范允许事务被嵌套。\nControl 接口提供对特定事务的访问，实际上包装了事务 Coordinator 和 Terminator 接口，分别用于 enlist 参与者和结束事务。把这个功能分成两个接口的原因之一是，为了更精细的控制可终止事务的实体。\nResource/SubtransactionAwareResource 接口代表事务参与者，可以兼容任何两阶段提交协议的实现，包括 X/Open XA。\n每个top-level 事务都有一个相关的RecoveryCoordinator，参与者可以使用它来进行故障恢复。\nTransaction Context 主要作用是存储和传递与当前事务相关的信息。通过使用 Transaction Context，OTS 中的事务参与者可以共享同一事务上下文，从而实现对事务的正确协调和管理。\n使用 OTS 接口进行事务划分和传播时，有两种使用模式：\nIndirect/Implicit 模式，事务使用 Current 接口创建、提交和回滚事务。事务传播根据目标对象 POA 中的策略自动进行。 Direct/Explicit 模式，事务使用 TransactionFactory 创建，并使用 Control 对象进行提交或回滚。事务传播是通过向每个 IDL 操作添加参数（例如，事务的控制对象）来完成。 大多数应用程序的首选 Indirect/Implicit 模式，Direct/Explicit 模式提供了更大的灵活性，但更难管理。\n# JTS Java Transaction Service（JTS）规范是 OTS 规范的Java语言映射。使用符合 JTS 的实现在理论上允许与其他 JTS 实现的互操作。\nJTS API 通过规范提供 IDL 生成，主要的接口在 org.omg.CosTransactions 和 org.omg.CosTSPortability 包中。 Java 应用服务器通过 JTA 接口访问事务管理功能，JTA 通过 JTS 与事务管理的实现进行交互。同样，JTS 可以通过 JTA XA 接口访问资源，也可以访问启用 OTS 的非 XA 资源。JTS 实现可以通过 CORBA OTS 接口进行互操作。JTS 必须支持扁平事务模型。JTS 可以支持嵌套事务模型，但不是必需的。\n从 Transaction Manager 的角度来看，JTS 的实现是不需要公开。上图 Transaction Manager 框中的虚线说明了JTA 和 JTS 之间的专用接口，允许 JTA 与底层 OTS 实现进行交互。\nJTS 使用 CORBA OTS 接口来实现互操作性和可移植性（即通过 CosTransactions 和 CosTSPortability），这些接口为利用 IIOP 在 JTS 之间生成和传播事务上下文的实现定义了标准机制。\n总之，JTA 是暴露给用户和应用服务器使用的接口，应用服务器内部可以使用 JTS 作为其底层事务系统的实现，应用服务器间的事务互操作性是通过底层使用 JTS 实现获得的。\n","date":"2023-04-20T18:35:13+08:00","permalink":"https://mazhen.tech/p/java-ee%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86/","title":"Java EE应用服务器的事务管理"},{"content":"数据库连接池是应用服务器的基本功能，但有时用户因为性能、监控等需求，想使用第三方的连接池。如果只是使用第三方连接池管理数据库连接，那么直接在应用中引入就可以了，但如果用户同时还需要应用服务器的分布式事务和安全服务，就没那么简单了。\n为了讲清楚，首先需要了解一下 JDBC 基本概念。\n# Connection 从 JDBC driver 的角度来看，Connection 表示客户端会话。应用程序可以通过以下两种方式获取连接：\nDriverManager 最初的JDBC 1.0 API中被引入，当应用程序首次尝试通过指定URL连接到数据源时，DriverManager将自动加载在 CLASSPATH 中找到的任何 JDBC driver。 DataSource 是在 JDBC 2.0 可选包API中引入的接口。它允许应用程序对底层数据源的细节是透明的。DataSource 对象的属性被设置为表示特定数据源。当调用其 getConnection方法时，DataSource 实例将返回到该数据源的连接。通过简单地更改DataSource对象的属性，可以将应用程序定向到不同的数据源；无需更改应用程序代码。同样，可以更改 DataSource 实现而不更改使用它的应用程序代码。 JDBC 还定义了 DataSource 接口的两个重要扩展：\nConnectionPoolDataSource - 支持物理连接的缓存和重用，从而提高应用程序的性能和可扩展性 XADataSource - 提供可以参与分布式事务的连接 从 DriverManager 和 DataSource 都可以获得 Connection。\nDataSource、ConnectionPoolDataSource 和 XADataSource 都继承自 CommonDataSource，但它们之间没有继承关系。\n从 ConnectionPoolDataSource 获得的是 PooledConnection，PooledConnection 并没有继承 Connection，但可以获得Connection。\n从 XADataSource 获得的是 XAConnection，XAConnection 继承了 PooledConnection，除了能获得 Connection，还可以获得 XAResource。\n# Application Server DataSource 应用服务器会为其客户端提供了一个 DataSource 接口的实现，并通过 JNDI 暴露给用户。这个 DataSource 包装了 jdbc driver 连接数据库的能力，并在此基础上提供连接池、事务和安全等服务。\n在配置应用服务器的 DataSource 时，一般需要指定 Connection 的获取方式：\njava.sql.Driver\njavax.sql.DataSource\njavax.sql.ConnectionPoolDataSource\njavax.sql.XADataSource\n这四种连接获取方式都是 JDBC driver 提供的能力，Driver 和 DataSource 是最基本方式。如果应用服务器的 DataSource 想要具备连接池化、分布式事务等服务，除了自身要实现这些功能以外，还需要底层 driver 提供相应的能力配合。\n# ConnectionPoolDataSource 以连接池为例，JDBC driver 提供了 ConnectionPoolDataSource 的实现，应用服务器使用它来构建和管理连接池。客户端在使用相同的 JNDI 和 DataSource API 的同时获得更好的性能和可扩展性。\n应用服务器维护维护一个从 ConnectionPoolDataSource 对象返回的 PooledConnection 对象池。应用服务器的实现还可以向 PooledConnection 对象注册ConnectionEventListener，以获得连接事件的通知，如连接关闭和错误事件。\n我们看到，应用程序客户端通过 JNDI 查找一个 DataSource 对象，并请求从 DataSource 获得一个连接。当连接池没有可用连接时，DataSource 的实现从 JDBC driver 的 ConnectionPoolDataSource 中请求一个新的 PooledConnection 。应用服务器的 DataSource 实现会向 PooledConnection 注册一个ConnectionEventListener，随后获得一个新的 Connection 对象。应用客户端在完成操作后调用 Connection.close()，会生成一个 ConnectionEvent 实例，该实例会返回给应用服务器的数据源实现。在收到连接关闭的通知后，应用服务器可以将连接对象放回连接池中。\n注意 ConnectionPoolDataSource 本身不是连接池，它是 driver 提供给应用服务器的接口契约，意思是你从 ConnectionPoolDataSource 获得的PooledConnection可以放心的缓存起来，同时连接关闭的时候，driver 会发送事件通知给应用服务器，真正的关闭连接还是放回连接池，由你自己决定。 一般 JDBC driver 提供的 ConnectionPoolDataSource 实现并没有内置连接池功能，需要配合应用服务器或其他第三方连接池一起使用。可以参考 MySQL Connector 的文档。\n# XADataSource 同样，如果想要分布式事务支持，应用服务器的 DataSource 需要依赖 driver 提供的 XADataSource 实现，同时通过 XAResource 和 Transaction Manager 交互。\nXADataSource 对象返回 XAConnection ，该对象扩展了 PooledConnection ，增加了对分布式事务的参与能力。应用服务器的 DataSource 实现在XAConnection 对象上调用 getXAResource() 以获得传递给事务管理器的 XAResource 对象。事务管理器使用 XAResource 来管理分布式事务。\n就像池化连接一样，这种分布式事务管理的标准API对应用程序客户端也是透明的。因此，应用服务器可以使用不同 JDBC driver 实现的XADataSource， 来组装可扩展的分布式事务支持的数据访问方案。\n# 直接整合外部连接池 如果想在应用服务器中直接整合第三方的连接池实现是比较困难的，下面分析一下原因。\nJTA 规范要求连接必须能够同时处理多个事务，这个功能被称为事务多路复用或事务交错。我们看一个例子：\n1 2 3 4 5 6 7 8 9 10 1. UserTransaction ut = getUserTransaction(); 2. DataSource ds = getDataSource(); 3. 4. ut.begin(); 5. 6. Connection c1 = ds.getConnection(); 7. // do some SQL 8. c1.close(); 9. 10. ut.commit(); 在第8行，连接将释放回连接池，另外一个线程就可以通过 getConnection() 获得刚释放的连接。但此时 c1 上的事务还没有提交，如果被其他线程获取，就有可能加入另一个事务，这就是为什么连接必须能够一次支持多个事务。\n大多数数据库都不支持事务多路复用，那么一种变通的做法是让事务独占连接，在 JTA 事务完成之前，连接不要释放连接回池中。\n因此，需要应用服务器的连接池实现能感知到事务，在第8行不会释放连接，而是连接被标记为关闭。在第10行事务提交后，标记为已关闭的所有连接才释放回连接池。\n现实中，应用服务器管理的连接池都是能够感知事务的存在，并通过 XAResource 和 Transaction Manager 进行交互：\n另外，应用服务器都实现了对 **JCA（Java EE Connector Architecture）**规范的支持。JCA 将应用服务器的事务、安全和连接管理等功能，与事务资源管理器集成，定义了一个标准的 SPI(Service Provider Interface) ，因此，一般应用服务器的连接池都在 JCA 中实现，JDBC DataSource 作为一种资源，被 JCA 统一管理。\n而外部连接池不能感知事务的存在，所以没办法做到事务对连接的独占，因此应用服务器不能简单的直接整合第三方连接池。\n# 解决方案 如果外部连接池实现了 XADataSource，那么我们可以把它当作普通的 JDBC driver，在配置应用服务器的 DataSource 时使用。需要注意几点：\n为外部连接池配置真正的 JDBC driver 时，要使用 driver的 XADataSource 作为连接的获取方式\n外部连接池作为特殊的 driver，已经内置了池化功能，连接池的相关参数最好和应用服务器的DataSource保持一致，因为连接池的实际大小受到外部连接池的约束\n外部连接池在使用前，一般需要进行初始化，同时，应用服务器在关闭 DataSource 时，也要关闭内置的外部连接池，避免连接泄漏。\n这个解决方案的问题是，应用服务器和外部连接池都对连接做了池化，实际上是建立了两个连接池，存在较大的浪费。一种变通的做法是，设置应用服务器连接池的空闲连接数为0，这样应用服务器的连接池不会持有连接，连接在使用完毕后会释放到外部连接池。连接由外部连接池管理，同时经过应用服务器 datasource的包装，能够享受应用服务器内置的事务和安全服务。\n当然更优的做法是，对外部连接池进行适当改造，让它能感知事务的存在，例如 Agroal 连接池能够被注入Transaction Manager，通过 Transaction Manager 感知到事务的存在，做到事务对连接的独占。\n","date":"2023-03-10T22:35:11+08:00","permalink":"https://mazhen.tech/p/%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%95%B4%E5%90%88%E7%AC%AC%E4%B8%89%E6%96%B9%E8%BF%9E%E6%8E%A5%E6%B1%A0/","title":"应用服务器整合第三方连接池"},{"content":"高度模块化的设计是 Nginx 的架构基础。Nginx主框架中只提供了少量的核心代码，大量强大的功能是在各模块中实现的。\n# 模块数据结构 # ngx_module_t 结构 Nginx 的模块化架构最基本的数据结构为 ngx_module_t，所有的模块都遵循着同样的接口设计规范。\nngx_module_t 是 ngx_module_s 的别名，定义在 src/core/ngx_core.h 中：\n1 typedef struct ngx_module_s ngx_module_t; 而 ngx_module_s 在 src/core/ngx_module.h 中定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 struct ngx_module_s { ngx_uint_t ctx_index; // 模块在同类型模块数组中的索引序号 ngx_uint_t index; // 模块在所有模块数组中的索引序号 char *name; // 模块的名称 ngx_uint_t spare0; // 保留变量 ngx_uint_t spare1; // 保留变量 ngx_uint_t version; // 模块的版本号 目前只有一种，默认为1 const char *signature; void *ctx; // 模块的上下文 不同的模块指向不同的上下文 ngx_command_t *commands; // 模块的命令集，指向一个 ngx_command_t 结构数组 ngx_uint_t type; // 模块类型 ngx_int_t (*init_master)(ngx_log_t *log); // master进程启动时回调 ngx_int_t (*init_module)(ngx_cycle_t *cycle); // 初始化模块时回调 ngx_int_t (*init_process)(ngx_cycle_t *cycle);// worker进程启动时回调 ngx_int_t (*init_thread)(ngx_cycle_t *cycle); // 线程启动时回调（nginx暂时无多线程模式） void (*exit_thread)(ngx_cycle_t *cycle); // 线程退出时回调 void (*exit_process)(ngx_cycle_t *cycle);// worker进程退出时回调 void (*exit_master)(ngx_cycle_t *cycle); // master进程退出时回调 uintptr_t spare_hook0; // 保留字段 uintptr_t spare_hook1; uintptr_t spare_hook2; uintptr_t spare_hook3; uintptr_t spare_hook4; uintptr_t spare_hook5; uintptr_t spare_hook6; uintptr_t spare_hook7; }; ngx_module_t定义了init_master，init_module，init_process，init_thread，exit_thread，exit_process，exit_master 这7个回调方法，分别在初始化 master、初始化模块、初始化 worker 进程、初始化线程、退出线程、退出 worker 进程、退出 master 时被调用。事实上，init_master、init_thread、exit_thread 这3个方法目前都没有使用。\n# ngx_command_t 结构 ngx_command_t 类型的 commands 数组指定了模块处理配置项的方法，在解析时配置文件会查找该表。ngx_command_t 是 ngx_command_s的别名，定义在 src/core/ngx_core.h 中：\n1 typedef struct ngx_command_s ngx_command_t; 而 ngx_command_s 在 src/core/ngx_conf_file.h 中定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 struct ngx_command_s { // 配置项的名称 ngx_str_t name; // 配置项的类型 ngx_uint_t type; // 配置项解析处理函数 char *(*set)(ngx_conf_t *cf, ngx_command_t *cmd, void *conf); // 用于指示配置项所处内存的相对偏移位置 ngx_uint_t conf; // 表示当前配置项在整个存储配置项的结构体中的偏移位置 ngx_uint_t offset; // 配置项读取后的处理方法 必须是ngx_conf_post_t结构体的指针 void *post; }; # 模块类型 Nginx 模块共有6种类型，由 ngx_module_t-\u0026gt;type 字段表示。类型常量分别定义在：\nsrc/core/ngx_conf_file.h 1 2 #define NGX_CORE_MODULE 0x45524F43 /* \u0026#34;CORE\u0026#34; */ #define NGX_CONF_MODULE 0x464E4F43 /* \u0026#34;CONF\u0026#34; */ src/event/ngx_event.h 1 #define NGX_EVENT_MODULE 0x544E5645 /* \u0026#34;EVNT\u0026#34; */ src/http/ngx_http_config.h 1 #define NGX_HTTP_MODULE 0x50545448 /* \u0026#34;HTTP\u0026#34; */ src/stream/ngx_stream.h 1 #define NGX_STREAM_MODULE 0x4d525453 /* \u0026#34;STRM\u0026#34; */ src/mail/ngx_mail.h 1 #define NGX_MAIL_MODULE 0x4C49414D /* \u0026#34;MAIL\u0026#34; */ 所有模块间分层次、分类别的。Nginx 官方共有6大类型的模块：核心模块、配置模块、事件模块、HTTP模块、mail模块和stream模块。\nngx_module_t 中有一个类型为 void* 的 ctx成员，其定义了该模块的公共接口，每类模块都有各自特有的属性，通过 void* 类型的ctx 变量进行抽象，同类型的模块遵循同一套通用性接口。\n模块都具备相同的 ngx_module_t 接口，但 ctx 指向不同的结构。由于配置类型 NGX_CONF_MODULE的模块只拥有1个模块 ngx_conf_module，所以没有具体化 ctx上下文成员。\n配置模块和核心模块这两种模块类型是由Nginx的框架代码所定义的。\n配置模块 ngx_conf_module 是所有模块的基础，它实现了最基本的配置项解析功能（解析 nginx.conf文件），其他模块在生效前都需要依赖配置模块处理配置指令并完成各自的准备工作。\n核心模块的模块类型是 NGX_CORE_MODULE。目前官方的核心类型模块中共有6个具体模块，分别是 ngx_core_module、ngx_errlog_module、ngx_events_module、ngx_openssl_module、ngx_http_module 和 ngx_mail_module。Nginx 框架代码只关注 6个核心模块，而大部分模块都是非核心模块。\n核心模块的 ctx 变量指向的是名为 ngx_core_module_t （src/core/ngx_module.h）的结构体。这个结构体很简单，除了一个 name 成员就只有 create_conf和 init_conf两个方法。\n1 2 3 4 5 6 7 typedef struct { ngx_str_t name; // 解析配置项前Nginx框架会调用 void *(*create_conf)(ngx_cycle_t *cycle); // 解析配置项完成后，Nginx框架会调用 char *(*init_conf)(ngx_cycle_t *cycle, void *conf); } ngx_core_module_t; ngx_core_module_t 是以配置项的解析作为基础的。 create_conf 回调方法来创建存储配置项的数据结构，init_conf回调方法使用解析出的配置项初始化核心模块功能。例如核心模块ngx_core_module （src/core/nginx.c）的 ctx 实例化为 ngx_core_module_ctx，定义了 ngx_core_module_create_conf 和 ngx_core_module_init_conf 回调方法。\n1 2 3 4 5 static ngx_core_module_t ngx_core_module_ctx = { ngx_string(\u0026#34;core\u0026#34;), ngx_core_module_create_conf, ngx_core_module_init_conf }; 核心模块可以定义全新的模块类型。例如核心模块 ngx_http_module（src/http/ngx_http.c）定义了 NGX_HTTP_MODULE 模块类型，所有HTTP类型的模块都由 ngx_http_module核心模块管理。同样的，ngx_events_module 定义了 NGX_EVENT_MODULE 模块类型，ngx_mail_module 定义了 NGX_MAIL_MODULE 模块类型。\n核心模块 ngx_http_module 作为所有 HTTP 模块的 “代言”，负责加载所有的 HTTP 模块。同时，在类型为NGX_HTTP_MODULE 的模块中，ngx_http_core_module（src/http/ngx_http_core_module.c）作为 HTTP 核心业务与管理功能的模块，决定了 HTTP 业务的核心逻辑，以及对于具体的请求该选用哪一个HTTP 模块处理这样的工作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 static ngx_http_module_t ngx_http_core_module_ctx = { ngx_http_core_preconfiguration, /* preconfiguration */ ngx_http_core_postconfiguration, /* postconfiguration */ ngx_http_core_create_main_conf, /* create main configuration */ ngx_http_core_init_main_conf, /* init main configuration */ ngx_http_core_create_srv_conf, /* create server configuration */ ngx_http_core_merge_srv_conf, /* merge server configuration */ ngx_http_core_create_loc_conf, /* create location configuration */ ngx_http_core_merge_loc_conf /* merge location configuration */ }; ngx_module_t ngx_http_core_module = { NGX_MODULE_V1, \u0026amp;ngx_http_core_module_ctx, /* module context */ ngx_http_core_commands, /* module directives */ NGX_HTTP_MODULE, /* module type */ NULL, /* init master */ NULL, /* init module */ NULL, /* init process */ NULL, /* init thread */ NULL, /* exit thread */ NULL, /* exit process */ NULL, /* exit master */ NGX_MODULE_V1_PADDING }; 事件模块、mail模块和HTTP模块类似，它们都在核心模块中各有1个模块作为自己的“代言人”，并在同类模块中有1个作为核心业务与管理功能的模块。\n# 模块初始化 在Nginx的编译阶段执行 configure 后，会在 objs 目录下生成 nginx_modules.c 源文件。这个源文件中有两个很重要的全局变量 ngx_modules 和 ngx_module_names，前者保存了 Nginx 将要使用的全部模块，后者则记录了这些模块的名称。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ngx_module_t *ngx_modules[] = { \u0026amp;ngx_core_module, \u0026amp;ngx_errlog_module, \u0026amp;ngx_conf_module, \u0026amp;ngx_openssl_module, \u0026amp;ngx_regex_module, \u0026amp;ngx_events_module, \u0026amp;ngx_event_core_module, ... }; char *ngx_module_names[] = { \u0026#34;ngx_core_module\u0026#34; \u0026#34;ngx_errlog_module\u0026#34;, \u0026#34;ngx_conf_module\u0026#34;, \u0026#34;ngx_openssl_module\u0026#34;, \u0026#34;ngx_regex_module\u0026#34;, \u0026#34;ngx_events_module\u0026#34;, \u0026#34;ngx_event_core_module\u0026#34;, ... }; # main函数入口 main函数入口定义在 src/core/nginx.c 文件中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 int ngx_cdecl main(int argc, char *const *argv) { ngx_buf_t *b; ngx_log_t *log; ngx_uint_t i; ngx_cycle_t *cycle, init_cycle; ngx_conf_dump_t *cd; ngx_core_conf_t *ccf; ngx_debug_init(); ... } # 预初始化 ngx_preinit_module 负责初始化 ngx_modules 数组所有模块的 index 和 name，计算ngx_max_module和 ngx_modules_n。\n1 2 3 4 5 6 7 8 9 int ngx_cdecl main(int argc, char *const *argv) { ... if (ngx_preinit_modules() != NGX_OK) { return 1; } ... } # ngx_cycle_t 结构体 ngx_cycle_t 是 Nginx 框架最核心的一个结构体，其存储在系统运行过程中的所有信息，包括配置文件信息、模块信息、客户端连接、读写事件处理函数等信息。Nginx 围绕着ngx_cycle_t (src/core/ngx_cycle.h) 来控制进程的运行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 typedef struct ngx_cycle_s ngx_cycle_t; struct ngx_cycle_s { // 保存着所有模块存储配置项的结构体的指针，它首先是一个数组，每个数组成员又是一个指针，这个指针指向另一个存储着指针的数组 void ****conf_ctx; ngx_pool_t *pool; ngx_log_t *log; ngx_log_t new_log; ngx_uint_t log_use_stderr; /* unsigned log_use_stderr:1; */ ngx_connection_t **files; ngx_connection_t *free_connections; ngx_uint_t free_connection_n; // 模块数组 ngx_module_t **modules; ngx_uint_t modules_n; ngx_uint_t modules_used; /* unsigned modules_used:1; */ ngx_queue_t reusable_connections_queue; ngx_uint_t reusable_connections_n; time_t connections_reuse_time; ngx_array_t listening; ngx_array_t paths; ngx_array_t config_dump; ngx_rbtree_t config_dump_rbtree; ngx_rbtree_node_t config_dump_sentinel; ngx_list_t open_files; ngx_list_t shared_memory; ngx_uint_t connection_n; ngx_uint_t files_n; ngx_connection_t *connections; ngx_event_t *read_events; ngx_event_t *write_events; ngx_cycle_t *old_cycle; ngx_str_t conf_file; ngx_str_t conf_param; ngx_str_t conf_prefix; ngx_str_t prefix; ngx_str_t error_log; ngx_str_t lock_file; ngx_str_t hostname; }; 模块初始化仅关注 cycle-\u0026gt;modules 和 cycle-\u0026gt;conf_ctx 。\n# ngx_init_cycle 模块的初始化是在 main 中调用函数 ngx_init_cycle 完成的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 int ngx_cdecl main(int argc, char *const *argv) { ... cycle = ngx_init_cycle(\u0026amp;init_cycle); if (cycle == NULL) { if (ngx_test_config) { ngx_log_stderr(0, \u0026#34;configuration file %s test failed\u0026#34;, init_cycle.conf_file.data); } return 1; } ... } 函数 ngx_init_cycle 定义在 src/ngx_cycle.c 文件中。\n# 创建核心模块配置解析上下文 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ngx_cycle_t * ngx_init_cycle(ngx_cycle_t *old_cycle) { ... //配置上下文 cycle-\u0026gt;conf_ctx = ngx_pcalloc(pool, ngx_max_module * sizeof(void *)); ... //处理core模块，cycle-\u0026gt;conf_ctx用于存放所有CORE模块的配置 for (i = 0; cycle-\u0026gt;modules[i]; i++) { if (cycle-\u0026gt;modules[i]-\u0026gt;type != NGX_CORE_MODULE) { //跳过非核心模块 continue; } module = cycle-\u0026gt;modules[i]-\u0026gt;ctx; //只有ngx_core_module有create_conf回调函数,这个会调用函数会创建ngx_core_conf_t结构， //用于存储整个配置文件main scope范围内的信息，比如worker_processes，worker_cpu_affinity等 if (module-\u0026gt;create_conf) { rv = module-\u0026gt;create_conf(cycle); if (rv == NULL) { ngx_destroy_pool(pool); return NULL; } cycle-\u0026gt;conf_ctx[cycle-\u0026gt;modules[i]-\u0026gt;index] = rv; } } ... } # 配置文件解析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ngx_cycle_t * ngx_init_cycle(ngx_cycle_t *old_cycle) { ... //conf表示当前解析到的配置命令上下文，包括命令，命令参数等 conf.args = ngx_array_create(pool, 10, sizeof(ngx_str_t)); ... conf.temp_pool = ngx_create_pool(NGX_CYCLE_POOL_SIZE, log); ... conf.ctx = cycle-\u0026gt;conf_ctx; conf.cycle = cycle; conf.pool = pool; conf.log = log; conf.module_type = NGX_CORE_MODULE; //conf.module_type指示将要解析这个类型模块的指令 conf.cmd_type = NGX_MAIN_CONF; //conf.cmd_type指示将要解析的指令的类型 //真正开始解析配置文件中的每个命令 if (ngx_conf_parse(\u0026amp;conf, \u0026amp;cycle-\u0026gt;conf_file) != NGX_CONF_OK) { ... } ... } cycle-\u0026gt;conf_ctx 是一个长度为 ngx_max_module 的数组，每个元素是各个模块的配置结构体。conf_ctx 的创建过程和组织方式会在 ngx_conf_parse 中完成。\nngx_conf_parse 解析配置文件中的命令，conf 存放解析配置文件的上下文信息，如 module_type 表示将要解析模块的类型，cmd_type 表示将要解析的指令的类型，ctx指向解析出来信息的存放地址，args 存放解析到的指令和参数。具体每个模块配信息的存放如下图所示，NGX_MAIN_CONF表示的是全局作用域对应的配置信息，NGX_EVENT_CONF表示的是 EVENT模块对应的配置信息，NGX_HTTP_MAIN_CONF，NGX_HTTP_SRV_CONF，NGX_HTTP_LOC_CONF表示的是HTTP模块对应的main、server和local域的配置信息。\n# 初始化核心模块的配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ngx_cycle_t * ngx_init_cycle(ngx_cycle_t *old_cycle) { ... //初始化所有core module模块的config结构。调用ngx_core_module_t的init_conf, //在所有core module中，只有ngx_core_module有init_conf回调， //用于对ngx_core_conf_t中没有配置的字段设置默认值 for (i = 0; cycle-\u0026gt;modules[i]; i++) { if (cycle-\u0026gt;modules[i]-\u0026gt;type != NGX_CORE_MODULE) { continue; } module = cycle-\u0026gt;modules[i]-\u0026gt;ctx; if (module-\u0026gt;init_conf) { if (module-\u0026gt;init_conf(cycle, cycle-\u0026gt;conf_ctx[cycle-\u0026gt;modules[i]-\u0026gt;index]) == NGX_CONF_ERROR) { environ = senv; ngx_destroy_cycle_pools(\u0026amp;conf); return NULL; } } } ... } # 初始化各个模块 1 2 3 4 5 6 7 8 9 10 ngx_cycle_t * ngx_init_cycle(ngx_cycle_t *old_cycle) { ... if (ngx_init_modules(cycle) != NGX_OK) { /* fatal */ exit(1); } ... } ngx_init_modules 定义在 src/core/ngx_module.c中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 ngx_int_t ngx_init_modules(ngx_cycle_t *cycle) { ngx_uint_t i; for (i = 0; cycle-\u0026gt;modules[i]; i++) { if (cycle-\u0026gt;modules[i]-\u0026gt;init_module) { if (cycle-\u0026gt;modules[i]-\u0026gt;init_module(cycle) != NGX_OK) { return NGX_ERROR; } } } return NGX_OK; } ","date":"2022-11-23T15:06:21+08:00","permalink":"https://mazhen.tech/p/nginx%E7%9A%84%E6%A8%A1%E5%9D%97%E5%8C%96%E8%AE%BE%E8%AE%A1/","title":"Nginx的模块化设计"},{"content":" # 下载Nginx源码 在 nginx: download 选择当前稳定版本下载\n1 2 curl -OL https://nginx.org/download/nginx-1.22.1.tar.gz tar -zxvf nginx-1.22.1.tar.gz # 下载Nginx依赖 PCRE rewrite 模块依赖 从 sourceforge 下载 pcre-8.45.tar.gz，和Nginx源码解压到同级目录。\nzlib gzip 模块依赖 1 2 curl -OL https://zlib.net/zlib-1.2.13.tar.gz tar -zxvf zlib-1.2.13.tar.gz OpenSSL 1 2 curl -OL https://www.openssl.org/source/openssl-1.1.1s.tar.gz tar -zxvf openssl-1.1.1s.tar.gz # 修改默认配置 Nginx默认以 daemon 形式运行，会使用 double fork 技巧，调用 fork() 创建子进程并且把父进程直接丢弃，达到将 daemon 进程与会话的控制终端分离的目的。同时，Nginx 默认是多进程架构，有一个 master 父进程和多个 worker 子进程。为了调试方便，可以修改默认配置 conf/nginx.conf，关闭 daemon，并以单进程模式运行：\n1 2 daemon off; master_process off; # 编译选项配置 使用 configure 命令进行相关编译参数配置：\n--with-debug 启用 debugging log --with-cc-opt='-O0 -g' ，使用 -g 包含 debug 符号信息，-O0标志禁用编译器优化 --prefix 指定安装目录 --with-... 指定依赖的源码位置 1 2 3 4 5 6 ./configure --with-debug --with-cc-opt=\u0026#39;-O0 -g\u0026#39; \\ --prefix=./dist \\ --with-http_ssl_module \\ --with-pcre=../pcre-8.45 \\ --with-zlib=../zlib-1.2.13 \\ --with-openssl=../openssl-1.1.1s # 编译和安装 1 2 make make install # 配置VSCode 首先参考 VSCode 官方文档，完成 VS Code C++ 开发环境的配置。\n确认本机是否已经安装了 Clang 编译器： 1 2 3 4 # 确认是否安装了Clang $ clang --version # 安装开发者命令行工具，包括Clang、git等 $ xcode-select --install 安装 C++ extension for VS Code。 完成C++开发环境准备后，使用 VSCode 打开 nginx 源码，点击菜单 \u0026ldquo;Run -\u0026gt; Starting Debugging\u0026rdquo;，在提示中选择 LLDB，创建出 launch.json，编辑该文件进行 debug 配置。将 \u0026ldquo;program\u0026rdquo; 设置为上一步编译出带有debug信息的nginx。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;lldb\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Debug\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/dist/sbin/nginx\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34; } ] } 现在就可以在代码中设置断点，再次点击 \u0026ldquo;Run -\u0026gt; Starting Debugging\u0026rdquo;，开始调试 Nginx 吧。\n","date":"2022-11-21T11:18:25+08:00","permalink":"https://mazhen.tech/p/%E5%9C%A8macos%E4%B8%8A%E4%BD%BF%E7%94%A8vscode%E8%B0%83%E8%AF%95nginx/","title":"在macOS上使用VSCode调试NGINX"},{"content":" # Nginx 进程模型 Nginx其实有两种进程结构，一种是单进程结构，一种是多进程结构。单进程结构只适合我们做开发调试，在生产环境下，为了保持 Nginx 足够健壮，以及可以利用到 CPU 的多核特性，我们用到的是多进程架构的Nginx。\n多进程架构的Nginx，有一个父进程 master process，master 会有很多子进程，这些子进程分为两类，一类是worker 进程，一类是 cache 相关的进程。\n在一个四核的Linux服务器上查看Nginx进程：\n1 2 3 4 5 6 7 8 9 $ ps -ef --forest | grep nginx mazhen 20875 13073 0 15:25 pts/0 00:00:00 | \\_ grep --color=auto nginx mazhen 20862 1 0 15:25 ? 00:00:00 nginx: master process ./sbin/nginx mazhen 20863 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20864 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20865 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20866 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20867 20862 0 15:25 ? 00:00:00 \\_ nginx: cache manager process mazhen 20868 20862 0 15:25 ? 00:00:00 \\_ nginx: cache loader process 可以看到，Nginx 的 master 进程创建了4个 worker 进程，以及用来管理磁盘内容缓存的缓存helper进程。\n为什么Nginx使用的是多进程结构，而不是多线程结构呢？因为多线程结构，线程之间是共享同一个进程地址空间，当某一个第三方模块出现了地址空间的断错误时，会导致整个Nginx进程挂掉，而多进程模型就不会出现这样的问题，Nginx的第三方模块通常不会在 master 进程中加入自己的功能代码。\nmaster 进程执行一些特权操作，比如读取配置以及绑定端口，它管理 worker 进程的，负责监控每个 worke进程是否在正常工作，是否需要重载配置文件，以及做热部署等。\nworker 进程处理真正的请求，从磁盘读取内容或往磁盘中写入内容，以及与上游服务器通信。\ncache manager 进程会周期性地运行，从磁盘缓存中删除条目，以保证缓存没有超过配置的大小。\ncache loader 进程在启动时运行，用于将磁盘上的缓存加载到内存中，随后退出。\nNginx 采用了事件驱动的模型，它希望 worker 进程的数量和 CPU 一致，并且每一个 worker 进程与某一颗CPU绑定，worker 进程以非阻塞的方式处理多个连接，减少了上下文切换，同时更好的利用到了 CPU 缓存，减少缓存失效。\n# 请求处理流程 Nginx 使用的是非阻塞的事件驱动处理引擎，需要用状态机来把这个请求正确的识别和处理。Nginx 内部有三个状态机，分别是处理4层 TCP 流量的传输层状态机，处理7层流量的HTTP状态机和处理邮件的email 状态机。\nworker 进程首先等待监听套接字上的事件，新接入的连接会触发事件，然后连接分配到一个状态机。\n状态机本质上是告诉 Nginx 如何处理请求的指令集。解析出的请求是要访问静态资源，那么就去磁盘加载静态资源，更多的时候 Nginx 是作为负载均衡或者反向代理使用，这个时候请求会通过4层或7层协议，传输到上游服务器。对于每一个处理完成的请求，Nginx会记录 access 日志和 error 日志。\n# Nginx 进程管理 Linux 上多进程之间进行通讯，可以使用共享内存和信号。Nginx 在做进程间的管理时，使用了信号。我们可以使用 kill 命令直接向 master 进程和 worker 进程发送信号，也可以使用 nginx 命令行。\nmaster 进程接收处理的信号：\nCHLD 在 Linux 系统中，当子进程终止的时候，会向父进程发送 CHLD 信号。master 进程启动的 worker 进程，所以 master 是 worker 的父进程。如果 worker 进程由于一些原因意外退出，那么 master 进程会立刻收到通知，可以重新启动一个新的 worker进程。\nTERM 和 INT 立刻终止 worker 和 master 进程。\nQUIT 优雅的停止 worker 和 master 进程。worker 不会向客户端发送 reset 立即结束连接。\nHUP 重新加载配置文件\nUSR1 重新打开日志文件，做日志文件的切割\nUSR2 通知 master 开始进行热部署\nWINCH 在热部署过程中，通知旧的 master ，让它优雅关闭 worker 进程\n我们也可以通过 nginx -s 命令向 master 进程发送信号。在 Nginx 启动过程中， Nginx 会把 master 的 PID 记录在文件中，这个文件的默认位置是 $nginx/logs/nginx.pid 。 当我们执行 nginx -s 命令时，nginx 命令会去读取 nginx.pid 文件中 master 进程的 PID，然后向 master 进程发送对应的信号。下面是 nginx -s 命令对应的信号：\nreload - HUP reopen - USR1 stop - TERM quit - QUIT 使用 nginx -s 和 直接使用 kill 命令向 master 进程发送信号，效果是一样的。\n注意，USR2 和 WINCH 没有对应的 nginx -s 命令，只能通过 kill 命令直接向 master 进程发送。\nworker 进程能接收的信号：\nTERM 和 INT QUIT USR1 WINCH worker 进程收到这些信号，会产生和发给 master 一样的效果。但我们通常不会直接向 worker 进程发送信号，而是通过 master 进程来管理 worker 进程，master 进程收到信号以后，会再把信号转发给 worker 进程。\n# Nginx 配置更新流程 当更改了 Nginx 配置文件后，我们都会执行 nginx -s reload 命令重新加载配置文件。Nginx 不会停止服务，在处理新的请求的同时，平滑的进行配置文件的更新。\n执行 nginx -s reload 命令，会向 master 进程发送 SIGHUP 信号。当 master 进程接收 SIGHUP信号后，会做如下处理：\n检查配置文件语法是否正确。 master 加载配置，启动一组新的 worker 进程。这些 worker 进程马上开始接收新连接和处理网络请求。子进程可以共享使用父进程已经打开的端口，所以新的 worker 可以和老的worker监听同样的端口。 master 向旧的 worker 发送 QUIT 信号，让旧的 worker 优雅退出。 旧的 worker 进程停止接收新连接，完成现有连接的处理后结束进程。 # Nginx 热部署流程 Nginx 支持热部署，在升级的过程中也实现了高可用性，不导致任何连接丢失，停机时间或服务中断。热部署的流程如下：\n备份旧的 nginx 二进制文件，将新的nginx二进制文件拷贝到 $nginx_home/sbin目录。 向 master 进程发送 USR2 信号。 master 进程用新的nginx文件启动新的master进程，新的master进程会启动新的worker进程。 向旧的 master 进程发送 WINCH 信号，让它优雅的关闭旧的 worker 进程。此时旧的 master 仍然在运行。 如果想回滚到旧版本，可以向旧的 master 发送 HUP 信号，向新的master 发送QUIT信号。 如果一切正常，可以向旧的 master 发送 QUIT 信号，关闭旧的 master。 ","date":"2022-11-18T11:16:48+08:00","permalink":"https://mazhen.tech/p/nginx%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/","title":"Nginx架构基础"},{"content":"反向代理（reverse proxy）是指用代理服务器来接受外部的访问请求，然后将请求转发给内网的上游服务器，并将从上游服务器上得到的结果返回外部客户端。作为反向代理是 Nginx 的一种常见用法。\n这里的负载均衡是指选择一种策略，尽量把请求平均地分布到每一台上游服务器上。下面介绍负载均衡的配置项。\n# upstream 作为反向代理，一般都需要向上游服务器的集群转发请求。upstream 块定义了一个上游服务器的集群，便于反向代理中的 proxy_pass使用。\n1 2 3 4 5 6 7 http { ... upstream backend { server 127.0.0.1:8080; } ... } upstream 定义了一组上游服务器，并命名为 backend。\n# proxy_pass proxy_pass 指令设置代理服务器的协议和地址。协议可以指定 \u0026ldquo;http \u0026ldquo;或 \u0026ldquo;https\u0026rdquo;。地址可以指定为域名或IP地址，也可以配置为 upstream 定义的上游服务器：\n1 2 3 4 5 6 7 8 9 10 http { server { listen 6888; server_name localhost; location / { proxy_pass http://backend; } } } # proxy_set_header 在传递给上游服务器的请求头中，可以使用proxy_set_header 重新定义或添加字段。一般我们使用 proxy_set_header 向上游服务器传递一些必要的信息。\n1 2 3 4 5 6 location / { proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://backend; } 上面的配置使用 proxy_set_header 添加了三个 HTTP header：\nHost Host 是表明请求的主机名。默认情况下，Nginx 向上游服务器发送请求时，请求头中的 Host 字段是上游真实服务器的IP和端口号。如果我们想让传递给上游服务器的 Host 字段，包含的是用户访问反向代理时使用的域名，就需要通过 proxy_set_header 设置 Host 字段，值可以为 $host 或 $http_host，区别是前者只包含IP，而后者包含IP和端口号。\nX-Real-IP 经过反向代理后，上游服务器无法直接拿到客户端的 ip，也就是说，在应用中使用request.getRemoteAddr() 获得的是 Nginx 的地址。通过 proxy_set_header X-Real-IP $remote_addr;，将客户端的 ip 添加到了 HTTP header中，让应用可以使用 request.getHeader(“X-Real-IP”) 获取客户端的真实ip。\nX-Forwarded-For 如果配置了多层反向代理，当一个请求经过多层代理到达上游服务器时，上游服务器通过 X-Real-IP 获得的就不是客户端的真实IP了。那么这个时候就要用到 X-Forwarded-For ，设置 X-Forwarded-For 时是增加，而不是覆盖，从客户的真实IP为起点，穿过多层级代理 ，最终到达上游服务器，都会被记录下来。\n# proxy_cache Nginx 作为反向代理支持的所有特性和内置变量都可以在 ngx_http_proxy_module 的文档页面找到：\n其中一个比较重要的特性是 proxy cache，对访问上游服务器的请求进行缓存，极大减轻了对上游服务的压力。\n配置示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 http { ... proxy_cache_path /tmp/nginx/cache levels=1:2 keys_zone=myzone:10m inactive=1h max_size=10g use_temp_path=off; server { ... location / { ... proxy_cache myzone; proxy_cache_key $host$uri$is_args$args; proxy_cache_valid 200 304 302 12h; } } } 配置说明：\nproxy_cache_path 缓存路径，要把缓存放在哪里\nlevels=1:2：缓存的目录结构 keys_zone=myzone:10m：定义一块用于存放缓存key的共享内存区，命名为myzone，并分配 10MB 的内存；配至10MB的zone 大约可以存放 80000个key。 inactive=1d：不活跃的缓存文件 1 小时后将被清除 max_size=10g：缓存所占磁盘空间的上限 use_temp_path=off：不另设临时目录 proxy_cache myzone;：代表要使用上面定义的 myzone\nproxy_cache_key：用于生成缓存键，区分不同的资源。key 是决定缓存命中率的因素之一。\n$host：request header中的 Host字段 $uri：请求的uri $is_args 反映请求的 URI 是否带参数，若没有即为空值。 $args：请求中的参数 proxy_cache_valid：控制缓存有效期，可以针对不同的 HTTP 状态码可以设定不同的有效期。示例针对 200，304，302 状态码的缓存有效期为12小时。\n检验缓存配置的效果。\n首先查看缓存路径，没有存放任何内容：\n1 2 3 4 $ tree /tmp/nginx/cache/ /tmp/nginx/cache/ 0 directories, 0 files 然后访问Nginx反向代理服务器：\n1 2 3 ❯ curl -v http://172.21.32.84:6888/ ... 再次查看缓存路径：\n1 2 3 4 5 6 7 $ tree /tmp/nginx/cache/ /tmp/nginx/cache/ └── 6 └── ed └── 5e9596b7783c532f541535dd1a60eed6 2 directories, 1 file 经过请求后，缓存路径中已经有内容，并且目录结构是我们配置的 level=1:2。\n","date":"2022-11-16T10:56:47+08:00","permalink":"https://mazhen.tech/p/nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE/","title":"Nginx反向代理配置"},{"content":" # 配置文件语法 Nginx的配置文件是一个文本文件，由指令和指令块构成。\n# 指令 指令以分号 ; 结尾，指令和参数间以空格分割。\n指令块作为容器，将相关的指令组合在一起，用大括号 {} 将它们包围起来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 http { include mime.types; default_type application/octet-stream; server { listen 8080; server_name localhost; location / { root html; index index.html index.htm; } } } 上面配置中的http、server、location等都是指令块。指令块配置项之后是否如参数（例如 location /），取决于解析这个块配置项的模块。\n指令块配置项是可以嵌套的。内层块会继承父级块包含的指令的设置。有些指令可以出现在多层指令块内，你可以通过在内层指令块包含该指令，来覆盖从父级继承的设置。\n# Context 一些 top-level 指令被称为 context，将适用于不同流量类型的指令组合在一起。\nevents – 通用的连接处理 http – HTTP流量 mail – Mail 流量 stream – TCP 和 UDP 流量 放在这些 context 之外的指令是在 main context中。\n在每个流量处理 context 中，可以包括一个或多个 server 块，用来定义控制请求处理的虚拟服务器。\n对于HTTP流量，每个 server 指令块是对特定域名或IP地址访问的控制。通过一个活多个 location 定义如何处理特定的URI。\n对于 Mail 和 TCP/UDP 流量，server 指令块是对特定 TCP 端口流量的控制。\n# 静态资源服务 将个人网站的静态资源 clone 到 nginx 根目录：\n1 git clone https://github.com/mz1999/mazhen.git 在 conf/nginx.conf 文件中配置监听端口和 location：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 http { server { listen 8080; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { alias mazhen/; #index index.html index.htm; } } location 的语法格式为：\n1 location [ = | ~ | ~* | ^~ ] uri { ... } location 会尝试根据用户请求中的 URI 来匹配上面的 uri 表达式，如果可以匹配，就选择这个 location 块中的配置来处理用户请求。\nlocation 指定文件路径有两种方式：root和alias。\nroot 与alias 会以不同的方式将请求映射到服务器的文件上，它们的主要区别在于如何解释 location 后面的 uri 。\nroot的处理结果是，root＋location uri。 alias的处理结果是，使用 alias 替换 location uri。 alias 作为一个目录别名的定义。 例如：\n1 2 3 location /i/ { root /data/w3; } 如果一个请求的 URI 是 /i/top.gif ，Nginx 将会返回服务器上的 /data/w3/i/top.gif 文件。\n1 2 3 location /i/ { alias /data/w3/images/; } 如果一个请求的 URI 是 /i/top.gif，Nginx 将会返回服务器上的 /data/w3/images/top.gif文件。alias 会把 location 后面配置的 uri 替换为 alias 定义的目录。\n最后要注意，使用 alias 时，目录名后面一定要加 /。\n# 开启gzip Nginx 的 ngx_http_gzip_module 模块是一个过滤器，它使用 \u0026ldquo;gzip \u0026ldquo;方法压缩响应。可以在 http context 下配置 gzip：\n1 2 3 4 5 6 7 8 9 10 11 http { ... gzip on; gzip_min_length 1000; gzip_comp_level 2; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; server { ... } } gzip_min_length：设置允许压缩的页面最小字节数 gzip_comp_level： 设置 gzip 压缩比，1 压缩比最小处理速度最快，9 压缩比最大但处理最慢 gzip_types：匹配MIME类型进行压缩。 更多的配置项，可以参考官方文档。\n# autoindex Nginx 的 ngx_http_autoindex_module 模块处理以斜线字符 / 结尾的请求，并产生一个目录列表。通常情况下，当 ngx_http_index_module 模块找不到index文件时，请求会被传递给 ngx_http_autoindex_module 模块。\nautoindex 的配置很简单：\n1 2 3 4 location / { alias mazhen/; autoindex on; } 注意，只有 index 模块找不到index文件时，请求才会被 autoindex 模块处理。我们可以把 mazhen 目录下的 index 文件删掉，或者为 index 指令配置一个不存在的文件。\n# limit_rate 由于带宽的限制，我们有时候需要限制某些资源向客户端传输响应的速率，例如可以对大文件限速，避免传输大文件占用过多带宽，从而影响其他更重要的小文件（css，js）的传输。我们可以使用 set 指令配合内置变量 $limit_rate 实现这个功能：\n1 2 3 4 location / { ... set $limit_rate 1k; } 上面的指令限制了Nginx向客户端发送响应的速率为 1k/秒。\n$limit_rate是Nginx的内置变量，Nginx的文档详细列出了每个模块的内置变量。以 ngx_http_core_module 为例，在 Nginx文档首页的 Modules reference 部分，点击进入 ngx_http_core_module ：\n在 ngx_http_core_module 文档目录的最下方，点击 Embedded Variables ，会跳转到 ngx_http_core_module 内置变量列表：\n这里有 http module 所有内置变量的说明，包括我们刚才使用 $limit_rate。\n# access log Nginx 的 access log 功能由 ngx_http_log_module 模块提供。ngx_http_log_module 提供了两个指令：\nlog_format 指定日志格式 access_log 设置日志写入的路径 举例说明：\n1 2 3 4 5 6 7 8 9 10 http { ... log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; server { ... access_log logs/mazhen.access.log main; } } log_format 使用内置变量定义日志格式，示例中的 log_format 可以使用 http module 定义的内置变量。log_format 还指定了这个日志格式的名称为 main，这样让我们定义多种格式的日志，为不同的 server 配置特定的日志格式。\naccess_log 设置了日志路径为 logs/mazhen.access.log，并指定了日志格式为 main。示例中的 access_log 定义在 server 下，那所有发往这个 server 的请求日志都使用 main 格式，被记录在 logs/mazhen.access.log文件中。\n","date":"2022-11-15T14:40:19+08:00","permalink":"https://mazhen.tech/p/nginx%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E6%9C%8D%E5%8A%A1%E7%9A%84%E9%85%8D%E7%BD%AE/","title":"Nginx静态资源服务的配置"},{"content":" # Nginx 常用命令 Nginx的指令格式为 nginx [options argument]。\n查看帮助 1 ./sbin/nginx -? 使用指定的配置文件 1 ./sbin/nginx -c filename 指定运行目录 1 ./sbin/nginx -p /home/mazhen/nginx/ 设置配置指令，覆盖配置文件中的指令 1 ./sbin/nginx -g directives 向 Nginx 发送信号 我们可以向 Nginx 进程发送信号，控制运行中的 Nginx。一种方法是使用 kill 命令，也可以使用 nginx -s ：\n1 2 3 4 5 6 7 8 9 10 11 # 重新加载配置 $ ./sbin/nginx -s reload # 立即停止服务 $ ./sbin/nginx -s stop # 优雅停止服务 $ ./sbin/nginx -s quit # 重新开始记录日志文件 $ ./sbin/nginx -s reopen 测试配置文件是否有语法错误 1 ./sbin/nginx -t/-T 打印nginx版本 1 ./sbin/nginx -v/-V # 热部署 在不停机的情况下升级正在运行的 Nginx 版本，就是热部署。\n首先查看正在运行的 Nginx：\n1 2 3 4 5 6 7 $ ps aux | grep nginx mazhen 4376 0.0 0.0 9896 2372 ? Ss 16:47 0:00 nginx: master process ./sbin/nginx mazhen 4402 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4403 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4404 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4405 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4407 0.0 0.0 12184 2316 pts/0 S+ 16:51 0:00 grep --color=auto nginx 备份现有 Nginx 的二进制文件：\n1 cp nginx nginx.old 将构建好的最新版 Nginx 的二进制文件拷贝到 $nginx/sbin 目录：\n1 cp ~/works/nginx-1.22.1/objs/nginx ~/nginx/sbin/ -f 给正在运行的Nginx的 master 进程发送信号，通知它我们要开始进行热部署：\n1 kill -USR2 4376 这时候 Nginx master 进程会使用新的二进制文件，启动新的 master 进程。新的 master 会生成新的 worker，同时，老的worker并没有退出，也在运行中，但不再监听 80/443 端口，请求会平滑的过度到新 worker 中。\n1 2 3 4 5 6 7 8 9 10 11 12 $ ps aux | grep nginx mazhen 4376 0.0 0.0 9896 2536 ? Ss 16:47 0:00 nginx: master process ./sbin/nginx mazhen 4402 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4403 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4404 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4405 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4454 0.0 0.0 9768 6024 ? S 16:59 0:00 nginx: master process ./sbin/nginx mazhen 4455 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4456 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4457 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4458 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4461 0.0 0.0 12184 2436 pts/0 S+ 16:59 0:00 grep --color=auto nginx 向老的 Nginx master 发送信号，让它优雅关闭 worker 进程。\n1 kill -WINCH 4376 这时候再查看 Nginx 进程：\n1 2 3 4 5 6 7 8 $ ps aux | grep nginx mazhen 4376 0.0 0.0 9896 2536 ? Ss 16:47 0:00 nginx: master process ./sbin/nginx mazhen 4454 0.0 0.0 9768 6024 ? S 16:59 0:00 nginx: master process ./sbin/nginx mazhen 4455 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4456 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4457 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4458 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4475 0.0 0.0 12184 2292 pts/0 S+ 17:07 0:00 grep --color=auto nginx 老的 worker 已经优雅退出，所有的请求已经切换到了新升级的 Nginx 中。\n老的 master 仍然在运行，如果需要，我们可以向它发送 reload 信号，回退到老版本的 Nginx。\n# 日志切割 首先使用 mv 命令，备份旧的日志：\n1 mv access.log bak.log Linux 文件系统中，改名并不会影响已经打开文件的写入操作，因为内核 inode 不变，这样操作不会出现丢日志的情况。\n然后给运行中的 Nginx 发送 reopen 信号：\n1 ./nginx -s reopen Nginx 会重新生成 access.log 日志文件。\n一般会写一个 bash 脚本，通过配置 crontab，每日进行日志切割。\n","date":"2022-11-11T17:26:02+08:00","permalink":"https://mazhen.tech/p/nginx%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","title":"Nginx的基本使用"},{"content":"Nginx 是最流行的Web服务器，根据 W3Techs 最新的统计，世界上三分之一的网站在使用Nginx。\n# 准备工作 # Linux 版本 Nginx 需要 Linux 的内核为 2.6 及以上的版本，因为Linux 内核从 2.6 开始支持 epoll。可以使用 uname -a 查看 Linux 内核版本：\n1 2 $ uname -a Linux mazhen-laptop 5.15.0-52-generic #58-Ubuntu SMP Thu Oct 13 08:03:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux 从输出看到内核版本为 5.15.0，满足要求。现在已经很难找到内核 2.6 以下的服务器了吧。\n# 安装依赖 为了编译 Nginx 源码，需要安装一些依赖包。本文以 Ubuntu 为例。\nGCC编译器 GCC（GNU Compiler Collection）是必需的编译工具。使用下面的命令安装： 1 sudo apt install build-essential build-essential 是所谓的 meta-package，包含了 g++/GNU 编译器集合，GNU调试器，以及一些编译程序所需的工具和库。\nPCRE库 PCRE库支持正则表达式。如果我们在配置文件nginx.conf中使用了正则表达式，那么在编译Nginx时就必须把PCRE库编译进 Nginx，因为 Nginx的 HTTP 模块需要靠它来解析正则表达式。另外，pcre-devel 是使用PCRE做二次开发时所需要的开发库，包括头文件等，这也是编译Nginx所必须使用的。使用下面的命令安装： 1 sudo apt install libpcre3 libpcre3-dev zlib库 zlib库用于对 HTTP 包的内容做 gzip 格式的压缩，如果我们在 nginx.conf 中配置了gzip on，并指定对于某些类型（content-type）的 HTTP 响应使用 gzip 来进行压缩以减少网络传输量，则在编译时就必须把 zlib 编译进 Nginx。zlib-devel 是二次开发所需要的库。使用下面的命令安装：\n1 sudo apt install zlib1g-dev OpenSSL库 如果我们需要 Nginx 支持 SSL 加密传输，需要安装 OpenSSL 库。另外，如果我们想使用MD5、SHA1等散列函数，那么也需要安装它。使用下面的命令安装：\n1 sudo apt install openssl libssl-dev # 下载Nginx源码 从 http://nginx.org/en/download.html下载当前稳定版本的源码。\n当前稳定版为 1.22.1：\n1 wget https://nginx.org/download/nginx-1.22.1.tar.gz # Nginx配置文件的语法高亮 为了 Nginx 的配置文件在 vim 中能语法高亮，需要经过如下配置。\n解压 Nginx 源码：\n1 tar -zxvf nginx-1.22.1.tar.gz 将 Nginx 源码目录 contrib/vim/ 下的所有内容，复制到 $HOME/.vim 目录：\n1 2 mkdir ~/.vim cp -r contrib/vim/* ~/.vim/ 现在使用 vim 打开 nginx.conf，可以看到配置文件已经可以语法高亮了。\n# 编译前的配置 编译前需要使用 configure 命令进行相关参数的配置。\n使用 configure --help 查看编译配置支持的参数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ ./configure --help | more --help print this message --prefix=PATH set installation prefix --sbin-path=PATH set nginx binary pathname --modules-path=PATH set modules path --conf-path=PATH set nginx.conf pathname --error-log-path=PATH set error log pathname --pid-path=PATH set nginx.pid pathname --lock-path=PATH set nginx.lock pathname ...... --with-libatomic force libatomic_ops library usage --with-libatomic=DIR set path to libatomic_ops library sources --with-openssl=DIR set path to OpenSSL library sources --with-openssl-opt=OPTIONS set additional build options for OpenSSL --with-debug enable debug logging --with开头的模块缺省不包括在编译结果中，如果想使用需要在编译配置时显示的指定。--without开头的模块则相反，如果不想包含在编译结果中需要显示设定。\n例如我们可以这样进行编译前设置：\n1 ./configure --prefix=/home/mazhen/nginx --with-http_ssl_module 设置了Nginx的安装目录，以及需要http_ssl模块。\nconfigure命令执行完后，会生成中间文件，放在目录objs下。其中最重要的是ngx_modules.c文件，它决定了最终那些模块会编译进nginx。\n# 编译和安装 执行编译 在nginx目录下执行make编译：\n1 $ make 编译成功的nginx二进制文件在objs目录下。如果是做nginx的升级，可以直接将这个二进制文件copy到nginx的安装目录中。\n安装 在nginx目录下执行make install进行安装：\n1 $ make install 安装完成后，我们到 --prefix 指定的目录中查看安装结果：\n1 2 3 4 5 6 $ tree -L 1 /home/mazhen/nginx nginx/ ├── conf ├── html ├── logs └── sbin # 验证安装结果 编辑 nginx/conf/nginx.conf 文件，设置监听端口为8080：\n1 2 3 4 5 6 7 http { ... server { listen 8080; server_name localhost; ... 启动 nginx\n1 ./sbin/nginx 访问默认首页：\n1 2 3 4 5 6 7 8 9 10 $ curl -I http://localhost:8080 HTTP/1.1 200 OK Server: nginx/1.22.1 Date: Fri, 11 Nov 2022 08:04:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 08 Nov 2022 09:54:09 GMT Connection: keep-alive ETag: \u0026#34;636a2741-267\u0026#34; Accept-Ranges: bytes ","date":"2022-11-11T16:06:29+08:00","permalink":"https://mazhen.tech/p/nginx%E7%9A%84%E7%BC%96%E8%AF%91%E5%92%8C%E5%AE%89%E8%A3%85/","title":"Nginx的编译和安装"},{"content":"Tower 是一个专注于对网络编程进行抽象的框架，最核心的抽象为 Service trait。Service::call 接受一个 request 进行处理，成功则返回 response，否则返回 error。\n1 fn call(Request) -\u0026gt; Result\u0026lt;Response, Error\u0026gt; # Service trait 的定义 我们希望 Service 是异步编程风格，也就是 call 为 async 方法：\n1 async fn call(Request) -\u0026gt; Result\u0026lt;Response, Error\u0026gt; 我们就可以在 Service::call 上 await：\n1 service.call(request).await 然而当前 Rust 不支持 async trait 方法。\n我们可以让 call 作为普通方法，返回一个实现了 Future 的类型：\n1 2 3 4 5 6 7 trait Service\u0026lt;Request\u0026gt; { type Response; type Error; // ERROR: `impl Trait` not allowed outside of function and inherent // method return types fn call(\u0026amp;mut self, req: Request) -\u0026gt; impl Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;; } 还是不行，目前 Rust 也不支持在 Trait 中使用 impl Trait 做返回值，上一篇文章\u0026lt;impl Trait 的使用\u0026gt;分析过原因。\nTower 在定义 Service 时，使用了关联类型 type Future ，其实是将问题留给了 Service 的实现者，由用户选择 type Future 的实际类型：\n1 2 3 4 5 6 7 8 9 pub trait Service\u0026lt;Request\u0026gt; { type Response; type Error; type Future: Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;; fn poll_ready(\u0026amp;mut self, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Result\u0026lt;(), Self::Error\u0026gt;\u0026gt;; fn call(\u0026amp;mut self, req: Request) -\u0026gt; Self::Future; } # 实现自己的 Future 类型 我们在实现 Service 时，仍然需要为type Future 设置具体的类型。\n既然没法让 call 直接返回 impl Future，一种方法是定义自己的 Future 类型，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 pub struct HttpRequest { url: String, } pub struct HttpResponse { code: u32, } pub struct ResponseFuture { request: HttpRequest, } impl Future for ResponseFuture { type Output = Result\u0026lt;HttpResponse, Error\u0026gt;; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut std::task::Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; { println!(\u0026#34;process url:{}\u0026#34;, \u0026amp;self.as_ref().get_ref().request.url); Poll::Ready(Ok(HttpResponse { code: 200 })) } } 在实现 Service 时，关联类型 type Future 设置为我们手工实现的 Future：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 struct RequestHandler; impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { type Response = HttpResponse; type Error = Error; type Future = ResponseFuture; fn poll_ready(\u0026amp;mut self, cx: \u0026amp;mut std::task::Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Result\u0026lt;(), Self::Error\u0026gt;\u0026gt; { Poll::Ready(Ok(())) } fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { ResponseFuture { request: req } } } 然后就可以像正常的 Future 一样，使用 Service ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 #[tokio::main] async fn main() { let mut service = RequestHandler {}; match service .call(HttpRequest { url: \u0026#34;/user/mazhen\u0026#34;.to_owned(), }) .await { Ok(r) =\u0026gt; println!(\u0026#34;Response code: {}\u0026#34;, r.code), Err(e) =\u0026gt; println!(\u0026#34;process failed. {:?}\u0026#34;, e), } } # 使用 async blocks 手工实现 Future 会比较麻烦，我们一般都是用 async fn/async blocks语法糖生成 Future，那么这时 Service::call 返回什么类型呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 struct RequestHandler; impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { ... type Future = ??? fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { async move { println!(\u0026#34;process url {:?}\u0026#34;, \u0026amp;req.url); Ok(HttpResponse { code: 200 }) } } } 既然不能返回 impl Trait ，可以让 call 返回 trait object，用 trait object 统一返回值的类型。\n1 2 3 4 5 6 7 8 9 10 11 impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { ... type Future = Box\u0026lt;dyn Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;\u0026gt;; fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { Box::new(async move { println!(\u0026#34;process url {:?}\u0026#34;, \u0026amp;req.url); Ok(HttpResponse { code: 200 }) }) } } 这时候会报错，说 dyn Future 没有实现 Unpin：\n1 2 3 4 | 51 | type Future = Box\u0026lt;dyn Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;\u0026gt;; | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ the trait `Unpin` is not implemented for `(dyn std::future::Future\u0026lt;Output = Result\u0026lt;HttpResponse, std::io::Error\u0026gt;\u0026gt; + \u0026#39;static)` | 编译错误提示的没有实现 Unpin trait 是什么意思？那要先从 Pin 说起。\n# 一句话解释Pin Pin 本质上解决的问题是保证 Pin\u0026lt;P\u0026lt;T\u0026gt;\u0026gt; 中的 T 不会被 move，除非 T 满足 T: Unpin。\n# 什么是move 所有权转移的这个过程就是 move，例如：\n1 2 let s1 = \u0026#34;Hello, world\u0026#34;.to_owned(); let s2 = s1; s2 浅copy s1 的内容，同时 String 的所有权转移给了 s2。\n通过std::mem::swap()方法交换了两个可变借用 \u0026amp;mut 的内容，也发生了move。\n# 为什么要Pin 自引用结构体，move了以后会出问题。\n所以需要 Pin，不能move。\n# 怎么 Pin 住的 保证 T 不会被move，需要避免两种情况：\n不能暴露 T ，否则赋值、方法调用等都会move 不能暴露 \u0026amp;mut T，开发者可以调用 std::mem::swap() 或 std::mem::replace() 这类方法来 move 掉 T Pin\u0026lt;P\u0026lt;T\u0026gt;\u0026gt;没有暴露T，而且没法让你获得 \u0026amp;mut T，所以就 Pin 住了T。但注意有个前提条件：T 没有实现 Unpin\n# 谁没有实现 Unpin Unpin 是一个auto trait，编译器默认会给所有类型实现 Unpin。唯独有几个例外，他们实现的是 !Unpin。\nPhantomPinned 1 2 3 4 5 6 7 8 9 /// A marker type which does not implement `Unpin`. /// /// If a type contains a `PhantomPinned`, it will not implement `Unpin` by default. #[stable(feature = \u0026#34;pin\u0026#34;, since = \u0026#34;1.33.0\u0026#34;)] #[derive(Debug, Default, Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)] pub struct PhantomPinned; #[stable(feature = \u0026#34;pin\u0026#34;, since = \u0026#34;1.33.0\u0026#34;)] impl !Unpin for PhantomPinned {} 如果你使用了 PhantomPinned，你的类型自动实现 !Unpin 1 2 3 4 5 6 7 8 9 10 use std::marker::PhantomPinned; #[derive(Debug)] struct SelfReference { name: String, // 在初始化后指向 name name_ptr: *const String, // PhantomPinned 占位符 _marker: PhantomPinned, } 编译器为 async fn 生成的匿名结构体实现的是 !Unpin # async fn async fn 是语法糖，在编译时，编译器使用Generator为 async fn 生成匿名结构体，这个结构体实现了 Future。\n这个匿名结构体是自引用的。原因是，如果 async fn 内有多个 await，执行到 await 可能因为资源没准备好而让出 CPU 暂停执行，随后该 Future 可能被调度到其他线程接着执行。所以这个匿名结构体需要保存跨 await 的数据，形成了自引用结构。\n由于为 async fn 生成的结构体是自引用的，所以这个结构体实现了 !Unpin，表示它不能被 move。\n这也是为什么不能给Future::poll 直接传 \u0026amp;mut Self 的原因：生成的匿名结构体不能被move，而拿到 \u0026amp;mut Self就可以使用 swap 或 replace之类的方法进行move，这样不安全，所以必须使用 Pin\u0026lt;\u0026amp;mut Self\u0026gt;。\n# Future 都是 !Unpin 的吗 不一定。\nasync fn 语法糖生成的实现了 Future 的匿名结构，内部包含自引用，它会明确实现 !Unpin，不能 move。\n但如果你自己实现的 Future，内部没有自引用，它就不是 !Unpin，当然可以 move。\n也就是说，Future 和 !Unpin 是两个 trait，虽然它们经常联系在一起，但并不是实现了 Future 的类型都必须同时实现 !Unpin，没有包含自引用的 Future当然可以安全的 move 了。\n# 实现了 Unpin 的 Future 如果是可以 move 的 Future，也就是实现了 Unpin 的 Future，在调用 Future::poll 的时候，要求传入 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，会不会有什么问题呢？\n首先，如果 T：Unpin，那么 Pin\u0026lt;\u0026amp;mut T\u0026gt; 就完全等同于 \u0026amp;mut T。换句话说，Unpin 意味着这个类型可以被移动，即使是在 Pin 住的情况下，所以 Pin 对这样的类型没有影响。因为 Pin 是智能指针，它实现了 Deref/DerefMut，只要满足 T: Unpin，你就能拿到\u0026amp;mut T：\n1 2 3 4 5 6 #[stable(feature = \u0026#34;pin\u0026#34;, since = \u0026#34;1.33.0\u0026#34;)] impl\u0026lt;P: DerefMut\u0026lt;Target: Unpin\u0026gt;\u0026gt; DerefMut for Pin\u0026lt;P\u0026gt; { fn deref_mut(\u0026amp;mut self) -\u0026gt; \u0026amp;mut P::Target { Pin::get_mut(Pin::as_mut(self)) } } 其次，为了用户方便，FutureExt 提供了 poll_unpin，让你直接在 Unpin 的 Future 上 poll：\n1 2 3 4 5 6 7 8 9 pub trait FutureExt: Future { /// A convenience for calling `Future::poll` on `Unpin` future types. fn poll_unpin(\u0026amp;mut self, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; where Self: Unpin, { Pin::new(self).poll(cx) } } 所以，如果你的 Future 是 Unpin，那么即使Future::poll 要求传入的是 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，对你也没有任何影响。\n# Box\u0026lt;dyn Futrue\u0026lt;\u0026gt;\u0026gt; 的问题 回到上面的问题，我们想让 Service::call 返回 trait object，也就是 Box\u0026lt;dyn Futrue\u0026lt;\u0026gt;\u0026gt;，会编译不过，为什么呢？\n因为标准库为 Box 实现了 Future，但要求 Box 包装的 Future 必须同时实现了 Unpin ：\n1 2 3 4 impl\u0026lt;F, A\u0026gt; Future for Box\u0026lt;F, A\u0026gt; where F: Future + Unpin + ?Sized, A: Allocator + \u0026#39;static, 前面已经讲过，async 解语法糖生成的 Future 没有实现 Unpin，所以Box::new(async{...}) 不满足类型约束，它没有实现 Future，不能在它上面await。\n# Pin 实现了 Future 前面讲过，在 Future 上 poll 的时候，不能直接传入\u0026amp;mut Self，而需要传入 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，需要这样调用 Future::poll(Pin::new(\u0026amp;mut future), ctx)。如果 Pin 实现了 Future ，我们就可以直接这样 poll 了： Pin::new(\u0026amp;mut future).poll(ctx)。\n标准库也确实为 Pin 实现了 Future：\n1 2 3 4 impl\u0026lt;P\u0026gt; Future for Pin\u0026lt;P\u0026gt; where P: DerefMut, \u0026lt;P as Deref\u0026gt;::Target: Future, 我们来看对 P 的约束，P 可解引用为 Future，也就是说，P是 Future 的引用 \u0026amp;mut future，或者是智能指针 Box\u0026lt;dyn Future\u0026gt; 都可以满足约束。因为 Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt; 实现了 Future，我们可以用它作为 Service::call 返回值类型。\nBox 提供了 pin 方法，让用户构建 Pin\u0026lt;Box\u0026lt;T\u0026gt;\u0026gt;：\n1 pub fn pin(x: T) -\u0026gt; Pin\u0026lt;Box\u0026lt;T, Global\u0026gt;\u0026gt; 使用 Box::pin，RequestHanlder 可以这么修改：\n1 2 3 4 5 6 7 8 9 10 11 impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { ... type Future = Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;\u0026gt;\u0026gt;; fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { Box::pin(async move { println!(\u0026#34;process url {:?}\u0026#34;, \u0026amp;req.url); Ok(HttpResponse { code: 200 }) }) } } 这回终于可以了。事实上，在异步场景下，我们经常会看到使用 Box::pin 去包装 async block。\n# Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt; Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt; 除了实现了Future，也实现了 Unpin。\n因为 Pin 实现了 Unpin，只要 P 是 Unpin 的：\n1 2 3 impl\u0026lt;P\u0026gt; Unpin for Pin\u0026lt;P\u0026gt; where P: Unpin 而 Box 正好是 Unpin 的：\n1 2 3 4 impl\u0026lt;T, A\u0026gt; Unpin for Box\u0026lt;T, A\u0026gt; where A: Allocator + \u0026#39;static, T: ?Sized, 因此 Pin\u0026lt;Box\u0026lt;T\u0026gt;\u0026gt;是 Unpin 的。可以这么理解，Pin 钉住了 T，但 Pin 本身是 Unpin的，可以安全的 move。\n很多异步方法需要你的 Future 同时实现了 Unpin ，例如tokio::select!()，而 async fn 返回的 Future 显然不满足 Unpin，这个时候仍然可以用 Box::pin把 Future pin 住，得到的Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;...\u0026gt;\u0026gt;\u0026gt; 同时实现了 Future 和 Unpin，满足你的要求。\n简单总结，在异步编程场景，我们经常会用Box::pin 包装 async block，获得同时实现了 Future 和 Unpin 的Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt;。\n","date":"2022-10-12T09:14:10+08:00","permalink":"https://mazhen.tech/p/pinboxdyn-future%E8%A7%A3%E6%9E%90/","title":"Pin\u003cBox\u003cdyn Future\u003c\u003e\u003e\u003e解析"},{"content":"Rust 通过 RFC conservative impl trait 增加了新的语法 impl Trait，它被用在函数返回值的位置上，表示返回的类型将实现这个 Trait。随后的 RFC expanding impl Trait 更进一步，允许 impl Trait 用在函数参数的位置，表示由调用者决定参数的具体类型，其实就等价于函数的泛型参数。\n# impl Trait 作为函数参数 根据 RFC on expanding impl Trait， impl Trait 可以用在函数参数中，作用是作为函数的匿名泛型参数。\nExpand impl Trait to allow use in arguments, where it behaves like an anonymous generic parameter.\n也就是说，impl Trait 作为函数参数，和泛型参数是等价的：\n1 2 3 // These two are equivalent fn map\u0026lt;U\u0026gt;(self, f: impl FnOnce(T) -\u0026gt; U) -\u0026gt; Option\u0026lt;U\u0026gt; fn map\u0026lt;U, F\u0026gt;(self, f: F) -\u0026gt; Option\u0026lt;U\u0026gt; where F: FnOnce(T) -\u0026gt; U 不过，impl Trait和泛型参数有一个不同的地方，impl Trait 作为参数，不能明确指定它的类型：\n1 2 3 4 5 fn foo\u0026lt;T: Trait\u0026gt;(t: T) fn bar(t: impl Trait) foo::\u0026lt;u32\u0026gt;(0) // this is allowed bar::\u0026lt;u32\u0026gt;(0) // this is not 除了这个差别，可以认为impl Trait 作为函数参数，和使用泛型参数是等价的。\n# impl Trait 作为函数返回值 impl Trait 作为函数的返回值，表示返回的类型将实现这个 Trait。\n1 2 3 4 5 6 7 8 fn foo(n: u32) -\u0026gt; impl Iterator\u0026lt;Item = u32\u0026gt; { (0..n).map(|x| x * 100) } fn main() { for x in foo(10) { println!(\u0026#34;{}\u0026#34;, x); } } 在这种情况下，需要注意函数的所有返回路径必须返回完全相同的具体类型。\n1 2 3 4 5 6 7 8 // 编译错误，即使这两个类型都实现了Bar fn f(a: bool) -\u0026gt; impl Bar { if a { Foo { ... } } else { Baz { ... } } } 可以把函数返回值位置的 impl Trait 替换为泛型吗？\n1 2 3 4 // 不能编译 fn bar\u0026lt;T: Iterator\u0026lt;Item = u32\u0026gt;\u0026gt;(n: u32) -\u0026gt; T { (0..n).map(|x| x * 100) } 编译器给的错误信息是，期待返回值的类型是泛型类型 T，却实际却返回了一个具体类型。编译器很智能的给出了使用 impl Iterator\u0026lt;Item = u32\u0026gt;作为返回类型的建议：\n1 2 3 4 5 6 7 8 9 10 11 12 --\u0026gt; src/main.rs:6:5 | 5 | fn bar\u0026lt;T: Iterator\u0026lt;Item = u32\u0026gt;\u0026gt;(n: u32) -\u0026gt; T { | - - | | | | | expected `T` because of return type | this type parameter help: consider using an impl return type: `impl Iterator\u0026lt;Item = u32\u0026gt;` 6 | (0..n).map(|x| x * 100) | ^^^^^^^^^^^^^^^^^^^^^^^ expected type parameter `T`, found struct `Map` | = note: expected type parameter `T` found struct `Map\u0026lt;std::ops::Range\u0026lt;u32\u0026gt;, [closure@src/main.rs:6:16: 6:27]\u0026gt;` # Universals vs. Existentials 在 RFC on expanding impl Trait 中使用了两个术语，Universal 和 Existential：\nUniversal quantification, i.e. \u0026ldquo;for any type T\u0026rdquo;, i.e. \u0026ldquo;caller chooses\u0026rdquo;. This is how generics work today. When you write fn foo\u0026lt;T\u0026gt;(t: T), you\u0026rsquo;re saying that the function will work for any choice of T, and leaving it to your caller to choose the T. Existential quantification, i.e. \u0026ldquo;for some type T\u0026rdquo;, i.e. \u0026ldquo;callee chooses\u0026rdquo;. This is how impl Trait works today (which is in return position only). When you write fn foo() -\u0026gt; impl Iterator, you\u0026rsquo;re saying that the function will produce some type T that implements Iterator, but the caller is not allowed to assume anything else about that type. 简单来说：\nimpl Trait 用在参数位置是 universal type，也就是泛型类型，它可以是任意类型，由函数的调用者指定具体的类型。\nimpl Trait 用在返回值位置是 existential type，它不能是任意类型，而是由函数的实现者指定，一个实现了 Trait 的具体类型。调用者不能对这个类型做任何假设。\n也就是说，impl Trait 用在返回位置不是泛型，编译时不需要单态化，抽象类型可以简单地替换为调用代码中的具体类型。\n# 在 Trait 中使用 impl Trait Rust 目前还不支持在 Trait 里使用 impl Trait 做返回值：\n1 2 3 4 5 trait Foo { // ERROR: `impl Trait` not allowed outside of function and inherent // method return types fn foo(\u0026amp;self) -\u0026gt; impl Iterator\u0026lt;Item=u8\u0026gt;; } 因为 impl Trait 用在返回值位置是 existential type，意味着这个函数将返回一个实现了这个 Trait 的单一类型，而函数定义在 Trait 中，意味着每个实现了 Trait 的类型，都可以让这个函数返回不同类型，对编译器来说这很难处理，因为它需要知道被返回类型的具体大小。\n一个简单的解决方法是让函数返回 trait object：\n1 2 3 trait Foo { fn foo(\u0026amp;self) -\u0026gt; Box\u0026lt;dyn Iterator\u0026lt;Item=u8\u0026gt;\u0026gt;; } 带有 trait object 的函数不是泛型函数，它只带有单一类型，这个类型就是 trait object 类型。Trait object 本身被实现为胖指针，其中，一个指针指向数据本身，另一个则指向虚函数表（vtable）。\n这样定义在 Trait 中的函数，返回的不再是泛型，而是一个单一的 trait object 类型，大小固定（两个指针大小），编译器可以处理。\n","date":"2022-10-08T17:58:36+08:00","permalink":"https://mazhen.tech/p/impl-trait-%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"impl Trait 的使用"},{"content":"Rust开发生态最重要的两个工具\nrustup Rust 官方的跨平台安装工具 Cargo Rust 构建系统和包管理器 它们的下载源都位于国外，为了改善在国内的使用体验，可以为它们配置国内源。\nrustup 国内源\n目前国内 Rust 工具链镜像源有清华大学源、中国科学技术大学源、上海交通大学源等，以清华大学源为例，设置环境变量：\n1 2 export RUSTUP_DIST_SERVER=https://mirrors.tuna.tsinghua.edu.cn/rustup export RUSTUP_UPDATE_ROOT=https://mirrors.tuna.tsinghua.edu.cn/rustup/rustup crates.io 国内源\nCargo 默认的源服务器为 crates.io，同样可以配置为国内的镜像源，以清华大学源为例，编辑 ~/.cargo/config 文件，添加以下内容：\n1 2 3 4 5 [source.crates-io] replace-with = \u0026#39;tuna\u0026#39; [source.tuna] registry = \u0026#34;https://mirrors.tuna.tsinghua.edu.cn/git/crates.io-index.git\u0026#34; 这样可加快 Cargo 读取软件包索引的速度。\n","date":"2022-09-30T22:36:25+08:00","permalink":"https://mazhen.tech/p/%E9%85%8D%E7%BD%AErustup%E5%92%8Ccargo%E5%9B%BD%E5%86%85%E6%BA%90/","title":"配置rustup和Cargo国内源"},{"content":"原文链接\nLinux 是内核，由Linus在90年代创造。不包括驱动程序，内核本身在编译时只有几兆字节。\nGNU是一个自由软件的大集合，可以和操作系统一起使用，包括你非常熟悉的 grep、sed、gcc等等。GNU 还包括 glibc，C 语言标准库的实现。\nGNU/Linux 是任何基于 GNU 集合的Linux发行版（内核+用户态应用）。Debian、Ubuntu、CentOS，甚至RHEL在技术上都是 GNU/Linux。因为有共同的C语言标准库和系统工具，通常你可以在 GNU/Linux 发行版之间跳来跳去，不会有什么麻烦。\nAlpine Linux 是另一个类型 Linux 发行版，它不是基于 GNU 集合。\n为了替代 GNU，Alpine 使用：\nBusyBox：一个小型软件套件 (~2MB)，在单个可执行文件中提供了多个 Unix 实用程序。 musl：一种现代且更强大的 C 标准库实现 因此，Alpine Linux 不是 Debian 或 CentOS 等 GNU/Linux 发行版的直接替代品，musl 的行为可能与 glibc 不同！\n","date":"2022-09-30T10:23:23+08:00","permalink":"https://mazhen.tech/p/linux-gnu/linux%E4%BB%A5%E5%8F%8Aalpine-linux/","title":"Linux, GNU/Linux以及Alpine Linux"},{"content":" # Future trait Rust 异步编程最核心的是 Future trait：\n1 2 3 4 5 6 7 8 pub trait Future { type Output; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt;; } pub enum Poll\u0026lt;T\u0026gt; { Ready(T), Pending, } Future 代表一个异步计算，它会产生一个值。通过调用 poll 方法来推进 Future 的运行，如果 Future 完成了，它将返回 Poll::Ready(result)，我们拿到运算结果。如果 Future 还不能完成，可能是因为需要等待其他资源，它返回 Poll::Pending。等条件具备，如资源已经准备好，这个 Future 将被唤醒，再次进入 poll，直到计算完成获得结果。\n# async/.await 如果产生一个 Future 呢，使用 async 是产生 Future 最方便的方法。使用 async 有两种方式：async fn 和 async blocks。每种方法都返回一个实现了Future trait 的匿名结构：\n1 2 3 4 5 6 7 8 9 10 // `foo()` returns a type that implements `Future\u0026lt;Output = u8\u0026gt;`. async fn foo() -\u0026gt; u8 { 5 } fn bar() -\u0026gt; impl Future\u0026lt;Output = u8\u0026gt; { // This `async` block results in a type that implements // `Future\u0026lt;Output = u8\u0026gt;`. async { 5 } } 这两种方式是等价的，都返回了 impl Future\u0026lt;Output = u8\u0026gt;。async 关键字相当于一个返回 impl Future\u0026lt;Output\u0026gt; 的语法糖。\n调用 async fn 并不会让函数执行，而是返回 impl Future\u0026lt;Output\u0026gt;，你只有在返回值上使用 .await，才能触发函数的实际执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 async fn say_world() { println!(\u0026#34;world\u0026#34;); } #[tokio::main] async fn main() { // Calling `say_world()` does not execute the body of `say_world()`. let op = say_world(); // This println! comes first println!(\u0026#34;hello\u0026#34;); // Calling `.await` on `op` starts executing `say_world`. op.await; } 上面的程序输出为：\n1 2 hello world 在 Future 上调用 await，相当于执行 Future::poll。如果 Future 被某些条件阻塞，它将放弃对当前线程的控制。当条件准备好后， Future会被唤醒恢复执行。\n简单总结，我们用async 生成 Future，用 await 来触发 Future 的执行。尽管其他语言也实现了async/.await，但 Rust 的 async 是 lazy 的，只有在主动 await 后才开始执行。\n我们当然也可以手工为数据结构实现 Future：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 struct Delay { when: Instant, } impl Future for Delay { type Output = \u0026amp;\u0026#39;static str; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;\u0026amp;\u0026#39;static str\u0026gt; { if Instant::now() \u0026gt;= self.when { println!(\u0026#34;Hello world\u0026#34;); Poll::Ready(\u0026#34;done\u0026#34;) } else { // Ignore this line for now. cx.waker().wake_by_ref(); Poll::Pending } } } 同样用 await 触发 Future 的实际执行：\n1 2 3 4 5 6 7 8 #[tokio::main] async fn main() { let when = Instant::now() + Duration::from_millis(10); let future = Delay { when }; let out = future.await; assert_eq!(out, \u0026#34;done\u0026#34;); } # Pin Future 被每个 await 分成多段，执行到 await 可能因为资源没准备好而让出 CPU 暂停执行，随后该 future 可能被调度到其他线程接着执行。所以 future 结构中需要保存跨await的数据，形成了自引用结构。\n自引用结构不能被移动，否则内部引用因为指向移动前的地址，引发不可控的问题。所以future需要被pin住，不能移动。\n如何让 future 不被move？ 方法调用时只传递引用，那么就没有移动 future。但是通过可变引用仍然可以使用 replace，swap 等方式移动数据。那么用 Pin 包装可变引用 Pin\u0026lt;\u0026amp;mut T\u0026gt;，让用户没法拿到 \u0026amp;mut T，就把这个漏洞堵上了。\n总之 Pin\u0026lt;\u0026amp;mut T\u0026gt; 不是数据的 owner，也没法获得 \u0026amp;mut T，所以就不能移动 T。\n注意，Pin 拿住的是一个可以解引用成 T 的指针类型 P，而不是直接拿原本的类型 T。Pin 本身是可 move 的，T 被 pin 住，不能 move。\n# async runtime的内部实现 要运行异步函数，必须将最外层的 Future 提交给 executor。 executor 负责调用Future::poll，推动异步计算的前进。\nexecutor 内部会有一个 Task 队列，executor 在 run 方法内，不停的从 receiver 获取 Task，然后执行。\nTask 包装了一个 future，同时内部持有一个 sender，用于将自身放回 executor 的 Task 队列。\nFuture 的 poll 方法，接收的是 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，而不是 \u0026amp;mut Self。所以在向 executor 提交 Future 时，需要先 pin 住，然后才能用来初始化 Task：\n1 2 3 4 5 6 7 8 9 10 11 fn spawn\u0026lt;F\u0026gt;(future: F, sender: \u0026amp;channel::Sender\u0026lt;Arc\u0026lt;Task\u0026gt;\u0026gt;) where F: Future\u0026lt;Output = ()\u0026gt; + Send + \u0026#39;static, { let task = Arc::new(Task { future: Mutex::new(Box::pin(future)), executor: sender.clone(), }); let _ = sender.send(task); } 保存在 Task 字段中的 Future 是 Pin\u0026lt;Box\u0026lt;Future\u0026gt;\u0026gt;，保证了以后每次调用 poll 传入的是 Pin\u0026lt;\u0026amp;mut Self\u0026gt;。注意，Pin 是可以移动的，Task 也是可以移动的，只是 Future 不能移动。\n在执行 Future 时，如果遇到资源未准备好，需要让出 CPU，那么 Task 可以将自己放入 Reactor。Task 实现了 ArcWake trait，实际上放入 Reactor 的 Waker 就是 Task 的包装：\n1 2 3 4 5 6 7 8 9 10 11 12 fn poll(self: Arc\u0026lt;Self\u0026gt;) { // Get a waker referencing the task. let waker = task::waker(self.clone()); // Initialize the task context with the waker. let mut cx = Context::from_waker(\u0026amp;waker); // This will never block as only a single thread ever locks the future. let mut future = self.future.try_lock().unwrap(); // Poll the future let _ = future.as_mut().poll(\u0026amp;mut cx); } 当 Reactor 得到了满足条件的事件，它会调用 Waker.wake() 唤醒之前挂起的任务。Waker.wake 会调用 Task::wake_by_ref 方法，将 Task 放回 executor 的任务队列：\n1 2 3 4 5 impl ArcWake for Task { fn wake_by_ref(arc_self: \u0026amp;Arc\u0026lt;Self\u0026gt;) { let _ = arc_self.executor.send(arc_self.clone()); } } # Stream trait 对于 Iterator，可以不断调用其 next() 方法，获得新的值，直到 Iterator 返回 None。Iterator 是阻塞式返回数据的，每次调用 next()，必然独占 CPU 直到得到一个结果，而异步的 Stream 是非阻塞的，在等待的过程中会空出 CPU 做其他事情。\nStream::poll_next() 方法和 Future::poll() 类似, 除了它可以被重复调用，以便从 Stream 中接收多个值。然而，poll_next() 调用起来不方便，我们需要自己处理 Poll 状态。也就是说，await 语法糖只能应用在 Future 上，没法使用 stream.await 。所以，我们要想办法用 Future 包装 Stream，在 Future::poll() 中调用 Stream::poll_next()，这样就可以使用 await。 StreamExt 提供了 next() 方法，返回一个实现了 Future trait 的 Next 结构，这样，我们就可以直接通过 stream.next().await 来获取下一个值了。看一下 next() 方法以及 Next 结构的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 pub trait StreamExt: Stream { fn next(\u0026amp;mut self) -\u0026gt; Next\u0026lt;\u0026#39;_, Self\u0026gt; where Self: Unpin { assert_future::\u0026lt;Option\u0026lt;Self::Item\u0026gt;, _\u0026gt;(Next::new(self)) } } // next 返回的 Next 结构 pub struct Next\u0026lt;\u0026#39;a, St: ?Sized\u0026gt; { stream: \u0026amp;\u0026#39;a mut St, } // Next 实现了 Future，每次 poll() 实际上就是从 stream 中 poll_next() impl\u0026lt;St: ?Sized + Stream + Unpin\u0026gt; Future for Next\u0026lt;\u0026#39;_, St\u0026gt; { type Output = Option\u0026lt;St::Item\u0026gt;; fn poll(mut self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; { self.stream.poll_next_unpin(cx) } } 当手动实现一个 stream 时，它通常是通过合成 futures 和其他stream 来完成的。例如下面的例子，将 Lines 封装为 Stream，在 Stream::poll_next() 中利用了 Lines::poll_next_line()：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #[pin_project] struct LineStream\u0026lt;R\u0026gt; { #[pin] lines: Lines\u0026lt;BufReader\u0026lt;R\u0026gt;\u0026gt;, } impl\u0026lt;R: AsyncRead\u0026gt; LineStream\u0026lt;R\u0026gt; { /// 从 BufReader 创建一个 LineStream pub fn new(reader: BufReader\u0026lt;R\u0026gt;) -\u0026gt; Self { Self { lines: reader.lines(), } } } /// 为 LineStream 实现 Stream trait impl\u0026lt;R: AsyncRead\u0026gt; Stream for LineStream\u0026lt;R\u0026gt; { type Item = std::io::Result\u0026lt;String\u0026gt;; fn poll_next(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Option\u0026lt;Self::Item\u0026gt;\u0026gt; { self.project() .lines .poll_next_line(cx) .map(Result::transpose) } } Stream 可以是 unpin 的，Future 也可以是 unpin 的，如果他们内部包含了其他 !Unpin 的 Stream 或 Future，只需要把他们用 pin 包装，外面的 Stream 和 Future 就可以是 unpin 的。\n一般我们使用的 Stream 都是 unpin 的，如果不是，就用 pin 把它变成 unpin 的。为啥我们用的都是 unpin 的？因为能 move 的 Stream 更加灵活，可以作为参数和返回值。\n# AsyncRead 和 AsyncWrite 所有同步的 Read / Write / Seek trait，前面加一个 Async，就构成了对应的异步 IO 接口。\nAsyncRead / AsyncWrite 的方法会返回一个实现了 Future 的 struct，这样我们才能使用 await ，将 future 提交到 async runtime，触发 future 的执行。例如 AsyncReadExt::read_to_end()方法，返回 ReadToEnd 结构，而 ReadToEnd 实现了 Future：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 pub trait AsyncReadExt: AsyncRead { ... fn read_to_end\u0026lt;\u0026#39;a\u0026gt;(\u0026amp;\u0026#39;a mut self, buf: \u0026amp;\u0026#39;a mut Vec\u0026lt;u8\u0026gt;) -\u0026gt; ReadToEnd\u0026lt;\u0026#39;a, Self\u0026gt; where Self: Unpin, { read_to_end(self, buf) } } pin_project! { #[derive(Debug)] #[must_use = \u0026#34;futures do nothing unless you `.await` or poll them\u0026#34;] pub struct ReadToEnd\u0026lt;\u0026#39;a, R: ?Sized\u0026gt; { reader: \u0026amp;\u0026#39;a mut R, buf: VecWithInitialized\u0026lt;\u0026amp;\u0026#39;a mut Vec\u0026lt;u8\u0026gt;\u0026gt;, // The number of bytes appended to buf. This can be less than buf.len() if // the buffer was not empty when the operation was started. read: usize, // Make this future `!Unpin` for compatibility with async trait methods. #[pin] _pin: PhantomPinned, } } impl\u0026lt;A\u0026gt; Future for ReadToEnd\u0026lt;\u0026#39;_, A\u0026gt; where A: AsyncRead + ?Sized + Unpin, { type Output = io::Result\u0026lt;usize\u0026gt;; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; { let me = self.project(); read_to_end_internal(me.buf, Pin::new(*me.reader), me.read, cx) } } ","date":"2022-09-24T16:48:29+08:00","permalink":"https://mazhen.tech/p/rust-%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/","title":"Rust 异步编程笔记"},{"content":" # 读写流程 Ledger 的元数据信息保存在zookeeper中。\n# 写入流程 当写入发生时，首先 entry 被写入一个 journal 文件。journal 是一个 write-ahead log（WAL），它帮助 BookKeeper 在发生故障时避免数据丢失。这与关系型数据库实现数据持久化的机制相同。\n同时 entry 会被加入到 write cache 中。write cache 中累积的 entry 会定期排序，异步刷盘到 entry log 文件中。同一个 ledger 的 entry 排序后会被放在一起，这样有利于提高读取的性能。\nwrite cache 中的 entry 也将被写入 RocksDB， RocksDB 记录了每个 entry 在 entry log 文件中的位置，是 (ledgerId, entryId) 到 (entry log file, offset) 的映射，即可以通过(ledgerId, entryId)，在 entry log 文件中定位到 entry。\n# 读取流程 读取时会首先查看 write cache ，因为 write cache 中有最新的 entry。如果 write cache 中没有，那么接着查看 read cache。如果 read cache 中还是没有，那么就通过 (ledgerId, entryId) 在 RocksDB 中查找到该 entry 所在的 entry log 文件及偏移量，然后从 entry log 文件中读取该 entry ，并更新到 read cache 中，以便后续请求能在 read cache 中命中。两层缓存让绝大多数的读取通常是从内存获取。\n# 读写隔离 BookKeeper 中的写入都是顺序写入 journal 文件，该文件可以存储在专用磁盘上，可以获得更大的吞吐量。write cache 中的 entry 会异步批量写入 entry log 文件和 RocksDB，通常配置在另外一块磁盘。因此，一个磁盘用于同步写入（ journal 文件），另一个磁盘用于异步优化写入，以及所有的读取。\n# 数据一致性 Bookie 操作基本都是在客户端完成和实现的，比如副本复制、读写 entry 等操作。这些有助于确保 BookKeeper 的一致性。\n客户端在创建 ledger 时，会出现 Ensemble、Write Quorum 和 Ack Quorum 这些数据指标。\nEnsemble —— 用哪几台 bookie 去存储 ledger 对应的 entry\nWrite Quorum ——对于一条 entry，需要存多少副本\nAck Quorum —— 在写 entry 时，要等几个 response\n我们会用（5,3,2）的实例进行讲述\n（5,3,2) 代表了对于一个 ledger ，会挑 5 台 bookie 去存储所有的 entry。所以当 entry 0 生成时，可以根据hash模型计算出应该放置到哪台 bookie。比如 E0 对应 B1，B2，B3，那 E1 就对应 B2，B3，B4，以此类推。\n虽然总体是需要 5 台 bookie，但是每条 entry 只会占用 3 台 bookie 去存放，并只需等待其中的 2 台 bookie 给出应答即可。\n# LastAddConfirm LAC（LastAddConfirm）是由 LAP（LastAddPush） 延伸而来是根据客户端进行维护并发布的最后一条 entry id，从 0 开始递增。所以 LAC 是由应答确认回来的最后一条 entry id 构成，如下图右侧显示。 LAC 以前的 entry ID （比它本身小的）都已确认过，它其实是一致性的边界，LAC 之前和之后的都是一致的。 同时 LAC 作为 bookie 的元数据，可以根据此来判断 entry 的确认与否。这样做的好处是，LAC 不会受限于一个集中的元数据管理，可以分散存储在存储节点。 # Ensemble change 当其中的某个 bookie 挂掉时，客户端会进行一个 ensemble change 的操作，用新的 bookie 替换挂掉的旧 bookie。比如 当bookie 4 挂掉时，可以使用 bookie 6 进行替换。 整个过程，只要有新的存储节点出现，就会保证不会中断读写操作是，即过程中随时补新。\nEnsemble change 对应到元数据存储，即对元数据的修改。之前的 E0-E6 是写在 B1～B5 上，E7 以后写在了 B1、B2、B3、B6、B5 上。这样就可以通过元数据的方式，看到数据到底存储在那个bookie上。 # Bookie Fencing BookKeeper 有一个极其重要的功能，叫做 fencing。fencing 功能让 BookKeeper 保证只有一个写入者（Pulsar broker）可以写入一个 ledger。\nBroker（B1）挂掉，Broker（B2）接管 B1 上topic X的流程：\n当前拥有 topic X 所有权的Pulsar Broker（B1）被认为已经挂掉或不可用（通过ZooKeeper）。\n另一个Broker（B2）将topic X 的当前 ledger 的状态从 OPEN 更新为 IN_RECOVERY。\nB2 向当前 ledger 的所有 bookie 发送 fencing LAC read 请求，并等待(Write Quorum - Ack Quorum)+1的响应。一旦收到这个数量的回应，ledger 就已经被 fencing。B1就算还活着，也不能再写入这个ledger，因为B1无法得到 Ack Quorum 的确认。\nB2采取最高的LAC响应，然后开始执行从 LAC+1 的恢复性读取。它确保从该点开始的所有 entry 都被复制到 Write Quorum 个bookie。当 B2 不能再读取和复制任何entry，ledger 完成恢复。\nB2将 ledger 的状态改为 CLOSED。\nB2打开一个新的 ledger，现在可以接受对Topic X的写入。\n整个失效恢复的过程，是没有回头复用 ledger 的。因为复用意味着所有元素都处于相同状态且都同意后才能继续去读写，这个是很难控制的。\n我们从主从复制方式进行切入，将其定义为物理文件。数据从主复制到从，由于复制过程的速度差异，为了保证所有的一致性，需要做一些「删除/清空类」的操作。但是这个过程中一旦包含覆盖的操作，就会在过程中更改文件状态，容易出现 bug。\nBookKeeper 在运行的过程中，不是一个物理文件，而是逻辑上的序。同时在失效恢复过程中，没有进行任何的复用，使得数据恢复变得简单又清晰。其次它在整个修复过程中，没有去额外动用 ledger X 的数据。\n# 自动恢复 当一个 bookie 失败时，这个 bookie 上所有 ledger 的 fragments 都将被复制到其他节点，以确保每个 ledger 的复制系数（Write quorum）得到保证。\nrecovery 不是 BookKeeper 复制协议的一部分，而是在 BookKeeper 外部运行，作为一种异步修复机制使用。\n有两种类型的recovery：手动或自动。\n自动恢复进程 AutoRecoveryMain 可以在独立的服务器上运行，也可以和bookie跑在一起。其中一个自动恢复进程被选为审核员，然后进行如下操作：\n从 zookeeper 读取所有的 ledger 列表，并找到位于失败 bookie上的 ledger。\n对于第一步找到的所有ledger，在ZooKeeper上创建一个复制任务。\nAutoRecoveryMain 进程会发现 ZooKeeper 上的复制任务，锁定任务，进行复制，满足Write quorum，最后删除任务。\n通过自动恢复，Pulsar集群能够在面对存储层故障时进行自我修复，只需确保部署了适量的bookie就可以了。\n如果审计节点失败，那么另一个节点就会被提升为审计员。\n","date":"2022-09-17T16:42:59+08:00","permalink":"https://mazhen.tech/p/bookkeeper%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/","title":"BookKeeper实现分析"},{"content":" Previous Next \u0026nbsp; \u0026nbsp; Page: / ","date":"2022-08-22T16:24:52+08:00","permalink":"https://mazhen.tech/p/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/","title":"深入浅出容器技术"},{"content":"GCC 会为不同 CPU 架构预定义宏，如 __x86_64__ 代表Intel 64位CPU， __aarch64__代表 ARM64。 网上已经有文档对 GCC 为 CPU 的预定义宏进行了总结。\n这些预定义的宏有什么用呢？我们在代码中可以判断出当前的 CPU 架构，那么可以针对 不同CPU的特性，进行优化实现。例如RocksDB 对于获取当前时间，在 x86 平台上，会用到 Time Stamp Counter (TSC) 寄存器，使用 RDTSC 指令提取 TSC 中值。对于 ARM 64 也有类似的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Get the value of tokutime for right now. We want this to be fast, so we // expose the implementation as RDTSC. static inline tokutime_t toku_time_now(void) { #if defined(__x86_64__) || defined(__i386__) uint32_t lo, hi; __asm__ __volatile__(\u0026#34;rdtsc\u0026#34; : \u0026#34;=a\u0026#34;(lo), \u0026#34;=d\u0026#34;(hi)); return (uint64_t)hi \u0026lt;\u0026lt; 32 | lo; #elif defined(__aarch64__) uint64_t result; __asm __volatile__(\u0026#34;mrs %[rt], cntvct_el0\u0026#34; : [ rt ] \u0026#34;=r\u0026#34;(result)); return result; #elif defined(__powerpc__) return __ppc_get_timebase(); #elif defined(__s390x__) uint64_t result; asm volatile(\u0026#34;stckf %0\u0026#34; : \u0026#34;=Q\u0026#34;(result) : : \u0026#34;cc\u0026#34;); return result; #else #error No timer implementation for this platform #endif } 而在将 RocksDB 移植到龙芯的过程中，需要修改上面的代码，判断出当前是龙芯 loongarch64 架构。\n网上没有搜到 GCC 对龙芯 CPU 的预定宏的文档说明，只能从源码中找答案：\n1 2 3 4 5 6 7 void loongarch_cpu_cpp_builtins (cpp_reader *pfile) { ... builtin_define (\u0026#34;__loongarch__\u0026#34;); ... } 可以看到，__loongarch__代表龙芯CPU。 在暂时不知道龙芯是否支持RDTSC的情况下，只能给出通用的实现，以后再查龙芯的CPU手册进行优化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 #if defined(__x86_64__) || defined(__i386__) ... #elif defined(__aarch64__) ... #elif defined(__powerpc__) ... #elif defined(__loongarch__) struct timeval tv; gettimeofday(\u0026amp;tv,NULL); return tv.tv_sec*(uint64_t)1000000+tv.tv_usec; #else #error No implementation for this platform #endif ","date":"2022-06-17T16:19:54+08:00","permalink":"https://mazhen.tech/p/gcc-%E4%B8%BA%E9%BE%99%E8%8A%AF-cpu%E7%9A%84%E9%A2%84%E5%AE%9A%E4%B9%89%E5%AE%8F/","title":"GCC 为龙芯 CPU的预定义宏"},{"content":"我们经常会使用 kill 命令杀掉运行中的进程，对多次杀不死的进程进一步用 kill -9 干掉它。你可能知道这是在用 kill 命令向进程发送信号，优雅或粗暴的让进程退出。我们能向进程发送很多类型的信号，其中一些常见的信号 SIGINT 、SIGQUIT、 SIGTERM 和 SIGKILL 都是通知进程退出，但它们有什么区别呢？很多人经常把它们搞混，这篇文章会让你了解 Linux 的信号机制，以及一些常见信号的作用。\n# 什么是信号 信号（Signal）是 Linux 进程收到的一个通知。当进程收到一个信号时，该进程会中断其执行，并执行收到信号对应的处理程序。\n信号机制作为 Linux 进程间通信的一种方法。Linux 进程间通信常用的方法还有管道、消息、共享内存等。\n信号的产生有多种来源：\n硬件来源，例如 CPU 内存访问出错，当前进程会收到信号 SIGSEGV；按下 Ctrl+C 键，当前运行的进程会收到信号 SIGINT 而退出； 软件来源，例如用户通过命令 kill [pid]，直接向一个进程发送信号。进程使用系统调用 int kill(pid_t pid, int sig) 显示的向另一个进程发送信号。内核在某些情况下，也会给进程发送信号，例如当子进程退出时，内核给父进程发送 SIGCHLD 信号。 你可以使用 kill -l 命令查看系统实现了哪些信号：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ kill -l 1) SIGHUP\t2) SIGINT\t3) SIGQUIT\t4) SIGILL\t5) SIGTRAP 6) SIGABRT\t7) SIGBUS\t8) SIGFPE\t9) SIGKILL\t10) SIGUSR1 11) SIGSEGV\t12) SIGUSR2\t13) SIGPIPE\t14) SIGALRM\t15) SIGTERM 16) SIGSTKFLT\t17) SIGCHLD\t18) SIGCONT\t19) SIGSTOP\t20) SIGTSTP 21) SIGTTIN\t22) SIGTTOU\t23) SIGURG\t24) SIGXCPU\t25) SIGXFSZ 26) SIGVTALRM\t27) SIGPROF\t28) SIGWINCH\t29) SIGIO\t30) SIGPWR 31) SIGSYS\t34) SIGRTMIN\t35) SIGRTMIN+1\t36) SIGRTMIN+2\t37) SIGRTMIN+3 38) SIGRTMIN+4\t39) SIGRTMIN+5\t40) SIGRTMIN+6\t41) SIGRTMIN+7\t42) SIGRTMIN+8 43) SIGRTMIN+9\t44) SIGRTMIN+10\t45) SIGRTMIN+11\t46) SIGRTMIN+12\t47) SIGRTMIN+13 48) SIGRTMIN+14\t49) SIGRTMIN+15\t50) SIGRTMAX-14\t51) SIGRTMAX-13\t52) SIGRTMAX-12 53) SIGRTMAX-11\t54) SIGRTMAX-10\t55) SIGRTMAX-9\t56) SIGRTMAX-8\t57) SIGRTMAX-7 58) SIGRTMAX-6\t59) SIGRTMAX-5\t60) SIGRTMAX-4\t61) SIGRTMAX-3\t62) SIGRTMAX-2 63) SIGRTMAX-1\t64) SIGRTMAX 使用 man 7 signal 命令查看系统对每个信号作用的描述：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Signal Standard Action Comment ──────────────────────────────────────────────────────────────────────── SIGABRT P1990 Core Abort signal from abort(3) SIGALRM P1990 Term Timer signal from alarm(2) SIGBUS P2001 Core Bus error (bad memory access) SIGCHLD P1990 Ign Child stopped or terminated SIGCLD - Ign A synonym for SIGCHLD SIGCONT P1990 Cont Continue if stopped SIGEMT - Term Emulator trap SIGFPE P1990 Core Floating-point exception SIGHUP P1990 Term Hangup detected on controlling terminal or death of controlling process SIGILL P1990 Core Illegal Instruction ... # 信号和中断 信号处理是一种典型的异步事件处理方式：进程需要提前向内核注册信号处理函数，当某个信号到来时，内核会就执行相应的信号处理函数。\n我们知道，硬件中断也是一种内核的异步事件处理方式。当外部设备出现一个必须由 CPU 处理的事件，如键盘敲击、数据到达网卡等，内核会收到中断通知，暂时打断当前程序的执行，跳转到该中断类型对应的中断处理程序。中断处理程序是由 BIOS 和操作系统在系统启动过程中预先注册在内核中的。\n中断和信号通知都是在内核产生。中断是完全在内核里完成处理，而信号的处理则是在用户态完成的。也就是说，内核只是将信号保存在进程相关的数据结构里面，在执行信号处理程序之前，需要从内核态切换到用户态，执行完信号处理程序之后，又回到内核态，再恢复进程正常的运行。\n可以看出，中断和信号的严重程度不一样。信号影响的是一个进程，信号处理出了问题，最多是这个进程被干掉。而中断影响的是整个系统，一旦中断处理程序出了问题，可能整个系统都会挂掉。\n# 信号处理 一旦有信号产生，进程对它的处理都有下面三个选择。\n执行缺省操作（Default）。Linux 为每个信号都定义了一个缺省的行为。例如，信号 SIGKILL 的缺省操作是 Term，也就是终止进程的意思。信号 SIGQUIT 的缺省操作是 Core，即终止进程后，通过 Core Dump 将当前进程的运行状态保存在文件里面。 捕捉信号（Catch）。这个是指让用户进程可以注册自己针对这个信号的处理函数。当信号发生时，就执行我们注册的信号处理函数。 忽略信号（Ignore）。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。 有两个信号例外，对于 SIGKILL 和 SIGSTOP 这个两个信号，进程是无法捕捉和忽略，它们用于在任何时候中断或结束某一进程。SIGKILL 和 SIGSTOP 为内核和超级用户提供了删除任意进程的特权。\n如果我们不想让信号执行缺省操作，可以对特定的信号注册信号处理函数：\n1 2 3 4 5 6 #include \u0026lt;signal.h\u0026gt; typedef void (*sighandler_t)(int); sighandler_t signal(int signum, sighandler_t handler); 例如下面的例子，程序捕获了信号 SIGINT ，并且只是输出不做其他处理，这样在键盘上按 Ctrl+C 并不能让程序退出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; void sig_handler(int signo) { if (signo == SIGINT) { printf(\u0026#34;received SIGINT\\n\u0026#34;); } } int main(int argc, char *argv[]) { signal(SIGINT, sig_handler); printf(\u0026#34;Process is sleeping\\n\u0026#34;); while (1) { sleep(1000); } return 0; } 通过 signal 注册的信号处理函数，会保存在进程内核的数据结构 task_struct 中。由于信号都发给进程，并由进程在用户态处理，所以发送给进程的信号也保存在 task_struct 中。\ntask_struct-\u0026gt;sighand 和 task_struct-\u0026gt;signal 是线程组内共享，而 task_struct-\u0026gt;pending 是线程私有的。\nstask_struct-\u0026gt;sighand 里面有一个 action，这是一个数组，下标是信号，数组内容就是注册的信号处理函数。\ntask_struct-\u0026gt;pending 内包含了一个链表，保存了本线程所有的待处理信号。task_struct-\u0026gt;signal-\u0026gt;shared_pending 上也有一个待处理信号链表，这个链表保存的是线程组内共享的信号。\n# 常见信号 下面的列表列举了一些常见的信号。\nSingal Value Action comment key binding SIGHUP 1 Term Hangup detected on controlling terminal or death of controlling process SIGINT 2 Term Interrupt from keyboard Ctrl-c SIGQUIT 3 Core Quit from keyboard Ctrl-\\ SIGKILL 9 Term Kill signal SIGSEGV 11 Core Invalid memory reference SIGPIPE 13 Term Broken pipe: write to pipe with no readers SIGTERM 15 Term Termination signal SIGCHLD 17 Ign Child stopped or terminated SIGCONT 18 Cont Continue if stopped SIGSTOP 19 Stop Stop process SIGTSTP 20 Stop Stop typed at terminal Ctrl-z SIGTTIN 21 Stop Terminal input for background process SIGTTOU 22 Stop Terminal output for background process 第一列是信号名称，第二列是信号编号。使用 kill 向进程发送信号时，用信号名称和编号都可以，例如：\n1 2 kill -1 [pid] kill -SIGHUP [pid] Action 列是信号的缺省行为，主要有如下几个：\nTerm 终止进程 Core 终止进程并core dump Ign 忽略信号 Stop 停止进程 Cont 如果进程是已停止，则恢复进程执行 有一些信号在 TTY 终端做了键盘按键绑定，例如 CTRL+c 会向终端上运行的前台进程发送 SIGINT 信号。\n# SIGHUP 运行在终端中，由 bash 启动的进程，都是 bash 的子进程。终端退出结束时会向 bash 的每一个子进程发送 SIGHUP 信号。由于 SIGHUP 的缺省行为是 Term，因此，即使运行在后台的进程也会和终端一起结束。\n使用 nohup 命令可解决这个问题，它的作用是让进程忽略 SIGHUP 信号：\n1 $ nohup command \u0026gt;cmd.log 2\u0026gt;\u0026amp;1 \u0026amp; 这样，即使我们退出了终端，运行在后台的程序会忽视 SIGHUP 信号而继续运行。由于作为父进程的 bash 进程已经结束，因此 /sbin/init 就成为孤儿进程新的父进程。\n# SIGINT, SIGQUIT, SIGTERM 和 SIGKILL SIGTERM 和 SIGKILL 是通用的终止进程请求，SIGINT 和 SIGQUIT 是专门用于来自终端的终止进程请求。他们的关键不同点是：SIGINT 和 SIGQUIT 可以是用户在终端使用快捷键生成的，而 SIGTERM 和 SIGKILL 必须由另一个程序以某种方式生成（例如通过 kill 命令）。\n当用户按下 ctrl-c 时，终端将发送 SIGINT 到前台进程。 SIGINT 的缺省行为是终止进程（Term），但它可以被捕获或忽略。 信号 SIGINT 的目的是为进程提供一种有序、优雅的关闭机制。\n当用户按下 ctrl-\\ 时，终端将发送 SIGQUIT 到前台进程。 SIGQUIT 的缺省行为是终止进程并 core dump，它同样可以被捕获或忽略。 你可以认为 SIGINT 是用户发起的愉快的终止，而 SIGQUIT 是用户发起的不愉快终止，需要生成 Core Dump ，方便用户事后进行分析问题在哪里。\n在 ubuntu 上由 systemd-coredump 系统服务处理 core dump。我们可以使用 coredumpctl 命令行工具查询和处理 core dump 文件。\n1 2 3 $ coredumpctl list TIME PID UID GID SIG COREFILE EXE SIZE Tue 2022-04-12 22:09:52 CST 6754 1000 1000 SIGQUIT present /usr/bin/cat 17.1K core dump 文件缺省保存在 /var/lib/systemd/coredump 目录下。\nSIGTERM 默认行为是终止进程，但它也可以被捕获或忽略。SIGTERM 的目的是杀死进程，它允许进程有机会在终止前进行清理，优雅的退出。当我们使用 kill 命令时，SIGTERM 是默认信号。\nSIGKILL 唯一的行为是立即终止进程。 由于 SIGKILL 是特权信号，进程无法捕获和忽略，因此进程在收到该信号后无法进行清理，立刻退出。\n例如 docker 在停止容器的时候，先给容器里的1号进程发送 SIGTERM，如果不起作用，那么等待30秒后会会发送 SIGKILL，保证容器最终会被停止。\n# SIGSTOP 、 SIGTSTP 和 SIGCONT SIGSTOP 和 SIGTSTP 这两个信号都是为了暂停一个进程，但 SIGSTOP 是特权信息，不能被捕获或忽略。\nSIGSTOP 必须由另一个程序以某种方式生成（例如：kill -SIGSTOP pid），而SIGTSTP 也可以由用户在键盘上键入快捷键 Ctrl-z 生成。\n被暂停的进程通过信号 SIGCONT 恢复。当用户调用 fg 命令时，SIGCONT 由 shell 显式发送给被暂停的进程。\nLinux 使用他们进行作业控制，让你能够手动干预和停止正在运行的应用程序，并在未来某个时间恢复程序的执行。\n# SIGTTOU 和 SIGTTIN Linux 系统中可以有多个会话（session），每个会话可以包含多个进程组，每个进程组可以包含多个进程。\n会话是用户登录系统到退出的所有活动，从登录到结束前创建的所有进程都属于这次会话。会话有一个前台进程组，还可以有一个或多个后台进程组。只有前台进程可以从终端接收输入，也只有前台进程才被允许向终端输出。如果一个后台作业中的进程试图进行终端读写操作，终端会向整个作业发送 SIGTTOU 或 SIGTTIN 信号，默认的行为是暂停进程。\n# JVM 对信号的处理 如果你使用 strace 追踪 Java 应用，发现 Java 程序会抛出大量 SIGSEGV。\n1 2 3 4 5 6 7 8 9 10 11 $ strace -fe \u0026#39;trace=!all\u0026#39; java [app] ... [pid 21746] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061680} --- [pid 21872] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061480} --- [pid 21943] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061500} --- [pid 21844] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061780} --- [pid 21728] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061c00} --- [pid 21906] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061980} --- [pid 21738] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061100} --- [pid 21729] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061e00} --- ... SIGSEGV 信号的意思是 “分段错误”（segmentation fault），是当系统检测到进程试图访问不属于它的内存地址时，内核向进程发送的信号。SIGSEGV 对于一般应用来说是很严重的错误，但 Java 进程中的 SIGSEGV 几乎总是正常和安全的。\n在常规的 C/C++ 程序中，当你期望指针是指向某个结构，但实际指向的是 NULL，会导致应用程序崩溃。这种崩溃实际上是内核向进程发送了信号 SIGSEGV。如果应用程序没有为该信号注册信号处理程序，则信号会返回到内核，然后内核会终止应用。实际上 JVM 为 SIGSEGV 注册了一个信号处理程序，因为 JVM 想使用 SIGSEGV 和其他一些信号来实现自己的目的。\n这篇文档 描述了 JVM 对信号的特殊处理：\nSignal Description SIGSEGV, SIGBUS, SIGFPE, SIGPIPE, SIGILL These signals are used in the implementation for implicit null check, and so forth. SIGQUIT This signal is used to dump Java stack traces to the standard error stream. (Optional) SIGTERM, SIGINT, SIGHUP These signals are used to support the shutdown hook mechanism (java.lang.Runtime.addShutdownHook) when the VM is terminated abnormally. (Optional) SIGUSR1 This signal is used in the implementation of the java.lang.Thread.interrupt method. Not used starting with Oracle Solaris 10 reserved on Linux. (Configurable) SIGUSR2 This signal is used internally. Not used starting with Oracle Solaris 10 operating system. (Configurable) SIGABRT The HotSpot VM does not handle this signal. Instead it calls the abort function after fatal error handling. If an application uses this signal then it should terminate the process to preserve the expected semantics. 实际上，JVM 是使用 SIGSEGV、SIGBUS、SIGPIPE 等进行代码中的各种 NULL 检查。\n同样，我们在终端上键入 ctrl-\\，也不会让前台运行的 Java 进程终止并 core dump，而是会将 Java 进程的 stack traces 输出到终端的标准错误流。\n那么如何对 Java 进程进行 core dump 呢？需要在 Java 的启动命令里增加 JVM 选项 -Xrs ，它会让 JVM 不自己处理 SIGQUIT 信号，这样 SIGQUIT 会触发缺省行为 core dump。\n一般 Java 进程的运行时内存占用都比较大，在进行 core dump 时很容易超过缺省大小而被truncated，因此需要修改配置文件 /etc/systemd/coredump.conf，合理设置 ProcessSizeMax 和 ExternalSizeMax 的大小。\n","date":"2022-04-13T16:17:35+08:00","permalink":"https://mazhen.tech/p/linux-%E4%BF%A1%E5%8F%B7signal/","title":"Linux 信号(Signal)"},{"content":"长时间运行的Linux服务器，通常 free 的内存越来越少，让人觉得 Linux 特别能“吃”内存，甚至有人专门做了个网站 LinuxAteMyRam.com解释这个现象。实际上 Linux 内核会尽可能的对访问过的文件进行缓存，来弥补磁盘和内存之间巨大的延迟差距。缓存文件内容的内存就是 Page Cache。\nGoogle 的大神 Jeffrey Dean总结过一个Latency numbers every programmer should know，其中提到从磁盘读取 1MB 数据的耗时是内存的80倍，即使换成 SSD 也是内存延迟的 4 倍。\n我在本机做了实验，来体会一下 Page Cache 的作用。首先生成一个 1G 大小的文件：\n1 # dd if=/dev/zero of=/root/dd.out bs=4096 count=262144 清空 Page Cache：\n1 # sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches 统计第一次读取文件的耗时：\n1 2 3 4 5 # time cat /root/dd.out \u0026amp;\u0026gt; /dev/null real\t0m2.097s user\t0m0.010s sys\t0m0.638s 再此读取同一个文件，由于系统已经将读取过的文件内容放入了 Page Cache ，这次耗时大大缩短：\n1 2 3 4 5 # time cat /root/dd.out \u0026amp;\u0026gt; /dev/null real\t0m0.186s user\t0m0.004s sys\t0m0.182s Page Cache 不仅能加速对文件内容的访问，对共享库建立 Page Cache，可以在多个进程间共享，避免每个进程都单独加载，造成宝贵内存资源的浪费。\n# Page Cache 是什么 Page Cache 是由内核管理的内存，位于 VFS(Virtual File System) 层和具体文件系统层（例如ext4，ext3）之间。应用进程使用 read/write 等文件操作，通过系统调用进入到 VFS 层，根据 O_DIRECT 标志，可以使用 Page Cache 作为文件内容的缓存，也可以跳过 Page Cache 不使用内核提供的缓存功能。\n另外，应用程序可以使用 mmap ，将文件内容映射到进程的虚拟地址空间，可以像读写内存一样直接读写硬盘上的文件。进程的虚拟内存直接和 Page Cache 映射。\n为了了解内核是怎么管理 Page Cache 的，我们先看一下 VFS 的几个核心对象：\nfile 存放已打开的文件信息，是进程访问文件的接口； dentry 使用 dentry 将文件组织成目录树结构； inode 唯一标识文件系统中的文件。对于同一个文件，内核中只会有一个 inode 结构。 对于每一个进程，打开的文件都有一个文件描述符，内核中进程数据结构 task_struct 中有一个类型为 files_struct 的 files 字段，保存着该进程打开的所有文件。files_struct 结构的fd_array 字段是 file 数组， 数组的下标是文件描述符，内容指向一个 file 结构，表示该进程打开的文件。file 与打开文件的进程相关联，如果多个进程打开同一个文件，那么每个进程都有自己的 file ，但这些 file 指向同一个 inode。\n如上图所示，进程通过文件描述符与 VFS 中的 file 产生联系， 每个 file 对象又与一个 dentry 对应，根据 dentry 能找到 inode，而 inode 则代表文件本身。上图中进程 A 和进程 B 打开了同一个文件，进程 A 和进程 B 都维护着各自的 file ，但它们指向同一个 inode。\ninode 通过 address_space 管理着文件已加载到内存中的内容，也就是 Page Cache。address_space 的字段 i_pages 指向一棵 xarray 树，与这个文件相关的 Page Cache 页都挂在这颗树上。我们在访问文件内容的时候，根据指定文件和相应的页偏移量，就可以通过 xarray 树快速判断该页是否已经在 Page Cache 中。如果该页存在，说明文件内容已经被读取到了内存，也就是存在于 Page Cache 中；如果该页不存在，就说明内容不在 Page Cache 中，需要从磁盘中去读取。\n由于文件和 inode 一一对应，我们可以认为 inode 是 Page Cache 的宿主（host），内核通过 inode-\u0026gt;imapping-\u0026gt;i_pages 指向的树，管理维护着 Page Cache。\nPage Cache 是如何产生和释放，又是如何与进程相关联的呢？我们需要先了解进程虚拟地址空间。\n# 进程虚拟地址空间 Linux 是多任务系统，它支持多个进程的的并发执行。操作系统和 CPU 联手制造了一个假象：每个进程都独享连续的虚拟内存空间，并且各个进程的地址空间是完全隔离的，因此进程并不会意识到彼此的存在。从进程的角度来看，它会认为自己是系统中唯一的进程。\n进程看到的是虚拟内存的地址空间，它也不能直接访问物理地址。当进程访问某个虚拟地址的时候，该虚拟地址由内核负责转换成物理内存地址，即完成虚拟地址到物理地址的映射。这样不同进程在运行的时候，即使访问相同的虚拟地址，但内核会将它们映射到不同的物理地址，因此不会发生冲突。\n进程在 Linux 内核由 task_struct 所描述。估计 task_struct 是你学习内核时第一个熟悉的数据结构，因为它实在太重要了。 task_struct描述了进程相关的所有信息，包括进程状态，运行时统计信息，进程亲缘关系，进度调度信息，信号处理，进程内存管理，进程打开的文件等等。我们这里关注的进程虚拟内存空间，是由 task_struct 中的 mm 字段指向的 mm_struct 所描述，它是一个进程内存的运行时摘要信息。\n进程的虚拟地址是线性的，使用结构体 vm_area_struct 来描述。内核将每一段具有相同属性的内存区域当作一个 vm_area_struct 进行管理，每个 vm_area_struct 是一个连续的虚拟地址范围，这些区域不会互相重叠。 mm_struct 里面有一个单链表 mmap，用于将 vm_area_struct 串联起来，另外还有一颗红黑树 mm_rb ，vm_area_struct 根据起始地址挂在这颗树上。使用红黑树可以根据地址，快速查找一个内存区域。\nvm_area_struct 可以直接映射到物理内存，也可以关联文件。如果 vm_area_struct 是文件映射，由成员 vm_file 指向对应的文件指针。一个没有关联文件的 vm_area_struct 是匿名内存。\n开发者使用 malloc 等 glibc 库函数分配内存的时候，不是直接分配物理内存，而是在进程的虚拟内存空间中申请一段虚拟内存，生成相应的数据结构 vm_area_struct ，然后将它插进 mm_struct 的链表 mmap，同时挂在红黑树 mm_rb 上，就算完成了工作，根本没有涉及到物理内存的分配。只有当第一次对这块虚拟内存进行读写时，发现该内存区域没有映射到物理内存，这时会触发缺页中断，然后由内核填写页表，完成虚拟内存到物理内存的映射。\n当开发者使用 mmap 进行文件映射时，内核根据 vm_area_struct 中代表文件映射关系 vm_file，将文件内容从磁盘加载到物理内存，也就是 Page Cache 中，最后建立这段虚拟地址到物理地址的映射。\n另外，在虚拟内存中连续的页面，在物理内存中不必是连续的。只要维护好从虚拟内存页到物理内存页的映射关系，你就能正确地使用内存。由于每个进程都有独立的地址空间，为了完成虚拟地址到物理地址的映射，每个进程都要有独立的进程页表。在一个实际的进程里面，虚拟内存占用的地址空间，通常是两段连续的空间，而不是完全散落的随机的内存地址。基于这个特点，内核使用多级页表保存映射关系，可以大大减少页表本身的空间占用。最顶级的页表保存在 mm_struct 的 pgd 字段中。\n好了，我们对进程虚拟地址空间有了基本的了解，下面看看 Page Cache 的产生和释放，以及如何与进程空间发生联系的。\n# Page Cache 的产生和释放 Page Cache 的产生有两种不同的方式：\nBuffered I/O Memory-Mapped file 使用这两种方式访问磁盘上的文件时，内核会根据指定的文件和相应的页偏移量，判断文件内容是否已经在 Page Cache 中，如果内容不存在，需要从磁盘中去读取并创建 Page Cache 页。\n这两种方式的不同之处在于，使用 Buffered I/O，要先将数据从 Page Cache 拷贝到用户缓冲区，应用才能从用户缓冲区读取数据。而对于 Memory-Mapped file 而言，则是直接将 Page Cache 页映射到进程虚拟地址空间，用户可以直接读写 Page Cache 中的内容。由于少了一次 copy，使用 Memory-Mapped file 要比 Buffered I/O 的效率高一些。\n随着服务器运行时间的增加，系统中空闲内存会越来越少，其中很大一部分都被 Page Cache 占用。访问过的文件都被 Page Cache 缓存，内存最终会被耗尽，那什么时候回收 Page Cache 呢？ 内核认为，Page Cache 是可回收内存，当应用在申请内存时，如果没有足够的空闲内存，就会先回收 Page Cache，再尝试申请。回收的方式主要是两种：直接回收和后台回收。\n使用 Buffered I/O 时，Page Cache 并没有和进程的虚拟内存空间产生直接的关联，而是通过用户缓冲区作为中转。效率更好的Memory-Mapped file方式，看着比较简单，但背后的实现却有些复杂。下面我们看一下内核是如何实现 Memory-Mapped file 的。\n# 内存文件映射 前面我们介绍过， inode 是 Page Cache 的宿主（host），内核通过 inode-\u0026gt;imapping-\u0026gt;i_pages 指向的树，管理维护着 Page Cache。那么内核是如何完成内存文件映射，直接把缓存了文件内容的 Page Cache 映射到进程虚拟内存空间的呢？\n我们知道，进程结构体 task_struct 中的字段 mm 指向该进程的虚拟地址空间 mm_struct ，而一段虚拟内存由结构体 vm_area_struct 所描述，将 vm_area_struct 串在一起的链表 mmap 就代表了已经申请分配的虚拟内存。\n如果是进行内存文件映射，那么映射了文件的虚拟内存区域 vm_area_struct ，它的 vm_file 会指向被映射的文件结构体 file。file 表示进程打开的文件，它的成员 f_mapping 指向 address_space，这样就和管理文件着 Page Cache 的 address_space 关联了起来。\n当第一次访问文件映射的虚拟内存区域时，这段虚拟内存并没有映射到物理内存，这时会触发缺页中断。内核在处理缺页中断时，发现代表这段虚拟内存的 vm_area_struct 有关联的文件，即 vm_file 字段指向一个文件结构体 file。内核拿到该文件的 address_space，根据要访问内容的页偏移量，对 address_space-\u0026gt;i_pages 指向的 xarray 树进行查找。这颗树上挂的都是页偏移量对应的内存页，如果没找到，就说明文件内容还没加载进内存，就会分配内存页，将文件内容加载到内存中，然后把内存页挂在 xarray 树上。下次再访问同样的页偏移量时，文件内容已经在树上，可直接返回。 address_space-\u0026gt;i_pages 指向的树就是内核管理的 Page Cache。\n将文件内容加载到 Page Cache 后，内核就可以填写进程相关的页表项，将这块文件映射的虚拟地址区域，直接映射到 Page Cache 页，完成缺页中断的处理。\n当内存紧张需要回收 Page Cache 时，内核需要知道这些 Page Cache 页映射到了哪些进程，这样才能修改进程的页表，解除虚拟内存和物理内存的映射。我们知道，同一个文件可以映射到多个进程空间，所以需要保存反向映射关系，即根据 Page Cache 页找到进程。\nPage Cache 页的反向映射关系保存在 address_space 维护的另一颗树 i_mmap。address_space-\u0026gt;i_mmap 是一个优先查找树（Priority Search Tree），关联了这个文件 Page Cache 页的 vm_area_struct 就挂在这棵树上，而这些 vm_area_struct都将指向各自的进程空间描述符 mm_struct，从而建立了 Page Cache 页到进程的联系。\n当需要解除一个 Page Cache 页的映射时，利用 address_space-\u0026gt;i_mmap 指向的树，查找 Page Cache 页映射到哪些进程的哪些 vm_area_struct，从而确定需要修改的进程页表项内容。\n简单总结一下，一个文件对应的 address_space 主要管理着两颗树：i_pages 指向的 xarray 树，维护着的所有 Page Cache 页；i_mmap 指向的 PST 树，维护着文件映射所形成的 vm_area_struct 虚拟内存区域，用来在释放 Page Cache 页时，查找映射了该文件的进程。如果文件没有被映射到进程空间，那么 i_mmap 对应的 PST 树为空。\n# Page Cache 的观测 可以通过查看 /proc/meminfo 文件获知 Page Cache 相关的各种指标。\n/proc 是伪文件系统（Pseudo filesystems ）。Linux 通过伪文件系统，让系统和内核信息在用户空间可用。使用 free、vmstat等命令查看到的内存信息，数据实际上都来自 /proc/meminfo。\n我们看一个示例：\n$ cat /proc/meminfo MemTotal: 8052564 kB MemFree: 129804 kB MemAvailable: 4956164 kB Buffers: 175932 kB Cached: 4896824 kB SwapCached: 40 kB Active: 2748728 kB \u003c- Active(anon) + Active(file) Inactive: 4676540 kB \u003c- Inactive(anon) +Inactive(file) Active(anon): 3432 kB Inactive(anon): 2513172 kB Active(file): 2745296 kB Inactive(file): 2163368 kB Unevictable: 65496 kB Mlocked: 0 kB SwapTotal: 2097148 kB SwapFree: 2095868 kB Dirty: 12 kB Writeback: 0 kB AnonPages: 2411440 kB Mapped: 761076 kB Shmem: 170868 kB ... 关于 /proc/meminfo 每一项的详细解释，可以查看 [Linux 内核文档 - The /proc Filesystem](The /proc Filesystem — The Linux Kernel documentation)。我们重点看一下 Page Cache 相关的字段。\n当前系统 Page Cache 等于 Buffers + Cached 之和 ：\n1 Buffers + Cached = 5072756 kB 前面讨论过，如果 vm_area_struct 关联到文件，那么这段内存区域就是 File-backed 内存。没有关联文件的 vm_area_struct 内存区域是匿名内存。我们是否可以认为，和磁盘文件相关联的 File-backed 内存总和，应该等于 Page Cache 呢？\n1 Active(file) + Inactive(file) = 4908664 kB 好像有点对不上，还差了一些，差的这部分是共享内存（Shmem）。\nLinux 为了实现“共享内存”（shared memory）功能，即多个进程共同使用同一内存中的内容，需要使用虚拟文件系统。虚拟文件并不是真实存在于磁盘上的文件，它只是由内核模拟出来的。但虚拟文件也有自己的 inode 和 address_space结构。内核在创建共享匿名映射区域时，会创建出一个虚拟文件，并将这个文件与 vm_area_struct关联起来，这样多个进程的 vm_area_struct 会关联到同一个虚拟文件，最终映射到同样的物理内存页，从而实现了共享内存功能。这就是共享内存（Shmem）的实现原理。\n由于 Shmem 没有关联磁盘上的文件，因此它不属于 File-backed 内存，而是被记录在匿名内存（Active(anon) 或 Inactive(anon)）部分。但因为 Shmem 有自己的 inode ，inode-\u0026gt;address_sapce 维护的 Page Cache 页挂在 address_space-\u0026gt;i_pages 指向的 xarray 树上，因此 Shmem 部分的内存也应该算在 Page Cache 里。\n此外 File-backed 内存还有 Active 和 Inactive 的区别。刚被使用过的数据的内存空间被认为是 Active 的，长时间未被使用过的数据的内存空间则被认为是 Inactive 的。当物理内存不足，不得不释放正在使用的内存时，会首先释放 Inactive 的内存。\nPage Cache 和 匿名内存以及 File-backed 内存等之间的关系，如图下图所示。虽然难免存在误差，但大体来说下面的关系式是成立的：\n值得注意的是，AnonPages != Active(anon) + Inactive(anon)。Active(anon) 和 Inactive(anon) 是用来表示不可回收但是可以被交换到 swap 分区的内存，而 AnonPages 则是指没有对应文件的内存，两者的角度不一样。 Shmem 虽然属于Active(anon) 或者 Inactive(anon)，但是 Shmem 有对应的内存虚拟文件，所以它不属于 AnonPages。\n总之，Page Cache 肯定关联了文件，不管是真实存在的磁盘文件，还是虚拟内存文件。AnonPages 则没有关联任何文件。Shmem 关联了虚拟文件，它属于 Active(anon) 或者 Inactive(anon)，同时也算在 Page Cache 中。\n如果我们想知道某个文件有多少内容被缓存在 Page Cache ，可以使用 [fincore](fincore（1） - Linux 手册页 (man7.org)) 命令。例如：\n1 2 3 $ fincore /usr/lib/x86_64-linux-gnu/libc.so.6 RES PAGES SIZE FILE 2.1M 542 2.1M /usr/lib/x86_64-linux-gnu/libc.so.6 RES 是文件内容被加载进物理内存占用的内存空间大小。PAGES 是换算成文件内容占用了多少内存页。 在上面的例子中，文件 /usr/lib/x86_64-linux-gnu/libc.so.6 的全部内容，都被加载进了 Page Cache。\n结合 lsof 命令，我们可以查看某一进程打开的文件占用了多少 Page Cache：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ sudo lsof -p 1270 | grep REG | awk \u0026#39;{print $9}\u0026#39; | xargs sudo fincore RES PAGES SIZE FILE 64.8M 16580 89.9M /usr/bin/dockerd 32K 8 32K /var/lib/docker/buildkit/cache.db 16K 4 16K /var/lib/docker/buildkit/metadata_v2.db 16K 4 16K /var/lib/docker/buildkit/snapshots.db 16K 4 16K /var/lib/docker/buildkit/containerdmeta.db 284K 71 282.4K /usr/lib/x86_64-linux-gnu/libnss_systemd.so.2 244K 61 594.7K /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0.10.2 156K 39 154.2K /usr/lib/x86_64-linux-gnu/libgpg-error.so.0.29.0 24K 6 20.6K /usr/lib/x86_64-linux-gnu/libpthread.so.0 908K 227 906.5K /usr/lib/x86_64-linux-gnu/libm.so.6 ... 另外，对于所有缓存类型，缓存命中率都是一个非常重要的指标。我们可以使用 bcc 内置的工具 cachestat 追踪整个系统的 Page Cache 命中率：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ sudo cachestat-bpfcc HITS MISSES DIRTIES HITRATIO BUFFERS_MB CACHED_MB 2059 0 32 100.00% 74 1492 522 0 0 100.00% 74 1492 32 0 7 100.00% 74 1492 135 0 69 100.00% 74 1492 97 1 3 98.98% 74 1492 512 0 82 100.00% 74 1492 303 0 86 100.00% 74 1492 2474 7 1028 99.72% 74 1494 815 0 964 100.00% 74 1497 2786 0 1 100.00% 74 1497 1051 0 0 100.00% 74 1497 ^C 502 0 0 100.00% 74 1497 Detaching... 使用 cachetop 可以按进程追踪 Page Cache 命中率：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ sudo cachetop-bpfcc 14:20:41 Buffers MB: 86 / Cached MB: 2834 / Sort: HITS / Order: descending PID UID CMD HITS MISSES DIRTIES READ_HIT% WRITE_HIT% 14237 mazhen java 12823 4594 3653 52.6% 13.2% 14370 mazhen ldd 869 0 0 100.0% 0.0% 14371 mazhen grep 596 0 0 100.0% 0.0% 14376 mazhen ldd 536 0 0 100.0% 0.0% 14369 mazhen env 468 0 0 100.0% 0.0% 14377 mazhen ldd 467 0 0 100.0% 0.0% 14551 mazhen grpc-default-ex 466 0 0 100.0% 0.0% 14375 mazhen ldd 435 0 0 100.0% 0.0% 14479 mazhen ldconfig 421 0 0 100.0% 0.0% 14475 mazhen BookieJournal-3 417 58 132 60.0% 6.1% ... # mmap系统调用 系统调用 mmap是最重要的内存管理接口。使用 mmap 可以创建文件映射，从而产生 Page Cache。使用 mmap 还可以用来申请堆内存。glibc 提供的 malloc，内部使用的就是 mmap 系统调用。 由于 mmap 系统调用分配内存的效率比较低， malloc 会先使用 mmap 向操作系统申请一块比较大的内存，然后再通过各种优化手段让内存分配的效率最大化。\nmmap 根据参数的不同， 可以从是不是文件映射，以及是不是私有内存这两个不同的维度来进行组合：\n私有匿名映射 在调用 mmap(MAP_ANON | MAP_PRIVATE) 时，只需要在进程虚拟内存空间分配一块内存，然后创建这块内存所对应的 vm_area_struct 结构，这次调用就结束了。当访问到这块虚拟内存时，由于这块虚拟内存都没有映射到物理内存上，就会发生缺页中断。 vm_area_struct关联文件属性为空，所以是匿名映射。内核会分配一个物理内存，然后在页表里建立起虚拟地址到物理地址的映射关系。\n私有文件映射 进程通过 mmap(MAP_FILE | MAP_PRIVATE) 这种方式来申请的内存，比如进程将共享库（Shared libraries）和可执行文件的代码段（Text Segment）映射到自己的地址空间就是通过这种方式。\n如果文件是只读的话，那这个文件在物理页的层面上其实是共享的。也就是进程 A 和进程 B 都有一页虚拟内存被映射到了相同的物理页上。但如果要写文件的时候，因为这一段内存区域的属性是私有的，所以内核就会做一次写时复制，为写文件的进程单独地创建一份副本。这样，一个进程在写文件时，并不会影响到其他进程的读。\n私有文件映射的只读页是多进程间共享的，可写页是每个进程都有一个独立的副本，创建副本的时机仍然是写时复制。\n共享文件映射 进程通过 mmap(MAP_FILE | MAP_SHARED) 这种方式来申请的内存。在私有文件映射的基础上，共享文件映射就很简单了：对于可写的页面，在写的时候不进行复制就可以了。这样的话，无论何时，也无论是读还是写，多个进程在访问同一个文件的同一个页时，访问的都是相同的物理页面。\n共享匿名映射 进程通过 mmap(MAP_ANON | MAP_SHARED) 这种方式来申请的内存。借助虚拟文件系统，多个进程的 vm_area_struct 会关联到同一个虚拟文件，最终映射到同样的物理内存页，实现进程间共享内存的功能。\nmmap 的四种映射类型，和上面介绍的 /proc/meminfo 内存指标之间的关系：\n私有映射都属于 AnonPages，共享映射都是 Page cache。前面讨论过，共享的匿名映射 Shmem，虽然没有关联真实的磁盘文件，但是关联了虚拟内存文件，所以也属于 Page Cache。\n私有文件映射，如果文件是只读的话，这块内存属于 Page Cache。如果有进程写文件，因为这一段内存区域的属性是私有的，所以内核就会做一次写时复制，为写文件的进程单独地创建一份副本，这个副本就属于 AnonPages 了。\n# 写在最后 Page Cache 机制涉及了进程空间，文件系统，内存管理等多个内核功能，Page Cache 就像一条线将这几部分串在了一起。因此深入理解 Page Cache 机制，对学习内核会有很大的帮助。\n","date":"2022-04-01T16:06:54+08:00","permalink":"https://mazhen.tech/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-page-cache/","title":"深入理解 Page Cache"},{"content":" # 进程ID 进程相关的 ID 有多种，除了进程标识 PID 外，还包括：线程组标识 TGID，进程组标识 PGID，回话标识 SID。TGID/PGID/SID 分别是相关线程组长/进程组长/回话 leader 进程的 PID。\n下面分别介绍这几种ID。\n# PID 进程总是会被分配一个唯一标识它们的进程ID号，简称 PID。\n用 fork 或 clone 产生的每个进程都由内核自动地分配了一个唯一的 PID 。\nPID 保存在 task_struct-\u0026gt;pid中。\n# TGID 进程以 CLONE_THREAD 标志调用 clone 方法，创建与该进程共享资源的线程。线程有独立的task_struct，但它 task_struct内的 files_struct、fs_struct 、sighand_struct、signal_struct和mm_struct 等数据结构仅仅是对进程相应数据结构的引用。\n由进程创建的所有线程都有相同的线程组ID(TGID)。线程有自己的 PID，它的TGID 就是进程的主线程的 PID。如果进程没有使用线程，则其 PID 和 TGID 相同。\n在内核中进程和线程都用 task_struct表示，而有了 TGID，我们就可以知道 task_struct 代表的是一个进程还是一个线程。\nTGID 保存在 task_struct-\u0026gt;tgid 中。\n当 task_struct 代表一个线程时，task_struct-\u0026gt;group_leader 指向主线程的 task_struct。\n# PGID 如果 shell 具有作业管理能力，则它所创建的相关进程构成一个进程组，同一进程组的进程都有相同的 PGID。例如，用管道连接的进程包含在同一个进程组中。\n进程组简化了向组的所有成员发送信号的操作。进程组提供了一种机制，让信号可以发送给组内的所有进程，这使得作业控制变得简单。\n当 task_struct 代表一个进程，且该进程属于某一个进程组，则 task_struct-\u0026gt;group_leader 指向组长进程的 task_struct。\nPGID 保存在 task_struct-\u0026gt;signal-\u0026gt;pids[PIDTYPE_PGID].pid中。 pids[] 的数组下标是枚举类型，在 include/linux/pid.h 中定义了 PID 的类型：\n1 2 3 4 5 6 7 8 enum pid_type { PIDTYPE_PID, PIDTYPE_TGID, PIDTYPE_PGID, PIDTYPE_SID, PIDTYPE_MAX, }; task_struce-\u0026gt;signal 是 signal_struct 类型，维护了进程收到的信号，task_struce-\u0026gt;signal 被该进程的所有线程共享。从 PGID 保存在 task_struct-\u0026gt;signal-\u0026gt;pids[PIDTYPE_PGID]中可以看出进程组和信号处理相关。 # SID 用户一次登录所涉及所有活动称为一个会话（session），其间产生的所有进程都有相同的会话ID（SID），等于会话 leader 进程的 PID。\nSID 保存在 task_struct-\u0026gt;signal-\u0026gt;pids[PIDTYPE_SID].pid中。\n# PID/TGID/PGID/SID总结 用一幅图来总结 PID/TGID/PGID/SID ：\n# 进程间关系 内核中所有进程的 task_struct 会形成多种组织关系。根据进程的创建过程会有亲属关系，进程间的父子关系组织成一个进程树；根据用户登录活动会有会话和进程组关系。\n# 亲属关系 进程通过 fork() 创建出一个子进程，就形成来父子关系，如果创建出多个子进程，那么这些子进程间属于兄弟关系。可以用 pstree 命令查看当前系统的进程树。\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ pstree -p systemd(1)─┬─ModemManager(759)─┬─{ModemManager}(802) │ └─{ModemManager}(806) ├─NetworkManager(685)─┬─{NetworkManager}(743) │ └─{NetworkManager}(750) ├─acpid(675) ├─agetty(814) ├─avahi-daemon(679)───avahi-daemon(712) ├─bluetoothd(680) ├─canonical-livep(754)─┬─{canonical-livep}(1224) │ ├─{canonical-livep}(1225) │ ├─{canonical-livep}(1226) ... 进程描述符 task_struct 的 parent 指向父进程，children指向子进程链表的头部，sibling 把当前进程插入到兄弟链表中。\n通常情况下，real_parent 和 parent 是一样的。如果在 bash 上使用 GDB 来 debug 一个进程，这时候进程的 parent 是 GDB ，进程的 real_parent 是 bash。\n当一个进程创建了子进程后，它应该通过系统调用 wait() 或者 waitpid() 等待子进程结束，回收子进程的资源。而子进程在结束时，会向它的父进程发送 SIGCHLD 信号。因此父进程还可以注册 SIGCHLD 信号的处理函数，异步回收资源。\n如果父进程提前结束，那么子进程将把1号进程 init 作为父进程。总之，进程都有父进程，负责进程结束后的资源回收。在子进程退出且父进程完成回收前，子进程变成僵尸进程。僵尸进程持续的时间通常比较短，在父进程回收它的资源后就会消亡。如果父进程没有处理子进程的终止，那么子进程就会一直处于僵尸状态。\n# 会话、进程组关系 Linux 系统中可以有多个会话（session），每个会话可以包含多个进程组，每个进程组可以包含多个进程。\n会话是用户登录系统到退出的所有活动，从登录到结束前创建的所有进程都属于这次会话。登录后第一个被创建的进程（通常是 shell），被称为 会话 leader。\n进程组用于作业控制。一个终端上可以启动多个作业，也就是进程组，并能控制哪个作业在前台，前台作业可以访问终端，哪些作业运行在后台，不能读写终端。\n我们来看一个会话和进程组的例子。\n1 2 3 4 5 6 7 8 9 10 11 12 $ cat | head hello hello ^Z [1]+ 已停止 cat | head $ ps j | more PPID PID PGID SID TTY TPGID STAT UID TIME COMMAND 1522 1532 1532 1532 pts/0 1762 Ss 1000 0:00 -bash 1532 1760 1760 1532 pts/0 1762 T 1000 0:00 cat 1532 1761 1760 1532 pts/0 1762 T 1000 0:00 head 1532 1762 1762 1532 pts/0 1762 R+ 1000 0:00 ps j 1532 1763 1762 1532 pts/0 1762 S+ 1000 0:00 more 上面的命令通过 cat | head 创建了第一个进程组，包含 cat 和 head 两个进程。这时这个作业是前台任务，可以控制终端。当我们按下 Ctrl + z，会发送信号 SIGTSTP 给前台进程组的所有进程，该信号的缺省行为是暂停作业执行。暂停的作业会让出终端，并且进程不会再被调度，直到它们收到 SIGCONT 信号恢复执行。\n然后我们通过 ps j | more 创建了另一个进程组，包含 ps 和 more 两个进程。ps 的参数 j 表示用任务格式显示进程。输出中的 STAT 列是进程的状态码，前面的大写字母表示进程状态，我们可以从 ps 的 man page 查看其含义：\n1 2 3 4 5 6 7 8 9 D uninterruptible sleep (usually IO) I Idle kernel thread R running or runnable (on run queue) S interruptible sleep (waiting for an event to complete) T stopped by job control signal t stopped by debugger during the tracing W paging (not valid since the 2.6.xx kernel) X dead (should never be seen) Z defunct (\u0026#34;zombie\u0026#34;) process, terminated but not reaped by its parent 某些进程除了大写字母代表的进程状态，还跟着一个附加符号：\ns ：进程是会话 leader 进程 + ：进程位于前台进程组中 从输出可以看出，bash 是这个会话的 leader 进程，它的 PID、PGID 和 SID 相同，都是1532 。这个会话其他所有进程的 SID 也都是 1532。\ncat | head 进程组的 PGID 是 1760，ps j | more 进程组的 PGID 是 1762。用管道连接的进程包含在同一个进程组中，每个进程组内第一个进程成为 Group Leader，并以 Group Leader 的 PID 作为组内进程的 PGID。\n会话有一个前台进程组，还可以有一个或多个后台进程组，只有前台作业可以从终端读写数据。示例的进程组关系如图：\n注意到上图中显示，终端设备可以向进程组发送信号。我们可以在终端输入特殊字符向前台进程发送信号：\nCtrl + c 发送 SIGINT 信号，默认行为是终止进程； Ctrl + \\ 发送 SIGQUIT 信号，默认行为是终止进程，并进行 core dump； Ctrl + z 发送 SIGTSTP 信号，暂停进程。 只有前台进程可以从终端接收输入，也只有前台进程才被允许向终端输出。如果一个后台作业中的进程试图进行终端读写操作，终端会向整个作业发送 SIGTTOU 或 SIGTTIN 信号，默认的行为是暂停进程。\n当终端关闭时，会向整个会话发送 SIGHUP 信号，通常情况下，这个会话的所有进程都会被终止。如果想让运用在后台的进程不随着 session 的结束而退出，可以使用 nohup 命令忽略 SIGHUP 信号：\n1 $ nohup command \u0026gt;cmd.log 2\u0026gt;\u0026amp;1 \u0026amp; 即使 shell 结束，运行于后台的进程也能无视 SIGHUP 信号继续执行。另外一个方法是可以让进程运行在 screen 或 tmux 这种终端多路复用器（terminal multiplexer）中。\n","date":"2021-11-14T16:02:22+08:00","permalink":"https://mazhen.tech/p/%E8%BF%9B%E7%A8%8Bid%E5%8F%8A%E8%BF%9B%E7%A8%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/","title":"进程ID及进程间的关系"},{"content":"我们通常会使用 load average 了解服务器的健康状况，检查服务器的负载是否正常。但 load average 有几个缺点：\nload average 的计算包含了 TASK_RUNNING 和 TASK_UNINTERRUPTIBLE 两种状态的进程。TASK_RUNNING 是进程处于运行、或等待分配 CPU 的准备运行状态。TASK_UNINTERRUPTIBLE 是进程处于不可中断的等待，一般是等待磁盘的输入输出。因此 load average 值飙高，可能是因为 CPU 资源不够，让很多处于 TASK_RUNNING 状态的进程等待 CPU，也可能是由于磁盘 I/O 资源紧张，造成很多进程因为等待 IO 而处于 TASK_UNINTERRUPTIBLE 状态。你可以通过 load average 发现系统很忙，但没法区分是因为争夺 CPU 还是 IO 引起的。 load average 最短的时间窗口为1分钟，没法观察更短窗口的负载平均值，例如想了解最近10秒的load average。 load average 报告的是活跃进程数的原始数据，你还需要知道可用的 CPU 核数，这样 load average 的值才有意义。 所以，当用户遇到服务器 load average 飙高的时候，还需要继续查看 CPU、I/O 和内存等资源的统计数据，才能进一步分析问题。\n于是，Facebook的工程师 Johannes Weiner 发明了一个新的指标 PSI(Pressure Stall Information)，并向内核提交了这个patch。\n# PSI 概览 当 CPU、内存或 IO 设备争夺激烈的时候，系统会出现负载的延迟峰值、吞吐量下降，并可能触发内核的 OOM Killer。PSI(Pressure Stall Information) 字面意思就是由于资源（CPU、内存和 IO）压力造成的任务执行停顿。PSI 量化了由于硬件资源紧张造成的任务执行中断，统计了系统中任务等待硬件资源的时间。我们可以用 PSI 作为指标，来衡量硬件资源的压力情况。停顿的时间越长，说明资源面临的压力越大。\n如果持续监控 PSI 指标并绘制变化曲线图，可以发现吞吐量下降与资源短缺的关系，让用户在资源变得紧张前，采取更主动的措施，例如将任务迁移到其他服务器，杀死低优先级的任务等。\nPSI 已经包含在 4.20及以上版本的 Linux 内核中。\n# PSI 接口文件 CPU、内存和 IO 的压力信息导出到了 /proc/pressure/ 目录下对应的文件，你可以使用 cat 命令查询资源的压力统计信息：\n1 2 3 4 5 6 7 8 9 10 $ cat /proc/pressure/cpu some avg10=0.03 avg60=0.07 avg300=0.06 total=8723835 $ cat /proc/pressure/io some avg10=0.00 avg60=0.00 avg300=0.00 total=56385169 full avg10=0.00 avg60=0.00 avg300=0.00 total=54915860 $ cat /proc/pressure/memory some avg10=0.00 avg60=0.00 avg300=0.00 total=149158 full avg10=0.00 avg60=0.00 avg300=0.00 total=34054 内存和 IO 显示了两行指标：some 和 full，CPU 只有一行指标 some。关于 some 和 full 的定义下一节解释。\navg 给出了任务由于硬件资源不可用而被停顿的时间百分比。avg10、avg60和avg300分别是最近10秒、60秒和300秒的停顿时间百分比。\n例如上面 /proc/pressure/cpu 的输出，avg10=0.03 意思是任务因为CPU资源的不可用，在最近的10秒内，有0.03%的时间停顿等待 CPU。如果 avg 大于 40 ，也就是有 40% 时间在等待硬件资源，就说明这种资源的压力已经比较大了。\ntotal 是任务停顿的总时间，以微秒（microseconds）为单位。通过 total 可以检测出停顿持续太短而无法影响平均值的情况。\n# some 和 full 的定义 some 指标说明一个或多个任务由于等待资源而被停顿的时间百分比。在下图的例子中，在最近的60秒内，任务A的运行没有停顿，而由于内存紧张，任务B在运行过程中花了30秒等待内存，则 some 的值为50%。\nsome 表明了由于缺乏资源而造成至少一个任务的停顿。\nfull 指标表示所有的任务由于等待资源而被停顿的时间百分比。在下图的例子中，在最近的60秒内，任务 B 等待了 30 秒的内存，任务 A 等待了 10 秒内存，并且和任务 B 的等待时间重合。在这个重合的时间段10秒内，任务 A 和 任务 B 都在等待内存，结果是 some 指标为 50%，full 指标为 10/60 = 16.66%。\nfull 表明了总吞吐量的损失，在这种状态下，所有任务都在等待资源，CPU 周期将被浪费。\n请注意，some 和 full 的计算是用整个时间窗口内累计的等待时间，等待时间可以是连续的，也可能是离散的。\n理解了 some 和 full 的含义，就明白了 CPU 为什么没有 full 指标，因为不可能所有的任务都同时饿死在 CPU 上，CPU 总是在执行一个任务。\n# PSI 阈值监控 用户可以向 PSI 注册触发器，在资源压力超过自定义的阈值时获得通知。一个触发器定义了特定时间窗口内最大累积停顿时间，例如，在任何 500ms 的窗口内，累计 100ms 的停顿时间会产生一个通知事件。\n如何向 PSI 注册触发器呢？打开 /proc/pressure/ 目录下资源对应的 PSI 接口文件，写入想要的阈值和时间窗口，然后在打开的文件描述符上使用 select()、poll() 或 epoll() 方法等待通知事件。写入 PSI 接口文件的数据格式为：\n1 \u0026lt;some|full\u0026gt; \u0026lt;停顿阈值\u0026gt; \u0026lt;时间窗口\u0026gt; 阈值和时间窗口的单位都是微秒（us）。内核接受的窗口大小范围为500ms到10秒。\n举个例子，向 /proc/pressure/io 写入 \u0026ldquo;some 500000 1000000\u0026rdquo;，代表着在任何 1 秒的时间窗口内，如果一个或多个进程因为等待 IO 而造成的时间停顿超过了阈值 500ms，将触发通知事件。\n当用于定义触发器的 PSI 接口文件描述符被关闭时，触发器将被取消注册。\n我们通过一个例子演示触发器的使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 #include \u0026lt;errno.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;poll.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main() { const char trig[] = \u0026#34;some 500000 1000000\u0026#34;; struct pollfd fds; int n; fds.fd = open(\u0026#34;/proc/pressure/io\u0026#34;, O_RDWR | O_NONBLOCK); if (fds.fd \u0026lt; 0) { printf(\u0026#34;/proc/pressure/io open error: %s\\n\u0026#34;, strerror(errno)); return 1; } fds.events = POLLPRI; if (write(fds.fd, trig, strlen(trig) + 1) \u0026lt; 0) { printf(\u0026#34;/proc/pressure/io write error: %s\\n\u0026#34;, strerror(errno)); return 1; } printf(\u0026#34;waiting for events...\\n\u0026#34;); while (1) { n = poll(\u0026amp;fds, 1, -1); if (n \u0026lt; 0) { printf(\u0026#34;poll error: %s\\n\u0026#34;, strerror(errno)); return 1; } if (fds.revents \u0026amp; POLLERR) { printf(\u0026#34;got POLLERR, event source is gone\\n\u0026#34;); return 0; } if (fds.revents \u0026amp; POLLPRI) { printf(\u0026#34;event triggered!\\n\u0026#34;); } else { printf(\u0026#34;unknown event received: 0x%x\\n\u0026#34;, fds.revents); return 1; } } return 0; } 在服务器上编译并运行该程序，如果当前服务器比较空闲，我们会看到程序一直在等待 IO 压力超过阈值的通知：\n1 2 $ sudo ./monitor waiting for events... 我们为服务器制造点 IO 压力，生成一个5G大小的文件：\n1 $ dd if=/dev/zero of=/home/mazhen/testfile bs=4096 count=1310720 再回到示例程序的运行窗口，会发现已经收到事件触发的通知：\n1 2 3 4 5 6 7 8 $ sudo ./monitor waiting for events... event triggered! event triggered! event triggered! event triggered! event triggered! ... # PSI 应用案例 Facebook 是因为一些实际的需求开发了 PSI。其中一个案例是为了避免内核 OOM(Out-Of-Memory) killer 的触发。\n应用在申请内存的时候，如果没有足够的 free 内存，可以通过回收 Page Cache 释放内存，如果这时 free 内存还是不够，就会触发内核的 OOM Killer，挑选一个进程 kill 掉释放内存。这个过程是同步的，申请分配内存的进程一直被阻塞等待，而且内核选择 kill 掉哪个进程释放内存，用户不可控。因此，Facebook 开发了用户空间的 OOM Killer 工具 oomd。\noomd 使用 PSI 阈值作为触发器，在内存压力增加到一定程度时，执行指定的动作，避免最终 OOM 的发生。oomd 作为第一道防线，确保服务器工作负载的健康，并能自定义复杂的清除策略，这些都是内核做不到的。\ncgroup2 也支持 group 内任务的 PSI 指标追踪，这样就可以知道容器内 CPU、内存和 IO 的真实压力情况，进行更精细化的容器调度，在资源利用率最大化的同时保证任务的延迟和吞吐量。\n","date":"2021-08-01T11:51:05+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8psipressure-stall-information%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%B5%84%E6%BA%90/","title":"使用PSI（Pressure Stall Information）监控服务器资源"},{"content":"你可能听说过 TTY 和 PTY 这些缩写，也在 /dev 目录下看到过 /dev/tty[n] 设备，大概知道它们和 Linux 终端的概念有关。可是你清楚 TTY、PTY 具体指的是什么，它们有什么区别，以及它们和 shell 又是什么关系呢？为了理解这些，我们需要先回顾一下历史。\n# 回顾历史 在计算机诞生之前，人们发明了 Teleprinter(电传打字机)，通过长长的电线点对点连接，发送和接收打印的信息，用于远距离传输电报信息。\nTeleprinter 也可以写成 teletypewriter 或 teletype。\n后来人们将 Teleprinter 连接到早期的大型计算机上，作为输入和输出设备，将输入的数据发送到计算机，并打印出响应。\n在今天你很难想象程序的运行结果需要等到打印出来才能看到，Teleprinter 设备已经进了计算机博物馆。现在我们用 TTY 代表计算机终端（terminal），只是沿用了历史习惯，电传打字机（teletypewriter）曾经是计算机的终端，它的缩写便是 TTY(TeleTYpewriter)。\n为了把不同型号的电传打字机接入计算机，需要在操作系统内核安装驱动，为上层应用屏蔽所有的低层细节。\n电传打字机通过两根电缆连接：一根用于向计算机发送指令，一根用于接收计算机的输出。这两根电缆插入 UART （Universal Asynchronous Receiver and Transmitter，通用异步接收和发送器）的串行接口连接到计算机。\n操作系统包含一个 UART 驱动程序，管理字节的物理传输，包括奇偶校验和流量控制。然后输入的字符序列被传递给 TTY 驱动，该驱动包含一个 line discipline。\nline discipline 负责转换特殊字符（如退格、擦除字、清空行），并将收到的内容回传给电传打字机，以便用户可以看到输入的内容。line discipline 还负责对字符进行缓冲，当按下回车键时，缓冲的数据被传递给与 TTY 相关的前台用户进程。用户可以并行的执行几个进程，但每次只与一个进程交互，其他进程在后台工作。\n# 终端模拟器(terminal emulator) 今天电传打字机已经进了博物馆，但 Linux/Unix 仍然保留了当初 TTY驱动和 line discipline 的设计和功能。终端不再是一个需要通过 UART 连接到计算机上物理设备。终端成为内核的一个模块，它可以直接向 TTY 驱动发送字符，并从 TTY 驱动读取响应然后打印到屏幕上。也就是说，用内核模块模拟物理终端设备，因此被称为终端模拟器(terminal emulator)。\n上图是一个典型的Linux桌面系统。终端模拟器就像过去的物理终端一样，它监听来自键盘的事件将其发送到 TTY 驱动，并从 TTY 驱动读取响应，通过显卡驱动将结果渲染到显示器上。TTY驱动 和 line discipline的行为与原先一样，但不再有 UART 和 物理终端参与。\n如何看到一个终端模拟器呢？在 Ubuntu 20 桌面系统上，按 Ctrl+Alt+F3 就会得到一个由内核模拟的 TTY。Linux上这种模拟的文本终端也被称为虚拟终端（Virtual consoles）。每个虚拟终端都由一个特殊的设备文件 /dev/tty[n] 所表示，与这个虚拟终端的交互，是通过对这个设备文件的读写操作，以及使用ioctl系统调用操作这个设备文件进行的。通过执行 tty 命令可以查看代表当前虚拟终端的设备文件：\n1 2 $ tty /dev/tty3 可以看到，当前终端的设备文件是 /dev/tty3，也就是通过 Ctrl+Alt+F3 得到的虚拟终端。\n你可以通过 Ctrl+Alt+F3 到 Ctrl+Alt+F6 在几个虚拟终端之间切换。按 Ctrl+Alt+F2 回到桌面环境。X 系统也是运行在一个终端模拟器上，在 Ubuntu 20 上它对应的设备是 /dev/tty2，这也是为什么使用 Ctrl+Alt+F2 可以切换到 X 系统的原因。\n我们可以看看 X 系统打开的文件中是否包含了设备文件 /dev/tty2。先查找 X 系统的 PID：\n1 2 # ps aux | grep Xorg mazhen 1404 0.1 0.6 741884 49996 tty2 Sl+ 08:07 0:13 /usr/lib/xorg/Xorg vt2 -displayfd 3 -auth /run/user/1000/gdm/Xauthority -background none -noreset -keeptty -verbose 3 再看看这个进程(1404)打开了哪些文件：\n1 2 3 4 5 6 7 8 # ll /proc/1404/fd 总用量 0 dr-x------ 2 mazhen mazhen 0 7月 10 08:07 ./ dr-xr-xr-x 9 mazhen mazhen 0 7月 10 08:07 ../ lrwx------ 1 mazhen mazhen 64 7月 10 08:07 0 -\u0026gt; /dev/tty2 lrwx------ 1 mazhen mazhen 64 7月 10 08:07 1 -\u0026gt; \u0026#39;socket:[39965]\u0026#39; lrwx------ 1 mazhen mazhen 64 7月 10 10:09 10 -\u0026gt; \u0026#39;socket:[34615]\u0026#39; ... 可以看到，X 系统确实打开了 /dev/tty2。\n再做一个有趣的实验，在 tty3 下以 root 用户身份执行 echo 命令：\n1 # echo \u0026#34;hello from tty3\u0026#34; \u0026gt; /dev/tty4 再按 Ctrl+Alt+F4 切换到 tty4，能看到从 tty3 发送来的信息。\n# 伪终端（pseudo terminal, PTY） 终端模拟器(terminal emulator) 是运行在内核的模块，我们也可以让终端模拟程序运行在用户区。运行在用户区的终端模拟程序，就被称为伪终端（pseudo terminal, PTY）。\nPTY 运行在用户区，更加安全和灵活，同时仍然保留了 TTY 驱动和 line discipline 的功能。常用的伪终端有 xterm，gnome-terminal，以及远程终端 ssh。我们以 Ubuntu 桌面版提供的 gnome-terminal 为例，介绍伪终端如何与 TTY 驱动交互。\nPTY 是通过打开特殊的设备文件 /dev/ptmx 创建，由一对双向的字符设备构成，称为 PTY master 和 PTY slave。\ngnome-terminal 持有 PTY master 的文件描述符 /dev/ptmx。 gnome-terminal 负责监听键盘事件，通过PTY master接收或发送字符到 PTY slave，还会在屏幕上绘制来自PTY master的字符输出。\ngnome-terminal 会 fork 一个 shell 子进程，并让 shell 持有 PTY slave 的设备文件 /dev/pts/[n]，shell 通过 PTY slave 接收字符，并输出处理结果。\nPTY master 和 PTY slave 之间是 TTY 驱动，会在 master 和 slave 之间复制数据，并进行会话管理和提供 line discipline 功能。\n在 gnome-terminal 中执行 tty 命令，可以看到代表PTY slave的设备文件：\n1 2 $ tty /dev/pts/0 执行 ps -l 命令，也可以确认 shell 关联的伪终端是 pts/0：\n1 2 3 4 $ ps -l F S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD 0 S 1000 1842 1832 0 80 0 - 3423 do_wai pts/0 00:00:00 bash 0 R 1000 1897 1842 0 80 0 - 3626 - pts/0 00:00:00 ps 注意到 TTY 这一列指出了当前进程的终端是 pts/0。\n我们以实际的例子，看看在 terminal 执行一个命令的全过程。\n我们在桌面启动终端程序 gnome-terminal，它向操作系统请求一个PTY master，并把 GUI 绘制在显示器上 gnome-terminal 启动子进程 bash bash 的标准输入、标准输出和标准错误都设置为 PTY slave gnome-terminal 监听键盘事件，并将输入的字符发送到PTY master line discipline 收到字符，进行缓冲。只有当你按下回车键时，它才会把缓冲的字符复制到PTY slave。 line discipline 在接收到字符的同时，也会把字符写回给PTY master。gnome-terminal 只会在屏幕上显示来自 PTY master 的东西。因此，line discipline 需要回传字符，以便让你看到你刚刚输入的内容。 当你按下回车键时，TTY 驱动负责将缓冲的数据复制到PTY slave bash 从标准输入读取输入的字符（例如 ls -l ）。注意，bash 在启动时已经将标准输入被设置为了PTY slave bash 解释从输入读取的字符，发现需要运行 ls bash fork 出 ls 进程。bash fork 出的进程拥有和 bash 相同的标准输入、标准输出和标准错误，也就是PTY slave ls 运行，结果打印到标准输出，也就是PTY slave TTY 驱动将字符复制到PTY master gnome-terminal 循环从 PTY master 读取字节，绘制到用户界面上。 # Shell 我们经常不去区分 terminal 和 Shell，会说打开一个 terminal，或打开一个 Shell。从前面介绍的命令执行过程可以看出，Shell 不处理键盘事件，也不负责字符的显示，这是 terminal 要为它处理好的。\nShell是用户空间的应用程序，通常由 terminal fork出来，是 terminal 的子进程。Shell用来提示用户输入，解释用户输入的字符，然后处理来自底层操作系统的输出。\n通常我们使用较多的 shell 有 Bash、Zsh 和 sh。\n# 配置 TTY 设备 内核将使用 TTY 驱动来处理 terminal 和 Shell 之间的通信。line discipline 是 TTY 驱动的一个逻辑组件。line discipline 主要有以下功能：\n当用户输入时，字符会被回传到PTY master line discipline 会在内存中缓冲这些字符。当用户按回车键时，它才将这些字符发送到PTY slave line discipline 可以拦截处理一些特殊的功能键，例如： 当用户按 CTRL+c 时，它向连接到 PTY slave 的进程发送 kill -2（SIGINT） 信号 当用户按 CTRL+w 时，它删除用户输入的最后一个字 当用户按 CTRL+z 时，它向连接到 PTY slave 的进程发送 kill -STOP信号 当用户按退格键时，它从缓冲区中删除该字符，并向PTY master发送删除最后一个字符的指令 我们可以使用命令行工具 stty 查询和配置 TTY，包括 line discipline 规则。在 terminal 执行 stty -a 命令：\n1 2 3 4 5 6 7 8 9 10 11 $ stty -a speed 38400 baud; rows 40; columns 80; line = 0; intr = ^C; quit = ^\\; erase = ^?; kill = ^U; eof = ^D; eol = \u0026lt;undef\u0026gt;; eol2 = \u0026lt;undef\u0026gt;; swtch = \u0026lt;undef\u0026gt;; start = ^Q; stop = ^S; susp = ^Z; rprnt = ^R; werase = ^W; lnext = ^V; discard = ^O; min = 1; time = 0; -parenb -parodd -cmspar cs8 -hupcl -cstopb cread -clocal -crtscts -ignbrk -brkint -ignpar -parmrk -inpck -istrip -inlcr -igncr icrnl ixon -ixoff -iuclc -ixany -imaxbel iutf8 opost -olcuc -ocrnl onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0 isig icanon iexten echo echoe echok -echonl -noflsh -xcase -tostop -echoprt echoctl echoke -flusho -extproc -a 标志告诉 stty 返回所有的设置，包括TTY 的特征和 line discipline 规则。\n让我们看一下第一行：\nspeed 表示波特率。当 terminal 和计算机通过物理线路连接时，speed 后的数字表示物理线路的波特率。波特率对 PTY 来说是没有意义。 rows, columns 表示 terminal 的行数和列数，以字符为单位。 line 表示line discipline 的类型。0 是 N_TTY。 stty 能够对 terminal 进行设置，让我们做个简单的测试验证一下。在第一个 terminal 中使用 vi 编辑一个文件。vi 在启动时会查询当前 terminal 的大小，以便 vi 能填满整个窗口。这时候我们在另一个 terminal 中输入：\n1 # stty -F /dev/pts/0 rows 20 这个命令将终端 pts/0 的行数设置为原来的一半，这将更新内核中 TTY 的数据结构，并向 vi 发送一个 SIGWINCH 信号，vi 接收到该信号后将根据 TTY 新的行列数重新绘制自己，这时 vi 就只使用了可用窗口区域的上半部分。\nstty -a 输出的第二行给出了 line discipline 能处理的所有特殊字符，包含了键的绑定。例如 intr = ^C 是指将 CTRL+c 映射到 kill -2 (SIGINT) 信号。你也可以更改这个绑定，例如执行 stty intr o 命令，将发送 SIGINT 信号的键从 CTRL+c 换成了字符 o。\n最后，stty -a 列出了一系列 line discipline 规则的开关。- 表示开关是关闭的，否则开关就是打开的。所有的开关在 man stty中都有解释。我举其中一个简单的例子，echo 是指示 line discipline 将字符回传的规则，我们可以执行命令关闭 echo 规则：\n1 $ stty -echo 这时候你再输入一些东西，屏幕上什么也不会出现。line discipline 不会将字符回传给 PTY master，因此 terminal 不会再显示我们输入的内容。然而其他一切都照常进行。例如你输入 ls，在输入时看不到字符 ls，然后你输入回车后，仍然会看到 ls 的输出。执行命令恢复 echo 规则：\n1 $ stty echo 可以通过 stty raw 命令来禁用所有的 line discipline 规则，这样的终端被称为 raw terminal。像 vi 这样的编辑器会将终端设置为 raw ，因为它需要自己处理字符。后面介绍的远程终端也是需要一个 raw terminal，同样会禁用所有的 line discipline 规则。\n# 远程终端 我们经常通过 ssh 连接到一个远程主机，这时候远程主机上的 ssh server 就是一个伪终端 PTY，它同样持有 PTY master，但 ssh server 不再监听键盘事件，以及在屏幕上绘制输出结果，而是通过 TCP 连接，向 ssh client 发送或接收字符。\n我们简单梳理一下远程终端是如何执行命令的。\n用户在客户端的 terminal 中输入 ssh 命令，经过 PTY master、TTY 驱动，到达 PTY slave。bash 的标准输入已经设置为了 PTY slave，它从标准输入读取字符序列并解释执行，发现需要启动 ssh 客户端，并请求和远程服务器建 TCP 连接。\n服务器端接收客户端的 TCP 连接请求，向内核申请创建 PTY，获得一对设备文件描述符。让 ssh server 持有 PTY master，ssh server fork 出的子进程 bash 持有 PTY slave。bash 的标准输入、标准输出和标准错误都设置为了PTY slave。\n当用户在客户端的 terminal 中输入命令 ls -l 和回车键，这些字符经过 PTY master 到达 TTY 驱动。我们需要禁用客户端 line discipline 的所有规则，也就是说客户端的 line discipline 不会对特殊字符回车键做处理，而是让命令 ls -l 和回车键一起到达 PTY slave。ssh client 从 PTY slave 读取字符序列，通过网络，发送给 ssh server。\nssh server 将从 TCP 连接上接收到的字节写入PTY master。TTY 驱动对字节进行缓冲，直到收到特殊字符回车键。\n由于服务器端的 line discipline 没有禁用 echo 规则，所以 TTY 驱动还会将收到的字符写回PTY master，ssh server 从 PTY master 读取字符，将这些字符通过 TCP 连接发回客户端。注意，这是发回的字符不是 ls -l 命令的执行结果，而是 ls -l 本身的回显，让客户端能看到自己的输入。\n在服务器端 TTY 驱动将字符序列传送给 PTY slave，bash 从 PTY slave读取字符，解释并执行命令 ls -l。bash fork 出 ls 子进程，该子进程的标准输入、标准输出和标准错误同样设置为了 PTY slave。ls -l 命令的执行结果写入标准输出 PTY slave，然后执行结果通过 TTY 驱动到达 PTY master，再由 ssh server 通过 TCP 连接发送给 ssh client。\n注意在客户端，我们在屏幕上看到的所有字符都来自于远程服务器。包括我们输入的内容，也是远程服务器上的 line discipline 应用 echo 规则的结果，将这些字符回显了回来。表面看似简单的在远程终端上执行了一条命令，实际上底下确是波涛汹涌。\n# 写在最后 简单回顾总结一下本文的主要内容：\n电传打字机（TTY）是物理设备，最初是为电报设计的，后来被连接到计算机上，发送输入和获取输出。 电传打字机（TTY）现在被运行在内核中的模块所模拟，被称为终端模拟器(terminal emulator)。 伪终端（pseudo terminal, PTY） 是运行在用户区的终端模拟程序。 Shell 由 terminal fork 出来，是 terminal 的子进程。Shell 不处理键盘事件，也不负责字符的显示，这些是由 terminal 处理。Shell 负责解释执行用户输入的字符。 可以使用 stty 命令对 TTY 设备进行配置。 远程终端 ssh 也是一种伪终端 PTY。 相信通过这篇文章，你已经能够理解终端、终端模拟器和伪终端的区别和联系。如果想进一步探究低层实现，可以阅读 TTY 驱动的源码 drivers/tty/tty_io.c和 line discipline 的源码 drivers/tty/n_tty.c。\n","date":"2021-07-12T11:46:39+08:00","permalink":"https://mazhen.tech/p/%E7%90%86%E8%A7%A3linux-%E7%BB%88%E7%AB%AF%E7%BB%88%E7%AB%AF%E6%A8%A1%E6%8B%9F%E5%99%A8%E5%92%8C%E4%BC%AA%E7%BB%88%E7%AB%AF/","title":"理解Linux 终端、终端模拟器和伪终端"},{"content":"操作系统内核对应用开发工程师来说就像一个黑盒，似乎很难窥探到其内部的运行机制。其实Linux内核很早就内置了一个强大的tracing工具：Ftrace，它几乎可以跟踪内核的所有函数，不仅可以用于调试和分析，还可以用于观察学习Linux内核的内部运行。虽然Ftrace在2008年就加入了内核，但很多应用开发工程师仍然不知道它的存在。本文就给你介绍一下Ftrace的基本使用。\n# Ftrace初体验 先用一个例子体验一下Ftrace的使用简单，且功能强大。使用 root 用户进入/sys/kernel/debug/tracing目录，执行 echo 和 cat 命令：\n# echo _do_fork \u003e set_graph_function # echo function_graph \u003e current_tracer # cat trace | head -20 # tracer: function_graph # # CPU DURATION FUNCTION CALLS # | | | | | | | 3) | _do_fork() { 3) | copy_process() { 3) 0.895 us | _raw_spin_lock_irq(); 3) | recalc_sigpending() { 3) 0.740 us | recalc_sigpending_tsk(); 3) 2.248 us | } 3) | dup_task_struct() { 3) 0.775 us | tsk_fork_get_node(); 3) | kmem_cache_alloc_node() { 3) | _cond_resched() { 3) 0.740 us | rcu_all_qs(); 3) 2.117 us | } 3) 0.701 us | should_failslab(); 3) 2.023 us | memcg_kmem_get_cache(); 3) 0.889 us | memcg_kmem_put_cache(); 3) + 12.206 us | } 我们使用Ftrace的function_graph功能显示了内核函数 _do_fork() 所有子函数调用。左边的第一列是执行函数的 CPU，第二列 DURATION 显示在相应函数中花费的时间。我们注意到最后一行的耗时之前有个 + 号，提示用户注意延迟高的函数。+ 代表耗时大于 10 μs。如果耗时大于 100 μs，则显示 ! 号。\n我们知道，fork 是建立父进程的一个完整副本，然后作为子进程执行。那么_do_fork()的第一件大事就是调用 copy_process() 复制父进程的数据结构，从上面输出的调用链信息也验证了这一点。\n使用完后执行下面的命令关闭function_graph：\n1 2 # echo nop \u0026gt; current_tracer # echo \u0026gt; set_graph_function 使用 Ftrace 的 function_graph 功能，可以查看内核函数的子函数调用链，帮助我们理解复杂的代码流程，而这只是 Ftrace 的功能之一。这么强大的功能，我们不必安装额外的用户空间工具，只要使用 echo 和 cat 命令访问特定的文件就能实现。Ftrace 对用户的使用接口正是tracefs文件系统。\n# tracefs 文件系统 用户通过tracefs文件系统使用Ftrace，这很符合一切皆文件的Linux哲学。tracefs文件系统一般挂载在/sys/kernel/tracing目录。由于Ftrace最初是debugfs文件系统的一部分，后来才被拆分为自己的tracefs。所以如果系统已经挂载了debugfs，那么仍然会保留原来的目录结构，将tracefs挂载到debugfs的子目录下。我们可以使用 mount 命令查看当前系统debugfs和tracefs挂载点：\n1 2 3 4 # mount -t debugfs,tracefs debugfs on /sys/kernel/debug type debugfs (rw,nosuid,nodev,noexec,relatime) tracefs on /sys/kernel/tracing type tracefs (rw,nosuid,nodev,noexec,relatime) tracefs on /sys/kernel/debug/tracing type tracefs (rw,nosuid,nodev,noexec,relatime) 我使用的系统是Ubuntu 20.04.2 LTS，可以看到，为了保持兼容，tracefs同时挂载到了/sys/kernel/tracing和/sys/kernel/debug/tracing。\ntracefs下的文件主要分两类：控制文件和输出文件。这些文件的名字都很直观，像前面例子通过 current_tracer 设置当前要使用的 tracer，然后从 trace中读取结果。还有像 available_tracers 包含了当前内核可用的 tracer，可以设置 trace_options 自定义输出。\n# ls -F /sys/kernel/tracing/ available_events max_graph_depth stack_max_size available_filter_functions options/ stack_trace available_tracers per_cpu/ stack_trace_filter buffer_percent printk_formats synthetic_events buffer_size_kb README timestamp_mode buffer_total_size_kb saved_cmdlines trace current_tracer saved_cmdlines_size trace_clock dynamic_events saved_tgids trace_marker dyn_ftrace_total_info set_event trace_marker_raw enabled_functions set_event_notrace_pid trace_options error_log set_event_pid trace_pipe events/ set_ftrace_filter trace_stat/ free_buffer set_ftrace_notrace tracing_cpumask function_profile_enabled set_ftrace_notrace_pid tracing_max_latency hwlat_detector/ set_ftrace_pid tracing_on instances/ set_graph_function tracing_thresh kprobe_events set_graph_notrace uprobe_events kprobe_profile snapshot uprobe_profile 本文后面的示例假定你已经处在了/sys/kernel/tracing或/sys/kernel/debug/tracing目录下。\n# 函数跟踪 Ftrace 实际上代表的就是function trace（函数跟踪），因此函数追踪是Ftrace最初的一个主要功能。\nFtrace 可以跟踪几乎所有内核函数调用的详细信息，这是怎么做到的呢？简单来说，在编译内核的时候使用了 gcc 的 -pg 选项，编译器会在每个内核函数的入口处调用一个特殊的汇编函数“mcount” 或 “__fentry__”，如果跟踪功能被打开，mcount/fentry 会调用当前设置的 tracer，tracer将不同的数据写入ring buffer。\n从上图可以看出，Ftrace 提供的 function hooks 机制在内核函数入口处埋点，根据配置调用特定的 tracer， tracer将数据写入ring buffer。Ftrace实现了一个无锁的ring buffer，所有的跟踪信息都存储在ring buffer中。用户通过 tracefs 文件系统接口访问函数跟踪的输出结果。\n你可能已经意识到，如果每个内核函数入口都加入跟踪代码，必然会非常影响内核的性能，幸好Ftrace支持动态跟踪功能。如果启用了CONFIG_DYNAMIC_FTRACE选项，编译内核时所有的mcount/fentry调用点都会被收集记录。在内核的初始化启动过程中，会根据编译期记录的列表，将mcount/fentry调用点替换为NOP指令。NOP就是 no-operation，不做任何事，直接转到下一条指令。因此在没有开启跟踪功能的情况下，Ftrace不会对内核性能产生任何影响。在开启追踪功能时，Ftrace才会将NOP指令替换为mcount/fentry。\n启用函数追踪功能，只需要将 current_tracer 文件的内容设置为 \u0026ldquo;function\u0026rdquo;：\n# echo function \u003e current_tracer # cat trace | head -20 # tracer: function # # entries-in-buffer/entries-written: 204981/2728851 #P:4 # # _-----=\u003e irqs-off # / _----=\u003e need-resched # | / _---=\u003e hardirq/softirq # || / _--=\u003e preempt-depth # ||| / delay # TASK-PID CPU# |||| TIMESTAMP FUNCTION # | | | |||| | | sshd-1388 [000] .... 44388.890787: _cond_resched \u003c-__flush_work curl-7231 [001] .... 44389.399226: PageHuge \u003c-find_get_entry curl-7231 [001] .... 44389.399227: fsnotify_parent \u003c-vfs_read curl-7231 [001] .... 44389.399227: _cond_resched \u003c-copy_page_to_iter curl-7231 [001] .... 44389.399227: rcu_all_qs \u003c-_cond_resched curl-7231 [001] .... 44389.399228: vmacache_find \u003c-find_vma curl-7231 [001] .... 44389.399228: atime_needs_update \u003c-touch_atime curl-7231 [001] .... 44389.399228: current_time \u003c-atime_needs_update # echo nop \u003e current_tracer 文件头已经很好的解释了每一列的含义。前两项是被追踪的任务名称和 PID，大括号内是执行跟踪的CPU。TIMESTAMP 是启动后的时间，后面是被追踪的函数，它的调用者在 \u0026lt;- 之后。\n我们可以设置 set_ftrace_filter 选择想要跟踪的函数：\n# echo '*sleep' \u003e set_ftrace_filter # echo function \u003e current_tracer # cat trace_pipe sleep-9445 [001] .... 45978.125872: common_nsleep \u003c-__x64_sys_clock_nanosleep sleep-9445 [001] .... 45978.125873: hrtimer_nanosleep \u003c-common_nsleep sleep-9445 [001] .... 45978.125873: do_nanosleep \u003c-hrtimer_nanosleep cron-568 [002] .... 45978.504262: __x64_sys_clock_nanosleep \u003c-do_syscall_64 cron-568 [002] .... 45978.504264: common_nsleep \u003c-__x64_sys_clock_nanosleep cron-568 [002] .... 45978.504264: hrtimer_nanosleep \u003c-common_nsleep cron-568 [002] .... 45978.504264: do_nanosleep \u003c-hrtimer_nanosleep sleep-9448 [001] .... 45978.885085: __x64_sys_clock_nanosleep \u003c-do_syscall_64 sleep-9448 [001] .... 45978.885087: common_nsleep \u003c-__x64_sys_clock_nanosleep sleep-9448 [001] .... 45978.885087: hrtimer_nanosleep \u003c-common_nsleep # echo nop \u003e current_tracer # echo \u003e set_ftrace_filter trace_pipe 包含了与 trace 相同的输出，从这个文件的读取会返回一个无尽的事件流，它也会消耗事件，所以在读取一次后，它们就不再在跟踪缓冲区中了。\n也许你只想跟踪一个特定的进程，可以通过设置 set_ftrace_pid 内容为PID指定想追踪的特定进程。让 tracer 只追踪PID列在这个文件中的线程：\n# echo [PID] \u003e set_ftrace_pid # echo function \u003e current_tracer 如果设置了 function-fork 选项，那么当一个 PID 被列在 set_ftrace_pid 这个文件中时，其子任务的 PID 将被自动添加到这个文件中，并且子任务也将被 tracer 追踪。\n# echo function-fork \u003e trace_options 取消function-fork 选项：\n# echo nofunction-fork \u003e trace_options # cat trace_options ... noevent-fork nopause-on-trace function-trace nofunction-fork nodisplay-graph nostacktrace ... 取消 set_ftrace_pid 的设置：\n# echo \u003e set_ftrace_pid # Ftrace function_graph 文章开始例子已经展示过，function_graph 可以打印出函数的调用图，揭示代码的流程。function_graph 不仅跟踪函数的输入，而且跟踪函数的返回，这使得 tracer 能够知道被调用的函数的深度。function_graph 可以让人更容易跟踪内核的执行流程。\n我们再看一个例子：\n# echo try_to_wake_up \u003e set_graph_function # echo function_graph \u003e current_tracer # cat trace | head -20 # tracer: function_graph # # CPU DURATION FUNCTION CALLS # | | | | | | | 0) | try_to_wake_up() { 0) 1.083 us | ttwu_queue_wakelist(); 0) 0.622 us | update_rq_clock(); 0) | ttwu_do_activate() { 0) | enqueue_task_fair() { 0) | enqueue_entity() { 0) 0.616 us | update_curr(); 0) 0.602 us | update_cfs_group(); 0) 0.662 us | account_entity_enqueue(); 0) 0.652 us | place_entity(); 0) 0.697 us | __enqueue_entity(); 0) + 12.890 us | } 0) 0.672 us | hrtick_update(); 0) + 17.781 us | } 0) | ttwu_do_wakeup() { 0) | check_preempt_curr() { # echo nop \u003e current_tracer # echo \u003e set_graph_function 前面提到过，函数耗时大于 10 μs，前面会有 + 号提醒用户注意，其他的符号还有：\n$ ：延迟大于1秒 @ ：延迟大于 100 ms * ：延迟大于 10 ms # ：延迟大于 1 ms ! ：延迟大于 100 μs + ：延迟大于 10 μs # 函数Profiler 函数Profiler提供了内核函数调用的统计数据，可以观察哪些内核函数正在被使用，并能发现哪些函数的执行耗时最长。\n# echo nop \u003e current_tracer # echo 1 \u003e function_profile_enabled # echo 0 \u003e function_profile_enabled # echo \u003e set_ftrace_filter 这里有一个要注意的地方，确保使用的是 0 \u0026gt;，而不是 0\u0026gt;。这两者的含义不一样，0\u0026gt;是对文件描述符 0 的重定向。同样要避免使用 1\u0026gt;，因为这是对文件描述符 1 的重定向。\n现在可以从 trace_stat 目录中读取 profile 的统计数据。在这个目录中，profile 数据按照 CPU 保存在名为 function[n] 文件中。我使用的4核CPU，看一下profile 结果：\n# ls trace_stat/ function0 function1 function2 function3 # head trace_stat/function* ==\u003e trace_stat/function0 \u003c== Function Hit Time Avg s^2 -------- --- ---- --- --- tcp_sendmsg 202 3791.163 us 18.768 us 659.733 us tcp_sendmsg_locked 202 3521.863 us 17.434 us 638.307 us tcp_recvmsg 125 2238.773 us 17.910 us 1062.699 us tcp_push 202 2168.569 us 10.735 us 467.879 us tcp_write_xmit 47 2107.768 us 44.846 us 414.934 us tcp_v4_do_rcv 49 871.318 us 17.782 us 126.562 us tcp_send_ack 50 849.091 us 16.981 us 164.986 us tcp_rcv_established 49 827.212 us 16.881 us 117.427 us ==\u003e trace_stat/function1 \u003c== Function Hit Time Avg s^2 -------- --- ---- --- --- tcp_recvmsg 312 3110.497 us 9.969 us 281.015 us tcp_sendmsg 86 1412.005 us 16.418 us 370.310 us tcp_sendmsg_locked 86 1313.847 us 15.277 us 362.495 us tcp_send_ack 47 863.222 us 18.366 us 121.567 us tcp_v4_do_rcv 60 825.359 us 13.755 us 102.550 us tcp_write_xmit 28 807.609 us 28.843 us 336.106 us tcp_push 86 805.776 us 9.369 us 299.815 us tcp_rcv_established 60 777.510 us 12.958 us 99.129 us ==\u003e trace_stat/function2 \u003c== Function Hit Time Avg s^2 -------- --- ---- --- --- tcp_v4_rcv 1618 27858.95 us 17.218 us 253.487 us tcp_v4_do_rcv 1216 22528.58 us 18.526 us 226.243 us tcp_rcv_established 1184 20535.08 us 17.343 us 210.765 us tcp_send_ack 487 7558.698 us 15.520 us 111.035 us tcp_write_xmit 328 6281.810 us 19.151 us 656.192 us tcp_tasklet_func 162 4258.312 us 26.285 us 797.278 us tcp_ack 575 4148.714 us 7.215 us 27.061 us tcp_tsq_handler 162 4123.507 us 25.453 us 791.961 us ==\u003e trace_stat/function3 \u003c== Function Hit Time Avg s^2 -------- --- ---- --- --- tcp_recvmsg 567 5773.997 us 10.183 us 397.950 us tcp_send_ack 127 1881.700 us 14.816 us 133.317 us tcp_v4_do_rcv 133 1783.527 us 13.409 us 86.122 us tcp_rcv_established 133 1690.142 us 12.707 us 83.527 us tcp_sendmsg 54 1652.290 us 30.597 us 698.120 us tcp_sendmsg_locked 54 1574.276 us 29.153 us 666.451 us tcp_write_xmit 40 1184.827 us 29.620 us 354.719 us tcp_push 54 1129.465 us 20.916 us 486.157 us 第一行是每一列的名称，分别是函数名称（Function），调用次数（Hit），函数的总时间（Time）、平均函数时间（Avg）和标准差（s^2）。输出结果显示，tcp_sendmsg() 在3个 CPU 上都是最频繁的，tcp_v4_rcv() 在 CPU2 上被调用了1618次，平均延迟为 17.218 us。\n最后要注意一点，在使用 Ftrace Profiler 时，尽量通过 set_ftrace_filter 限制 profile 的范围，避免对所有的内核函数都进行 profile。\n# 追踪点 Tracepoints Tracepoints是内核的静态埋点。内核维护者在他认为重要的位置放置静态 tracepoints 记录上下文信息，方便后续排查问题。例如系统调用的开始和结束，中断被触发，网络数据包发送等等。\n在Linux的早期，内核维护者就一直想在内核中加入静态 tracepoints，尝试过各种策略。Ftrace 创造了Event Tracing 基础设施，让开发者使用 TRACE_EVENT() 宏添加内核 tracepoints，不用创建自定义内核模块，使用 Event Tracing 基础设施来注册埋点函数。\n现在内核中的Tracepoints都使用了 TRACE_EVENT() 宏来定义，tracepoints 记录的上下文信息作为 Trace events 进入 Event Tracing 基础设施，这样我们就可以复用 Ftrace 的 tracefs ，通过文件接口来配置 tracepoint events，并使用 trace 或 trace_pipe 文件查看事件输出。\n所有的 tracepoint events 的控制文件都在 events 目录下，按照类别以子目录形式组织：\n# ls -F events/ alarmtimer/ ftrace/ iwlwifi/ oom/ smbus/ block/ gpio/ iwlwifi_data/ page_isolation/ sock/ bpf_test_run/ gvt/ iwlwifi_io/ pagemap/ spi/ bridge/ hda/ iwlwifi_msg/ page_pool/ swiotlb/ btrfs/ hda_controller/ iwlwifi_ucode/ percpu/ sync_trace/ cfg80211/ hda_intel/ jbd2/ power/ syscalls/ cgroup/ header_event kmem/ printk/ task/ clk/ header_page kvm/ pwm/ tcp/ ... 我们以 events/sched/sched_process_fork 事件为例，该事件是在 include/trace/events/sched.h 中由 TRACE_EVENT 宏所定义：\n1 2 3 4 5 6 7 8 9 10 /* * Tracepoint for do_fork: */ TRACE_EVENT(sched_process_fork, TP_PROTO(struct task_struct *parent, struct task_struct *child), TP_ARGS(parent, child), ... ); TRACE_EVENT 宏会根据事件名称 sched_process_fork 生成 tracepoint 方法 trace_sched_process_fork()。你会在 kernel/fork.c 的 _do_fork() 中看到调用这个 tracepoint 方法。_do_fork() 是进程 fork 的主流程，在这里放置 tracepoint 是一个合适的位置，trace_sched_process_fork(current, p) 记录当前进程和 fork 出的子进程信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 /* * Ok, this is the main fork-routine. * * It copies the process, and if successful kick-starts * it and waits for it to finish using the VM if required. * * args-\u0026gt;exit_signal is expected to be checked for sanity by the caller. */ long _do_fork(struct kernel_clone_args *args) { ... p = copy_process(NULL, trace, NUMA_NO_NODE, args); add_latent_entropy(); /* * Do this prior waking up the new thread - the thread pointer * might get invalid after that point, if the thread exits quickly. */ trace_sched_process_fork(current, p); pid = get_task_pid(p, PIDTYPE_PID); ... } 在 events/sched/sched_process_fork 目录下，有这个事件的控制文件：\n# ls events/sched/sched_process_fork enable filter format hist id inject trigger 我们演示如何通过 enable 文件开启和关闭这个 tracepoint 事件：\n# echo 1 \u003e events/sched/sched_process_fork/enable # cat trace_pipe bash-14414 [000] .... 109721.823843: sched_process_fork: comm=bash pid=14414 child_comm=bash child_pid=24001 bash-14468 [002] .... 109730.405810: sched_process_fork: comm=bash pid=14468 child_comm=bash child_pid=24002 bash-14468 [002] .... 109737.925336: sched_process_fork: comm=bash pid=14468 child_comm=bash child_pid=24003 test.sh-24003 [000] .... 109737.968891: sched_process_fork: comm=test.sh pid=24003 child_comm=test.sh child_pid=24004 curl-24004 [002] .... 109737.975038: sched_process_fork: comm=curl pid=24004 child_comm=curl child_pid=24005 ... # echo 0 \u003e events/sched/sched_process_fork/enable 前五列分别是进程名称，PID，CPU ID，irqs-off 等标志位，timestamp 和 tracepoint 事件名称。其余部分是 tracepoint 格式字符串，包含当前这个 tracepoint 记录的重要信息。格式字符串可以在 events/sched/sched_process_fork/format 文件中查看：\n# cat events/sched/sched_process_fork/format name: sched_process_fork ID: 315 format: field:unsigned short common_type;\toffset:0;\tsize:2;\tsigned:0; field:unsigned char common_flags;\toffset:2;\tsize:1;\tsigned:0; field:unsigned char common_preempt_count;\toffset:3;\tsize:1;\tsigned:0; field:int common_pid;\toffset:4;\tsize:4;\tsigned:1; field:char parent_comm[16];\toffset:8;\tsize:16;\tsigned:1; field:pid_t parent_pid;\toffset:24;\tsize:4;\tsigned:1; field:char child_comm[16];\toffset:28;\tsize:16;\tsigned:1; field:pid_t child_pid;\toffset:44;\tsize:4;\tsigned:1; print fmt: \"comm=%s pid=%d child_comm=%s child_pid=%d\", REC-\u003eparent_comm, REC-\u003eparent_pid, REC-\u003echild_comm, REC-\u003echild_pid 通过这个 format 文件，我们可以了解这个 tracepoint 事件每个字段的含义。\n我们再演示一个使用 trigger 控制文件的例子：\n# echo 'hist:key=parent_pid' \u003e events/sched/sched_process_fork/trigger # [do some working] # cat events/sched/sched_process_fork/hist # event histogram # # trigger info: hist:keys=parent_pid:vals=hitcount:sort=hitcount:size=2048 [active] # { parent_pid: 572 } hitcount: 1 { parent_pid: 24494 } hitcount: 1 { parent_pid: 24497 } hitcount: 1 { parent_pid: 14414 } hitcount: 1 { parent_pid: 24505 } hitcount: 1 { parent_pid: 14053 } hitcount: 1 { parent_pid: 24527 } hitcount: 1 { parent_pid: 24501 } hitcount: 1 { parent_pid: 24510 } hitcount: 2 { parent_pid: 24508 } hitcount: 3 { parent_pid: 24493 } hitcount: 24 Totals: Hits: 37 Entries: 11 Dropped: 0 # remove triger # echo '!hist:key=parent_pid' \u003e events/sched/sched_process_fork/trigger 这个例子使用了 hist triggers，通过 sched_process_fork 事件来统计 _do_fork 的次数，并按照进程ID生成直方图。输出显示了 PID 24493 在追踪期间 fork 了24个子进程，最后几行显示了统计数据。\n关于 Hist Triggers 的详细介绍可以参考文档 Event Histograms。\n我的系统内核版本是 5.8.0-59-generic，当前可用的 tracepoints events 有2547个：\n# cat available_events btrfs:btrfs_transaction_commit btrfs:btrfs_inode_new btrfs:btrfs_inode_request btrfs:btrfs_inode_evict btrfs:btrfs_get_extent btrfs:btrfs_handle_em_exist btrfs:btrfs_get_extent_show_fi_regular btrfs:btrfs_truncate_show_fi_regular btrfs:btrfs_get_extent_show_fi_inline ... # cat available_events | wc -l 2547 Event Tracing 基础设施应该是 Ftrace 的另一大贡献，它提供的 TRACE_EVENT 宏统一了内核 tracepoint 的实现方式，为 tracepoint events 提供了基础支持。perf 的 tracepoint events 也是基于 Ftrace 实现的。\n# 利用 Tracepoints 理解内核代码 由于 tracepoints 是内核维护者在流程重要位置设置的埋点，因此我们可以从 tracepoints 入手来学习内核代码。所有的 tracepoints 都定义在 include/trace/events/ 目录下的头文件中，例如进程调度相关的 tracepoints 定义在 include/trace/events/sched.h中，我们以 sched_switch 为例：\n1 2 3 4 5 6 7 8 9 10 /* * Tracepoint for task switches, performed by the scheduler: */ TRACE_EVENT(sched_switch, TP_PROTO(bool preempt, struct task_struct *prev, struct task_struct *next), TP_ARGS(preempt, prev, next), TRACE_EVENT 宏会根据事件名称 sched_switch 生成 tracepoint 方法 trace_sched_switch()，在源码中查找该方法，发现在 kernel/sched/core.c 的 __schedule()中调用了trace_sched_switch() ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /* * __schedule() is the main scheduler function. *... */ static void __sched notrace __schedule(bool preempt) { ... if (likely(prev != next)) { rq-\u0026gt;nr_switches++; ... trace_sched_switch(preempt, prev, next); ... else { ... } balance_callback(rq); } 这样我们就找到了 scheduler 的主流程，可以从这里开始阅读进程调度的源码。\n# 写在最后 Ftrace 就包含在内核源码中 kernel/trace，理解了 Ftrace 内核不再是黑箱，你会有豁然开朗的感觉，内核源码忽然有条理了起来。让我们从 Ftrace 开始内核探索之旅吧。\n","date":"2021-06-21T11:44:02+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8Eftrace%E5%BC%80%E5%A7%8B%E5%86%85%E6%A0%B8%E6%8E%A2%E7%B4%A2%E4%B9%8B%E6%97%85/","title":"从Ftrace开始内核探索之旅"},{"content":"GDB（GNU Debugger）是Linux上的调试程序，可用于C/C++、Go、Rust等多种语言。GDB可以让你在被调试程序执行时看到它的”内部“情况，观察程序在特定断点上的状态，并逐行运行代码。\nGDB还提供了“远程”模式，使用GDB协议通过网络或串行设备与被调试程序进行通信。程序需要链接GDB提供的stub，这个stub实现了GDB协议。或者可以使用GDBserver，这时程序不需要进行任何更改。\n类似的，Linux内核开发者可以使用GDB的远程模式，与调试应用程序几乎相同的方式来调试Linux内核。KGDB是Linux内核的源代码级调试器，你可以使用GDB作为KGDB的前端，在我们熟悉且功能强大的GDB调试界面中调试内核。\n使用KGDB需要两台机器，一台作为开发机，另一台是目标机器，要调试的内核在目标机器上运行。在开发机上使用gdb运行包含符号信息的vmlinux，然后通过指定网络地址和端口，连接到目标机器的KGDB。我们也可以使用QEMU/KVM虚拟机作为目标机器，让待调试的内核运行在虚拟机中，然后在宿主机上运行gdb，连接到虚拟机中的KGDB。\n本文将介绍如何在本机搭建Linux内核调试环境，步骤比较繁琐，还会涉及到编译内核。作为内核小白，我会尽量写的详细些，毕竟我折腾了很久才成功。\n# 本机环境 我使用的Ubuntu 20.04.2 LTS，gdb版本为9.2。\n# 安装QEMU/KVM和Virsh Virsh是Virtual Shell的缩写，是一个用于管理虚拟机的命令行工具。你可以使用Virsh创建、编辑、启动、停止、关闭和删除VM。Virsh目前支持KVM，LXC，Xen，QEMU，OpenVZ，VirtualBox和VMware ESX。这里我们使用Virsh管理QEMU/KVM虚拟机。\n在安装之前，首先要确认你的CPU是否支持虚拟化技术。使用grep查看cpuinfo是否有\u0026quot;vmx\u0026quot;(Intel-VT 技术)或\u0026quot;svm\u0026quot;(AMD-V 支持)输出：\n1 egrep \u0026#34;(svm|vmx)\u0026#34; /proc/cpuinfo 某些CPU型号在默认情况下，BIOS中可能禁用了VT支持。我们需要再检查BIOS设置是否启用了VT的支持。使用kvm-ok命令进行检查：\n1 2 $ sudo apt install cpu-checker $ kvm-ok 如果输出为：\n1 2 INFO: /dev/kvm exists KVM acceleration can be used 证明CPU的虚拟化支持已经在BIOS中启用。\n运行下面的命令安装QEMU/KVM和Virsh：\n1 $ sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager 检查libvirt守护程序是否已经启动：\n1 2 $ sudo systemctl is-active libvirtd active 如果没有输出active，运行下面的命令启动libvertd服务：\n1 2 $ sudo systemctl enable libvirtd $ sudo systemctl start libvirtd # 创建虚拟机镜像 创建一个虚拟机镜像，大小为40G，qcow2 格式为动态分配磁盘占用空间。\n1 qemu-img create -f qcow2 ubuntutest.img 40G # 创建虚拟机，安装操作系统 使用下面的命令启动虚拟机，-cdrom参数为虚拟机挂载了Ubuntu的安装光盘：\n1 qemu-system-x86_64 -enable-kvm -name ubuntutest -m 4096 -hda ubuntutest.img -cdrom ubuntu-20.04.2-live-server-amd64.iso -boot d -vnc :19 我们使用VNC客户端连接进虚拟机，完成Ubuntu的安装。注意上面的命令通过-vnc :19设置了虚拟机的VNC监听端口为5919。\n我使用的VNC客户端是VNC Viewer，支持Windows、macOS和Linux等主流平台。按照正常步骤，完成Ubuntu在虚拟机上的安装。\n安装完成后，可以用ctrl+c退出qemu-system-x86_64命令的执行来停止虚拟机。再次启动虚拟机，需要把 -cdrom 参数去掉。\n1 qemu-system-x86_64 -enable-kvm -name ubuntutest -m 4096 -hda ubuntutest.img -boot d -vnc :19 # 配置虚拟机网络 为了让虚拟机能访问外部网络，我们需要形成下面的结构：\n在宿主机上创建网桥br0，并设置一个IP地址：\n1 2 3 $ sudo brctl addbr br0 $ sudo ip link set br0 up $ sudo ifconfig br0 192.168.57.1/24 编辑宿主机的/etc/sysctl.conf文件，设置IP转发生效：\n1 net.ipv4.ip_forward=1 使用sysctl -p重新加载sysctl.conf配置使其生效。\n在宿主机上增加SNAT规则。\n1 sudo iptables -t nat -A POSTROUTING -o wlp2s0 -j MASQUERADE 虚拟机的IP地址外部并不认识，如果它要访问外网，需要在数据包离开前将源地址替换为宿主机的IP，这样外部主机才能用宿主机的IP作为目的地址发回响应。\n上面的命令的含义是：在nat表的POSTROUTING链增加规则，出口设备为wlp2s0时，就执行MASQUERADE动作。MASQUERADE是一种源地址转换动作，它会动态选择宿主机的一个IP做源地址转换。\n注意上面命令中的 -o 参数，指定了数据包的出口设备为wlp2s0。你需要使用ip link命令在你的机器上查看具体设备的名称：\n如果想进一步了解iptables，可以参见我的另一篇文章《Docker单机网络模型动手实验》。\n接着我们需要将虚拟机的网卡连接到网桥br0。后面我们使用libvirt来管理QEMU/KVM虚拟机，这样可以把虚拟机的配置参数记录在XML文件中，易于维护。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \u0026lt;domain type=\u0026#39;kvm\u0026#39; xmlns:qemu=\u0026#39;http://libvirt.org/schemas/domain/qemu/1.0\u0026#39;\u0026gt; \u0026lt;name\u0026gt;ubuntutest\u0026lt;/name\u0026gt; \u0026lt;uuid\u0026gt;0f0806ab-531d-6134-5def-c5b4955292aa\u0026lt;/uuid\u0026gt; \u0026lt;memory unit=\u0026#39;GiB\u0026#39;\u0026gt;4\u0026lt;/memory\u0026gt; \u0026lt;currentMemory unit=\u0026#39;GiB\u0026#39;\u0026gt;4\u0026lt;/currentMemory\u0026gt; \u0026lt;vcpu placement=\u0026#39;static\u0026#39;\u0026gt;2\u0026lt;/vcpu\u0026gt; \u0026lt;os\u0026gt; \u0026lt;type arch=\u0026#39;x86_64\u0026#39; machine=\u0026#39;pc-i440fx-trusty\u0026#39;\u0026gt;hvm\u0026lt;/type\u0026gt; \u0026lt;boot dev=\u0026#39;hd\u0026#39;/\u0026gt; \u0026lt;/os\u0026gt; \u0026lt;features\u0026gt; \u0026lt;acpi/\u0026gt; \u0026lt;apic/\u0026gt; \u0026lt;pae/\u0026gt; \u0026lt;/features\u0026gt; \u0026lt;clock offset=\u0026#39;utc\u0026#39;/\u0026gt; \u0026lt;on_poweroff\u0026gt;destroy\u0026lt;/on_poweroff\u0026gt; \u0026lt;on_reboot\u0026gt;restart\u0026lt;/on_reboot\u0026gt; \u0026lt;on_crash\u0026gt;restart\u0026lt;/on_crash\u0026gt; \u0026lt;devices\u0026gt; \u0026lt;emulator\u0026gt;/usr/bin/kvm\u0026lt;/emulator\u0026gt; \u0026lt;disk type=\u0026#39;file\u0026#39; device=\u0026#39;disk\u0026#39;\u0026gt; \u0026lt;driver name=\u0026#39;qemu\u0026#39; type=\u0026#39;qcow2\u0026#39;/\u0026gt; \u0026lt;source file=\u0026#39;/home/mazhen/works/ubuntutest.img\u0026#39;/\u0026gt; \u0026lt;target dev=\u0026#39;vda\u0026#39; bus=\u0026#39;virtio\u0026#39;/\u0026gt; \u0026lt;/disk\u0026gt; \u0026lt;controller type=\u0026#39;pci\u0026#39; index=\u0026#39;0\u0026#39; model=\u0026#39;pci-root\u0026#39;/\u0026gt; \u0026lt;interface type=\u0026#39;bridge\u0026#39;\u0026gt; \u0026lt;mac address=\u0026#39;fa:16:3e:6e:89:ce\u0026#39;/\u0026gt; \u0026lt;source bridge=\u0026#39;br0\u0026#39;/\u0026gt; \u0026lt;target dev=\u0026#39;tap1\u0026#39;/\u0026gt; \u0026lt;model type=\u0026#39;virtio\u0026#39;/\u0026gt; \u0026lt;/interface\u0026gt; \u0026lt;serial type=\u0026#39;pty\u0026#39;\u0026gt; \u0026lt;target port=\u0026#39;0\u0026#39;/\u0026gt; \u0026lt;/serial\u0026gt; \u0026lt;console type=\u0026#39;pty\u0026#39;\u0026gt; \u0026lt;target type=\u0026#39;serial\u0026#39; port=\u0026#39;0\u0026#39;/\u0026gt; \u0026lt;/console\u0026gt; \u0026lt;graphics type=\u0026#39;vnc\u0026#39; port=\u0026#39;5919\u0026#39; autoport=\u0026#39;no\u0026#39; listen=\u0026#39;0.0.0.0\u0026#39;\u0026gt; \u0026lt;listen type=\u0026#39;address\u0026#39; address=\u0026#39;0.0.0.0\u0026#39;/\u0026gt; \u0026lt;/graphics\u0026gt; \u0026lt;video\u0026gt; \u0026lt;model type=\u0026#39;cirrus\u0026#39;/\u0026gt; \u0026lt;/video\u0026gt; \u0026lt;/devices\u0026gt; \u0026lt;qemu:commandline\u0026gt; \u0026lt;qemu:arg value=\u0026#39;-s\u0026#39;/\u0026gt; \u0026lt;/qemu:commandline\u0026gt; \u0026lt;/domain\u0026gt; 我们可以看到，source file指定的文件/home/mazhen/works/ubuntutest.img就是虚拟机镜像。devices中的interface定义了虚拟网卡，br0是我们前面创建的网桥，libvirt帮我们创建的虚拟网卡会连接到网桥br0上。\n将XML文件保存为domain.xml，然后在libvirt定义虚拟机：\n1 $ virsh define domain.xml 接着我们可以使用virsh list --all查看虚拟机列表：\n1 2 3 4 $ virsh list --all Id Name State ----------------------------- - ubuntutest shut off 使用命令virsh start ubuntutest启动虚拟机：\n1 2 3 4 5 6 7 $ virsh start ubuntutest Domain ubuntutest started $ virsh list Id Name State ---------------------------- 1 ubuntutest running 这时我们使用VNC Viewer连接进行虚拟机，为虚拟机配置IP地址。虚拟机安装的是ubuntu-20.04.2，编辑/etc/netplan/00-installer-config.yaml文件配置IP地址。\n1 2 3 4 5 6 7 8 9 10 network: ethernets: ens3: addresses: [192.168.57.100/24] gateway4: 192.168.57.1 dhcp4: no nameservers: addresses: [114.114.114.114] optional: true version: 2 我们可以看到，网关配置的就是br0的IP地址。然后，使用命令 netplan apply让配置生效。这样，虚拟机的网络就配置好了，可以在虚拟机里访问到外网。这时我们就可以在宿主机上使用ssh登录虚拟机，这样比使用VNC Viewer操作更方便一些。\n# 下载Linux内核源码 在虚拟机上下载Linux内核源码：\n1 $ sudo apt install linux-source-5.4.0 ubuntu-20.04.2对应的内核版本是5.4。可以使用uname -srm查看内核版本。\n源码被下载到来/usr/src/目录下，使用下面的命令解压缩：\n1 sudo tar vjxkf linux-source-5.4.0.tar.bz2 内核源码被解压缩到了/usr/src/linux-source-5.4.0目录下。\n# 编译Linux内核 首先我们需要安装编译内核用到的依赖包：\n1 $ sudo apt install libncurses5-dev libssl-dev bison flex libelf-dev gcc make openssl libc6-dev 编译前要定义内核编译选项。进入/usr/src/linux-source-5.4.0目录，运行下面的命令，会进入内核参数配置界面：\n1 $ sudo make menuconfig 为了构建能够调试的内核，我们需要配置以下几个参数。\nCONFIG_DEBUG_INFO 在内核和内核模块中包含调试信息，这个选项在幕后为gcc使用的编译器参数增加了-g选项。 这个选项的菜单路径为：\n1 2 3 Kernel hacking ---\u0026gt; Compile-time checks and compiler options ---\u0026gt; [*] Compile the kernel with debug info 实际上通过菜单进行设置比较麻烦。我们保存设置退出后，配置会保存在.config文件中。直接编辑这个文件会更方便一些。在.config中确认CONFIG_DEBUG_INFO的设置正确。\n1 CONFIG_DEBUG_INFO=y CONFIG_FRAME_POINTER 这个选项会将调用帧信息保存在寄存器或堆栈上的不同位置，使gdb在调试内核时可以更准确地构造堆栈回溯跟踪（stack back traces）。 在.config中设置：\n1 CONFIG_FRAME_POINTER=y 启用CONFIG_GDB_SCRIPTS，但要关闭CONFIG_DEBUG_INFO_REDUCED。 1 2 CONFIG_GDB_SCRIPTS=y CONFIG_DEBUG_INFO_REDUCED=n CONFIG_KGDB 启用内置的内核调试器，该调试器允许进行远程调试。 1 CONFIG_KGDB=y 关闭CONFIG_RANDOMIZE_BASE设置 1 CONFIG_RANDOMIZE_BASE=n KASLR会更改引导时放置内核代码的基地址。如果你在内核配置中启用了KASLR（CONFIG_RANDOMIZE_BASE=y），则无法从gdb设置断点。\n设置完必要的内核参数后，我们开始编译内核：\n1 2 3 sudo make -j8 sudo make modules_install sudo make install 编译的过程很漫长，可能需要数小时。当编译完毕之后，新内核的选项已经增加到了grub的配置中。我们可以查看配置文件/boot/grub/grub.cfg确认：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 submenu \u0026#39;Advanced options for Ubuntu\u0026#39; $menuentry_id_option \u0026#39;gnulinux-advanced-5506d28f-c9e7-46d4-a12e-42555d491eec\u0026#39; { menuentry \u0026#39;Ubuntu, with Linux 5.4.106\u0026#39; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option \u0026#39;gnulinux-5.4.106-advanced-5506d28f-c9e7-46d4-a12e-42555d491eec\u0026#39; { recordfail load_video gfxmode $linux_gfx_mode insmod gzio if [ x$grub_platform = xxen ]; then insmod xzio; insmod lzopio; fi insmod part_gpt insmod ext2 if [ x$feature_platform_search_hint = xy ]; then search --no-floppy --fs-uuid --set=root 5506d28f-c9e7-46d4-a12e-42555d491eec else search --no-floppy --fs-uuid --set=root 5506d28f-c9e7-46d4-a12e-42555d491eec fi echo \u0026#39;Loading Linux 5.4.106 ...\u0026#39; linux /boot/vmlinuz-5.4.106 root=UUID=5506d28f-c9e7-46d4-a12e-42555d491eec ro maybe-ubiquity echo \u0026#39;Loading initial ramdisk ...\u0026#39; initrd /boot/initrd.img-5.4.106 } vmlinuz-5.4.106就是我们新编译的内核。\n重启虚拟机。在GRUB界面选择 Ubuntu 高级选项，选择第一项进去，就进入了新的内核。\n# 启用gdb监听端口 QEMU有个命令行参数-s，它代表参数-gdb tcp::1234，意思是QEMU监听 1234端口，这样gdb 可以 attach 到这个端口上，调试QEMU里面的内核。\n实际上在前面的domain.xml中我们已经为QEMU加了-s参数。\n1 2 3 4 5 6 \u0026lt;domain type=\u0026#39;kvm\u0026#39; xmlns:qemu=\u0026#39;http://libvirt.org/schemas/domain/qemu/1.0\u0026#39;\u0026gt; ... \u0026lt;qemu:commandline\u0026gt; \u0026lt;qemu:arg value=\u0026#39;-s\u0026#39;/\u0026gt; \u0026lt;/qemu:commandline\u0026gt; \u0026lt;/domain\u0026gt; 所以这时运行在虚拟机里的内核已经可以被调试了。\n# 调试内核 在宿主机上运行gdb需要内核的二进制文件，这个文件就是在虚拟机GRUB里配置的/boot/vmlinuz-5.4.106。为了方便在调试过程中查看源代码，我们可以将虚拟机的/usr/src/linux-source-5.4.0整个目录都拷贝到宿主机上来。\n1 $ scp -r mazhen@virtual-node:/usr/src/linux-source-5.4.0 ./ 在/usr/src/linux-source-5.4.0目录下面的vmlinux文件也是内核的二进制文件。\n为了能让gdb在启动时能够加载Linux helper脚本，需要在~/.gdbinit文件中添加如下内容：\n1 add-auto-load-safe-path /path/to/linux-build /path/to/linux-build就是上面从虚拟机拷贝过来的Linux源码目录。\n必要的配置完成后，就可以启动gdb了。\n在宿主机的./linux-source-5.4.0目录下执行gdb vmlinux。\n然后在gdb的交互环境下使用target remote :1234命令attach到虚拟机的内核。\n1 2 3 4 5 6 $ gdb vmlinux ... Reading symbols from vmlinux... (gdb) target remote :1234 Remote debugging using :1234 0xffffffff81ade35e in native_safe_halt () at ./arch/x86/include/asm/irqflags.h:60 如果我们想调试进程fork的过程，可以用b _do_fork设置断点：\n1 2 (gdb) b _do_fork Breakpoint 1 at 0xffffffff81098450: file kernel/fork.c, line 2362. 我们可以看到，断点设置成功。如果你不确认fork的具体方法名，可以使用info functions命令搜索符号表：\n1 2 3 4 5 (gdb) info function do_fork All functions matching regular expression \u0026#34;do_fork\u0026#34;: File kernel/fork.c: 2361:\tlong _do_fork(struct kernel_clone_args *); 使用命令c让内核继续执行：\n1 2 (gdb) c Continuing. 这时在虚拟机里执行任意命令，例如ls，断点将被触发：\n1 2 3 4 5 6 (gdb) c Continuing. Thread 1 hit Breakpoint 1, _do_fork (args=0xffffc9000095fee0) at kernel/fork.c:2362 2362\t{ (gdb) 我们可以使用n执行下一条语句：\n1 2 3 4 5 6 7 (gdb) n 2376\tif (!(clone_flags \u0026amp; CLONE_UNTRACED)) { (gdb) n 2377\tif (clone_flags \u0026amp; CLONE_VFORK) (gdb) n 2379\telse if (args-\u0026gt;exit_signal != SIGCHLD) (gdb) l显示多行源码：\n1 2 3 4 5 6 7 8 9 10 11 (gdb) l 2374\t* for the type of forking is enabled. 2375\t*/ 2376\tif (!(clone_flags \u0026amp; CLONE_UNTRACED)) { 2377\tif (clone_flags \u0026amp; CLONE_VFORK) 2378\ttrace = PTRACE_EVENT_VFORK; 2379\telse if (args-\u0026gt;exit_signal != SIGCHLD) 2380\ttrace = PTRACE_EVENT_CLONE; 2381\telse 2382\ttrace = PTRACE_EVENT_FORK; 2383\tbt查看函数调用信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 (gdb) bt #0 _do_fork (args=0xffffc9000095fee0) at kernel/fork.c:2379 #1 0xffffffff810989f4 in __do_sys_clone (tls=\u0026lt;optimized out\u0026gt;, child_tidptr=\u0026lt;optimized out\u0026gt;, parent_tidptr=\u0026lt;optimized out\u0026gt;, newsp=\u0026lt;optimized out\u0026gt;, clone_flags=\u0026lt;optimized out\u0026gt;) at kernel/fork.c:2544 #2 __se_sys_clone (tls=\u0026lt;optimized out\u0026gt;, child_tidptr=\u0026lt;optimized out\u0026gt;, parent_tidptr=\u0026lt;optimized out\u0026gt;, newsp=\u0026lt;optimized out\u0026gt;, clone_flags=\u0026lt;optimized out\u0026gt;) at kernel/fork.c:2525 #3 __x64_sys_clone (regs=\u0026lt;optimized out\u0026gt;) at kernel/fork.c:2525 #4 0xffffffff81003fd7 in do_syscall_64 (nr=\u0026lt;optimized out\u0026gt;, regs=0xffffc9000095ff58) at arch/x86/entry/common.c:290 #5 0xffffffff81c0008c in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:175 #6 0x00005621191e2da0 in ?? () #7 0x000056211a7de450 in ?? () #8 0x00007ffc9f31a3e0 in ?? () #9 0x0000000000000000 in ?? () p用于打印内部变量值：\n1 2 (gdb) p clone_flags $1 = 18874368 你现在可以像调试普通应用程序一样，调试Linux内核了！\n# 写在最后 在本机搭建Linux内核调试环境的步骤有点繁杂，但使用GDB能调试内核，会成为我们学习内核的利器，进程管理、内存管理、文件系统，对源码有什么困惑就可以debug一下。 Enjoy it!\n","date":"2021-05-21T11:37:22+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8gdb%E8%B0%83%E8%AF%95linux%E5%86%85%E6%A0%B8/","title":"使用GDB调试Linux内核"},{"content":"Atomikos是一个轻量级的分布式事务管理器，实现了Java Transaction API (JTA)规范，可以很方便的和Spring Boot集成，支持微服务场景下跨节点的全局事务。\n本文为一个微服务的示例应用，通过引入Atomikos增加全局事务能力。\n示例代码可以在这里查看。\n用户访问Business服务，它通过RPC调用分别调用Order和Storage创建订单和减库存。三个服务需要加入到一个全局事务中，要么全部成功，任何一个服务失败，都会造成事务回滚，数据的状态始终保持一致性。\n蚂蚁金服开源的Seata就是为了解决这类问题，在微服务架构下提供分布式事务服务。传统的应用服务器通过JTA/JTS也能解决分布式场景下的事务问题，但需要和EJB绑定在一起才能使用。Atomikos是一个独立的分布式事务管理器，原先是为Spring和Tomcat提供事务服务，让用户不必只为了事务服务而引入应用服务器。\n现在Atomikos也能为微服务提供分布式事务服务，这时主要需要两个问题：\n事务上下文如何通过RPC在服务间传播 微服务如何参与进两阶段提交协议的过程 后面会结合示例应用介绍Atomikos是如何解决这两个问题。示例应用atomkos-sample的结构如下：\napi：定义了服务接口OrderService和StorageService order-service：OrderService的具体实现 storage-service：StorageService的具体实现 business-service：用户访问入口 # 事务上下文的传播 在项目主工程的pom文件中引入Atomikos依赖，注意要包括transactions-remoting，正是它才能让事务上下文在RPC调用时传递。\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.atomikos\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;transactions-remoting\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.0.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; transactions-remoting支持jaxrs，Spring Remoting和Spring rest等几种RPC方式，我们使用的是Spring Remoting。\n以order-service为例，通过TransactionalHttpInvokerServiceExporter将OrderService发布为远程服务：\n1 2 3 4 5 6 7 @Bean(name = \u0026#34;/services/order\u0026#34;) TransactionalHttpInvokerServiceExporter orderService(OrderServiceImpl orderService) { TransactionalHttpInvokerServiceExporter exporter = new TransactionalHttpInvokerServiceExporter(); exporter.setService(orderService); exporter.setServiceInterface(OrderService.class); return exporter; } OrderService的调用者business-service使用HttpInvokerProxyFactoryBean引入远程服务：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Bean public HttpInvokerProxyFactoryBean orderService() { HttpInvokerProxyFactoryBean orderService = new HttpInvokerProxyFactoryBean(); orderService.setHttpInvokerRequestExecutor(httpInvokerRequestExecutor()); orderService.setServiceUrl(\u0026#34;http://localhost:8082/services/order\u0026#34;); orderService.setServiceInterface(OrderService.class); return orderService; } @Bean public TransactionalHttpInvokerRequestExecutor httpInvokerRequestExecutor() { TransactionalHttpInvokerRequestExecutor httpInvokerRequestExecutor = new TransactionalHttpInvokerRequestExecutor(); return httpInvokerRequestExecutor; } business-service负责发起全局事务，它使用Spring标准的@Transactional标记方法开启事务：\n1 2 3 4 5 @Transactional public void createOrder(String userId, String commodityCode, Integer count) { orderService.create(userId, commodityCode, count); storageService.deduct(commodityCode, count); } Atomikos提供了TransactionalHttpInvokerRequestExecutor和TransactionalHttpInvokerServiceExporter拦截请求和响应，利用HTTP header传递事务上下文。\nbusiness-service在调用远程服务OrderService时，请求发送前会经过TransactionalHttpInvokerRequestExecutor.prepareConnection处理，增加HTTP header，携带事务上下文：\n1 2 3 4 5 6 7 @Override protected void prepareConnection(HttpURLConnection con, int contentLength) throws IOException { String propagation = template.onOutgoingRequest(); con.setRequestProperty(HeaderNames.PROPAGATION_HEADER_NAME, propagation); super.prepareConnection(con, contentLength); } OrderService会使用TransactionalHttpInvokerServiceExporter.decorateInputStream进行请求拦截，能从HTTP header中解析出事务上下文：\n1 2 3 4 5 6 7 8 9 10 11 @Override protected InputStream decorateInputStream(HttpServletRequest request, InputStream is) throws IOException { try { String propagation = request.getHeader(HeaderNames.PROPAGATION_HEADER_NAME); template.onIncomingRequest(propagation); } catch (IllegalArgumentException e) { ... } return super.decorateInputStream(request, is); } OrderService处理完成返回响应时，会将该节点加入全局事务包装成Event，放入HTTP header返回给business-service：\n1 2 3 4 5 6 7 8 9 10 11 12 @Override protected OutputStream decorateOutputStream(HttpServletRequest request, HttpServletResponse response, OutputStream os) throws IOException { ... response.addHeader(HeaderNames.EXTENT_HEADER_NAME, extent); ... return super.decorateOutputStream(request, response, os); } business-service接收到响应，利用TransactionalHttpInvokerRequestExecutor.validateResponse解析出Event，注册进事务管理器，这样在全局事务提交时，可以让该分支参与到两阶段提交协议：\n1 2 3 4 5 6 7 @Override protected void validateResponse(HttpInvokerClientConfiguration config, HttpURLConnection con) throws IOException { super.validateResponse(config, con); String extent = con.getHeaderField(HeaderNames.EXTENT_HEADER_NAME); template.onIncomingResponse(extent); } # 两阶段提交过程 在处理RPC调用的响应时，Atomikos会将参与到全局事务的远程节点注册为Participants(Extent.addRemoteParticipants)，在事务提交时，所有的Participants都会参与到两阶段提交：\n1 2 3 4 5 6 7 8 9 10 11 12 13 synchronized ( fsm_ ) { if ( commit ) { if ( participants_.size () \u0026lt;= 1 ) { commit ( true ); } else { int prepareResult = prepare (); // make sure to only do commit if NOT read only if ( prepareResult != Participant.READ_ONLY ) commit ( false ); } } else { rollback (); } 可以看出，如果Participants大于1，会走prepare和commit两阶段提交的完整过程。那么OrderService和StorageService如何参与进两阶段提交呢？\nAtomikos提供了REST入口com.atomikos.remoting.twopc.AtomikosRestPort，你可以将AtomikosRestPort注册到JAX-RS，例如本示例选择的是Apache CFX，在application.properties进行配置：\n1 2 3 cxf.path=/api cxf.jaxrs.classes-scan=true cxf.jaxrs.classes-scan-packages=com.atomikos.remoting.twopc business-service在进行全局事务提交时，会访问所有Participants相应的REST接口进行两阶段提交：\nbusiness-service是怎么知道AtomikosRestPort的访问地址的呢？上面提到了，business-service在访问OrderService时，返回的响应header中包含了Event，地址就随着Event返回给了调用者。AtomikosRestPort的访问地址配置在jta.properties中：\n1 com.atomikos.icatch.rest_port_url=http://localhost:8082/api/atomikos 至此，我们解释清楚了Atomikos如何为微服务提供分布式事务服务的，主要解决了两个问题：事务上下文如何通过RPC在服务间传播，以及微服务如何参与进两阶段提交协议的过程。\n下一步我准备为Atomikos增加dubbo的支持，即事务上下文可以通过dubbo进行传播。\n","date":"2020-05-15T11:30:26+08:00","permalink":"https://mazhen.tech/p/atomikos%E5%9C%A8%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"Atomikos在微服务场景下的使用"},{"content":" # 应用服务器的分布式事务支持 我们先看一下分布式事务的需求是如何产生的，以及应用服务器是如何支持分布式事务管理的。\n单体应用 首先看单体应用，所有的模块部署在一个应用服务器上，业务数据都保存在单个数据库中，这种场景本地事务就可以满足需求。\n数据库水平拆分 如果数据库按照业务模块进行水平拆分，完成一个业务请求会涉及到跨库的资源访问和更新，这时候就需要使用应用服务器的JTA进行两阶段提交，保证跨库操作的事务完整性。\n应用模块拆分 应用按照业务模块进一步拆分，每一个模块都作为EJB，部署在独立的应用服务器中。完成一个业务请求会跨越多个应用服务器节点和资源，如何在这种场景保证业务操作的事务呢？当访问入口EJB时JTA会自动开启全局事务，事务上下文随着EJB的远程调用在应用服务器之间传播，让被调用的EJB也加入到全局事务中。\n这就是应用因拆分而遇到分布式事务的问题，以及应用服务器是如何解决这个问题的。\n# 分布式事务中间件 微服务时代，没人再使用沉重的EJB，都是将Spring Bean直接暴露为远程服务。完成一个业务请求需要跨越多个微服务，同样需要面对分布式事务的问题。这时就需要引入分布式事务中间件。我们以蚂蚁金服开源的Seata为例，看看它是怎么解决微服务场景下的分布式事务问题。\n将上一小节跑在应用服务器上的业务，使用微服务 + Seata的重构后，部署架构如下：\n上图中黄色方框（RM，TM，TC）是Seata的核心组件，它们配合完成对微服务的分布式事务支持。可以看出，和应用服务器的EJB方案架构上类似，只是多了一个独立运行的TC组件。\n我们再看看Seata各组件的具体作用。\n# Seata的架构 Seata由三个组件构成：\nTransaction Coordinator (TC)： 事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚。 Transaction Manager (TM)： 控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议。 Resource Manager (RM)： 控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚。 # Seata vs. 应用服务器 Seata和应用服务器的分布式事务支持主要有以下四个差异：\nSeata和应用服务器都可以实现业务无侵入分布式事务支持。但应用服务器的XA方案实现的是实时一致性，而Seata的AT 模式实现的是最终一致性。 Seata引入了独立运行的Transaction Coordinator，维护全局事务的运行状态。而应用服务器的访问入口节点承担了维护全局事务状态的职责。 Seata自己实现了Resource Manager，不需要依赖数据库的XA driver。这样就有可能将没有实现XA接口的资源加入的分布式事务中，例如NoSQL。同时，RM的实现要比JTA中的XAResource复杂很多。RM需要拦截并解析SQL，生成回滚语句，在事务rollback时自动进行数据还原。XAResource是对XA driver的包装，资源参与分布式事务的能力，都是由数据库提供的。 事务上下文的传播机制不同。应用服务器使用标准的RMI-IIOP协议进行事务上下文的跨节点传播。Seata是对各种RPC框架提供了插件，拦截请求和响应，事务上下文随着RPC调用进行跨节点传播。目前Seata已经支持了dubbo、gRPC、Motan和sofa-rpc等多种RPC框架。 Seata和应用服务器都支持在分布式场景下的全局事务，都可以做到对业务无侵入。Seata实现的是最终一致性，因此性能比应用服务器的XA方案好很多，具备海量并发处理能力，这也是互联网公司选择它的原因。由于Seata不依赖数据库的XA driver，只使用数据库的本地事务，就完成了对分布式事务的支持，相当于承担了部分数据库的职责，因此Seata的实现难度要比应用服务器的JTA大。\n# 应用服务器进入微服务时代 那么应用服务器的分布式事务支持在微服务时代还有用吗？或者说我们应该怎样改进，才能让应用服务器进入微服务时代？\n首先我们要看到JTA/XA的优势：支持数据的实时一致性，对业务开发更加友好。客户对原有的系统进行微服务改造时，如果把业务模型假定成数据最终一致性，客户就不得不做出很大的妥协和变更。特别是有些金融客户对一致性的要求会比较高。\n我们可以学习Seata的架构，抛弃掉沉重的EJB/RMI-IIOP，让Spring Bean通过dubbo等RPC框架直接对外暴露服务，同时事务上下文可以在RPC调用时进行传递：\n我们甚至可以将JTA独立出来，和Tomcat这样的Web容器整合，为微服务架构提供分布式事务支持。相信通过这样的改造，应用服务器的分布式事务能力在微服务时代又能焕发第二春。\n","date":"2020-04-21T11:25:09+08:00","permalink":"https://mazhen.tech/p/%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%94%AF%E6%8C%81%E5%92%8Cseata%E7%9A%84%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/","title":"应用服务器的分布式事务支持和Seata的对比分析"},{"content":" # 性能分析工具的分类 性能分析的技术和工具可以分为以下几类：\nCounters 内核维护着各种统计信息，被称为Counters，用于对事件进行计数。例如，接收的网络数据包数量，发出的磁盘I/O请求，执行的系统调用次数。常见的这类工具有：\nvmstat: 虚拟和物理内存统计 mpstat: CPU使用率统计 iostat：磁盘的I/O使用情况 netstat：网络接口统计信息，TCP/IP协议栈统计信息，连接统计信息 Tracing Tracing是收集每个事件的数据进行分析。Tracing会捕获所有的事件，因此有比较大的CPU开销，并且可能需要大量存储来保存数据。\n常见的Tracing工具有：\ntcpdump: network packet tracing blktrace: block I/O tracing perf: Linux Performance Events, 跟踪静态和动态探针 strace: 系统调用tracing gdb: 源代码级调试器 Profiling Profiling 是通过收集目标行为的样本或快照，来了解目标的特征。Profiling可以从多个方面对程序进行动态分析，如CPU、Memory、Thread、I/O等，其中对CPU进行Profiling的应用最为广泛。\nCPU Profiling原理是基于一定频率对运行的程序进行采样，来分析消耗CPU时间的代码路径。可以基于固定的时间间隔进行采样，例如每10毫秒采样一次。也可以设置固定速率采样，例如每秒采集100个样本。\nCPU Profiling经常被用于分析代码的热点，比如“哪个方法占用CPU的执行时间最长”、“每个方法占用CPU的比例是多少”等等，然后我们就可以针对热点瓶颈进行分析和性能优化。\nLinux上常用的CPU Profiling工具有：\nperf的 record 子命令 BPF profile Monitoring 系统性能监控会记录一段时间内的性能统计信息，以便能够基于时间周期进行比较。这对于容量规划，了解高峰期的使用情况都很有帮助。历史值还为我们理解当前的性能指标提供了上下文。\n监控单个操作系统最常用工具是sar（system activity reporter，系统活动报告）命令。sar通过一个定期执行的agent来记录系统计数器的状态，并可以使用sar命令查看它们，例如：\n1 2 3 4 5 6 7 8 9 10 $ sar Linux 4.15.0-88-generic (mazhen) 03/19/2020 _x86_64_\t(4 CPU) 12:53:08 PM LINUX RESTART 12:55:01 PM CPU %user %nice %system %iowait %steal %idle 01:05:01 PM all 14.06 0.00 10.97 0.11 0.00 74.87 01:15:01 PM all 9.60 0.00 7.49 0.09 0.00 82.83 01:25:01 PM all 0.04 0.00 0.02 0.02 0.00 99.92 01:35:01 PM all 0.03 0.00 0.02 0.01 0.00 99.94 本文主要讨论如何使用perf和BPF进行CPU Profiling。\n# perf perf最初是使用Linux性能计数器子系统的工具，因此perf开始的名称是Performance Counters for Linux(PCL)。perf在Linux2.6.31合并进内核，位于tools/perf目录下。\n随后perf进行了各种增强，增加了tracing、profiling等能力，可用于性能瓶颈的查找和热点代码的定位。\nperf是一个面向事件（event-oriented）的性能剖析工具，因此它也被称为Linux perf events (LPE)，或perf_events。\nperf的整体架构如下：\nperf 由两部分组成：\nperf Tools：perf用户态命令，为用户提供了一系列工具集，用于收集、分析性能数据。 perf Event Subsystem：Perf Events是内核的子系统之一，和用户态工具共同完成数据的采集。 内核依赖的硬件，比如说CPU，一般会内置一些性能统计方面的寄存器（Hardware Performance Counter），通过软件读取这些特殊寄存器里的信息，我们也可以得到很多直接关于硬件的信息。perf最初就是用来监测CPU的性能监控单元（performance monitoring unit, PMU）的。\n# perf Events分类 perf支持多种性能事件：\n这些性能事件分类为：\nHardware Events: CPU性能监控计数器performance monitoring counters（PMC），也被称为performance monitoring unit（PMU） Software Events: 基于内核计数器的底层事件。例如，CPU迁移，minor faults，major faults等。 Kernel Tracepoint Events: 内核的静态Tracepoint，已经硬编码在内核需要收集信息的位置。 User Statically-Defined Tracing (USDT): 用户级程序的静态Tracepoint。 Dynamic Tracing: 用户自定义事件，可以动态的插入到内核或正在运行中的程序。Dynamic Tracing技术分为两类： kprobes：对于kernel的动态追踪技术，可以动态地在指定的内核函数的入口和出口等位置上放置探针，并定义自己的探针处理程序。 uprobes：对于用户态软件的动态追踪技术，可以安全地在用户态函数的入口等位置设置动态探针，并执行自己的探针处理程序。 可以使用perf的list子命令查看当前可用的事件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ sudo perf list List of pre-defined events (to be used in -e): branch-instructions OR branches [Hardware event] branch-misses [Hardware event] bus-cycles [Hardware event] cache-misses [Hardware event] cache-references [Hardware event] cpu-cycles OR cycles [Hardware event] ... alignment-faults [Software event] bpf-output [Software event] context-switches OR cs [Software event] cpu-clock [Software event] cpu-migrations OR migrations [Software event] ... alarmtimer:alarmtimer_cancel [Tracepoint event] alarmtimer:alarmtimer_fired [Tracepoint event] alarmtimer:alarmtimer_start [Tracepoint event] alarmtimer:alarmtimer_suspend [Tracepoint event] block:block_bio_backmerge [Tracepoint event] block:block_bio_bounce [Tracepoint event] ... # perf的使用 如果还没有安装perf，可以使用apt或yum进行安装：\n1 sudo apt install linux-tools-$(uname -r) linux-tools-generic perf的功能强大，支持硬件计数器统计，定时采样，静态和动态tracing等。本文只介绍几个常用的使用场景，如果想全面的了解perf的使用，可以参考perf.wiki。\nCPU Statistics 使用perf的stat命令可以收集性能计数器统计信息，精确统计一段时间内 CPU 相关硬件计数器数值的变化。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 -\u0026gt; % sudo perf stat dd if=/dev/zero of=/dev/null count=10000000 10000000+0 records in 10000000+0 records out 5120000000 bytes (5.1 GB, 4.8 GiB) copied, 12.2795 s, 417 MB/s Performance counter stats for \u0026#39;dd if=/dev/zero of=/dev/null count=10000000\u0026#39;: 12280.299325 task-clock (msec) # 1.000 CPUs utilized 16 context-switches # 0.001 K/sec 0 cpu-migrations # 0.000 K/sec 70 page-faults # 0.006 K/sec 41,610,802,323 cycles # 3.388 GHz 20,195,746,887 instructions # 0.49 insn per cycle 3,972,723,471 branches # 323.504 M/sec 90,061,565 branch-misses # 2.27% of all branches 12.280445133 seconds time elapsed CPU Profiling 可以使用perf record以任意频率收集快照。这通常用于CPU使用情况的分析。\nsudo perf record -F 99 -a -g sleep 10 对所有CPU（-a）进行call stacks（-g）采样，采样频率为99 Hertz（-F 99），即每秒99次，持续10秒（sleep 10）。\nsudo perf record -F 99 -a -g -p PID sleep 10 对指定进程（-p PID）进行采样。\nsudo perf record -F 99 -a -g -e context-switches -p PID sleep 10 perf可以和各种instrumentation points一起使用，以跟踪内核调度程序（scheduler）的活动。其中包括software events和tracepoint event（静态探针）。\n上面的例子对指定进程的上下文切换（-e context-switches）进行采样。\nreport perf record的运行结果保存在当前目录的perf.data文件中，采样结束后，我们使用perf report查看结果。\n交互式查看模式 1 $ sudo perf report 以+开头的行可以回车，展开详细信息。\n使用--stdio选项打印所有输出 1 $ sudo perf report --stdio context-switches的采样报告：\n后面我们会介绍火焰图，以可视化的方式展示stack traces，比perf report更加直观。\n# BPF BPF是Berkeley Packet Filter的缩写，最初是为BSD开发，第一个版本于1992年发布，用于改进网络数据包捕获的性能。BPF是在内核级别进行过滤，不必将每个数据包拷贝到用户空间，从而提高了数据包过滤的性能。tcpdump使用的就是BPF。\n2013年BPF被重写，被称为Extended BPF (eBPF)，于2014年包含进Linux内核中。改进后的BPF成为了通用执行引擎，可用于多种用途，包括创建高级性能分析工具。\nBPF允许在内核中运行mini programs，来响应系统和应用程序事件（例如磁盘I/O事件）。这种运作机制和JavaScript类似：JavaScript是运行在浏览器引擎中的mini programs，响应鼠标点击等事件。BPF使内核可编程化，使用户（包括非内核开发人员）能够自定义和控制他们的系统，以解决实际问题。\nBPF可以被认为是一个虚拟机，由指令集，存储对象和helper函数三部分组成。BPF指令集由位于Linux内核的BPF runtime执行，BPF runtime包括了解释器和JIT编译器。BPF是一种灵活高效的技术，可以用于networking，tracing和安全等领域。我们重点关注它作为系统监测工具方面的应用。\n和perf一样，BPF能够监测多种性能事件源，同时可以通过调用perf_events，使用perf已有的功能：\nBPF可以在内核运行计算和统计汇总，这样大大减少了复制到用户空间的数据量：\nBPF已经内置在Linux内核中，因此你无需再安装任何新的内核组件，就可以在生产环境中使用BPF。\n# BCC和bpftrace 直接使用BPF指令进行编程非常繁琐，因此很有必要提供高级语言前端方便用户使用，于是就出现了BCC和bpftrace。\nBCC（BPF Compiler Collection） 提供了一个C编程环境，使用LLVM工具链来把 C 代码编译为BPF虚拟机所接受的字节码。此外它还支持Python，Lua和C++作为用户接口。\nbpftrace 是一个比较新的前端，它为开发BPF工具提供了一种专用的高级语言。bpftrace适合单行代码和自定义短脚本，而BCC更适合复杂的脚本和守护程序。\nBCC和bpftrace没有在内核代码库，它们存放在GitHub上名为IO Visor的Linux Foundation项目中。\niovisor/bcc iovisor/bpftrace # BCC的安装 BCC可以参考官方的安装文档。以Ubuntu 18.04 LTS为例，建议从源码build安装：\n安装build依赖 1 2 3 4 sudo apt-get -y install bison build-essential cmake flex git libedit-dev \\ libllvm6.0 llvm-6.0-dev libclang-6.0-dev python zlib1g-dev libelf-dev sudo apt-get -y install luajit luajit-5.1-dev 编译和安装 1 2 3 4 5 git clone https://github.com/iovisor/bcc.git mkdir bcc/build; cd bcc/build cmake .. make sudo make install build python3 binding 1 2 3 4 5 cmake -DPYTHON_CMD=python3 .. pushd src/python/ make sudo make install popd make install完成后，BCC自带的工具都安装在了/usr/share/bcc/tools目录下。BCC已经包含70多个BPF工具，用于性能分析和故障排查。这些工具都可以直接使用，无需编写任何BCC代码。\n我们试用其中一个工具biolatency，跟踪磁盘I/O延迟：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 -\u0026gt; % sudo /usr/share/bcc/tools/biolatency Tracing block device I/O... Hit Ctrl-C to end. ^C usecs : count distribution 0 -\u0026gt; 1 : 0 | | 2 -\u0026gt; 3 : 0 | | 4 -\u0026gt; 7 : 0 | | 8 -\u0026gt; 15 : 0 | | 16 -\u0026gt; 31 : 2 |*** | 32 -\u0026gt; 63 : 0 | | 64 -\u0026gt; 127 : 3 |***** | 128 -\u0026gt; 255 : 7 |*********** | 256 -\u0026gt; 511 : 6 |********** | 512 -\u0026gt; 1023 : 11 |****************** | 1024 -\u0026gt; 2047 : 16 |************************** | 2048 -\u0026gt; 4095 : 24 |****************************************| 4096 -\u0026gt; 8191 : 1 |* | 8192 -\u0026gt; 16383 : 6 |********** | 16384 -\u0026gt; 32767 : 3 |***** | biolatency展示的直方图比iostat的平均值能更好的理解磁盘I/O性能。\nBCC已经自带了CPU profiling工具：\ntools/profile: Profile CPU usage by sampling stack traces at a timed interval. 此外，BCC还提供了Off-CPU的分析工具：\ntools/offcputime: Summarize off-CPU time by kernel stack trace 一般的CPU profiling都是分析on-CPU，即CPU时间都花费在了哪些代码路径。off-CPU是指进程不在CPU上运行时所花费的时间，进程因为某种原因处于休眠状态，比如说等待锁，或者被进程调度器（scheduler）剥夺了 CPU 的使用。这些情况都会导致这个进程无法运行在 CPU 上，但是仍然花费了时间。\noff-CPU分析是对on-CPU的补充，让我们知道线程所有的时间花费，更全面的了解程序的运行情况。\n后面会介绍profile，offcputime如何生成火焰图进行可视化分析。\n# bpftrace的安装 bpftrace 建议运行在Linux 4.9 kernel或更高版本。根据安装文档的说明，是因为kprobes、uprobes、tracepoints等主要特性是在4.x以上加入内核的：\n4.1 - kprobes 4.3 - uprobes 4.6 - stack traces, count and hist builtins (use PERCPU maps for accuracy and efficiency) 4.7 - tracepoints 4.9 - timers/profiling 可以运行scripts/check_kernel_features.sh脚本进行验证：\n1 2 $ ./scripts/check_kernel_features.sh All required features present! bpftrace对Linux的版本要求较高，以Ubuntu为例，19.04及以上才支持apt安装：\n1 sudo apt-get install -y libbpfcc-dev 18.04和18.10可以从源码build，但需要先build好BCC。\n安装依赖 1 2 3 sudo apt-get update sudo apt-get install -y bison cmake flex g++ git libelf-dev zlib1g-dev libfl-dev systemtap-sdt-dev binutils-dev sudo apt-get install -y llvm-7-dev llvm-7-runtime libclang-7-dev clang-7 编译和安装 1 2 3 4 5 git clone https://github.com/iovisor/bpftrace mkdir bpftrace/build; cd bpftrace/build; cmake -DCMAKE_BUILD_TYPE=Release .. make -j8 sudo make install make install完成后，bpftrace自带的工具安装在/usr/local/share/bpftrace/tools目录下，这些工具的说明文档可以在项目主页找到。\n我们同样试用查看Block I/O延迟直方图的工具：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -\u0026gt; % sudo bpftrace /usr/local/share/bpftrace/tools/biolatency.bt Attaching 4 probes... Tracing block device I/O... Hit Ctrl-C to end. ^C @usecs: [128, 256) 6 |@@@@@@@@@@ | [256, 512) 4 |@@@@@@ | [512, 1K) 8 |@@@@@@@@@@@@@ | [1K, 2K) 20 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ | [2K, 4K) 30 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@| [4K, 8K) 1 |@ | [8K, 16K) 3 |@@@@@ | [16K, 32K) 0 | | [32K, 64K) 2 |@@@ | 关于bpftrace脚本编写不在本文的讨论范围，感兴趣的可以参考reference_guide。\n# 火焰图 火焰图是Brendan Gregg发明的将stack traces可视化展示的方法。火焰图把时间和空间两个维度上的信息融合在一张图上，将频繁执行的代码路径以可视化的形式，非常直观的展现了出来。\n火焰图可以用于可视化来自任何profiler工具的记录的stack traces信息，除了用来CPU profiling，还适用于off-CPU，page faults等多种场景的分析。本文只讨论 on-CPU 和 off-CPU 火焰图的生成。\n要理解火焰图，先从理解Stack Trace开始。\n# Stack Trace Stack Trace是程序执行过程中，在特定时间点的函数调用列表。例如，func_a()调用func_b()，func_b()调用func_c()，此时的Stack Trace可写为：\n1 2 3 func_c func_b func_a # Profiling Stack Traces 我们做CPU profiling时，会使用perf或bcc定时采样Stack Trace，这样会收集到非常多的Stack Trace。前面介绍了perf report会将Stack Trace样本汇总为调用树，并显示每个路径的百分比。火焰图是怎么展示的呢？\n考虑下面的示例，我们用perf定时采样收集了多个Stack Trace，然后将相同的Stack Trace归纳合并，统计出次数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func_e func_d func_b func_a 1 func_b func_a 2 func_c func_b func_a 7 可以看到，总共收集了10个样本，其中代码路径func_a-\u0026gt;func_b-\u0026gt;func_c有7次，该路径上的func_c在CPU上运行。 func_a-\u0026gt;func_b进行了两次采样，func_b在CPU上运行。func_a-\u0026gt;func_b-\u0026gt;func_d-\u0026gt;func_e一次采样，func_e在CPU上运行。\n# 火焰图 根据前面对Stack Trace的统计信息，可以绘制出如下的火焰图：\n火焰图具有以下特性：\n每个长方块代表了函数调用栈中的一个函数 Y 轴显示堆栈的深度（堆栈中的帧数）。调用栈越深，火焰就越高。顶层方块表示 CPU 上正在运行的函数，下面的函数即为它的祖先。 X 轴的宽度代表被采集的样本数量，越宽表示采集到的越多，即执行的时间长。需要注意的是，X轴从左到右不代表时间，而是所有的调用栈合并后，按字母顺序排列的。 拿到火焰图，寻找最宽的塔并首先了解它们。顶层的哪个函数占据的宽度最大，说明它可能存在性能问题。\n可以使用Brendan Gregg开发的开源项目FlameGraph生成交互式的SVG火焰图。该项目提供了脚本，可以将采集的样本归纳合并，统计出Stack Trace出现的频率，然后使用flamegraph.pl生成SVG火焰图。\n我们先把FlameGraph项目clone下来，后面会用到：\n1 git clone https://github.com/brendangregg/FlameGraph.git # Java CPU Profiling 虽然有很多Java专用的profiler工具，但这些工具一般只能看到Java方法的执行，缺少了GC，JVM的CPU时间消耗，并且有些工具的Method tracing性能损耗比较大。\nperf和BCC profile的优点是它很高效，在内核上下文中对堆栈进行计数，并能完整显示用户态和内核态的CPU使用，能看到native libraries（例如libc），JVM（libjvm），Java方法和内核中花费的时间。\n但是，perf和BCC profile这种系统级的profiler不能很好地与Java配合使用，它们识别不了Java方法和stack traces。这是因为：\nJVM的JIT（just-in-time）没有给系统级profiler公开符号表 JVM还使用帧指针寄存器（frame pointer register，x86-64上的RBP）作为通用寄存器，打破了传统的堆栈遍历 为了能生成包含Java栈与Native栈的火焰图，目前有两种解决方式：\n使用JVMTI agent perf-map-agent，生成Java符号表，供perf和bcc读取（/tmp/perf-PID.map）。同时要加上-XX:+PreserveFramePointer JVM 参数，让perf可以遍历基于帧指针（frame pointer）的堆栈。 使用async-profiler，该项目将perf的堆栈追踪和JDK提供的AsyncGetCallTrace结合了起来，同样能够获得mixed-mode火焰图。同时，此方法不需要启用帧指针，所以不用加上-XX:+PreserveFramePointer参数。 下面我们就分别演示这两种方式。\n# perf-map-agent perf期望能从/tmp/perf-\u0026lt;pid\u0026gt;.map中获得在未知内存区域执行的代码的符号表。perf-map-agent可以为JIT编译的方法生成/tmp/perf-\u0026lt;pid\u0026gt;.map文件，以满足perf的要求。\n首先下载并编译perf-map-agent：\n1 2 3 4 git clone https://github.com/jvm-profiling-tools/perf-map-agent.git cd perf-map-agent cmake . make # 配合perf使用 perf-map-agent提供了perf-java-flames脚本，可以一步生成火焰图。\nperf-java-flames接收perf record命令参数，它会调用perf进行采样，然后使用FlameGraph生成火焰图，一步完成，非常方便。\n注意，记得要给被profiling的Java进程加上-XX:+PreserveFramePointer JVM 参数。\n设置必要的环境变量：\n1 2 export FLAMEGRAPH_DIR=[FlameGraph 所在的目录] export PERF_RECORD_SECONDS=[采样时间] ./bin/perf-java-flames [PID] -F 99 -a -g -p [PID] 对指定进程（-p PID），在所有CPU（-a）上进行call stacks（-g）采样，采样频率为99 Hertz （-F 99），持续时间为PERF_RECORD_SECONDS秒。命令运行完成后，会在当前目录生成名为flamegraph-pid.svg的火焰图。\n./bin/perf-java-flames [PID] -F 99 -g -a -e context-switches -p [PID] 对指定进程的上下文切换（-e context-switches）进行采样，并生成火焰图。\n当然也可以只为perf生成Java符号表，然后直接使用perf采样 1 2 3 4 5 6 ./bin/create-java-perf-map.sh [PID]; sudo perf record -F 99 -p [PID] -a -g -- sleep 15 ./bin/create-java-perf-map.sh [PID]; sudo perf record -g -a -e context-switches -p [PID] sleep 15 # 查看报告 sudo perf report --stdio # 配合bcc profile使用 FlameGraph项目提供了jmaps脚本，它会调用perf-map-agent为当前运行的所有Java进程生成符号表。\n首先为jmaps脚本设置好JAVA_HOME和perf-map-agent的正确位置：\n1 2 JAVA_HOME=${JAVA_HOME:-/usr/lib/jvm/java-8-oracle} AGENT_HOME=${AGENT_HOME:-/usr/lib/jvm/perf-map-agent} # from https://github.com/jvm-profiling-tools/perf-map-agent 运行jmaps，可以看到它会为当前所有的Java进程生成符号表：\n1 2 3 4 $ sudo ./jmaps Fetching maps for all java processes... Mapping PID 30711 (user adp): wc(1): 3486 10896 214413 /tmp/perf-30711.map 我们在做任何profiling之前，都需要调用jmaps，保持符号表是最新的。\nCPU Profiling火焰图 1 2 3 4 5 # Profiling sudo ./jmaps ; sudo /usr/share/bcc/tools/profile -dF 99 -afp [PID] 10 \u0026gt; out.profile01.txt # 生成火焰图 ./flamegraph.pl --color=java --hash \u0026lt;out.profile01.txt \u0026gt; flamegraph.svg off-CPU火焰图 1 2 3 4 5 # Profiling sudo ./jmaps ; sudo /usr/share/bcc/tools/offcputime -fp [PID] 10 \u0026gt; out.offcpu01.txt # 生成火焰图 ./flamegraph.pl --color=java --bgcolor=blue --hash --countname=us --width=1024 --title=\u0026#34;Off-CPU Time Flame Graph\u0026#34; \u0026lt; out.offcpu01.txt \u0026gt; out.offcpu01.svg off-CPU，并过滤指定的进程状态 Linux的进程状态有：\n状态 描述 TASK_RUNNING 意味着进程处于可运行状态。这并不意味着已经实际分配了CPU。进程可能会一直等到调度器选中它。该状态确保进程可以立即运行，而无需等待外部事件。 TASK_INTERRUPTIBLE 可中断的等待状态，主要为恢复时间无法预测的长时间等待。例如等待来自用户的输入。 TASK_UNINTERRUPTIBLE 不可中断的等待状态。用于因内核指示而停用的睡眠进程。它们不能由外部信号唤醒，只能由内核亲自唤醒。例如磁盘输入输出等待。 TASK_STOPPED 响应暂停信号而运行中断的状态。直到恢复前都不会被调度 TASK_ZOMBIE 僵尸状态，子进程已经终止，但父进程尚未执行wait()，因此该进程的资源没有被系统释放。 在状态TASK_RUNNING（0）会发生非自愿上下文切换，而我们通常感兴趣的阻塞事件是TASK_INTERRUPTIBLE（1）或TASK_UNINTERRUPTIBLE（2），offcputime可以用--state过滤指定的进程状态：\n1 2 3 4 5 # Profiling sudo ./jmaps ; sudo /usr/share/bcc/tools/offcputime -K --state 2 -f 30 \u0026gt; out.offcpu01.txt # 生成火焰图 ./flamegraph.pl --color=io --countname=ms \u0026lt; out.offcpu01.txt \u0026gt; out.offcpu01.svg # async-profiler async-profiler将perf的堆栈追踪和JDK提供的AsyncGetCallTrace结合了起来，做到同时采样Java栈与Native栈，因此也就可以同时分析Java代码和Native代码中存在的性能热点。\nAsyncGetCallTrace是JDK内部提供的一个函数，它的原型如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 typedef struct { jint lineno; // BCI in the source file jmethodID method_id; // method executed in this frame } ASGCT_CallFrame; typedef struct { JNIEnv *env_id //Env where trace was recorded jint num_frames; // number of frames in this trace ASGCT_CallFrame *frames; } ASGCT_CallTrace; void AsyncGetCallTrace(ASGCT_CallTrace *trace, // pre-allocated trace to fill jint depth, // max number of frames to walk up the stack void* ucontext) // signal context 可以看出，该函数直接通过ucontext就能获取到完整的Java调用栈。\n# async-profiler的使用 下载并解压好async-profiler安装包。\n从Linux 4.6开始，从non-root进程使用perf捕获内核的call stacks，需要设置如下两个内核参数：\n1 2 # echo 1 \u0026gt; /proc/sys/kernel/perf_event_paranoid # echo 0 \u0026gt; /proc/sys/kernel/kptr_restrict async-profiler的使用非常简单，一步就能生成火焰图。另外，也不需要为被profiling的Java进程设置-XX:+PreserveFramePointer参数。\n1 ./profiler.sh -d 30 -f /tmp/flamegraph.svg [PID] # 总结 为Java生成CPU profiling火焰图，基本的流程都是：\n使用工具采集样本 使用FlameGraph项目提供的脚本，将采集的样本归纳合并，统计出Stack Trace出现的频率 最后使用flamegraph.pl利用上一步的输出，绘制SVG火焰图 为了能够生成Java stacks和native stacks完整的火焰图，解决perf和bcc profile不能识别Java符号和Java stack traces的问题，目前有以下两种方式：\nperf-map-agent 加上 perf或bcc profile async-profiler（内部会使用到perf） 如果只是对Java进程做on-CPU分析，async-profiler更加方便好用。如果需要更全面的了解Java进程的运行情况，例如分析系统锁的开销，阻塞的 I/O 操作，以及进程调度器（scheduler）的工作，那么还是需要使用功能更强大的perf和bcc。\n# 参考资料 perf Examples Linux Extended BPF (eBPF) Tracing Tools BPF Performance Tools (book) Off-CPU Analysis Flame Graphs ","date":"2020-03-23T11:01:58+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8%E7%81%AB%E7%84%B0%E5%9B%BE%E8%BF%9B%E8%A1%8Cjava%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","title":"使用火焰图进行Java性能分析"},{"content":" Previous Next \u0026nbsp; \u0026nbsp; Page: / ","date":"2020-02-07T10:44:10+08:00","permalink":"https://mazhen.tech/p/consensus-and-distributed-transactions/","title":"Consensus and Distributed Transactions"},{"content":"在上一篇\u0026lt;Tomcat系统架构\u0026gt;中提到，Tomcat的网络通信层支持多种 I/O 模型。本文将介绍NioEndpoint，它是直接使用NIO实现了 I/O 多路复用。\n# NioEndpoint的处理流程 NioEndpoint的处理流程如下：\nAcceptor实现了Runnable接口，运行在一个独立的线程中。Acceptor的run方法在循环中调用ServerSocketChannel.accept()，将返回的SocketChannel包装成NioSocketWrapper，然后将NioSocketWrapper注册进Poller。\nPoller同样实现了Runnable接口，运行在一个独立的线程中。Poller的核心任务是检测I/O事件，它在无限循环中调用Selector.select()，会得到准备就绪的NioSocketWrapper列表，为每个NioSocketWrapper生成一个SocketProcessor任务，然后把任务扔进线程池Executor去处理。\nExecutor是可配置的线程池，负责运行SocketProcessor任务。SocketProcessor实现了Runnable接口，在run方法中会调用ConnectionHandler.process(NioSocketWrapper, SocketEvent)处理当前任务关联的NioSocketWrapper。\nConnectionHandler内部使用一个ConcurrentHashMap建立了NioSocketWrapper和Processor之间的映射。从上一篇\u0026lt;Tomcat系统架构\u0026gt;的介绍我们知道，Processor负责应用层协议的解析，那么我们需要为每个NioSocketWrapper创建并关联一个Processor。\n为什么要建立NioSocketWrapper和Processor之间的关联呢？因为Processor在从NioSocketWrapper中读取字节流进行协议解析时，数据可能并不完整，这时需要释放工作线程，当Poller再次触发I/O读取事件时，可以根据NioSocketWrapper找回关联的Processor，继续进行未完成的协议解析工作。\nProcessor解析的结果是生成Tomcat的Request对象，然后调用Adapter.service(request, response)方法。Adapter的职责是将Tomcat的Request对象转换为标准的ServletRequest后，传递给Servlet引擎，最终会调用到用户编写的Servlet.service(ServletRequest, ServletResponse)。\n# NioEndpoint的线程模型 我们注意到，在Tomcat 9的实现中，Acceptor和Poller都只有一个线程，并且不可配置。Poller检测到的I/O事件会被扔进Executor线程池中处理，最终Servlet.service也是在Executor中执行。这是一种常见的NIO线程模型，将I/O事件的检测和处理分开在不同的线程。\n但这种处理方式也有缺点。当Selector检测到数据就绪事件时，运行Selector线程的CPU已经在CPU cache中缓存了数据。这时切换到另外一个线程去读，这个读取线程很可能运行在另一个CPU核，此前缓存在CPU cache中的数据就没用了。同时这样频繁的线程切换也增加了系统内核的开销。\n同样是基于NIO，Jetty使用了不同的线程模型：线程自己产生的I/O事件，由当前线程处理，\u0026ldquo;Eat What You Kill\u0026rdquo;，同时，Jetty可能会新建一个新线程继续检测和处理I/O事件。\n这篇博客详细的介绍了Jetty的 \u0026ldquo;Eat What You Kill\u0026rdquo; 策略。Jetty也支持类似Tomcat的ProduceExecuteConsume策略，即I/O事件的产出和消费用不同的线程处理。\nExecuteProduceConsume策略，也就是 \u0026ldquo;Eat What You Kill\u0026rdquo;，I/O事件的生产者自己消费任务。\nJetty对比了这两种策略，使用ExecuteProduceConsume能达到更高的吞吐量。\n其实，Netty也使用了和 \u0026ldquo;Eat What You Kill\u0026rdquo; 类似的线程模型。\nChannel注册到EventLoop，一个EventLoop能够服务多个Channel。EventLoop仅在一个线程上运行，因此所有I/O事件均由同一线程处理。\n# blocking write的实现 当通过Response向客户端返回数据时，最终会调用NioSocketWrapper.write(boolean block, ByteBuffer from)或NioSocketWrapper.write(boolean block, byte[] buf, int off, int len)，将数据写入socket。\n我们注意到write方法的第一个参数block，它决定了write是使用blocking还是non-blocking方式。比较奇怪，虽然是NioEndpoint，但write动作也不全是non-blocking。\n一般NIO框架在处理write时都是non-blocking方式，先尝试SocketChannel.write(ByteBuffer)，如果buffer.remaining() \u0026gt; 0，将剩余数据以某种方式缓存，然后把SelectionKey.OP_WRITE添加到SelectionKey的interest set，等待被Selector触发时再次尝试写出，直到buffer中没有剩余数据。\n那是什么因素决定了NioSocketWrapper.write是blocking还是non-blocking呢？\n我们看一下Http11OutputBuffer.isBlocking的实现：\n1 2 3 4 5 6 7 /** * Is standard Servlet blocking IO being used for output? * @return \u0026lt;code\u0026gt;true\u0026lt;/code\u0026gt; if this is blocking IO */ protected final boolean isBlocking() { return response.getWriteListener() == null; } 如果response.getWriteListener()不为null，说明我们注册了WriteListener接收write事件的通知，这时我们肯定是在使用异步Servlet。\n也就是说，当我们使用异步Servlet时，才会使用NioSocketWrapper.write的non-blocking方式，普通的Servlet都是使用blocking方式的write。\nNioEndpoint在实现non-blocking的write时和一般的NIO框架类似，那它是如何实现blocking方式的write呢？\nTomcat的NIO connector有一个配置参数selectorPool.shared。selectorPool.shared的缺省值为true，这时会创建一个运行在独立线程中BlockPoller。调用者在发起blocking write时，会将SocketChannel注册到这个BlockPoller中，然后await在一个CountDownLatch上。当BlockPoller检测到准备就绪的SocketChannel，会通过关联的CountDownLatch唤醒被阻塞的调用者。这时调用者尝试往SocketChannel中写入，如果buffer中还有剩余数据，那么会再把SocketChannel注册回BlockPoller，并继续await，重复前面的过程，直到数据完全写出，最后调用者从blocking的write方法返回。\n当设置selectorPool.shared为false时，NioEndpoint会为每个发起blocking write的线程创建一个Selector，执行和上面类似的过程。当然NioEndpoint会使用NioSelectorPool来缓存Selector，并不是每次都创建一个新的Selector。NioSelectorPool中缓存的Selector的最大数量由selectorPool.maxSelectors参数控制。\n至此，相信你对NioEndpoint的内部实现已经有了整体的了解。\n","date":"2019-11-23T17:29:34+08:00","permalink":"https://mazhen.tech/p/tomcat%E7%9A%84nioendpoint%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/","title":"Tomcat的NioEndpoint实现分析"},{"content":" # Tomcat系统架构图 从架构图可以看出，顶层组件Server代表一个Tomcat Server实例，一个Server中有一个或者多个Service，每个Service有多个Connector，以及一个Engine。\nConnector和Engine是Tomcat最核心的两个组件。\nConnector负责处理网络通信，以及应用层协议(HTTP，AJP)的解析，生成标准的ServletRequest和ServletResponse对象，然后传递给Engine处理。每个Connector监听不同的网络端口。\nEngine代表整个Servlet引擎，可以包含多个Host，表示它可以管理多个虚拟站点。Host代表的是一个虚拟主机，而一个虚拟主机下可以部署多个Web应用程序，Context表示一个Web应用程序。Wrapper表示一个Servlet，一个Web应用程序中可能会有多个Servlet。\n从Tomcat的配置文件server.xml也能看出Tomcat的系统架构设计。\n1 2 3 4 5 6 7 8 9 10 \u0026lt;Server\u0026gt; \u0026lt;Service\u0026gt; \u0026lt;Connector /\u0026gt; \u0026lt;Connector /\u0026gt; \u0026lt;Engine\u0026gt; \u0026lt;Host\u0026gt; \u0026lt;/Host\u0026gt; \u0026lt;/Engine\u0026gt; \u0026lt;/Service\u0026gt; \u0026lt;/Server\u0026gt; # Connector 我们再仔细看一下Connector的内部实现。\nEndpoint 负责网络通信 Processor 实现应用层协议(HTTP，AJP)解析 Adapter 将Tomcat的Request/Response转换为标准的ServletRequest/ServletResponse Tomcat的网络通信层支持多种 I/O 模型：\nNIO：使用Java NIO实现 NIO.2：异步I/O，使用JDK NIO.2实现 APR：使用了Apache Portable Runtime (APR)实现 Tomcat实现支持了多种应用层协议：\nHTTP/1.1 HTTP/2 AJP：二进制协议，Web Server和Tomcat之间的通信协议 Processor解析网络字节流生成Tomcat的Request对象后，会调用Adapter.service(request, response)方法。Adapter是Servlet引擎的入口，Adapter负责将Tomcat的Request对象转换为标准的ServletRequest，然后再调用Servlet引擎的service方法。\n# ProtocolHandler Tomcat允许一个Engine对接多个Connector，每个Connector可以使用不同的 I/O 模型，实现不同的应用层协议解析。Connector屏蔽了 I/O 模型和协议的区别，传递给Engine的是标准的ServletRequest/ServletResponse对象。\n由于 I/O 模型和应用层协议解析可以自由组合，Tomcat使用ProtocolHandler实现这种组合。各种组合都有相应的具体实现类。比如：Http11NioProtocol 和 AjpNio2Protocol。\n关于NioEndpoint和Nio2Endpoint组件的内部实现，会在后续文章进行分析。\n","date":"2019-11-21T17:22:56+08:00","permalink":"https://mazhen.tech/p/tomcat%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/","title":"Tomcat系统架构简介"},{"content":"程序员的工作需要长期坐在电脑前，可能还会经常熬夜，作息时间不规律，所以码农是身体素质比较差的一群人。因此程序员健身锻炼的首要目标，不是为了有好看的身材，而是让你精力充沛，面对高强度工作游刃有余，同时还有精力去享受生活，这才是最关键的。\n# 心肺系统的重要性 这一切的基础是你必须要有一个好的心血管系统。心脏和全身的血管组成了心血管系统。通过持续不间断地跳动，心脏把富含氧气的血液不断输送到人体各个部位，并且将各个部位产生的废物和有害物质带到相应的排泄器官排出体外，这才维持起人体的各项机能。\n心血管系统就好比汽车的发动机，没了发动机，其他零件再好也没有用，这个发动机是最重要的。\n如果通过运动，让心肌得到有效的锻炼，那受益的是整个心血管系统。至于体重的減轻、体型的变化，这些都是随时而来的副产品。\n# 自测心肺功能水平 那么如何锻炼才能有效的增强心血管系统呢？\n首先我们要知道自己当前心血管系统的水平如何。一个专业的指标叫做最大摄氧量(VO2 max)。这个指标可以看出你现在心血管和心肺功能的水平。简单来说，最大摄氧量就是你在运动中能获取的最大氧气量。这个指标越高，说明你的心血管系统、心肺功能越好。\n对一个正常成年人，男性这个指标达到40，女性达到36才算是及格。普通人54以上可以算是优秀，而职业长跑运动员，最大摄氧量指标能达到88以上。\n如何测量自己的最大摄氧量呢？一般专业的心率表都会有这个指标。例如Apple watch可以通过你的体能训练，估算出最大摄氧量。\niOS系统自带的“健康”App里能查看到这个指标。\n# 合适强度的运动改善心肺功能 了解了自己当前的心肺功能水平后，如何提高改善你的心肺功能呢？需要选择合适的运动强度，不能一上来就强度过大。如果平时缺乏锻炼，心肺功能不达标，高强度的训练是比较危险的。\n建议使用“卡氏公式”计算一下你合适的运动心率区间。这个心率区间是和你的年龄，以及早上起来的静态心率状况有关。\n适合心肺功能训练的卡氏公式\n1 2 3 心肺训练心率 = (220 - 年龄 − 静态心率)×(55% ~ 65%) + 静态心率 如果是刚开始健身，比较推荐你在跑步机上进行走路。因为跑步机的速度是恒定的，把跑步机调成上坡的时候，你会发现很容易达到你想要达到的心率，只要在这个心率范围之内，就是你最合适的运动区间。随着你的心肺能力不断提高，你会逐渐提高坡度和速度。\n# 如何有效减脂 健身锻炼除了能充沛精力，另外一个大家关心的问题是，如何通过运动有效的减脂呢？\n实际上，饮食才是最有效的控制体重的方法。对于想减脂的人来说，最好的办法就是控制好你糖和脂肪的摄入，然后多吃一些蛋白质类食物，这样你会有足够的饱腹感。\n此外，可以做一些低强度运动，身体会消耗更多的脂肪。\n为什么低强度的运动能消耗脂肪呢？因为脂肪的消耗是需要氧气参与，运动过程中必须有充足的氧气才能消耗脂肪，因此这个运动强度应该比心肺训练的强度更低。\n减脂的运动强度就是卡氏公式的35%到55%，在这个强度运动是消耗脂肪最多的。\n适合减脂训练的卡氏公式\n1 2 3 减脂训练心率 = (220 - 年龄 − 静态心率)×(35% ~ 55%) + 静态心率 最后，男性的健康体脂率应该是15%到20%，女性的健康体脂率是20%到25%。推荐买一个体脂秤测试跟踪自己的体脂率变化。\n","date":"2019-11-11T10:36:27+08:00","permalink":"https://mazhen.tech/p/%E7%BB%99%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E5%81%A5%E8%BA%AB%E9%94%BB%E7%82%BC%E6%8C%87%E5%8D%97/","title":"给程序员的健身锻炼指南"},{"content":" # Outline 回顾HTTP的发展简史，理解HTTP在设计上的关键转变，以及每次转变的动机\nHTTP简史 HTTP/1.1的主要特性和问题 HTTP/2 的核心概念、主要特性 HTTP/2 的升级与发现 HTTP/2 的问题及展望 # HTTP简史 HTTP(HyperText Transfer Protocol，超文本传输协议)是互联网上最普遍采用的一种应用协议 由欧洲核子研究委员会CERN的英国工程师Tim Berners-Lee在1991年发明 Tim Berners-Lee也是WWW的发明者 # HTTP简史 HTTP/0.9：只有一行的协议 请求只有一行，包括GET方法和要请求的文档的路径 响应是一个超文本文档，没有首部，也没有其他元数据，只有HTML 服务器与客户端之间的连接在每次请求之后都会关闭 HTTP/0.9的设计目标传递超文本文档 # HTTP简史 HTTP/0.9演示 1 2 3 4 5 6 7 8 9 10 11 $\u0026gt; telnet apache.org 80 Trying 95.216.24.32... Connected to apache.org. Escape character is \u0026#39;^]\u0026#39;. GET /foundation/ \u0026lt;!DOCTYPE html\u0026gt; ... Connection closed by foreign host. # HTTP简史 1996年HTTP工作组发布了RFC 1945，这就是HTTP/1.0 提供请求和响应的各种元数据 不局限于超文本的传输，响应可以是任何类型：HTML文件、图片、音频等 支持内容协商、内容编码、字符集、认证、缓存等 从超文本到超媒体传输 # HTTP简史 HTTP/1.0演示 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $\u0026gt; telnet apache.org 80 Trying 95.216.24.32... Connected to apache.org. GET /foundation/ HTTP/1.0 Accept: */* HTTP/1.1 200 OK Server: Apache/2.4.18 (Ubuntu) Content-Length: 46012 Connection: close Content-Type: text/html \u0026lt;!DOCTYPE html\u0026gt; ... Connection closed by foreign host. # HTTP简史 1997年1月定义HTTP/1.1标准的RFC 2068发布 1999年6月RFC 2616发布，取代了RFC 2068 性能优化 持久连接 除非明确告知，默认使用持久连接 分块编码传输 请求管道，支持并行请求处理（应用的非常有限） 增强的缓存机制 # HTTP简史 HTTP/1.1演示 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt;$ telnet www.baidu.com 80 Trying 14.215.177.38... Connected to www.a.shifen.com. GET /s?wd=http2 HTTP/1.1 Accept: text/html,application/xhtml+xml,application/xml Accept-Language: zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7 Connection: keep-alive Host: www.baidu.com HTTP/1.1 200 OK Connection: Keep-Alive Content-Type: text/html;charset=utf-8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Date: Sun, 06 Oct 2019 12:49:28 GMT Server: BWS/1.1 Transfer-Encoding: chunked ffa \u0026lt;!DOCTYPE html\u0026gt; ... 1be7 ... 0 GET /img/bd_logo1.png HTTP/1.1 Accept: text/html,application/xhtml+xml,application/xml Accept-Language: zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7 Connection: close Host: www.baidu.com HTTP/1.1 200 OK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Content-Length: 7877 Content-Type: image/png Date: Sun, 06 Oct 2019 13:05:06 GMT Etag: \u0026#34;1ec5-502264e2ae4c0\u0026#34; Expires: Wed, 03 Oct 2029 13:05:06 GMT Last-Modified: Wed, 03 Sep 2014 10:00:27 GMT Server: Apache Set-Cookie: BAIDUID=0D01C3C9C00A6019C16F79CAEB1EFE91:FG=1 Connection: close . . . . . . Connection closed by foreign host. # HTTP简史 Google在2009年发布了实验性协议SPDY，主要目标是解决HTTP/1.1的性能限制 Google工程师在09年11月分享了实验结果 A 2x Faster Web So far we have only tested SPDY in lab conditions. The initial results are very encouraging: when we download the top 25 websites over simulated home network connections, we see a significant improvement in performance - pages loaded up to 55% faster.\n2012年，SPDY得到Chrome、Firefox和Opera的支持 HTTP-WG(HTTP Working Group)开始在SPDY的基础上制定官方标准 # HTTP简史 2015年正式发布HTTP/2 主要目标：改进传输性能，降低延迟，提高吞吐量 保持原有的高层协议语义不变 根据W3Techs的报告，截止2019年11月，全球已经有 42.1% 的网站开启了HTTP/2 # HTTP简史 Google在2012年设计开发了QUIC协议，让HTTP不再基于TCP 2018年底，HTTP/3标准发布 HTTP/3协议业务逻辑变化不大，可以简单理解为 HTTP/2 + QUIC # HTTP/1.1 持久连接 # HTTP/1.1 持久连接 # HTTP/1.1 持久连接 非持久HTTP连接的固定时间成本 至少两次网络往返： 握手、请求和响应 服务处理速度越快，固定延迟的影响就越大 持久连接避免TCP连接时的三次握手，消除TCP的慢启动 # HTTP/1.1 管道 多次请求必须满足先进先出(FIFO)的顺序 # HTTP/1.1 管道 尽早发送请求，不被每次响应阻塞 # HTTP/1.1 管道 HTTP/1.1的局限性 只能严格串行地返回响应，不允许一个连接上的多个响应交错到达 管道的问题 并行处理请求时，服务器必须缓冲管道中的响应，占用服务器资源 由于失败可能导致重复处理，非幂等的方法不能pipeline化 由于中间代理的兼容性，可能会破坏管道 管道的应用非常有限 # HTTP/1.1 的协议开销 每个HTTP请求都会携带500~800字节的header 如果使用了cookie，每个HTTP请求会增加几千字节的协议开销 HTTP header以纯文本形式发送，不会进行任何压缩 某些时候HTTP header开销会超过实际传输的数据一个数量级 例如访问RESTful API时返回JSON格式的响应 # HTTP/1.1性能优化建议 由于HTTP/1.1不支持多路复用 浏览器支持每个主机打开多个连接（例如Chrome是6个） 应用使用多域名，将资源分散到多个子域名 浏览器连接限制针对的是主机名，不是IP地址 缺点 消耗客户端和服务器资源 域名分区增加了额外的DNS查询 避免不了TCP的慢启动 # HTTP/1.1性能优化建议 使用多域名分区 # HTTP/1.1性能优化建议 减少请求次数 把多个JavaScript或CSS组合为一个文件 把多张图片组合为一个更大的复合的图片 inlining内联，将图片嵌入到CSS或者HTML文件中，减少网络请求次数 增加应用的复杂度，导致缓存、更新等问题，只是权宜之计\n# HTTP/2 的目标 性能优化 支持请求与响应的多路复用 支持请求优先级和流量控制 支持服务器端推送 压缩HTTP header降低协议开销 HTTP的语义不变 HTTP方法、header、状态码、URI # HTTP/2 二进制分帧层 引入新的二进制分帧数据层 将传输的信息分割为消息和帧，并采用二进制格式的编码 # HTTP/2 的核心概念 流(Stream) 已建立的连接上的双向字节流 该字节流可以携带一个或多个消息 消息(Message) 与请求/响应消息对应的一系列完整的数据帧 帧(Frame) 通信的最小单位 每个帧包含帧首部，标识出当前帧所属的流 # HTTP/2 的核心概念 # HTTP/2 的核心概念 所有HTTP/2通信都在一个TCP连接上完成 流是连接中的一个虚拟信道，可以承载双向的消息 一个连接可以承载任意数量的流，每个流都有一个唯一的整数标识符(1、2\u0026hellip;N) 消息是指逻辑上的HTTP消息，比如请求、响应等 消息由一或多个帧组成，这些帧可以交错发送，然后根据每个帧首部的流标识符重新组装 # HTTP/2请求与响应的多路复用 HTTP/1.x中，如果客户端想发送多个并行的请求，那么必须使用多个TCP连接 HTTP/2中，客户端可以使用多个流发送请求，同时HTTP消息被分解为互不依赖的帧，交错传输，最后在另一端重新组装 # HTTP/2 帧格式 详细说明请参考HTTP/2规范 # HTTP/2 帧类型 客户端通过HEADERS帧来发起新的流 服务器通过PUSH_PROMISE帧来发起推送流 帧类型 类型编码 用途 DATA 0x0 传输HTTP消息体 HEADERS 0x1 传输HTTP头部 PRIORITY 0x2 指定流的优先级 RST_STREAM 0x3 通知流的非正常 SETTINGS 0x4 修改连接或者流的配置 PUSH_PROMISE 0x5 服务端推送资源时的请求帧 PING 0x6 心跳检测，计算RTT往返时间 GOAWAY 0x7 优雅的终止连接，或者通知错误 WINDOW_UPDATE 0x8 针对流或者连接，实现流量控制 CONTINUATION 0x9 传递较大HTTP头部时的持续帧 # HTTP/2 请求优先级 HTTP/2允许每个流关联一个31bit的优先值 0 最高优先级 2^31 -1 最低优先级 浏览器会基于资源的类型、在页面中的位置等因素，决定请求的优先次序 服务器可以根据流的优先级，控制资源分配，优先将高优先级的帧发送给客户端 HTTP/2没有规定具体的优先级算法 # HTTP/2 流量控制 流量控制有方向性，即接收方可能根据自己的情况为每个流，乃至整个连接设置任意窗口大小 连接建立后，客户端与服务器交换SETTINGS帧，设置 双向的流量控制窗口大小 流量控制窗口大小通过WINDOW_UPDATE帧更新 HTTP/2流量控制和TCP流量控制的机制相同，但TCP流量控制不能对同一个连接内的多个流实施差异化策略 # HTTP/2 服务器端推送 服务器可以对一个客户端请求发送多个响应 服务器通过发送PUSH_PROMISE帧来发起推送流 客户端可以使用HTTP header向服务器发送信号，列出它希望推送的资源 服务器可以智能分析客户端的需求，自动推送关键资源 # HTTP header压缩 HTTP/2使用HPACK压缩格式压缩请求/响应头 通过静态霍夫曼码对发送的header字段进行编码，减小了它们的传输大小 客户端和服务器使用索引表来维护和更新header字段。对于相同的数据，不再重复发送 # HTTP header压缩 # HTTP/2 vs HTTP/1.1 https://http2.akamai.com/demo\n# HTTP/2的升级与发现 HTTP/1.x还将长期存在，客户端和服务器必须同时支持1.x和2.0 客户端和服务器在开始交换数据前，必须发现和协商使用哪个版本的协议进行通信 HTTP/2定义了两种协商机制 通过安全连接TLS和ALPN进行协商 基于TCP连接的协商机制 # HTTP/2的升级与发现 HTTP/2标准不要求必须基于TLS，但浏览器要求必须基于TLS Web上存在大量的代理和中间设备：缓存服务器、安全网关、加速器等等 如果任何中间设备不支持，连接都不会成功 建立TLS信道，端到端加密传输，绕过中间代理，实现可靠的部署 新协议一般都要依赖于建立TLS信道，例如WebSocket、SPDY # h2和h2c升级协商机制 基于TLS运行的HTTP/2被称为h2 直接在TCP之上运行的HTTP/2被称为h2c # h2c演示环境 客户端测试工具 curl (\u0026gt; 7.46.0) 服务器端 Tomcat 9.x 1 2 3 4 5 6 \u0026lt;Connector port=\u0026#34;8080\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34; connectionTimeout=\u0026#34;20000\u0026#34; redirectPort=\u0026#34;8443\u0026#34; \u0026gt; \u0026lt;UpgradeProtocol className=\u0026#34;org.apache.coyote.http2.Http2Protocol\u0026#34;/\u0026gt; \u0026lt;/Connector\u0026gt; # h2c协议升级 curl http://localhost:8080 --http2 -v 1 2 3 4 5 6 7 8 9 10 11 \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8080 \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; Connection: Upgrade, HTTP2-Settings \u0026gt; Upgrade: h2c \u0026gt; HTTP2-Settings: AAMAAABkAARAAAAAAAIAAAAA \u0026lt; HTTP/1.1 101 \u0026lt; Connection: Upgrade \u0026lt; Upgrade: h2c # HTTP/2连接建立 # HTTP/2连接建立 Magic帧 ASCII 编码，12字节 何时发送? 接收到服务器发送来的 101 Switching Protocols后 TLS 握手成功后 Preface 内容 # HTTP/2连接建立 交换settings帧(client -\u0026gt; server) # HTTP/2连接建立 交换settings帧(server -\u0026gt; client) # HTTP/2连接建立 settings ACK 帧 (client \u0026lt;-\u0026gt; server) # TLS协议的设计目标 保密性 完整性 身份验证 # TLS发展史 1994年，NetScape 设计了SSL协议(Secure Sockets Layer) 1.0，未正式发布 1995年，NetScape 发布 SSL 2.0 1996年，发布SSL 3.0 1999年，IETF标准化了SSL协议，更名为TLS(Transport Layer Security)，发布TLS 1.0 2006年4月，IETF 工作组发布了TLS 1.1 2008年8月，IETF 工作组发布了TLS 1.2 2018年8月，TLS 1.3正式发布 # TLS 1.2 握手过程 验证身份 达成安全套件共识 传递密钥 加密通讯 非对称加密只在建立TLS信道时使用，之后的通信使用握手时生成的共享密钥加密 # TLS 安全密码套件 密钥交换算法 双方在完全没有对方任何预先信息，通过不安全信道创建密钥 1976年，Diffie–Hellman key exchange，简称 DH 基于椭圆曲线(Elliptic Curve)升级DH协议，ECDHE 身份验证算法 非对称加密算法，Public Key Infrastructure(PKI) 对称加密算法、强度、工作模式 工作模式：将明文分成多个等长的Block模块，对每个模块分别加解密 hash签名算法 # TLS1.3的握手优化 An Overview of TLS 1.3 – Faster and More Secure # 测试TLS的支持情况 https://www.ssllabs.com/ssltest/index.html # Application-Layer Protocol Negotiation 基于TLS运行的HTTP/2使用ALPN扩展做协议协商\n客户端在ClientHello消息中增加ProtocolNameList字段，包含自己支持的应用协议 服务器检查ProtocolNameList字段，在ServerHello消息中以ProtocolName字段返回选中的协议 在TLS握手的同时协商应用协议，省掉了HTTP的Upgrade机制所需的额外往返时间\n# ALPN # h2演示环境 客户端：浏览器 服务器端：Tomcat 9.x Tomcat提供了三种不同的TLS实现 Java运行时提供的JSSE实现 使用了OpenSSL的JSSE实现 APR实现，默认情况下使用OpenSSL引擎 # Tomcat三种TLS实现 JSSE 非常慢 ALPN是因为HTTP/2才在2014年出现，JDK8不支持ALPN OpenSSL实现 只使用了OpenSSL，没有使用其他本地代码(native socket, poller等) 可以配合 NIO 和 NIO2 APR 大量的native code TLS同样使用了OpenSSL # TLS实现的性能对比 OpenSSL性能比纯Java实现好很多；使用TLS可以不再需要APR Linux上NIO.2是通过epoll来模拟实现的EPollPort.java # 使用JSSE 生成private key和自签名证书 keytool -genkey -alias tomcat -keyalg RSA 配置server.xml 1 2 3 4 5 6 7 8 9 10 11 \u0026lt;Connector protocol=\u0026#34;org.apache.coyote.http11.Http11NioProtocol\u0026#34; port=\u0026#34;8443\u0026#34; maxThreads=\u0026#34;200\u0026#34; sslImplementationName= \u0026#34;org.apache.tomcat.util.net.jsse.JSSEImplementation\u0026#34; scheme=\u0026#34;https\u0026#34; secure=\u0026#34;true\u0026#34; SSLEnabled=\u0026#34;true\u0026#34; keystoreFile=\u0026#34;${user.home}/.keystore\u0026#34; keystorePass=\u0026#34;changeit\u0026#34; clientAuth=\u0026#34;false\u0026#34; sslProtocol=\u0026#34;TLS\u0026#34;\u0026gt; \u0026lt;UpgradeProtocol className=\u0026#34;org.apache.coyote.http2.Http2Protocol\u0026#34; /\u0026gt; \u0026lt;/Connector\u0026gt; # 使用JSSE JDK8不支持ALPN 1 2 3 4 5 严重 [main] org.apache.coyote.http11.AbstractHttp11Protocol.configureUpgradeProtocol The upgrade handler [org.apache.coyote.http2.Http2Protocol] for [h2] only supports upgrade via ALPN but has been configured for the [\u0026#34;https-jsse-nio-8443\u0026#34;] connector that does not support ALPN. JDK11 1 2 3 4 信息 [main] org.apache.coyote.http11.AbstractHttp11Protocol.configureUpgradeProtocol The [\u0026#34;https-jsse-nio-8443\u0026#34;] connector has been configured to support negotiation to [h2] via ALPN # 使用OpenSSL 安装tomcat-native brew install tomcat-native 配置$CATALINA_HOME/bin/setenv.sh 1 CATALINA_OPTS=\u0026#34;$CATALINA_OPTS -Djava.library.path=/usr/local/opt/tomcat-native/lib\u0026#34; 配置server.xml 1 2 3 4 5 6 \u0026lt;Connector protocol=\u0026#34;org.apache.coyote.http11.Http11NioProtocol\u0026#34; sslImplementationName= \u0026#34;org.apache.tomcat.util.net.openssl.OpenSSLImplementation\u0026#34; ... \u0026gt; \u0026lt;/Connector\u0026gt; # 使用OpenSSL JDK8 \u0026amp; JDK11 1 2 3 4 5 6 7 8 9 10 信息 [main] org.apache.coyote.http11.AbstractHttp11Protocol.configureUpgradeProtocol The [\u0026#34;https-openssl-nio-8443\u0026#34;] connector has been configured to support negotiation to [h2] via ALPN ... 信息 [main] org.apache.coyote.AbstractProtocol.start 开始协议处理句柄 [\u0026#34;https-openssl-nio-8443\u0026#34;] # ALPN协议协商 ClientHello # ALPN协议协商 ServerHello # 使用Chrome开发者工具观察 # HTTP/2的问题 HTTP/2消除了HTTP协议的队首阻塞现象，但TCP层面上仍然存在队首阻塞 HTTP/2多请求复用一个TCP连接，丢包可能会block住所有的HTTP请求 # HTTP/2的问题 TCP及TCP+TLS建立连接需要多次round trips # QUIC Quick UDP Internet Connections 由Goolge开发，并已经在Google部署使用 # QUIC QUIC: next generation multiplexed transport over UDP\n# 参考资料 High Performance Browser Networking HTTP的前世今生 HTTP/3 的过去、现在和未来 HTTP/2协议 ","date":"2019-10-07T09:59:29+08:00","permalink":"https://mazhen.tech/p/http/2-%E7%AE%80%E4%BB%8B/","title":"HTTP/2 简介"},{"content":" # 安装helm客户端 在macOS上安装很简单：\n1 brew install kubernetes-helm 其他平台请参考Installing Helm\n# 配置RBAC 定义rbac-config.yaml文件，创建tiller账号，并和cluster-admin绑定：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system 执行命令：\n1 2 3 $ kubectl create -f rbac-config.yaml serviceaccount \u0026#34;tiller\u0026#34; created clusterrolebinding \u0026#34;tiller\u0026#34; created # 安装Tiller镜像 在强国环境内，需要参考kubernetes-for-china，将helm服务端部分Tiller的镜像下载到集群节点上。\n# 初始化helm 执行初始化命令，注意指定上一步创建的ServiceAccount：\n1 helm init --service-account tiller --history-max 200 命令执行成功，会在集群中安装helm的服务端部分Tiller。可以使用kubectl get pods -n kube-system命令查看：\n1 2 3 4 5 $kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE ... tiller-deploy-7fbf5fc745-lxzxl 1/1 Running 0 179m # Quickstart 增加Chart Repository（可选） 查看helm的Chart Repository：\n1 2 3 4 5 $ helm repo list NAME URL stable https://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts 如果你所处的网络环境无法访问缺省的Chart Repository，可以更换为其他repo，例如微软提供的 helm 仓库的镜像：\n1 2 3 4 5 $ helm repo add stable http://mirror.azure.cn/kubernetes/charts/ \u0026#34;stable\u0026#34; has been added to your repositories $ helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/ \u0026#34;incubator\u0026#34; has been added to your repositories 所有可用chart列表： 1 2 helm repo update helm search 搜索tomcat chart： 1 helm search tomcat 查看stable/tomcat的详细信息 1 helm inspect stable/tomcat stable/tomcat使用 sidecar 方式部署web应用，通过参数image.webarchive.repository指定war的镜像，不指定会部署缺省的sample应用。\n安装tomcat： 如果是在私有化集群部署，设置service.type为NodePort：\n1 helm install --name my-web --set service.type=NodePort stable/tomcat 测试安装效果 1 2 3 4 5 6 export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services my-web-tomcat) export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo http://$NODE_IP:$NODE_PORT # 访问sample应用 curl http://$NODE_IP:$NODE_PORT/sample/ 列表和删除 1 2 helm list helm del --purge my-web ","date":"2019-06-11T09:36:01+08:00","permalink":"https://mazhen.tech/p/helm%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","title":"Helm的安装和使用"},{"content":"Kubernetes 相关的docker镜像存放在Google的镜像仓库 k8s.gcr.io，在国内网络环境内无法访问。有人已经将这些镜像同步到了阿里云，你可以在阿里云容器镜像服务中搜索到它们。几乎所有的k8s镜像都已经同步到了阿里云。阿里云容器服务团队甚至还有一个开源项目sync-repo，专门做Docker Registry之间的同步。但是，如果你不放心别人同步的镜像，或者最新版本的镜像还没人同步过来，你可以按照本文将介绍的步骤，自己将gcr.io上的docker镜像搬到阿里云。\n# 安装配置shadowsocks客户端 先简单介绍下Shadowsocks协议，详细的工作原理可以参考这篇博客：\n当我们启动shadowsocks client时，实际上是启动了一个 ss-local 进程，左侧绿色的 Socks5 Client 可以是浏览器，也可以是Telegram等本地应用，它们和ss-local之间是使用 socks 协议进行通信。也就是说，浏览器像连接普通 socks 代理一样，连接到ss-local进程。ss-local 会将收到的请求，转发给ss-server，由ss-server完成实际的访问，并将结果通过ss-local返回给浏览器。ss-server部署在国内网络之外，和ss-local之间是加密传输，这样就实现了跨越长城。其实防火长城已经能够识别Shadowsocks协议，但发现我们是在努力学习先进技术，就先放我们过关。\n好了，我们现在首先要做的是在本机安装配置shadowsocks客户端。推荐使用shadowsocks-libev，纯C实现的shadowsocks协议，已经在很多操作系统的官方repository中 ，安装非常方便。\nmacOS 1 brew install shadowsocks-libev Ubuntu 1 sudo apt install shadowsocks-libev 接下来填写shadowsocks client配置文件，JSON格式，简单易懂：\n1 2 3 4 5 6 7 8 { \u0026#34;server\u0026#34;:\u0026#34;ss服务器IP\u0026#34;, \u0026#34;server_port\u0026#34;:443, // ss服务器port \u0026#34;local_port\u0026#34;:1080, // 本地监听端口 \u0026#34;password\u0026#34;:\u0026#34;xxxx\u0026#34;, // ss服务器密码 \u0026#34;timeout\u0026#34;:600, \u0026#34;method\u0026#34;:\u0026#34;aes-256-cfb\u0026#34; // ss服务器加密方法 } 至于shadowsocks服务器端，可以租用国内网络外的云主机自己搭建，也可以购买现成的机场服务，本文就不讨论了。\n然后启动shadowsocks client：\n1 nohup ss-local -c ss-client.conf \u0026amp; # 安装配置HTTP代理 shadowsocks client创建的是socks5代理，不过一些程序无法使用 socks5 ，它们需要通过 http_proxy 和 https_proxy 环境变量，使用 HTTP 代理。polipo 可以帮助我们将 socks5 代理转换为 HTTP 代理。\nmacOS下安装polipo 1 brew install polipo Ubuntu下安装polipo 1 2 3 4 5 sudo apt install polipo # 建议停掉polipo服务，需要的时候自己启动 sudo systemctl stop polipo.service sudo systemctl disable polipo.service 启动HTTP代理\n1 sudo polipo socksParentProxy=127.0.0.1:1080 proxyPort=1087 socksParentProxy配置为localhost和ss-local监听端口，proxyPort是启动的HTTP代理端口。\n我们可以在命令行终端测试HTTP代理的效果：\n1 2 3 $ export http_proxy=http://localhost:1087 $ export https_proxy=http://localhost:1087 $ curl https://www.google.com 应该可以正常访问到Google。\n# 设置Docker HTTP代理 如果是在macOS上使用Docker Desctop，可以在Preference中的Proxies设置上一步启动的HTTP代理：\n如果是Linux平台，请参考Docker的官方文档进行设置。\n# 在阿里云创建容器镜像的命名空间 为了将镜像同步到阿里云，首先需要在阿里云的容器镜像服务控制台创建镜像的命名空间。\n建议将仓库类型设置为“公开”，这样其他人也能搜索、下载到镜像。\n# 从gcr.io下载镜像 在本机从gcr.io下载镜像，我们以镜像pause:3.1为例：\n1 docker pull k8s.gcr.io/pause:3.1 # 给镜像标记新的tag 根据前面在阿里云创建的命名空间，给镜像标记新的tag：\n1 docker tag k8s.gcr.io/pause:3.1 registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 mz-k8s是在前面创建的命名空间。 查看tag结果：\n1 2 3 4 5 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/pause 3.1 da86e6ba6ca1 17 months ago 742kB registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause 3.1 da86e6ba6ca1 17 months ago 742kB 通过IMAGE ID可以看出，两个镜像为同一个。\n# 将镜像上传到阿里云 登录阿里云镜像仓库：\n1 $ docker login --username=(阿里云账号) registry.cn-shenzhen.aliyuncs.com 根据提示输入password，登录成功后，显示Login Succeeded。\n上传镜像：\n1 docker push registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 # 从阿里云下载镜像 现在可以在其他机器上从阿里云下载pause:3.1镜像，这时候已经不需要科学上网了：\n1 $ docker pull registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 给镜像打上原来的tag，这样kubeadm等工具就可以使用本地仓库中的pause:3.1镜像了：\n1 $ docker tag registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 k8s.gcr.io/pause:3.1 至此，我们跨越长城，将一个docker镜像从gcr.io搬到了Aliyun。\n如果是需要批量、定时的从gcr.io同步镜像，建议考虑使用阿里开源的sync-repo。\n","date":"2019-06-06T09:53:24+08:00","permalink":"https://mazhen.tech/p/%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E5%B0%86%E8%B0%B7%E6%AD%8Ck8s%E9%95%9C%E5%83%8F%E5%90%8C%E6%AD%A5%E5%88%B0%E9%98%BF%E9%87%8C%E4%BA%91/","title":"自己动手将谷歌k8s镜像同步到阿里云"},{"content":"用户在访问Kubernetes集群的API server时，访问请求需要经过身份验证、授权和准入控制这三个阶段的检查，才能真正到达API服务，如下图所示：\nKubernetes中的用户有两种类型：service accounts 和 normal users。service accounts 由 Kubernetes管理，它是Pod中的进程用于访问API服务的account，为Pod中的进程提供了一种身份标识。normal users是由外部系统管理，在Kubernetes中并没有对应的 user 对象，它为人类用户使用kubectl之类的工具访问API服务时提供身份标识。所有用户，不管是使用 kubectl、客户端lib、还是直接发起REST请求访问API server，都需要经过上述三个步骤的检查。\n本文将介绍Kubernetes集群的身份验证，即Kubernetes如何确认来访者的身份。\nKubernetes支持多种方式的身份验证：客户端证书，Password， Plain Tokens，JWT(JSON Web Token)，HTTP basic auth等。你可以同时启用多种认证，一般建议至少使用两种：\n为验证normal users身份的客户端证书方式 为验证Service accounts身份的 JWT Tokens方式 # 使用客户端证书进行身份验证 # 理解数字证书 非对称加密算法是证书的基础。数字签名、数字证书等一系列概念有点绕，但只要记住：公钥用来加密，私钥用来签名 就可以了。\n怎么理解呢？公钥可以随意分发，谁都可以持有，如果你用私钥加密，任何持有对应公钥的人都可以解密，这样做和没加密一样，没什么意义。因此，我们需要用公钥加密，只有持有私钥的那个人才能解密。私钥之所以称为私钥，一定会私密保存，不会向其他人泄漏。同时，用私钥加密虽然没有意义，但如果别人用公钥解开了私钥加密的信息，就能够证明信息是由私钥持有者发出的，验证了信息发送者的身份，这就是数字签名。\n每个人制作好自己的公钥和私钥，然后把公钥发布出去。两个人如果都有对方的公钥，就可以用对方的公钥给对方发送加密信息，同时附上用私钥加密的信息摘要作为数字签名，证明消息发送者的身份。\n通过加密防止了窃听风险，通过数字签名防止了冒充风险，数字签名内的消息摘要防止了篡改风险，一起看似很完美。\n等等，这里有个很重要的问题被忽略了：如何安全的将公钥发布出去？如果双方希望安全通信，最好当面交换公钥，以免被别人冒充，并且要保护好自己的电脑，避免公钥被别有用心的人替换。现实中不可能这样分发公钥，效率太低，几乎无法大规模实行。于是出现了CA(certificate authority)，为公钥做认证。CA用自己的私钥，对申请用户的公钥和一些身份信息加密，生成\u0026quot;数字证书\u0026quot;（Digital Certificate）。你现在可以用任何方式将内含公钥的数字证书发布出去，例如有客户发起请求，希望以HTTPS的方式访问你的WEB服务，你可以在第一次回复客户的响应中带上数字证书。客户拿到你的数字证书，用CA的公钥解开数字证书，安全的获得你的公钥。有了CA为你的数字证书背书，客户可以确定你的身份，不是有人在冒充你。\n那么CA的公钥如何安全的分发呢？首先，证书的签发是“链”式结构，给你签发证书的CA，它的证书可能还是由上一级CA机构签发的，这样一直往上追溯，最终会到某个“根证书”。如果“根证书”是被我们信任的，那么整条“链”上的证书都可信。\n其次，操作系统都内置了“受信任的根证书”。我们拿到某个证书，如果它的根证书在系统的“受信任的根证书”列表中，那么这个证书就是可信的。例如知乎的证书：\n可以看到，它的根证书是DigiCert Global Root CA，在操作系统的“受信任的根证书”列表中能找到它：\n根证书是通过预装的方式完成的分发，因此安装来源不明的操作系统有风险，可能潜伏了非法的根证书。一旦被植入了非法的根证书，一整套的安全体系瞬间土崩瓦解。同时，不能随意向系统中添加可信任的根证书，你很难验证根证书的真伪，它已经是root，没人能为它做背书了。12306网站早期的根证书就不在操作系统的“受信任根证书”列表中，需要用户手工安装，在网上引起轩然大波。最终12306在17年底的时候换成了Digicert的证书。\n简单总结一下基于非对称加密算法的公钥/私钥体系，公钥用来加密，私钥用来签名，引入CA保证公钥的安全分发。你可以找CA签发数字证书，那么你的客户就可以根据本地“受信任的根证书”验证你的数字证书，从而确认你的身份，然后用证书内包含的公钥给你发加密的信息。同样，你也可以要求对方的数字证书，以便确认对方的身份，并给他回加密的信息。\n理解了数字证书的基本原理，我们再看看Kubernetes中如何使用客户端证书进行身份验证。\n# 数字证书在Kubernetes中的应用 Kubernetes各组件之间的通信都是基于TLS，实现服务的加密访问，同时支持基于证书的双向认证。\n我们在搭建私有Kubernetes集群时，一般是自建root CA，因为参与认证的所有集群节点，包括远程访问集群的客户端桌面都完全由自己控制，我们可以安全的将根证书分发到所有节点。有了CA，我们再用CA的私钥/公钥为各个组件签发所需的证书。\nCA的创建，以及一系列客户端、服务端证书的签发，实际上是建立了Kubernetes集群的PKI(Public key infrastructure)。\nKubernetes中的组件比较多，所以需要的证书会非常多，这篇文档做了介绍。我按证书的用途归类总结一下：\nCA证书 Kubernetes 一般用途 etcd 集群根证书 aggregation 相关功能 服务端证书 API server etcd kubelet 访问API server时进行身份验证的客户端证书 kubelet　-\u0026gt; API server controller-manager -\u0026gt; API server kube-scheduler -\u0026gt; API server admin用户　-\u0026gt; API server API server 访问其他组件时进行身份验证的客户端证书 API server -\u0026gt; etcd API server -\u0026gt; kubelet API server -\u0026gt; aggregated API server etcd 相关功能 etcd 集群中节点互相通信使用的客户端证书 如果etcd是以Pod方式运行，针对etcd的 Liveness 需要的客户端证书 Service accounts 私钥/公钥对，用于生成Service accounts身份验证的 JWT Tokens 最后一个不是证书，不过也在Kubernetes PKI的管理范围。关于Service accounts 私钥/公钥对的作用，后面会讲到。\n理论上CA根证书可以只使用一个，不过为了安全和方便管理，官方强调在不同的上下文最好使用不同的CA：\nWarning: Do not reuse a CA that is used in a different context unless you understand the risks and the mechanisms to protect the CA’s usage.\n可以看出，API server是核心组件，其他组件、包括admin用户对它的访问都需要TLS双向认证，所以会有API server的服务端证书和各个组件的客户端证书。API server作为客户端需要访问etcd、kubelet和aggregated API server，所以也会有相应的服务端、客户端证书。\n当我们使用kubeadm安装Kubernetes时，kubeadm会为我们生成上述的一系列私钥和证书，放在/etc/kubernetes/目录下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # tree --dirsfirst /etc/kubernetes/ /etc/kubernetes/ ├── manifests 组件的配置文件，以Pod方式运行在集群中 │ ├── etcd.yaml │ ├── kube-apiserver.yaml │ ├── kube-controller-manager.yaml │ └── kube-scheduler.yaml ├── pki │ ├── etcd │ │ ├── ca.crt etcd 集群CA证书 │ │ ├── ca.key etcd 集群CA私钥 │ │ ├── healthcheck-client.crt Liveness 健康检查使用的客户端证书 │ │ ├── healthcheck-client.key │ │ ├── peer.crt etcd节点间通信使用的客户端证书 │ │ ├── peer.key │ │ ├── server.crt etcd服务端证书 │ │ └── server.key │ ├── apiserver.crt API Server 服务端证书 │ ├── apiserver.key │ ├── apiserver-etcd-client.crt API server -\u0026gt; etcd │ ├── apiserver-etcd-client.key │ ├── apiserver-kubelet-client.crt API server -\u0026gt; kubelet │ ├── apiserver-kubelet-client.key │ ├── ca.crt CA证书 │ ├── ca.key CA私钥 │ ├── front-proxy-ca.crt aggregation 相关功能CA证书 │ ├── front-proxy-ca.key aggregation 相关功能CA私钥 │ ├── front-proxy-client.crt API server -\u0026gt; aggregated API server │ ├── front-proxy-client.key │ ├── sa.key Service accounts 私钥 │ └── sa.pub Service accounts 公钥 ├── admin.conf admin -\u0026gt; API server ├── controller-manager.conf controller-manager -\u0026gt; API server ├── kubelet.conf kubelet　-\u0026gt; API server └── scheduler.conf kube-scheduler -\u0026gt; API server 注意，最后四个*.conf是kubeconfig file，内容包含了集群、用户、namespace等信息，还有用来认证的CA证书、客户端证书和私钥。例如admin.conf就是kubectl访问集群用到的kubeconfig file，缺省情况下kubectl会使用$HOME/.kube/config，你也可以通过KUBECONFIG环境变量，或kubectl 的 --kubeconfig 参数进行设置。\nkubelet运行在每个工作节点，无法提前预知 node 的 IP 信息，所以 kubelet 一般不会明确指定服务端证书, 而是只指定 CA 根证书, 让 kubelet 根据本机信息自动生成服务端证书，保存到配置参数指定的\u0026ndash;cert-dir目录中。cert-dir的缺省值是/var/lib/kubelet/pki。\n1 2 3 4 5 6 7 # tree /var/lib/kubelet/pki /var/lib/kubelet/pki ├── kubelet-client-2019-04-28-10-48-13.pem ├── kubelet-client-current.pem -\u0026gt; /var/lib/kubelet/pki/kubelet-client-2019-04-28-10-48-13.pem ├── kubelet.crt kubelet服务端证书 └── kubelet.key kubelet服务端私钥 另外，API server有很多认证相关的启动参数，参数名称让人容易混淆，有人还专门提了issue，这个回答根据用途对这些参数进行了分类，说明的非常清晰。\n# API server 如何用客户端证书进行身份验证 前面提到，当用户使用kubectl访问API server时，需要以某种方式进行身份验证，最常用的方式就是使用客户端证书。Kubernetes是没有 user 这种 API 对象，kubectl的用户身份信息就包含在客户端证书中。API server验证了客户端证书，也就可以从证书中获得用户名和所属的group。\n我们以/etc/kubernetes/admin.conf 为例，看看客户端证书中提供了那些信息。\n先查看admin.conf文件的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ kubectl --kubeconfig /etc/kubernetes/admin.conf config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://172.18.100.90:6443 name: test contexts: - context: cluster: test user: kubernetes-admin name: kubernetes-admin@test current-context: kubernetes-admin@test kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 这个文件提供了API server的地址，以及身份验证用到的证书：\ncertificate-authority-data ：CA证书 client-certificate-data ：客户端证书 client-key-data： 客户端私钥 客户端证书是以base64编码的方式保存在client-certificate-data字段中，我们将证书提取出来：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # cat /etc/kubernetes/admin.conf | grep client-certificate-data | cut -d \u0026#34; \u0026#34; -f 6 | base64 -d \u0026gt; admin.crt # cat admin.crt -----BEGIN CERTIFICATE----- MIIC8jCCAdqgAwIBAgIIaBuxevPYGaswDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE AxMKa3ViZXJuZXRlczAeFw0xOTA0MjgwMjQwMDBaFw0yMDA0MjcwMjQwMDNaMDQx FzAVBgNVBAoTDnN5c3RlbTptYXN0ZXJzMRkwFwYDVQQDExBrdWJlcm5ldGVzLWFk bWluMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvRTIQ4YEMh0mUWKP 17pLdUocgZMLVCK6tYmj0DJIihRk+wKvNzYSStfxsug9nnEqVVzmbW5/UxER776H 844y/1NGk/8LsDIkFGspf3cEmQ8OE8TlLNW7h9gWIGymLQ/K1qhYfNOPDYoJXPix eUWTJgn0+neJNbJ3JoJk2WRlDFwbE0uXgYYczuDcJablSdbb8Oc+E4qJ1U7u9YMN Bo/JBY68wYtdjXHl6Mg28aCioVZrs5eZWkNzNpXMVjQwFdZAdWbnS3OJGN1b6IrV gWk9PMoCE2TtFv5NdlHSYFtEAaBEwfl3/D3rGHKb4ZH/fgKWsepy8ffxxibM6pND pLnmAwIDAQABoycwJTAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0lBAwwCgYIKwYBBQUH AwIwDQYJKoZIhvcNAQELBQADggEBALPYorv2mlXyu6jpX/6gE1kTvpPGK4vylfSY 9jl4PtQZgRaXvmVsUKpyIdtVhdMZp9EFaYNC4AYkqaVEOoAbU96SYhdO/6h7Rn8T 0Ae+f1Vwt+8GxErEN3xp4noHfXM0eSEuFLPXt43BBJInYRyx1J0urAjYtNCvc9wX uQFVmNKsqgmjvHQsRkvKcb8HEzcaD1TqqnTpq3usGjNggVZFTChB58R909yGPEXL n7VsilmN86gom3fgqwCn2C00iKcuzCOwYN2T+Mi8KI2DraDDoVeRMSaYQNUfKNIX Ngeod/C4piq+OAdyrPPFEINdLi404EYHyod0CgiD6uhoX5W06O4= -----END CERTIFICATE----- 使用openssl查看证书内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # openssl x509 -in ./admin.crt -text Certificate: Data: Version: 3 (0x2) Serial Number: 7501784745950845355 (0x681bb17af3d819ab) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=kubernetes Validity Not Before: Apr 28 02:40:00 2019 GMT Not After : Apr 27 02:40:03 2020 GMT Subject: O=system:masters, CN=kubernetes-admin Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) ...... 注意这一行：\n1 Subject: O=system:masters, CN=kubernetes-admin API server会将证书中CN(Common Name)作为用户名，O(Organization)作为用户所属的group。API server从这个证书得到的信息是：admin用户所属的group是system:masters。至此，身份验证阶段完成。\n下一步是授权检查，也就是检查用户有没有权限执行这个操作。这是另外一个话题，本文不做详细讨论，只是简单介绍一下。根据官方文档，Kubernetes提供了缺省的 ClusterRole - group 绑定关系，已经将system:masters group 和 角色 cluster-admin绑定到了一起：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # kubectl get clusterrolebindings cluster-admin -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2019-04-28T02:40:17Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: cluster-admin resourceVersion: \u0026#34;98\u0026#34; selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin uid: f58a87a7-695e-11e9-91ca-005056ac1c1c roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:masters 这个绑定关系的意思是，属于system:mastersgroup的用户，都拥有cluster-admin角色包含的权限。我们再看看角色cluster-admin的具体权限信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # kubectl get clusterrole cluster-admin -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2019-04-28T02:40:17Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: cluster-admin resourceVersion: \u0026#34;44\u0026#34; selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-admin uid: f5523c21-695e-11e9-91ca-005056ac1c1c rules: - apiGroups: - \u0026#39;*\u0026#39; resources: - \u0026#39;*\u0026#39; verbs: - \u0026#39;*\u0026#39; - nonResourceURLs: - \u0026#39;*\u0026#39; verbs: - \u0026#39;*\u0026#39; 从rules列表可以看出，cluster-admin这个角色对所有资源都有无限制的操作权限。因此，使用了这个kubeconfig file的kubectl的请求就有了操控和管理整个集群的权限。\n# 使用JWT Tokens进行身份验证 运行在Pod中的进程需要访问API server时，同样需要进行身份验证和授权检查。如何让Pod具有用户身份呢？通过给Pod指定service account来实现。service account是Kubernetes内置的一种 “服务账户”，它为Pod中的进程提供了一种身份标识。如果Pod没有显式的指定service account，系统会自动为其关联default service account。\n我们自己创建service account对象非常简单：\n1 2 3 4 5 6 7 8 //serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: nginx-example # kubectl apply -f serviceaccount.yaml serviceaccount/nginx-example created 查看刚刚创建的service account：\n1 2 3 4 5 6 7 8 9 10 11 # kubectl describe serviceaccounts nginx-example Name: nginx-example Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;ServiceAccount\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;nginx-example\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;}} Image pull secrets: \u0026lt;none\u0026gt; Mountable secrets: nginx-example-token-r2cv6 Tokens: nginx-example-token-r2cv6 Events: \u0026lt;none\u0026gt; 当service account对象创建成功，controller-manager会发现这个新对象，然后为它生成token。token实际上是secret对象，内部包含了用来身份验证的token。service account对象的Tokens列引用的就是controller-manager为它创建的token。\n我们来看看token的内容：\n1 2 3 4 5 6 7 8 9 10 11 # kubectl get secrets nginx-example-token-r2cv6 -o yaml apiVersion: v1 data: ca.crt: (APISERVER\u0026#39;S CA BASE64 ENCODED) namespace: ZGVmYXVsdA== token: (BEARER TOKEN BASE64 ENCODED) kind: Secret metadata: ...... type: kubernetes.io/service-account-token 可以看到，这个secret对象的type是service-account-token，包含了三部分数据：\nca.crt： API Server的CA证书，用于Pod中的进程访问API Server时对服务端证书进行校验 namespace： secret所在namespace，使用了base64编码 token：JWT Tokens JWT Tokens 是 controller-manager 用 service account私钥sa.key签发的，其中包含了用户的身份信息，API Server可以用sa.pub验证token，拿到用户身份信息，从而完成身份验证。\n如果是使用kubeadm安装的Kubernetes，我们可以在/etc/kubernetes/manifests/目录中的配置文件确认sa.key和sa.pub的作用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # cat /etc/kubernetes/manifests/kube-controller-manager.yaml ... spec: containers: - command: - kube-controller-manager ... - --service-account-private-key-file=/etc/kubernetes/pki/sa.key ... # cat /etc/kubernetes/manifests/kube-apiserver.yaml ... spec: containers: - command: - kube-apiserver ... - --service-account-key-file=/etc/kubernetes/pki/sa.pub ... controller-manager用私钥sa.key签名，API Server用公钥sa.pub验签。\n运行在Pod中的进程在向API server发起HTTP请求时，HTTP header中会携带token，API server从header中拿到token，进行身份验证：\n1 Authorization: Bearer [token] JWT Tokens的是由用.分割的三部分组成：\nHeader Payload Signature 因此，一个JWT Tokens看起来是这样的：\n1 xxxxxxx.yyyyyyyy.zzzzzzz Header和Payload都是base64编码的JSON，以上面nginx-example关联的token为例，看看Header和Payload的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ kubectl describe secrets nginx-example-token-r2cv6 | grep token: | cut -d \u0026#34; \u0026#34; -f 7 eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im5naW54LWV4YW1wbGUtdG9rZW4tcjJjdjYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibmdpbngtZXhhbXBsZSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBmZWRlOWM1LTc2YjUtMTFlOS05MWNhLTAwNTA1NmFjMWMxYyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0Om5naW54LWV4YW1wbGUifQ.VOjVQBr5PKg1WyZYtIW0Fos5fxFN4cYE3Mz9p1eWbQP6rQRQGDEiGX-LBuM6ECI9cpSL-F4nYQAL9vmIlA4vbAgS4OFgC4nwu8SzLu2FVeE7RDpguvsdAsj-4T_LxEGX1RPljGTpvlt8HRjTnp9K8W4dy7PyJQEB5XvCf-IVNAs3zESgmuJ7wJwO7mXQe5WdeqhI5vXjcZiXP97oH0VRYT1vTKVP-GooC5YfaNhU7rHoJ0gmR10xNqZjwKGsHKkq5maC5BOrXFLlHRqVRwm9-hRn-ZLgAoCwujCIpLvPaFUR8HaatzX4GQ_HWev2soJnk1qcav0smxfjC-fu540vZA $ kubectl describe secrets nginx-example-token-r2cv6 | grep token: | cut -d \u0026#34; \u0026#34; -f 7 | cut -d \u0026#34;.\u0026#34; -f 1 | base64 -d | python -mjson.tool { \u0026#34;alg\u0026#34;: \u0026#34;RS256\u0026#34;, \u0026#34;kid\u0026#34;: \u0026#34;\u0026#34; } $ kubectl describe secrets nginx-example-token-r2cv6 | grep token: | cut -d \u0026#34; \u0026#34; -f 7 | cut -d \u0026#34;.\u0026#34; -f 2 | base64 -d | python -mjson.tool { \u0026#34;iss\u0026#34;: \u0026#34;kubernetes/serviceaccount\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/secret.name\u0026#34;: \u0026#34;nginx-example-token-r2cv6\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.name\u0026#34;: \u0026#34;nginx-example\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.uid\u0026#34;: \u0026#34;0fede9c5-76b5-11e9-91ca-005056ac1c1c\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;system:serviceaccount:default:nginx-example\u0026#34; } Header中的alg指明了签名用到的加密算法，Payload 中包含了用户的身份信息，可以知道这个service account属于的namespace为default，名称为nginx-example。\n第三部分Signature的构造方式如下，如果加密算法选择了PKCS SHA：\n1 2 3 4 PKCSSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), secret) controller-manager用sa.key签名，API Server用公钥sa.pub验签，进行身份验证。\n如果先深入了解JWT(JSON Web Token)，建议阅读这篇文档\u0026lt;JWT: The Complete Guide to JSON Web Tokens\u0026gt;\nPod中的进程如何获得这个token呢？Kubernetes在创建Pod时，会将service account token映射到Pod的/var/run/secrets/kubernetes.io/serviceaccount 目录下。我们通过一个例子演示一下。\n创建Pod：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // simple.yaml apiVersion: v1 kind: Pod metadata: name: firstpod spec: serviceAccountName: nginx-example containers: - image: nginx name: stan $ kubectl apply -f simple.yaml pod/firstpod created 查看Pod内/var/run/secrets/kubernetes.io/serviceaccount目录的内容：\n1 2 3 4 $ kubectl exec firstpod -- ls /var/run/secrets/kubernetes.io/serviceaccount ca.crt namespace token 查看Pod内文件/var/run/secrets/kubernetes.io/serviceaccount/token的内容：\n1 2 3 4 5 6 7 8 9 $ kubectl exec firstpod -- cat /var/run/secrets/kubernetes.io/serviceaccount/token | cut -d \u0026#34;.\u0026#34; -f 2 | base64 -d | python -mjson.tool { \u0026#34;iss\u0026#34;: \u0026#34;kubernetes/serviceaccount\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/secret.name\u0026#34;: \u0026#34;nginx-example-token-r2cv6\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.name\u0026#34;: \u0026#34;nginx-example\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.uid\u0026#34;: \u0026#34;0fede9c5-76b5-11e9-91ca-005056ac1c1c\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;system:serviceaccount:default:nginx-example\u0026#34; } 可以看到，映射进Pod中的token，正是我们在配置中通过serviceAccountName指定的nginx-example。Pod中的进程可以通过访问文件/var/run/secrets/kubernetes.io/serviceaccount/token拿到token。\n如何为service account授权？通过定义service account和role的绑定完成。本文简单演示一下，详细的说明参加官方文档。\n创建role：\n1 2 3 4 5 6 7 8 9 10 11 12 // example-role.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] $ kubectl apply -f example-role.yaml role.rbac.authorization.k8s.io/example-role created role可以理解为一组权限的集合，例如上面创建的example-role对default Namesapce内的Pods有get、watch和list操作权限。\n下一步就是将service account和role进行绑定：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //example-rolebinding.yaml kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: default subjects: - kind: ServiceAccount name: nginx-example namespace: default roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io $ kubectl apply -f example-rolebinding.yaml rolebinding.rbac.authorization.k8s.io/example-rolebinding created 通过绑定的创建，service account就拥有了role定义的权限。\n# 总结 用户对API server的访问需要通过身份验证、授权和准入控制这三个阶段的检查。\n一般集群外部用户访问API Server使用客户端证书进行身份验证。Kubernetes各组件之间的通信都使用了TLS加密传输，同时支持基于证书的双向认证。因此Kubernetes的安装过程涉及很多证书的创建，本文分类介绍了这些证书的作用。\n集群内Pod中的进程访问API server时，使用service account关联的token进行身份验证。\n每个Pod都会关联一个service account，没有明确指定时使用default。当我们创建service account对象，controller-manager会为这个service account生成Secret，内部包含了用来身份验证的JWT Tokens。Kubernetes会将token文件mount到Pod的/var/run/secrets/kubernetes.io/serviceaccount/token，Pod内的进程在向API server发起的HTTP时，就可以在请求头中携带这个token。\n","date":"2019-05-19T09:46:53+08:00","permalink":"https://mazhen.tech/p/kubernetes%E9%9B%86%E7%BE%A4%E7%9A%84%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81/","title":"Kubernetes集群的身份验证"},{"content":" # 界面概览 快捷键 描述 cmd + shift + e 文件资源管理器 cmd + shift + f 跨文件搜索 ctrl + shift + g 源代码管理 cmd + shift + d 启动和调试 cmd + shift + x 扩展管理 cmd + shift + p 查找并运行所有命令 cmd + j 打开、关闭panel # 命令行的使用 命令 描述 code $path 新窗口中打开这个文件或文件夹 code -r $path 窗口复用打开文件 code -r -g $file:lineno 打开文件，跳转到指定行 code -r -d $file1 $file2 比较两个文件 ls code - # 光标移动 快捷键 描述 option + 左/右方向键 针对单词的光标移动 cmd + 左/右方向键 移动到行首、行尾 cmd + shift + \\ 在花括号之间跳转 cmd + 上/下方向键 移动到文档的第一行、最后一行 # 文本选择 shift + 光标移动\n# 删除操作 可以先选择，再删除\n快捷键 描述 cmd + fn + del 删除到行尾 cmd + del 删除到行首 option + del 向前删除单词 option + fn + del 向后删除单词 # 代码行编辑 快捷键 描述 cmd + shift + k 删除行 cmd + x 剪切行 cmd + enter 在当前行下一行新开始一行 cmd + shift + enter 在当前行上一行新开始一行 option + 上/下方向键 将当前行上下移动 option + shift + 上/下方向键 将当前行上下复制 cmd + / 将一行代码注释 option + shift + a 注释整块代码 option + shift + f 代码格式化 cmd+k cmd+f 选中代码格式化 ctrl + t 光标前后字符调换位置 cmd+shift+p transform to up/low case 转换大小写 ctrl + j 合并代码行 cmd + u 撤销光标移动 # 创建多个光标 使用鼠标 option + 鼠标左键\n使用键盘 快捷键 描述 cmd + option + 上/下方向键 创建多个光标 cmd + d 选中相同单词，并创建多个光标 option + shift+ i 在选择的多行后创建光标 # 文件跳转 快捷键 描述 ctrl + tab 文件标签之间跳转 cmd + p 打开文件列表 # 行跳转 快捷键 描述 ctrl + g 跳转到指定行 # 符号跳转 快捷键 描述 cmd + shift + o 当前文件所有符号列表 @: 符号列表@后输入冒号，符号分类排列 cmd + t 在多个文件进行符号跳转 cmd + F12 跳转到函数的实现位置 shift + F12 函数引用列表 ctrl + - 跳回上一次光标所在位置 ctrl + shift + - 跳回下一次光标所在位置 # 代码自动补全 快捷键 描述 ctrl+ space 调出建议列表 cmd + shift + space 调出参数预览窗口 cmd + . 快速修复建议列表 F2 函数名重构 # 代码折叠 快捷键 描述 cmd+ option + [ 最内层折叠 cmd + option + ] 最内层展开 cmd+k cmd+0 全部折叠 cmd+k cmd+j 全部展开 # 搜索 快捷键 描述 cmd + f 搜索 cmd + g 搜索，光标在编辑器内跳转 cmd + option + f 查找替换 cmd + shift + f 多文件搜索 # 编辑器操作 快捷键 描述 cmd + \\ 拆分编辑器 option + cmd + 左/右方向键 编辑器间切换 cmd + num 在拆分的编辑器窗口跳转 Cmd +/- 缩放整个工作区 cmd + shift + p reset zoom 重置缩放 # 专注模式 快捷键 描述 cmd + b 打开或者关闭整个视图 cmd + j 打开或者关闭面板 cmd+shift+p Toggle Zen Mode 切换禅模式 cmd+shift+p Toggle Centered Layout 切换剧中布局 # 命令面板 快捷键 描述 cmd + shift + p 命令面板 命令面板的第一个符合对应着不同的功能：\n? 列出所有可用功能 \u0026gt; 用于显示所有的命令 @ 用于显示和跳转文件中的 “符号”（Symbols） @: 可以把符号们按类别归类 # 用于显示和跳转工作区中的 “符号”（Symbols）。 : 用于跳转到当前文件中的某一行。 edt 显示所有已经打开的文件 edt active 显示当前活动组中的文件 ext 插件的管理 ext install 搜索和安装插件。 task 任务 debug 调试功能 term创建和管理终端实例 view 打开各个 UI 组件 # 窗口管理 快捷键 描述 ctrl + w 窗口切换 ctrl + r 切换文件夹 ctrl+r cmd+enter 新建窗口打开文件夹 # 集成终端 快捷键 描述 ctrl + ` 切换集成终端 ctrl + shift + ` 新建集成终端 cmd+shift+p Run Active File In Active Terminal 在集成终端中运行当前脚本 cmd+shift+p Run Selected Text In Active Terminal 在集成终端中运行所选文本 # 任务管理 快捷键 描述 cmd+shift+p run task 自动检测当前项目中可运行的任务 cmd+shift+p Configure Task 配置任务 Cmd + Shift + b 运行默认的生成任务（build task） # 鼠标操作 文本选择 双击鼠标，选中单词 三击鼠标，选中一行 四击鼠标，选中整个文档 单击行号，选中行 文本编辑 选中后可以拖动文本到指定区域 拖动过程中按option，变成复制文本到指定区域 在悬停窗口上按下cmd，提示函数的实现 ","date":"2019-05-16T10:56:00+08:00","permalink":"https://mazhen.tech/p/vs-code-%E5%BF%AB%E6%8D%B7%E9%94%AE/","title":"vs code 快捷键"},{"content":"飞腾芯片 + 银河麒麟OS是目前国产自主可控市场上的主流基础平台。飞腾芯片是aarch64架构，是支持64位的ARM芯片。银河麒麟是基于Ubuntu的发行版。因此可以认为飞腾芯片 + 银河麒麟OS相当于 ARM64 + Ubuntu。\n本文介绍在飞腾平台上编译安装nginx的步骤。\n下载nginx源码 从http://nginx.org/en/download.html下载当前稳定版本的源码，例如\n1 wget http://nginx.org/download/nginx-1.14.2.tar.gz 解压nginx源码：\n1 tar -zxvf nginx-1.14.2.tar.gz nginx配置文件的语法高亮 将nginx源码目录下contrib/vim/的所有内容，copy到用户的$HOME/.vim目录，可以实现nginx配置文件在vim中的语法高亮。\n1 2 mkdir ~/.vim cp -r contrib/vim/* ~/.vim/ 再使用vim打开nginx.conf，可以看到配置文件已经可以语法高亮。\n编译前的配置 查看编译配置支持的参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 $ ./configure --help | more --help print this message --prefix=PATH set installation prefix --sbin-path=PATH set nginx binary pathname --modules-path=PATH set modules path --conf-path=PATH set nginx.conf pathname --error-log-path=PATH set error log pathname --pid-path=PATH set nginx.pid pathname --lock-path=PATH set nginx.lock pathname --user=USER set non-privileged user for worker processes --group=GROUP set non-privileged group for worker processes --build=NAME set build name --builddir=DIR set build directory --with-select_module enable select module --without-select_module disable select module --with-poll_module enable poll module --without-poll_module disable poll module ...... --with-libatomic force libatomic_ops library usage --with-libatomic=DIR set path to libatomic_ops library sources --with-openssl=DIR set path to OpenSSL library sources --with-openssl-opt=OPTIONS set additional build options for OpenSSL --with-debug enable debug logging --with开头的模块缺省不包括在编译结果中，如果想使用需要在编译配置时显示的指定。--without开头的模块则相反，如果不想包含在编译结果中需要显示设定。\n例如我们可以这样进行编译前设置：\n1 ./configure --prefix=/home/adp/nginx --with-http_ssl_module 设置了nginx的安装目录，需要http_ssl模块。\n如果报错缺少OpenSSL，需要先安装libssl。在/etc/apt/sources.list.d目录下增加支持ARM64的apt源，例如国内的清华，创建tsinghua.list，内容如下：\n1 2 3 4 5 6 7 8 deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial main multiverse restricted universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-security main multiverse restricted universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-updates main multiverse restricted universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-backports main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-security main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-updates main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-backports main multiverse restricted universe 执行命令安装OpenSSL：\n1 2 $ sudo apt-get update $ sudo apt-get install libssl-dev configure命令执行完后，会生成中间文件，放在目录objs下。其中最重要的是ngx_modules.c文件，它决定了最终那些模块会编译进nginx。\n执行编译 在nginx目录下执行make编译：\n1 $ make 编译成功的nginx二进制文件在objs目录下。如果是做nginx的升级，可以直接将这个二进制文件copy到nginx的安装目录中。\n安装 在nginx目录下执行make install进行安装：\n1 $ make install 安装完成后，我们到--prefix指定的目录中查看安装结果：\n1 2 3 4 5 6 7 $ tree -L 1 /home/adp/nginx nginx/ ├── conf ├── html ├── logs └── sbin 验证安装结果 编辑nginx/conf/nginx.conf文件，设置监听端口为8080：\n1 2 3 4 5 6 7 http { ... server { listen 8080; server_name localhost; ... 启动nginx\n1 ./sbin/nginx 访问默认首页：\n1 2 3 4 5 6 7 8 9 10 11 $ curl -I http://localhost:8080 HTTP/1.1 200 OK Server: nginx/1.14.2 Date: Tue, 02 Apr 2019 08:38:02 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Tue, 02 Apr 2019 08:30:04 GMT Connection: keep-alive ETag: \u0026#34;5ca31d8c-264\u0026#34; Accept-Ranges: bytes 其他常用命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 查看帮助 $ ./sbin/nginx -? # 重新加载配置 $ ./sbin/nginx -s reload # 立即停止服务 $ ./sbin/nginx -s stop # 优雅停止服务 $ ./sbin/nginx -s quit # 测试配置文件是否有语法错误 $ ./sbin/nginx -t/-T # 打印nginx版本、编译信息 $ ./sbin/nginx -v/-V ","date":"2019-04-02T09:45:05+08:00","permalink":"https://mazhen.tech/p/%E5%9C%A8%E5%9B%BD%E4%BA%A7%E9%A3%9E%E8%85%BE%E5%B9%B3%E5%8F%B0%E4%B8%8A%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85nginx/","title":"在国产飞腾平台上编译安装nginx"},{"content":"kubectl会使用$HOME/.kube目录下的config文件作为缺省的配置文件。我们可以使用kubectl config view查看配置信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://172.18.100.90:6443 name: cluster-1 contexts: - context: cluster: cluster-1 user: cluster-1-admin name: cluster-1-admin@cluster-1 current-context: cluster-1-admin@cluster-1 kind: Config preferences: {} users: - name: cluster-1-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 可以看到，配置文件主要包含了clusters，users和contexts三部分信息。context是访问一个kubernetes集群所需要的参数集合。每个context有三个参数：\ncluster：要访问的集群信息 namespace：用户工作的namespace，缺省值为default user：连接集群的认证用户 缺省情况下，kubectl会使用current-context指定的context作为当前的工作集群环境。不难想象，切换context就可以切换到不同的kubernetes集群。\n在不了解context的概念之前，想访问不同的集群，每次都要把集群对应的config文件copy到$HOME/.kube目录下，同时要记得使用kubectl cluster-info确认当前访问的集群：\n1 2 3 4 5 6 $kubectl cluster-info Kubernetes master is running at https://172.18.100.90:6443 KubeDNS is running at https://172.18.100.90:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. 在看了这篇文档后，才知道kubectl可以切换context来管理多个集群。如果你有多个集群的config文件，可以在系统环境变量KUBECONFIG中指定每个config文件的路径，例如：\n1 export KUBECONFIG=/home/mazhen/kube-config/config-cluster-1:/home/mazhen/kube-config/config-cluster-1 再使用kubectl config view查看集群配置时，kubectl会自动合并多个config的信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://172.20.51.11:6443 name: cluster-2 - cluster: certificate-authority-data: DATA+OMITTED server: https://172.18.100.90:6443 name: cluster-1 contexts: - context: cluster: cluster-2 user: cluster-2-admin name: cluster-2-admin@cluster-2 - context: cluster: cluster-1 user: cluster-1-admin name: cluster-1-admin@cluster-1 current-context: cluster-1-admin@cluster-1 kind: Config preferences: {} users: - name: cluster-2-admin user: client-certificate-data: REDACTED client-key-data: REDACTED - name: cluster-1-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 可以看到，配置中包含了两个集群，两个用户，以及两个context。我们可以使用kubectl config get-contexts查看配置中所有的context：\n1 2 3 4 5 $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE cluster-2-admin@cluster-2 cluster-2 cluster-2-admin * cluster-1-admin@cluster-1 cluster-1 cluster-1-admin 星号*标识了当前的工作集群。如果想访问另一个集群，使用kubectl config use-context进行切换：\n1 2 3 $ kubectl config use-context cluster-2-admin@cluster-2 Switched to context \u0026#34;cluster-2-admin@cluster-2\u0026#34;. 我们可以再次确认切换的结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * cluster-2-admin@cluster-2 cluster-2 cluster-2-admin cluster-1-admin@cluster-1 cluster-1 cluster-1-admin $ kubectl cluster-info Kubernetes master is running at https://172.20.51.11:6443 KubeDNS is running at https://172.20.51.11:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://172.20.51.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. 看吧，kubectl切换context管理多集群是多么的方便。\n","date":"2019-03-29T09:43:28+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8kubectl%E7%AE%A1%E7%90%86%E5%A4%9A%E9%9B%86%E7%BE%A4/","title":"使用kubectl管理多集群"},{"content":"先使用visudo 查看当前的配置，这个命令编辑的是/etc/sudoers文件。可以直接在这个文件中为用户设置sudo权限：\n1 2 3 # User privilege specification root ALL=(ALL:ALL) ALL adp ALL=(ALL) ALL 也可以看看哪个group有root权限，然后将用户加入这个group。例如下面的配置，admin组有root权限：\n1 2 # Members of the admin group may gain root privileges %admin ALL=(ALL) ALL 可以将用户加入admin组，自然就有了sudo权限：\n1 usermod -a -G admin [user] 如果提示admin不存在，可以先创建这个组，再将用户加入这个group：\n1 2 groupadd admin usermod -a -G admin [user] 如果不想编辑/etc/sudoers，可以在/etc/sudoers.d/目录下，为需要sudo权限的用户创建独立的文件，在文件中分别为用户授权，格式和/etc/sudoers一样：\n1 adp ALL=(ALL) ALL 修改文件权限：\n1 chmod 440 adp 这样做的好处每个用户都有独立的配置文件，是方便管理。\n最后，建议将/sbin 和 /usr/sbin 加入到用户路径。\n1 PATH=$PATH:/usr/sbin:/sbin ","date":"2019-03-28T09:41:19+08:00","permalink":"https://mazhen.tech/p/%E5%A6%82%E4%BD%95%E8%AE%A9%E7%94%A8%E6%88%B7%E6%8B%A5%E6%9C%89sudo%E6%9D%83%E9%99%90/","title":"如何让用户拥有sudo权限"},{"content":"为了方便开发者体验Kubernetes，社区提供了可以在本地部署的Minikube。由于在国内网络环境内，无法顺利的安装使用Minikube，我们可以从阿里云的镜像地址来获取所需Docker镜像和配置。\n安装VirtualBox sudo apt-get install virtualbox\n安装 Minikube 1 curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v0.35.0/minikube-linux-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ 启动Minikube 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ minikube start --registry-mirror=https://registry.docker-cn.com 😄 minikube v0.35.0 on linux (amd64) 🔥 Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... 📶 \u0026#34;minikube\u0026#34; IP address is 192.168.99.100 🐳 Configuring Docker as the container runtime ... ✨ Preparing Kubernetes environment ... 🚜 Pulling images required by Kubernetes v1.13.4 ... 🚀 Launching Kubernetes v1.13.4 using kubeadm ... ⌛ Waiting for pods: apiserver proxy etcd scheduler controller addon-manager dns 🔑 Configuring cluster permissions ... 🤔 Verifying component health ..... 💗 kubectl is now configured to use \u0026#34;minikube\u0026#34; 🏄 Done! Thank you for using minikube! 检查状态 1 2 3 4 5 6 $ minikube status host: Running kubelet: Running apiserver: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100 kubernetes已经成功运行，可以使用kubectl访问集群：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-89cc84847-2k67h 1/1 Running 0 18m coredns-89cc84847-95zsj 1/1 Running 0 18m etcd-minikube 1/1 Running 0 18m kube-addon-manager-minikube 1/1 Running 0 19m kube-apiserver-minikube 1/1 Running 0 18m kube-controller-manager-minikube 1/1 Running 0 18m kube-proxy-f66hz 1/1 Running 0 18m kube-scheduler-minikube 1/1 Running 0 18m kubernetes-dashboard-7d8d567b4d-h82vx 1/1 Running 0 18m storage-provisioner 1/1 Running 0 18m 停止Minikube 1 2 3 4 $ minikube stop ✋ Stopping \u0026#34;minikube\u0026#34; in virtualbox ... 🛑 \u0026#34;minikube\u0026#34; stopped. 删除本地集群 1 2 3 4 $ minikube delete 🔥 Deleting \u0026#34;minikube\u0026#34; from virtualbox ... 💔 The \u0026#34;minikube\u0026#34; cluster has been deleted. ","date":"2019-03-25T09:39:12+08:00","permalink":"https://mazhen.tech/p/%E5%9C%A8ubuntu%E4%B8%8A%E5%AE%89%E8%A3%85minikube/","title":"在Ubuntu上安装Minikube"},{"content":"刚接触Kubernetes时很容易被它繁多的概念（POD，Service，Deployment \u0026hellip;)以及比较复杂的部署架构搞晕，本文希望能通过一个简单的例子，讲解Kubernetes最基本的工作原理。\nKubernetes本质上是为用户提供了一个容器编排工具，可以管理和调度用户提交的作业。用户在 YAML 配置文件中描述应用所需的环境配置、参数等信息，以及应用期待平台提供的服务（负载均衡，水平扩展等），然后将 YAML 提交，Kubernetes会按照用户的要求，在集群上将应用运行起来。在遇到异常情况，或用户的主动调整时，Kubernetes 将始终保持应用实际的运行状态，符合用户的期待状态。\nKubernetes 是由 Master 和 Node 两种节点组成。Master由3个独立的组件组成：\n负责 API 服务的 kube-apiserver 负责容器编排的 kube-controller-manager 负责调度的 kube-scheduler Kubernetes 集群的所有状态信息都存储在 etcd，其他组件对 etcd 的访问，必须通过 kube-apiserver。\nKubelet 运行在所有节点上，它通过容器运行时（例如Docker），让应用真正的在节点上运行起来。\n下面通过一个简单的例子，描述 Kubernetes 的各个组件，是如何协作完成工作的。\n用户将 YAML 提交给 kube-apiserver，YAML 经过校验后转换为 API 对象，存储在 etcd 中。\nkube-controller-manager 是负责编排的组件，当它发现有新提交的应用，会根据配置的要求生成对应的 Pod 对象。Pod 是 Kubernetes 调度管理的最小单元，可以简单的认为，Pod 就是一个虚拟机，其中运行着关系紧密的进程，共同组成用户的应用。例如Web应用进程和日志收集agent，可以包含在一个Pod中。Pod 对象也存储在 etcd 中。本例子中用户定义 replicas 为2，也就是用户期待有两个 Pod 实例。\n其实kube-controller-manager 内部一直在做循环检查，只要发现有应用没有对应的 Pod，或者 Pod 的数量不满足用户的期望，它都会进行适当的调整，创建或删除Pod 对象。\nkube-scheduler 负责 Pod 的调度。kube-scheduler 发现有新的 Pod 出现，它会按照调度算法，为每个 Pod 寻找一个最合适的节点（Node）。kube-scheduler 对一个 Pod 的调度成功，实际上就是在 Pod 对象上记录了调度结果的节点名称。注意，Pod 调度成功，只是在 Pod 上标记了节点的名字，Pod 是否真正在节点上运行，就不是kube-scheduler的责任了。\nKubelet 运行在所有节点上，它会订阅所有 Pod 对象的变化，当发现一个 Pod 与 Node 绑定，也就是这个 Pod 上标记了Node的名字，而这个被绑定的 Node 就是它自己，Kubelet 就会在这个节点将 Pod 启动。\n至此，用户提交的应用在Kubernetes集群中就运行起来了。\n同时，上述的过程一直在循环往复。例如，用户更新了 YAML，将 replicas 改为3，并将更新后的 YAML 再次提交。kube-controller-manager会发现实际运行的 Pod 数量与用户的期望不符，它会生成一个新的 Pod 对象。紧接着 kube-scheduler 发现一个没有绑定节点的 Pod，它会按照调度算法为这个Pod寻找一个最佳节点完成绑定。最后，某个Kubelet 发现新绑定节点的 Pod 应该在本节点上运行，它会通过接口调用Docker完成 Pod 的启动。\n上面就是 Kubernetes 基本工作流程的简单描述，希望对你理解它的工作原理有所帮助。\n","date":"2019-02-24T17:30:11+08:00","permalink":"https://mazhen.tech/p/kubernetes%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/","title":"Kubernetes工作原理概述"},{"content":"Kubernetes在2017年赢得了容器编排之战，使得基于容器+Kubernetes来构建PaaS平台成为了云计算的主流方式。在人们把关注的目光都聚焦在Kubernetes上时，容器技术领域在2018年也发生了很多创新，包括amazon最近开源的轻量级虚拟机管理器 Firecracker，Google在今年5月份开源的基于用户态操作系统内核的 gVisor 容器，还有更早开源的虚拟化容器项目 KataContainers，可谓百花齐放。一般的开发者可能认为容器就等于Docker，没想到容器领域还在发生着这么多创新。我在了解这些项目时，发现如果没有一些背景知识，很难get到它们的创新点。我试着通过这篇文章进行一次背景知识的梳理。让我们先从最基本的问题开始：操作系统是怎么工作的？\n# 操作系统的工作方式 我们不去讨论操作系统的标准定义，而是想想操作系统的本质是什么，它是怎么为应用提供服务的呢？\n其实操作系统很“懒”，它不是一直运行着主动干活，而是躺在那儿等着被中断“唤醒”，然后根据中断它的事件类型做下一步处理：是不是有键盘敲击、网络数据包到达，还是时间片到了该考虑进程切换，或者是有应用向内核发出了服务请求。内核处理完唤醒它的事件，会将控制权返还给应用程序，然后等着再次被中断“唤醒”。内核就是这样由中断驱动，控制着CPU、内存等硬件资源，为应用程序提供服务。\n”唤醒“操作系统内核的事件主要分为三类：\n中断：来自硬件设备的处理请求； 异常：当前正在执行的进程，由于非法指令或者其他原因导致执行失败而产生的异常事情处理请求，典型的如缺页异常； 系统调用：应用程序主动向操作系统内核发出的服务请求。系统调用的本质其实也是中断，相对于硬件设备的中断，这种中断被称为软中断。 # CPU的特权等级 内核代码常驻在内存中，被每个进程映射到它的逻辑地址空间：\n进程逻辑地址空间的最大长度与实际可用的物理内存数量无关，由CPU的字长决定。进程的逻辑地址空间划分为两个部分，分别称为内核空间和用户空间。用户空间是彼此独立的，而逻辑地址空间顶部的内核空间是被所有进程共享。\n从每个进程的角度来看，地址空间中只有自身一个进程，它不会感知到其他进程的存在。由于内核空间被所有进程共享，为了防止进程修改彼此的数据而造成相互干扰，用户进程不能直接操作或读取内核空间中的数据。同时由于内核管理着所有硬件资源，也不能让用户进程直接执行内核空间中的代码。操作系统应该如何做到这种限制呢？\n实际上，操作系统的实现依赖 CPU 提供的功能。现代的CPU体系架构都提供几种特权级别，每个特权级别有各种限制。各级别可以看作是环，内环能够访问更多的功能，外环则较少，被称为protection rings：\nIntel 的 CPU 提供了4种特权级别， Linux 只使用了 Ring0 和 Ring3 两个级别。Ring 0 拥有最多的特权，它可以直接和CPU、内存等物理硬件交互。 Ring 0 被称为内核态，操作系统内核正是运行在Ring 0。Ring 3被称为用户态，应用程序运行在用户态。\n# 系统调用 在用户态禁止直接访问内核态，也就是说不同通过普通的函数调用方式调用内核代码，而必须使用系统调用陷入（trap）内核，完成从用户态到内核态的切换。内核首先检查进程是否允许执行想要的操作，然后代表进程执行所需的操作，完成后再返回到用户态。\n除了代表用户程序执行代码之外，内核还可以由硬件中断激活，然后在中断上下文中运行。另外除了普通进程，系统中还有内核线程在运行。内核线程不与任何特定的用户空间进程相关联。\nCPU 在任何时间点上的活动必然为下列三者之一 ：\n运行于用户空间，执行应用程序 运行于内核空间，处于进程上下文，即代表某个特定的进程执行 运行于内核空间，处于中断上下文，与任何进程无关，处理某个特定的中断 # 优化系统调用 从上面的讨论可以看出，由于系统调用会经过用户态到内核态的切换，开销要比普通函数调用大很多，因此在进行系统级编程时，减少用户态与内核态之间的切换是一个很重要的优化方法。\n例如同样是实现 Overlay网络，使用 VXLAN 完全在内核态完成封装和解封装，要比把数据包从内核态通过虚拟设备TUN传入用户态再进行处理要高效很多。\n对应的也可以从另外一个方向进行优化：使用 DPDK 跳过内核网络协议栈，数据从网卡直接到达用户态，在用户态处理数据包，也就是说网络协议栈完全运行在用户态，同样避免了用户态和内核态的切换。像腾讯开源的 F-Stack就是一个基于DPDK运行在用户空间的TCP/IP协议栈。\n了解了操作系统内核的基本工作方式，我们再看下一个话题：虚拟化。\n# 虚拟化技术 为了更高效灵活的使用硬件资源，同时能够实现服务间的安全隔离，我们需要虚拟化技术。运行虚拟化软件（Hypervisor或者叫VMM，virtual machine monitor）的物理设施我们称之为Host，安装在Hypervisor之上的虚拟机称为Guest。\n根据Hypervisor在系统中的位置，可以将它归类为type-1或type-2型。如果hypervisor直接运行在硬件之上，它通常被认为是Type-1型。如果hypervisor作为一个单独的层运行在操作系统之上，它将被认为是Type 2型。\nType-1型Hypervisor的概念图：\nType-2型Hypervisor的概念图：\n# KVM \u0026amp; QEMU 实际上Type-1和Type-2并没有严格的区分，像最常见的虚拟化软件 KVM（Kernel-based Virtual Machine）是一个Linux内核模块，加载KVM后Linux内核就转换成了Type-1 hypervisor。同时，Linux还是一个通用的操作系统，也可以认为KVM是运行在Linux之上的Type-2 hypervisor。\n为了在Host上创建出虚拟机，仅仅有KVM是不够的。对于 I/O 的仿真，KVM 还需要 QEMU的配合。QEMU 是一个运行在用户空间程序，它可以仿真处理器和一系列的物理设备：磁盘、网络、VGA、PCI、USB、串口/并口等等，基于QEMU可以构造出一个完整的虚拟PC。\n值得注意的是，QEMU 有两种运行模式：仿真模式和虚拟化模式。在仿真模式下，QEMU可以在一个Intel的Host上运行ARM或MIPS虚拟机。这是怎么做到的呢？实际上，QEMU 通过 TCG（Tiny Code Generator）技术进行了二进制代码转换，可以认为这是一种高级语言的VM，就像JVM。例如可以将运行在ARM上的二进制转换为一种中间字节码，然后让它运行在Host的Intel CPU上。很明显，这种二进制代码转换有着巨大的性能开销。\n相对应的，QEMU的另一种是虚拟化模式，它借助KVM完成处理器的虚拟化。由于和CPU的体系结构紧密关联，虚拟化模式能够带来更好的性能，限制是Guest必须使用和Host一样的CPU体系机构。这就是我们最常用到的虚拟化技术栈：KVM/QEMU\nKVM 和 QEMU 有两种交互方式：通过设备文件/dev/kvm 和通过内存映射页面。QEMU 和 KVM之间的大块数据传递会使用内存映射页面。/dev/kvm是KVM暴露的主要API，它支持一系列ioctl接口，QEMU 使用这些接口和KVM交互。/dev/kvm API分为三个层次：\nSystem Level: 用于KVM全局状态的维护，例如创建 VM； VM Level: 用于处理和特定VM相关工作的 API，vCPU 就是通过这个级别的API创建出来的； vCPU Level: 这是最细粒度的API，用于和特定vCPU的交互。QEMU会为每个vCPU分配一个专门的线程。 # CPU 虚拟化技术 VT-x KVM和QEMU配合完美，但和CPU的特权级别在一起就遇到了麻烦。我们知道，hypervisor需要管理宿主机的CPU、内存、I/O设备等资源，因此它需要运行在ring 0级别才能执行这些高特权操作。然而运行在VM中的操作系统希望得到访问所有资源的权限，它并不知道自己运行在虚拟机中。因为同一时间只有一个内核可以运行在ring 0，Guest OS不得不被“挤”到了ring 1，这一级别不能满足内核的需求。怎么解决？\n虽然我们可以使用软件的方式进行模拟，让hypervisor拦截应用发往ring 0的系统调用，再转发给Guest OS，但这么做会产生额外的性能损耗，而且方案复杂难以维护。Intel和AMD认识到了虚拟化的重要性，各自独立创建了X86架构的扩展指令集，分别称为 VT-x and AMD-V，从CPU层面支持虚拟化。\n以Intel CPU为例，VT-x不仅增加了虚拟化相关的指令集，还将CPU的指令划分会两种模式：root 和 non-root。hypervisor运行在 root 模式，而VM运行在non-root模式。指令在non-root模式的运行速度和root模式几乎一样，除了不能执行一些涉及CPU全局状态切换的指令。\nVMX（Virtual Machine Extensions）是增加到VT-x中的指令集，主要有四个指令：\nVMXON：在这个指令执行之前，CPU还没有root 和 non-root的概念。VMXON执行后，CPU进入虚拟化模式。 VMXOFF：VMXON的相反操作，执行VMXOFF退出虚拟化模式。 VMLAUNCH：创建一个VM实例，然后进入non-root模式。 VMRESUME：进入non-root模式，恢复前面退出的VM实例。当VM试图执行一个在non-root禁止的指令，CPU立即切换到root模式，类似前面介绍的系统调用trap方式，这就是VM的退出。 这样，VT-x/KVM/QEMU 构成了今天应用最广泛的虚拟化技术栈。\n有了上面的铺垫，终于要谈到容器技术了。\n# 容器的本质 虽然虚拟化技术在灵活高效的使用硬件资源方面前进了一大步，但人们还觉得远远不够。特别是在机器使用量巨大的互联网公司。因为虚拟机一旦创建，为它分配的资源就相对固定，缺乏弹性，很难再提高机器的利用率。而且创建、销毁虚拟机也是相对“重”的操作。这时候容器技术出现了。我们知道，容器依赖的底层技术，Linux Namesapce和Cgroups都是最早由Google开发，提交进Linux内核的。\n容器的本质就是一个进程，只不过对它进行了Linux Namesapce隔离，让它“看”不到外面的世界，用Cgroups限制了它能使用的资源，同时利用系统调用pivot_root或chroot切换了进程的根目录，把容器镜像挂载为根文件系统rootfs。rootfs中不仅有要运行的应用程序，还包含了应用的所有依赖库，以及操作系统的目录和文件。rootfs打包了应用运行的完整环境，这样就保证了在开发、测试、线上等多个场景的一致性。\n从上图可以看出，容器和虚拟机的最大区别就是，每个虚拟机都有独立的操作系统内核Guest OS，而容器只是一种特殊的进程，它们共享同一个操作系统内核。\n看清了容器的本质，很多问题就容易理解。例如我们执行 docker exec 命令能够进入运行中的容器，好像登录进独立的虚拟机一样。实际上这只不过是利用系统调用setns，让当前进程进入到容器进程的Namesapce中，它就能“看到”容器内部的情况了。\n由于容器就是进程，它的创建、销毁非常轻量，对资源的使用控制更加灵活，因此让Kubernetes这种容器编排和资源调度工具可以大显身手，通过合理的搭配，极大的提高了整个集群的资源利用率。\n# 虚拟化容器技术 前面提到，运行在一个宿主机上的所有容器共享同一个操作系统内核，这种隔离级别存在着很大的潜在安全风险。因此在公有云的多租户场景下，还是需要先用虚拟机进行租户强隔离，然后用户在虚拟机上再使用容器+Kubernetes部署应用。\n然而在Serverless的场景下，传统的先建虚拟机再创建容器的方式，在灵活性、执行效率方面难以满足需求。随着Serverless、FaaS（Function-as-a-Service）的兴起，各公有云厂商都将安全性容器作为了创新焦点。\n一个很自然能想到的方案，是结合虚拟机的强隔离安全性+容器的轻量灵活性，这就是虚拟化容器项目 KataContainers。\nOpenStack在2017年底发布的 KataContainers 项目，最初是由 Intel ClearContainer 和 Hyper runV 两个项目合并而产生的。在Kubernetes场景下，一个Pod对应于Kata Containers启动的一个轻量化虚拟机，Pod中的容器，就是运行在这个轻量级虚拟机里的进程。每个Pod都运行在独立的操作系统内核上，从而达到安全隔离的目的。\n可以看出，KataContainers 依赖 KVM/QEMU技术栈。\namazon最近开源的Firecracker也是为了实现在 functions-based services 场景下，多租户安全隔离的容器。\nFirecracker同样依赖KVM，然后它没有用到QEMU，因为Firecracker本身就是QEMU的替代实现。Firecracker是一个比QEMU更轻量级的VMM，它只仿真了4个设备：virtio-net，virtio-block，serial console和一个按钮的键盘，仅仅用来停止microVM。理论上，KataContainers可以用Firecracker换掉它现在使用的QEMU，从而将 Firecracker整合进Kubernetes生态圈。\n其实Google早就没有使用QEMU，而且对KVM进行了深度定制。我们可以从这篇介绍看出端倪：7 ways we harden our KVM hypervisor at Google Cloud: security in plaintext\nNon-QEMU implementation: Google does not use QEMU, the user-space virtual machine monitor and hardware emulation. Instead, we wrote our own user-space virtual machine monitor that has the following security advantages over QEMU\n\u0026hellip;\n# gVisor Google 开源的gVisor为了实现安全容器另辟蹊径，它用 Go 实现了一个运行在用户态的操作系统内核，作为容器运行的Guest Kernel，每个容器都依赖独立的操作系统内核，实现了容器间安全隔离的目的。\n虽然 gVisor 今年才开源，但它已经在Google App Engine 和 Google Cloud Functions运行了多年。\ngVisor作为运行应用的安全沙箱，扮演着Virtual kernel的角色。同时gVisor 包含了一个兼容Open Container Initiative (OCI) 的运行时runsc，因此可以用它替换掉 Docker 的 runc，整合进Kubernetes生态圈，为Kubernetes带来另一种安全容器的实现方案。\nKata Containers和gVisor的本质都是为容器提供一个独立的操作系统内核，避免容器共享宿主机的内核而产生安全风险。KataContainers使用了传统的虚拟化技术，gVisor则自己实现了一个运行在用户态、极小的内核。gVisor比KataContainers更加轻量级，根据这个分享的介绍，gVisor目前只实现了211个Linux系统调用，启动时间150ms，内存占用15MB。\ngVisor实现原理，简单来说是模拟内核的行为，使用某种方式拦截应用发起的系统调用，经过gVisor的安全控制，代替容器进程向宿主机发起可控的系统调用。目前gVisor实现了两种拦截方式：\n基于Ptrace 机制的拦截 使用 KVM 来进行系统调用拦截。 因为gVisor基于拦截系统调用的实现原理，它并不适合系统调用密集的应用。\n最后，对于像我这样没有读过Linux内核代码的后端程序员，gVisor是一个很好的窥探内核内部实现的窗口，又激起了我研究内核的兴趣。Twitter上看到有人和我有类似的看法：\n希望下次能分享gVisor深入研究系列。保持好奇心，Stay hungry. Stay foolish.\n","date":"2018-12-16T22:03:21+08:00","permalink":"https://mazhen.tech/p/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E6%BC%AB%E8%B0%88/","title":"容器技术创新漫谈"},{"content":"容器的跨主机通信主要有两种方式：封包模式和路由模式。上一篇文章演示了使用VXLAN协议的封包模式，这篇将介绍另一种方式，利用三层网络的路由转发实现容器的跨主机通信。\n# 路由模式概述 宿主机将它负责的容器IP网段，以某种方式告诉其他节点，然后每个节点根据收到的\u0026lt;宿主机-容器IP网段\u0026gt;映射关系，配置本机路由表。\n这样对于容器间跨节点的IP包，就可以根据本机路由表获得到达目的容器的网关地址，即目的容器所在的宿主机地址。接着在把IP包封装成二层网络数据帧时，将目的MAC地址设置为网关的MAC地址，IP包就可以通过二层网络送达目的容器所在的宿主机。\n至于用什么方式将\u0026lt;宿主机-容器IP网段\u0026gt;映射关系发布出去，不同的项目采用了不同的实现方案。\nFlannel是将这些信息集中存储在etcd中，每个节点从etcd自动获取数据，更新宿主机路由表。\nCalico则使用BGP（Border Gateway Protocol）协议交换共享路由信息，每个宿主机都是运行在它之上的容器的边界网关。\n如果宿主机之间跨了网段怎么办？宿主机之间的二层网络不通，虽然知道目的容器所在的宿主机，但没办法将目的MAC地址设置为那台宿主机的MAC地址。\nCalico有两种解决方案：\nIPIP 模式，在跨网段的宿主机之间建立“隧道” 让宿主机之间的路由器“学习”到容器路由规则，每个路由器都知道某个容器IP网段是哪个宿主机负责的，容器间的IP包就能正常路由了。 # 动手实验 路由模式的实验比较简单，关键在于宿主机上路由规则的配置。为了简化实验，这些路由规则都是我们手工配置，而且两个节点之间二层网络互通，没有跨网段。\n参照Docker跨主机Overlay网络动手实验，创建“容器”，veth pairs，bridge，设置IP，激活虚拟设备。\n然后在node-1上增加路由规则：\n1 sudo ip route add 172.18.20.0/24 via 192.168.31.192 在node-2上增加路由规则：\n1 sudo ip route add 172.18.10.0/24 via 192.168.31.183 当docker1访问docker2时，IP包会从veth到达br0，然后根据node-1上刚设置的路由规则，访问172.18.20.0/24网段的网关地址为node-2，这样，IP包就能路由到node-2了。\n同时，node-2的路由表中包含这样的一条规则：\n1 2 3 4 $ ip route ... 172.18.20.0/24 dev br0 proto kernel scope link src 172.18.20.1 到达node-2的IP包，会根据这条规则路由到网桥br0，最终到达docker-2。反过来从docker2访问docker1的过程也是类似。\n# 总结 两种容器跨主机的通信方案我们都实验了一下，现在做个简单总结对比：\n封包模式对基础设施要求低，三层网络通就可以了。但封包、解包带来的性能损耗较大。 路由模式性能好，但要求二层网络连通，或者在跨网段的情况下，要求路由器能配合“学习”路由规则。 至此，容器网络的三篇系列完成：\nDocker单机网络模型动手实验 Docker跨主机Overlay网络动手实验 Docker跨主机通信路由模式动手实验（本篇） ","date":"2018-11-11T22:00:22+08:00","permalink":"https://mazhen.tech/p/docker%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E4%BF%A1%E8%B7%AF%E7%94%B1%E6%A8%A1%E5%BC%8F%E5%8A%A8%E6%89%8B%E5%AE%9E%E9%AA%8C/","title":"Docker跨主机通信路由模式动手实验"},{"content":"上一篇文章我演示了docker bridge网络模型的实验，这次我将展示如何利用Overlay 网络实现跨主机容器的通信。\n两个容器docker1和docker2分别位于节点Node-1和Node-2，如何实现容器的跨主机通信呢？一般来说有两种实现方式：\n封包模式：利用Overlay网络协议在节点间建立“隧道”，容器之间的网络包被封装在外层的网络协议包中进行传输。 路由模式：容器间的网络包传输全部用三层网络的路由转发来实现。 本文主要介绍封包模式。Overlay网络主要有两种方式，一种是使用UDP在用户态封装，一种是利用VXLAN 在内核态封装。由于减少了用户态到内核态的切换，封包解包逻辑都在内核态进行，VXLAN 的性能更好，成为了容器网络的主流方案。\n关于路由模式，会在下一篇文章介绍。\n# VXLAN VXLAN（Virtual Extensible LAN）是一种网络虚拟化技术，它将链路层的以太网包封装到UDP包中进行传输。VXLAN最初是由VMware、Cisco开发，主要解决云环境下多租户的二层网络隔离。我们常听到公有云厂商宣称支持VPC（virtual private cloud），实际底层就是使用VXLAN实现的。\nVXLAN packet的结构：\n我们可以看到，最内部是原始的二层网络包，外面加上一个VXLAN header，其中最重要的是VNI（VXLAN network identifier)字段，它用来唯一标识一个VXLAN。也就是说，使用不同的VNI来区分不同的虚拟二层网络。VNI有24位，基本够公用云厂商使用了。要知道原先用来网络隔离的虚拟局域网VLAN只支持4096个虚拟网络。\n在 VXLAN header外面封装了正常的UDP包。VXLAN在UDP之上，实现了一个虚拟的二层网络，连接在这个虚拟二层网络上的主机，就像连接在普通的局域网上一样，可以互相通信。\n介绍完背景知识，我们可以开始动手实验了。\n# 实现方案一 参照Flannel的实现方案：\n配置内核参数，允许IP forwarding 分别在Node-1、Node-2上执行：\n1 sudo sysctl net.ipv4.conf.all.forwarding=1 创建“容器” 在Node-1上执行：\n1 sudo ip netns add docker1 在Node-2上执行：\n1 sudo ip netns add docker2 为什么创建个Namesapce就说是“容器”？请参考上一篇文章。\n创建Veth pairs 分别在Node-1、Node-2上执行：\n1 sudo ip link add veth0 type veth peer name veth1 将Veth的一端放入“容器” 在Node-1上执行：\n1 sudo ip link set veth0 netns docker1 在Node-2上执行：\n1 sudo ip link set veth0 netns docker2 创建bridge 分别在Node-1、Node-2上创建bridge br0：\n1 sudo brctl addbr br0 将Veth的另一端接入bridge 分别在Node-1、Node-2上执行：\n1 sudo brctl addif br0 veth1 为\u0026quot;容器“内的网卡分配IP地址，并激活上线 在Node-1上执行：\n1 2 sudo ip netns exec docker1 ip addr add 172.18.10.2/24 dev veth0 sudo ip netns exec docker1 ip link set veth0 up 在Node-2上执行：\n1 2 sudo ip netns exec docker2 ip addr add 172.18.20.2/24 dev veth0 sudo ip netns exec docker2 ip link set veth0 up Veth另一端的网卡激活上线 分别在Node-1、Node-2上执行：\n1 sudo ip link set veth1 up 为bridge分配IP地址，激活上线 在Node-1上执行：\n1 2 sudo ip addr add 172.18.10.1/24 dev br0 sudo ip link set br0 up 在Node-2上执行：\n1 2 sudo ip addr add 172.18.20.1/24 dev br0 sudo ip link set br0 up 将bridge设置为“容器”的缺省网关 在Node-1上执行：\n1 sudo ip netns exec docker1 route add default gw 172.18.10.1 veth0 在Node-2上执行：\n1 sudo ip netns exec docker2 route add default gw 172.18.20.1 veth0 创建VXLAN虚拟网卡 VXLAN需要在宿主机上创建一个虚拟网络设备对 VXLAN 的包进行封装和解封装，实现这个功能的设备称为 VTEP（VXLAN Tunnel Endpoint）。宿主机之间通过VTEP建立“隧道”，在其中传输虚拟二层网络包。\n在Node-1创建vxlan100：\n1 2 3 4 5 6 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.183 \\ dev enp0s3 \\ dstport 4789 \\ nolearning 为vxlan100分配IP地址，然后激活：\n1 2 sudo ip addr add 172.18.10.0/32 dev vxlan100 sudo ip link set vxlan100 up 为了让Node-1上访问172.18.20.0/24网段的数据包能进入“隧道”，我们需要增加如下的路由规则：\n1 sudo ip route add 172.18.20.0/24 dev vxlan100 在Node-2上执行相应的命令：\n1 2 3 4 5 6 7 8 9 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.192 \\ dev enp0s3 \\ dstport 4789 \\ nolearning sudo ip addr add 172.18.20.0/32 dev vxlan100 sudo ip link set vxlan100 up sudo ip route add 172.18.10.0/24 dev vxlan100 scope global 手动更新ARP和FDB 虚拟设备vxlan100会用ARP和FDB (forwarding database) 数据库中记录的信息，填充网络协议包，建立节点间转发虚拟网络数据包的“隧道”。\n我们知道，在二层网络上传输IP包，需要先根据目的IP地址查询到目的MAC地址，这就是ARP（Address Resolution Protocol）协议的作用。我们应该可以通过ARP查询到其他节点上容器IP地址对应的MAC地址，然后填充在VXLAN内层的网络包中。\nFDB是记录网桥设备转发数据包的规则。虚拟网络数据包根据上面定义的路由规则，从br0进入了本机的vxlan100“隧道”入口，应该可以在FDB中查询到“隧道”出口的MAC地址应该如何到达，这样，两个VTEP就能完成”隧道“的建立。\nvxlan为了建立节点间的“隧道”，需要一种机制，能让一个节点的加入、退出信息通知到其他节点，可以采用multicast的方式进行节点的自动发现，也有很多Unicast的方案，这篇文章\u0026lt;VXLAN \u0026amp; Linux\u0026gt;有很详细的介绍。总之就是要找到一种方式，能够更新每个节点的ARP和FDB数据库。\n如果是使用Flannel，它在节点启动的时候会采用某种机制自动更新其他节点的ARP和FDB数据库。现在我们的实验只能在两个节点上手动更新ARP和FDB。\n首先在两个节点上查询到设备vxlan100的MAC地址，例如在我当前的环境：\nNode-1上vxlan100的MAC地址是3a:8d:b8:69:10:3e Node-2上vxlan100的MAC地址是0e:e6:e6:5d:c2:da\n然后在Node-1上增加ARP和FDB的记录：\n1 2 sudo ip neighbor add 172.18.20.2 lladdr 0e:e6:e6:5d:c2:da dev vxlan100 sudo bridge fdb append 0e:e6:e6:5d:c2:da dev vxlan100 dst 192.168.31.192 我们可以确认下执行结果：\nARP中已经记录了Node-2上容器IP对应的MAC地址。再看看FDB的情况：\n根据最后一条新增规则，我们可以知道如何到达Node-2上“隧道”的出口vxlan100。“隧道”两端是使用UDP进行传输，即容器间通讯的二层网络包是靠UDP在宿主机之间通信。\n类似的，在Node-2上执行下面的命令：\n1 2 sudo ip neighbor add 172.18.10.2 lladdr 3a:8d:b8:69:10:3e dev vxlan100 sudo bridge fdb append 3a:8d:b8:69:10:3e dev vxlan100 dst 192.168.31.183 测试容器的跨节点通信 现在，容器docker1和docker1之间就可以相互访问了。\n我们从docker1访问docker2，在Node-1上执行：\n1 sudo ip netns exec docker1 ping -c 3 172.18.20.2 同样可以从docker2访问docker1，在Node-2上执行：\n1 sudo ip netns exec docker2 ping -c 3 172.18.10.2 在测试过程中如果需要troubleshooting，可以使用tcpdump在veth1、br0、vxlan100等虚拟设备上抓包，确认网络包是按照预定路线在转发：\n1 sudo tcpdump -i vxlan100 -n 测试环境恢复 在两个节点上删除我们创建的虚拟设备：\n1 2 3 4 sudo ip link set br0 down sudo brctl delbr br0 sudo ip link del veth1 sudo ip link del vxlan100 # 实现方案二 Docker原生的overlay driver底层也是使用VXLAN技术，但实现方案和Flannel略有不同：\n我们可以看到，vxlan100被“插”在了虚拟交换机br0上，虚拟网络数据包从br0到vxlan100不是通过本机路由，而是vxlan100根据FDB直接进行了转发。\n执行的命令略有差异，我不再赘述过程，直接提供了命令，大家自己实验吧：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 # 在Node-1上执行 sudo sysctl net.ipv4.conf.all.forwarding=1 sudo ip netns add docker1 sudo ip link add veth0 type veth peer name veth1 sudo ip link set veth0 netns docker1 sudo brctl addbr br0 sudo brctl addif br0 veth1 sudo ip netns exec docker1 ip addr add 172.18.10.2/24 dev veth0 sudo ip netns exec docker1 ip link set veth0 up sudo ip link set veth1 up sudo ip link set br0 up sudo ip netns exec docker1 route add default veth0 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.183 \\ dev enp0s5 \\ dstport 4789 \\ nolearning \\ proxy sudo ip link set vxlan100 up sudo brctl addif br0 vxlan100 sudo ip neigh add 172.18.20.2 lladdr [docker2的MAC地址] dev vxlan100 sudo bridge fdb append [docker2的MAC地址] dev vxlan100 dst 192.168.31.192 # 在Node-2上执行 sudo sysctl net.ipv4.conf.all.forwarding=1 sudo ip netns add docker2 sudo ip link add veth0 type veth peer name veth1 sudo ip link set veth0 netns docker2 sudo brctl addbr br0 sudo brctl addif br0 veth1 sudo ip netns exec docker2 ip addr add 172.18.20.2/24 dev veth0 sudo ip netns exec docker2 ip link set veth0 up sudo ip link set veth1 up sudo ip link set br0 up sudo ip netns exec docker2 route add default veth0 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.192 \\ dev enp0s5 \\ dstport 4789 \\ nolearning \\ proxy sudo ip link set vxlan100 up sudo brctl addif br0 vxlan100 sudo ip neigh add 172.18.10.2 lladdr [docker1的MAC地址] dev vxlan100 sudo bridge fdb append [docker1的MAC地址] dev vxlan100 dst 192.168.31.183 相信通过亲自动手实验，容器网络对你来说不再神秘。希望本文对你理解容器网络有所帮助。\n下一篇我将动手实验容器跨主机通信的路由模式。\n","date":"2018-11-07T21:52:36+08:00","permalink":"https://mazhen.tech/p/docker%E8%B7%A8%E4%B8%BB%E6%9C%BAoverlay%E7%BD%91%E7%BB%9C%E5%8A%A8%E6%89%8B%E5%AE%9E%E9%AA%8C/","title":"Docker跨主机Overlay网络动手实验"},{"content":" # 容器的本质 容器的本质就是一个进程，只不过对它进行了Linux Namesapce隔离，让它看不到外面的世界，用Cgroups限制了它能使用的资源，同时利用系统调用pivot_root或chroot切换了进程的根目录，把容器镜像挂载为根文件系统rootfs。rootfs中不仅有要运行的应用程序，还包含了应用的所有依赖库，以及操作系统的目录和文件。rootfs打包了应用运行的完整环境，这样就保证了在开发、测试、线上等多个场景的一致性。\n从上图可以看出，容器和虚拟机的最大区别就是，每个虚拟机都有独立的操作系统内核Guest OS，而容器只是一种特殊的进程，它们共享同一个操作系统内核。\n看清了容器的本质，很多问题就容易理解。例如我们执行 docker exec 命令能够进入运行中的容器，好像登录进独立的虚拟机一样。实际上这只不过是利用系统调用setns，让当前进程进入到容器进程的Namesapce中，它就能“看到”容器内部的情况了。\n关于容器涉及的基础技术，左耳朵耗子多年前写的系列文章仍然很有参考价值：\nDOCKER基础技术：LINUX NAMESPACE（上） DOCKER基础技术：LINUX NAMESPACE（下） DOCKER基础技术：LINUX CGROUP DOCKER基础技术：AUFS DOCKER基础技术：DEVICEMAPPER # 容器网络 如何让容器之间互相连接保持网络通畅，Docker有多种网络模型。对于单机上运行的多个容器，可以使用缺省的bridge网络驱动。而容器的跨主机通信，一种常用的方式是利用Overlay 网络，基于物理网络的虚拟化网络来实现。\n本文会在单机上实验展示bridge网络模型，揭示其背后的实现原理。下一篇文章会演示容器如何利用Overlay 网络进行跨主机通信。\n我们按照下图创建网络拓扑，让容器之间网络互通，从容器内部可以访问外部资源，同时，容器内可以暴露服务让外部访问。\n在开始动手实验之前，先简单介绍一下bridge网络模型会用到的Linux虚拟化网络技术。\n# Veth Pairs Veth是成对出现的两张虚拟网卡，从一端发送的数据包，总会在另一端接收到。利用Veth的特性，我们可以将一端的虚拟网卡\u0026quot;放入\u0026quot;容器内，另一端接入虚拟交换机。这样，接入同一个虚拟交换机的容器之间就实现了网络互通。\n# Linux Bridge 交换机是工作在数据链路层的网络设备，它转发的是二层网络包。最简单的转发策略是将到达交换机输入端口的报文，广播到所有的输出端口。当然更好的策略是在转发过程中进行学习，记录交换机端口和MAC地址的映射关系，这样在下次转发时就能够根据报文中的MAC地址，发送到对应的输出端口。\n我们可以认为Linux bridge就是虚拟交换机，连接在同一个bridge上的容器组成局域网，不同的bridge之间网络是隔离的。 docker network create [NETWORK NAME]实际上就是创建出虚拟交换机。\n# iptables 容器需要能够访问外部世界，同时也可以暴露服务让外界访问，这时就要用到iptables。另外，不同bridge之间的隔离也会用到iptables。\n我们说的iptables包含了用户态的配置工具(/sbin/iptables)和内核netfilter模块，通过使用iptables命令对内核的netfilter模块做规则配置。\nnetfilter允许在网络数据包处理流程的各个阶段插入hook函数，可以根据预先定义的规则对数据包进行修改、过滤或传送。\n从上图可以看出，网络包的处理流程有五个关键节点：\nPREROUTING：数据包进入路由表之前 INPUT：通过路由表后目的地为本机 FORWARDING：通过路由表后，目的地不为本机 OUTPUT：由本机产生，向外转发 POSTROUTIONG：发送到网卡接口之前 iptables 提供了四种内置的表 raw → mangle → nat → filter，优先级从高到低：\nraw 用于配置数据包，raw中的数据包不会被系统跟踪。不常用。 mangle 用于对特定数据包的修改。不常用。 nat: 用于网络地址转换（NAT）功能（端口映射，地址映射等）。 filter：一般的过滤功能，默认表。 每个表可以设置在多个指定的节点，例如filter表可以设置在INPUT、FORWARDING、OUTPUT等节点。同一个节点中的多个表串联成链。\niptables 是按照表的维度来管理规则，表中包含多个链，链中包含规则列表。例如我们使用sudo iptables -t filter -L 查看filter表：\n可以看到，filter表中包含三个链，每个链中定义了多条规则。由于filter是缺省表，上面的命令可以简化为：sudo iptables -L，即不通过-t指定表时，操作的就是filter表。\n在容器化网络场景，我们经常用到的是在nat表中设置SNAT和DNAT。源地址转换是发生在数据包离开机器被发送之前，因此SNAT只能设置在POSTROUTIONG阶段。DNAT是对目标地址的转换，需要在路由选择前完成，因此可以设置在PREROUTING和OUTPUT阶段。\n# 动手实验 有了前面的背景知识，我们就可以开始动手实验了。因为涉及到很多系统级设置，建议在一个“干净”的虚拟机内折腾，以免干扰到工作环境。我使用的实验环境是Ubuntu 18.04.1 LTS，不需要安装docker，我们使用系统命令模拟出容器网络环境。\n# 场景一：容器间的网络互通 创建“容器” 从前面的背景知识了解到，容器的本质是 Namespace + Cgroups + rootfs。因此本实验我们可以仅仅创建出Namespace网络隔离环境来模拟容器行为：\n1 2 sudo ip netns add docker0 sudo ip netns add docker1 查看创建出的网络Namesapce：\n1 2 3 $ ls -l /var/run/netns -r--r--r-- 1 root root 0 Nov 11 03:52 docker0 -r--r--r-- 1 root root 0 Nov 11 03:52 docker1 创建Veth pairs 1 2 sudo ip link add veth0 type veth peer name veth1 sudo ip link add veth2 type veth peer name veth3 查看创建出的Veth pairs：\n1 2 3 4 5 6 7 8 9 10 $ip addr show ... 3: veth1@veth0: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 3e:fe:2b:90:3e:b7 brd ff:ff:ff:ff:ff:ff 4: veth0@veth1: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 6a:a3:02:07:f4:92 brd ff:ff:ff:ff:ff:ff 5: veth3@veth2: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 76:14:e5:0e:26:98 brd ff:ff:ff:ff:ff:ff 6: veth2@veth3: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 6a:0a:84:0f:a7:f7 brd ff:ff:ff:ff:ff:ff 将Veth的一端放入“容器” 设置Veth一端的虚拟网卡的Namespace，相当于将这张网卡放入“容器”内：\n1 2 sudo ip link set veth0 netns docker0 sudo ip link set veth2 netns docker1 查看“容器” docker0 内的网卡：\n1 2 3 4 5 $ sudo ip netns exec docker0 ip addr show 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 4: veth0@if3: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 6a:a3:02:07:f4:92 brd ff:ff:ff:ff:ff:ff link-netnsid 0 ip netns exec docker0 ...的意思是在网络Namesapce docker0的限制下执行后面跟着的命令，相当于在“容器”内执行命令。\n可以看到，veth0已经放入了“容器”docker0内。同样使用命令sudo ip netns exec docker1 ip addr show查看“容器”docker1内的网卡。\n同时，在宿主机上查看网卡ip addr，发现veth0和veth2已经消失，确实是放入“容器”内了。\n创建bridge 安装bridge管理工具brctl\n1 sudo apt-get install bridge-utils 创建bridge br0：\n1 sudo brctl addbr br0 将Veth的另一端接入bridge 1 2 sudo brctl addif br0 veth1 sudo brctl addif br0 veth3 查看接入效果：\n1 sudo brctl show 两个网卡veth1和veth3已经“插”在bridge上。\n为\u0026quot;容器“内的网卡分配IP地址，并激活上线 docker0容器：\n1 2 sudo ip netns exec docker0 ip addr add 172.18.0.2/24 dev veth0 sudo ip netns exec docker0 ip link set veth0 up docker1容器：\n1 2 sudo ip netns exec docker1 ip addr add 172.18.0.3/24 dev veth2 sudo ip netns exec docker1 ip link set veth2 up Veth另一端的网卡激活上线 1 2 sudo ip link set veth1 up sudo ip link set veth3 up 为bridge分配IP地址，激活上线 1 2 sudo ip addr add 172.18.0.1/24 dev br0 sudo ip link set br0 up “容器”间的互通测试 我们可以先设置监听br0：\n1 sudo tcpdump -i br0 -n 从容器docker0 ping 容器docker1：\n1 sudo ip netns exec docker0 ping -c 3 172.18.0.3 br0上监控到的网络流量：\n1 2 3 4 5 6 7 8 9 10 05:53:10.859956 ARP, Request who-has 172.18.0.3 tell 172.18.0.2, length 28 05:53:10.859973 ARP, Reply 172.18.0.3 is-at 06:f4:01:c2:dd:6e, length 28 05:53:10.860030 IP 172.18.0.2 \u0026gt; 172.18.0.3: ICMP echo request, id 1310, seq 1, length 64 05:53:10.860050 IP 172.18.0.3 \u0026gt; 172.18.0.2: ICMP echo reply, id 1310, seq 1, length 64 05:53:11.878348 IP 172.18.0.2 \u0026gt; 172.18.0.3: ICMP echo request, id 1310, seq 2, length 64 05:53:11.878365 IP 172.18.0.3 \u0026gt; 172.18.0.2: ICMP echo reply, id 1310, seq 2, length 64 05:53:12.901334 IP 172.18.0.2 \u0026gt; 172.18.0.3: ICMP echo request, id 1310, seq 3, length 64 05:53:12.901350 IP 172.18.0.3 \u0026gt; 172.18.0.2: ICMP echo reply, id 1310, seq 3, length 64 05:53:16.006471 ARP, Request who-has 172.18.0.2 tell 172.18.0.3, length 28 05:53:16.006498 ARP, Reply 172.18.0.2 is-at c2:23:fe:ac:f5:4e, length 28 可以看到，先是172.18.0.2发起的ARP请求，询问172.18.0.3的MAC地址，然后是ICMP的请求和响应，最后是172.18.0.3的ARP请求。因为接在同一个bridge br0上，所以是二层互通的局域网。\n同样，从容器docker1 ping 容器docker0也是通的：\n1 sudo ip netns exec docker1 ping -c 3 172.18.0.2 # 场景二：从宿主机访问“容器”内网络 在“容器”docker0内启动服务，监听80端口：\n1 sudo ip netns exec docker0 nc -lp 80 在宿主机上执行telnet，可以连接到docker0的80端口：\n1 telnet 172.18.0.2 80 # 场景三：从“容器”内访问外网 配置内核参数，允许IP forwarding 1 sudo sysctl net.ipv4.conf.all.forwarding=1 配置iptables FORWARD规则 首先确认iptables FORWARD的缺省策略：\n1 sudo iptables -L 如果缺省策略是DROP，需要设置为ACCEPT：\n1 sudo iptables -P FORWARD ACCEPT 缺省策略的含义是，在数据包没有匹配到规则时执行的缺省动作。\n将bridge设置为“容器”的缺省网关 1 2 sudo ip netns exec docker0 route add default gw 172.18.0.1 veth0 sudo ip netns exec docker1 route add default gw 172.18.0.1 veth2 查看“容器”内的路由表：\n1 2 3 4 5 6 $sudo ip netns exec docker0 route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.18.0.1 0.0.0.0 UG 0 0 0 veth0 172.18.0.0 0.0.0.0 255.255.255.0 U 0 0 0 veth0 可以看出，“容器”内的缺省Gateway是bridge的IP地址，非172.18.0.0/24网段的数据包会路由给bridge。\n配置iptables的SNAT规则 容器的IP地址外部并不认识，如果它要访问外网，需要在数据包离开前将源地址替换为宿主机的IP，这样外部主机才能用宿主机的IP作为目的地址发回响应。\n另外一个需要注意的问题，内核netfilter会追踪记录连接，我们在增加了SNAT规则时，系统会自动增加一个隐式的反向规则，这样返回的包会自动将宿主机的IP替换为容器IP。\n1 sudo iptables -t nat -A POSTROUTING -s 172.18.0.0/24 ! -o br0 -j MASQUERADE 上面的命令的含义是：在nat表的POSTROUTING链增加规则，当数据包的源地址为172.18.0.0/24网段，出口设备不是br0时，就执行MASQUERADE动作。\nMASQUERADE也是一种源地址转换动作，它会动态选择宿主机的一个IP做源地址转换，而SNAT动作必须在命令中指定固定的IP地址。\n从“容器”内访问外部地址 1 2 sudo ip netns exec docker0 ping -c 3 123.125.115.110 sudo ip netns exec docker1 ping -c 3 123.125.115.110 我们确认在“容器”内是可以访问外部网络的。\n# 场景四：从外部访问“容器”内暴露的服务 配置iptables的DNAT规则 当外部通过宿主机的IP和端口访问容器内启动的服务时，在数据包进入PREROUTING阶段就要进行目的地址转换，将宿主机IP转换为容器IP。同样，系统会为我们自动增加一个隐式的反向规则，数据包在离开宿主机时自动做反向转换。\n1 sudo iptables -t nat -A PREROUTING ! -i br0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.18.0.2:80 上面命令的含义是：在nat表的PREROUTING链增加规则，当输入设备不是br0，目的端口为80时，做目的地址转换，将宿主机IP替换为容器IP。\n从远程访问“容器”内暴露的服务 在“容器”docker0内启动服务：\n1 sudo ip netns exec docker0 nc -lp 80 在和宿主机同一个局域网的远程主机访问宿主机IP:80\n1 telnet 192.168.31.183 80 确认可以访问到容器内启动的服务。\n# 测试环境恢复 删除虚拟网络设备\n1 2 3 4 sudo ip link set br0 down sudo brctl delbr br0 sudo ip link del veth1 sudo ip link del veth3 iptablers和Namesapce的配置在机器重启后被清除。\n# 总结 本文我们在介绍了veth、Linux bridge、iptables等概念后，亲自动手模拟出了docker bridge网络模型，并测试了几种场景的网络互通。实际上docker network 就是使用了上述技术，帮我们创建和维护网络。通过动手实验，相信你对docker bridge网络理解的更加深入。\n下一篇我将动手实验容器如何利用Overlay 网络进行跨主机通信。\n","date":"2018-10-26T21:40:03+08:00","permalink":"https://mazhen.tech/p/docker%E5%8D%95%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%8A%A8%E6%89%8B%E5%AE%9E%E9%AA%8C/","title":"Docker单机网络模型动手实验"},{"content":"当TiDB 源码阅读系列更新到第六篇《Select 语句概览》时，我发现需要一些关系数据库的基础知识才能更好的理解，例如逻辑查询计划优化其实就是：使用代数定律对查询语句的代数表达式树做等价转换，使改进后的代数表达式树预期可以生成更有效的物理查询计划。有了这些基础知识，看代码才能做到知其然知其所以然。本文希望通过梳理关系数据库背后的知识，为读懂 TiDB 查询处理器部分的源码扫清障碍。\n# 极简数据库发展史 数据库的应用及其广泛，已经成为信息系统的核心技术和重要的基础设施。简单说数据库需要做两件事：存储数据，以及随后在你需要的时候能访问读取数据。\n最早的数据库是基于文件系统，虽然它满足了长期存储数据的需求，但没有提供对文件的查询语言，读取访问非常不便利。于是人们在文件系统上引入一层抽象：数据模型。数据模型是对现实世界数据特征的抽象，能比较真实地模拟现实世界，容易为人所理解，也便于在计算机上实现。\n最早出现的是层次模型（Hierarchical Model），数据被组织为一棵树，类似于今天文档数据库使用的JSON的结构。层次模型很适合处理one-to-many关系，但要表现many-to-many关系则非常困难，一般也不支持join。使用层次模型最著名的数据库是 IBM 的Information Management System (IMS)，它最初是为了解决阿波罗飞船登月计划的需求，协调分散在全球制造的200万个阿波罗飞船零部件的生产进度。\n随后出现了不同的方案解决层次模型的限制，其中最突出的两个模型是网络模型（Network Model）和关系数据模型，最终关系数据模型胜出。\n今天最著名和使用最广泛的数据模型是由 Edgar Codd 博士提出的关系数据模型，他在1970年发布的论文《A Relational Model of Data for Large Shared Data Banks》，奠定了关系数据库的理论基础。ACM在1983年把这篇论文列为从1958年以来的四分之一世纪中具有里程碑式意义的最重要的25篇研究论文之一。到了80年代中期，基于关系数据模型的关系数据库已经成为人们存储、查询结构化数据的首选工具。\n到了2010年，NoSQL兴起，试图颠覆关系数据模型的统治地位。随着互联网的爆发式发展，数据库领域又一次发生了摇摆，伴随着互联网的特殊需求，一批有着新鲜血液的 NoSQL 数据库涌现了出来，层次模型又重新站在了大家面前。NoSQL为了应对海量数据存储和高并发访问，决定放弃关系数据模型和事务等关系数据数据库的关键特性。自从 NoSQL 概念横空出世，关系数据库似乎成了低效、高成本、速度慢的数据处理模式的代名词。然而，NoSQL在解决问题的同时也给使用者带来了很多困扰， 最终一致让应用开发者要面对各种复杂的场景。\n数据库技术的发展是螺旋式上升，Google发布的Spanner和F1两篇论文，让人们看到了关系数据模型 和 NoSQL 融合的可能性。以 TiDB 为代表的 NewSQL 数据库，让人们重新享受关系模型、强一致性事务等对使用者友好的特性，同时也具备了 NoSQL 的水平扩展能力。\n# 关系数据模型 和 关系代数 数据模型是对现实世界事物的抽象，而关系数据模型将一切事物都抽象为关系，并通过集合运算的方式规定了关系之间的运算过程，模型相对的比较简单，数据证明严谨，因此很快被大家广泛接受。\n这一节我将介绍关系数据库的数学基础：关系数据模型和关系代数。\n# 关系数据模型 关系模型为人们提供了一种描述数据的方法：一个称为关系（relation）的二维表。现实世界的实体以及实体间的各种联系都可以用关系来表示。我们通过例子来了解关系模型的重要术语：\n雇员表\nemp_no name birth_date gender hire_date 1 汤唯 1990-06-08 女 2015-08-01 2 刘亦菲 1994-09-10 女 2017-05-06 3 刘德华 1986-04-18 男 2008-09-01 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 关系（Relation） ：一个关系对应通常说的一张二维表 元组（Tuple） ： 表中的一行即为一个元组 属性（Attribute） ：表中的一列即为一个属性，给每一个属性起一个名称即属性名 键（Key）：表中的某个属性组，它可以唯一确定一个元组 域（Domain） : 是一组具有相同数据类型的值的集合。属性的取值范围来自某个域。例如性别的域是（男，女）。 关系模式（schema）：对关系的描述，先给出一个关系名，其后是用圆括号扩起来的所有属性，例如：employees（emp_no, name, birth_date, gender, hire_date） # 关系代数 一门代数是由一些操作符和操作数组成。例如算术代数的加、减、乘、除是操作符，变量x和常量8是操作数。任何一门代数都允许把操作符作用在操作数上构造出表达式（expression），例如算术表达式 (x+y)*3。\n关系代数 也是一门代数，它的操作数是关系，操作运算符有两类：集合运算符和专门的关系运算符。\n关系代数可以认为是一种抽象的查询语言，利用对关系的运算来表达查询，运算对象是关系，运算结果也是关系。因此，关系代数的表达式也被称为查询（query）。\n# 传统的集合运算 三个最常见的集合操作是：并（union）、交（intersection）、差（difference）。\nR ∪ S，表示关系R和S的并，得到的结果关系的元素来自R或者S，或R和S中都出现过。 R ∩ S，表示关系R和S的交，同时在R和S中存在的元素集合。 R - S，表示关系R和S的差，它是由在R中出现但不在S中出现的元素构成的集合。 另外一个集合操作是笛卡尔积（Cartesian Product）。\n关系R和S的笛卡尔积是一个有序对的集合，有序对的一个元素是关系R中的任何一个元组，第二个元素是关系S中的任何一个元组表示为 R × S。 关系R\nA B 1 2 3 4 关系S\nB C D 2 5 6 4 7 8 9 10 11 R × S的结果\nA R.B S.B C D 1 2 2 5 6 1 2 4 7 8 1 2 9 10 11 3 4 2 5 6 3 4 4 7 8 3 4 9 10 11 # 专门的关系运算 选择（selection)，当选择操作符应用到关系R上时，产生一个关系R的元组的子集合。结果关系元组必须满足某个涉及R中属性的条件C，表示为 σC( R )\n投影 （projection），用来从一个关系生成一个新的关系，这个关系只包含原来关系R中的部分列。表达式 πA1,A2,\u0026hellip;,An ( R ) 的值是这样一个关系，它只包含关系R属性A1,A2,...An所代表的列。\nθ连接，关系R和关系S满足条件C的θ连接可以用这样的符号来表示： R ⋈C S\nθ连接的结果这样构造：\n先得到R和S的笛卡尔积 在得到的关系中寻找满足条件C的元组 关系R\nA B C 1 2 3 6 7 8 9 7 8 关系S\nB C D 2 3 4 2 3 5 7 8 10 R ⋈ A\u0026lt;D AND R.B ≠ S.B S 的结果是：\nA R.B R.C S.B S.C D 1 2 3 7 8 10 有两类常用的连接运算：\n等值连接（equijoin）：比较运算符为 = 的连接运算称为等值连接。例如： R ⋈ R.A = S.B S 是从关系R与S的笛卡尔积中选取A、B属性值相等的那些元组。 自然连接（Natural join）：自然连接是一种特殊的等值连接，两个关系中进行比较的分量必须是相同的属性组，并在结果中把重复的属性列去掉。关系R和S的自然连接表示为 R ⋈ S 关系R\nA B 1 2 3 4 关系S\nB C D 2 5 6 4 7 8 9 10 11 R ⋈ S\nA B C D 1 2 5 6 3 4 7 8 两个关系R和S在做自然连接时，如果一个元组不能和另外关系中的任何一个元组配对的话，这个元组被称为悬浮元组（Dangling tuple）。上面的例子中，关系S的第三个元组就是悬浮元组。\n如果把悬浮元组也保存在结果关系中，而在其他属性上填空值(Null)，就叫做外连接（Outer Join）。\n左外连接(LEFT OUTER JOIN或LEFT JOIN)：只保留左边关系R中的悬浮元组 右外连接(RIGHT OUTER JOIN或RIGHT JOIN)：只保留右边关系S中的悬浮元组 # 关系代数的扩展操作符 消除重复操作符（duplicated-elimination operator）用 δ(R) 来返回一个没有重复元组的关系R 聚集操作符 （aggregation operator）用来汇总或者聚集关系某一列中出现的值，有 SUM，AVG，MIN，MAX，COUNT 等 分组操作（grouping）根据元组在一个或多个属性上的值把关系的元组拆成“组”。这样聚集操作就可以对分组的各个列进行计算。分组操作符 γ 是组合了分组和聚合操作的一个算子。例如表达式： γ gender, COUNT(emp_no)-\u0026gt;count(employees) 代表把性别（gender）作为分组属性，然后对每一个分组进行COUNT(emp_no)的操作。 排序算子（sorting operator）如果关系R的模式是 R(A,B,C)，那么 τC( R ) 就把R中的元组按照属性C的值排序。 # 关系代数小结 上面的知识有些枯燥，但非常容易理解，因为我们经常使用关系数据库，已经接受了这些概念。掌握了一些关系代数的知识，在阅读TiDB源码时，当看到selection、projection 这些术语就能一下想到它们对应的关系代数运算符。\n这里只介绍了关系代数最基本的概念，如果想完整学习，建议参考斯坦福大学大学的课程CS145: A First Course in Database Systems，对应的教材有中文版《数据库系统基础教程》。\n其实我们在查询时提交给数据库的就是关系代数表达式，它是关系运算符的组合，数据库会根据一些代数定律对最初的表达式做等价变换，得出一个最优的等价表达式（equivalent expression），即可以更快的被计算出结果的表达式。这个过程就是逻辑查询计划优化，后面我会简单的介绍相关概念。\n# SQL 的诞生 SQL(Structured Query Language) 结构化查询语言，是关系数据库的标准语言。\n在1970年Codd博士提出了关系模型之后，由于关系代数或者关系都太数学了，难以被普通用户接受。IBM在研制关系数据库管理系统原型System R的过程中，决定摈弃数学语言，以自然语言为方向，结果诞生了结构化英语查询语言（Structured English Query Language，SEQUEL），后来更名为SQL。System R 因此获得1988年度ACM“软件系统奖”。\nSQL是声明式查询语言，你只需要指定想要获得什么样的数据，而无须了解如何实现这个目标。SQL具体是如何执行的，取决于数据库系统的查询处理器，它来决定哪些索引和哪些连接方法可以使用，以及以什么样的顺序执行查询的各个部分。SQL隐藏了数据库引擎的实现细节，因此用户可以在不修改查询语句的情况下，享受到数据库性能优化带来的好处。\n下面我们来看看数据库的查询处理器。\n# 关系数据库的查询处理器 SQL是在很高层次上表达查询，那么数据库的查询处理器必须提供查询被如何执行的大量细节。下面我从概念上介绍查询处理器的处理流程，实际的数据库实现要复杂的多，特别是像 TiDB 这样的分布式数据库。如果想比较系统的了解数据库的实现技术，同样推荐斯坦福大学计算机科学专业的课程 CS245: Database System Implementation。上面提到的CS145是CS245的预修课。国内很少有讲数据库内部实现的书，这门课的教材值得阅读。当然最好的学习方法是理论联系实践，多去读 TiDB 的源代码:)\n一般查询处理可以简单的划分为以下几个步骤：\n对SQL进行语法分析，将查询语句转换成抽象语法树。 把抽象语法树转换成关系代数表达式树，这就是初始的逻辑查询计划。 使用关系代数中的多个代数定律改进初始的代数表达式树。利用一些代数定律，可以把代数表达式树转换成一个等价的表达式树，后者预期可以生成更有效的物理查询计划。这一步进行了查询重写，可以称为逻辑查询计划优化。 为逻辑查询计划的每一个操作符选择实现算法，并确定这些操作符的执行顺序，逻辑查询计划被转化为物理查询计划。物理查询计划指明了要执行的操作，操作的执行顺序，执行每步所用的算法，获取数据的方式，以及数据从一个操作传递给另一个操作的方式。 # 查询示例 本文准备以一个简单的例子来介绍查询处理的流程，下面是查询涉及的两张表：\nemployees\nemp_no name birth_date gender hire_date 1 汤唯 1990-06-08 女 2015-08-01 2 刘亦菲 1994-09-10 女 2017-05-06 3 刘德华 1986-04-18 男 2008-09-01 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; salaries\nemp_no salary last_modified 1 8000 2018-04-01 2 6000 2018-04-01 3 15000 2018-04-01 \u0026hellip; \u0026hellip; \u0026hellip; 想要获取工资大于7000的员工姓名列表，SQL语句可以这么写：\n1 2 3 4 SELECT name FROM employees, salaries WHERE employees.emp_no = salaries.emp_no AND salary \u0026gt; 7000; # SQL 语法分析 SQL Parser的功能是把SQL语句按照SQL语法规则进行解析，将文本转换成抽象语法树（AST）。具体的实现可以参考这篇文章《TiDB SQL Parser 的实现》。 示例SQL解析完成后得到下面的语法树：\n# 生成逻辑查询计划 现在将上一步生成的语法树转换成关系代数表达式树，也就是逻辑查询计划。对于示例SQL的抽象语法树转换过程如下：\n\u0026lt;FromList\u0026gt; 中涉及的关系employees和salaries做笛卡尔积运算 选择（selection）运算 σC，其中C被替换成\u0026lt;Condition\u0026gt;表达式，即employees.emp_no = salaries.emp_no AND salary \u0026gt; 7000 投影（projection） πL，其中L是\u0026lt;SelList\u0026gt;中的属性列表，对于这个查询只有一个属性name 我们得到下面的关系代数表达式树：\n# 逻辑查询计划的改进 当我们把查询语句转换成关系代数表达式时，得到了一个初始的逻辑查询计划。现在我们可以使用关系代数中的多个代数定律改进逻辑查询计划。\n这里仅仅列出一小部分这样的代数定律，它们可以将一个表达式树转换成一个等价的表达式树。\n交换律和结合律 R × S = S × R; (R X S) × T = R × (S × T)\nR ⋈ S = S ⋈ R; (R ⋈ S) ⋈ T = R ⋈ (S ⋈ T)\nR ∪ S = S ∪ R; (R ∪ S) ∪ T = R ∪ (S ∪ T)\nR ∩ S = S ∩ R; (R ∩ S) ∩ T = R ∩ (S ∩ T)\n涉及选择的定律 σC1 AND C2 = σC1(σC2( R ))\nσC1 OR C2 = (σC1( R ) ∪ (σC2( R ) )\nσC1(σC2( R )) = σC2(σC1( R ))\n下推选择 在表达式中下推选择是查询优化器最强有力的工具。\nσC( R × S ) = σC( R ) × S\nσC( R ⋈ S) = σC( R ) ⋈ S\nσC( R ⋈D S) = σC( R ) ⋈D S\n涉及连接和笛卡尔积的定律 R ⋈C S = σC( R × S)\n涉及消除重复的定律 δ(R×S) = δ( R ) × δ(S)\nδ(R⋈CS) = δ( R ) ⋈C δ(S)\n另外还有涉及投影的定律、涉及分组和聚集的定律。这部分有些理论化，可以参考这篇《TiDB 源码阅读系列文章（七）基于规则的优化》看看 TiDB 具体是怎么做的。\n对于本例使用到的定律比较简单。先将选择的两部分分解为 σemployees.emp_no = salaries.emp_no 和 σsalary \u0026gt; 7000，后者可以在树中下推。第一个条件涉及笛卡尔积两边的属性，可以把上面提到的定律 R ⋈C S = σC( R × S)\n从右向左使用，把笛卡尔积转换成连接。使用了两个定律后，得到优化后的逻辑查询计划如下图：\n# 物理查询计划的生成 这一步我们需要把逻辑查询计划转换成物理查询计划。通常由逻辑计划可以得到多个物理计划，我们需要对每个物理计划进行评估，选择具有最小估计代价的物理查询计划。\n# 基于代价的物理计划选择 在从逻辑计划构建物理计划的时候，因为可能得到多个物理计划，我们需要通过估计物理计划执行的代价来确定最优选择。\n如何计算这个代价呢？我们可以用物理计划每一步的任务执行时发生的磁盘I/O数、网络吞吐量、占用的内存空间大小等近似估算。\n这些资源的访问和占用，又是由什么决定的呢？可能包括的决定因素有：\n参与运算任务执行的数据大小 数据的分布位置（连续的磁盘空间、离散的磁盘空间、网络节点等） 关系中属性的不同值的数目 属性值的最大值、最小值、以及值的分布情况 数据库会收集统计这些信息，用来估算具体任务的代价。\n逻辑查询计划在转换成物理计划的时候，每一步的转换都会面临多种情况的选择，最容易想到的是使用穷举法，估算每一种情况的代价，最后确定最优的物理计划。但使用穷举法的话，很可能估算本身的代价变得非常大，实践中可以采用动态规划（dynamic programming）等算法。\n# 枚举物理查询计划 以上一步输出的逻辑查询计划为例，看看在枚举物理查询计划时需要做出哪些选择。\n首先，逻辑查询计划的每个结点转换成什么样的物理运算符会遇到多种选择。我们从逻辑查询计划树自底往上来看：\n逻辑计划的叶子结点 逻辑查询计划树的叶子结点被一个扫描运算符替代。这些运算符一般包括：\nTableScan( R )：以任意顺序读人所有元组 SortScan(R, L)：按照顺序读入R的元组，并以列L中的属性进行排列 IndexScan(R, C)：C是一个带有比较运算符的条件，例如 A = 100，A是R的一个属性，如果A上建立的索引，可以通过索引来访问R的元组。如果比较运算符不是等值比较，则索引必须是一个支持范围查询的索引，例如B+ Tree IndexScan(R, A)：这里A是R的一个属性，关系R通过A上的索引被检索。 如果R的数据在磁盘上不是占用连续的存储空间，该运算符可能比TableScan更有效。 逻辑计划的σ选择（selection)结点 σ选择结点一般有两种选择：\n可以简单的用物理过滤运算符Filter( C )替代 σC( R ) 如果C是一个带有比较运算符的条件，例如 A = 100，并且属性A上有索引，可以把比较运算合并到扫描运算符：IndexScan(R, A = 100)。 对于本例，salary 一般都不会建立索引，因此可以把σ( salary \u0026gt; 7000 ) 替换为 Filter( salary \u0026gt; 7000 )\n逻辑计划的连接结点 常见的等值连接满足结合律和交换律，因此连接可以得到很多候选的物理执行计划。其中最关键的问题是确定连接顺序。\n当两个关系连接，只有两种选择，一般我们应该将估计值较小的关系放在前面。当连接有2个以上关系时，可能的连接树的数量会迅速增加，例如4个关系的连接将会有4!=24种连接方式。这一部分很复杂，就不在本文讨论了。\n除了连接顺序，还需要确定具体使用的连接算法。常见的算法有：\nnested loops 基于排序的join（sort merge join） hash join 基于索引的join 逻辑计划的投影结点 投影结点的任务比较明确，输出包含指定列的关系。\n除了将逻辑查询计划的结点转换成物理运算符，在选择物理计划时还要考虑数据如何在运算符之间流动（中间结果保存到磁盘或流水线方式），物理运算符的执行顺序等。这些细节本文就不再讨论。\n最后，假定我们在所有选择的组合中，确定了其中一个作为最优的物理查询计划，然后就可以把它交给查询执行器真正的执行了：\n# 写在最后 本文把关系数据库查询处理涉及的基础知识进行了梳理，希望对你理解 TiDB 的代码能有所帮助。\n数据库是一个非常迷人的领域，它有很强的理论基础，同时又涉及大量的工程实践，可以说是最复杂的系统软件之一。我相信能开发数据库是很多程序员的梦想。梦想还是要有的，让我们一起努力吧！\n","date":"2018-07-01T17:04:31+08:00","permalink":"https://mazhen.tech/p/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%89%AB%E7%9B%B2/","title":"关系数据库查询处理基础知识扫盲"},{"content":" # 为什么XA事务建议用SERIALIZABLE隔离级别 在MySQL最新的官方文档中，关于XA Transactions的介绍有这么一段描述：\nAs with nondistributed transactions, SERIALIZABLE may be preferred if your applications are sensitive to read phenomena. REPEATABLE READ may not be sufficient for distributed transactions.\n这段话表达的意思是，对于分布式XA事务， REPEATABLE READ 隔离级别是不够的。\nMySQL旧版本的文档，对于这个问题表达的更加直接：\nHowever, for a distributed transaction, you must use the SERIALIZABLE isolation level to achieve ACID properties. It is enough to use REPEATABLE READ for a nondistributed transaction, but not for a distributed transaction.\n怎么理解呢？举个简单的例子：假设MySQL使用的是REPEATABLE READ 隔离级别，XA事务 T1 修改的数据涉及两个节点 A 和 B，当事务 T1 在 A 上完成commit，而在 B 上还没commit之前，也就是说这时事务 T1 并没有真正结束，另一个XA事务 T2 已经可以访问到 T1 在 A 上提交后数据，这不是出现脏读了吗？\n那么使用SERIALIZABLE就能保证吗？还是看例子：事务 T1 修改节点 A 上的数据 a -\u0026gt; a'，修改 B 上的数据 b -\u0026gt; b'，在提交阶段，可能被其他事务 T2 读取到了 a'， 因为使用了SERIALIZABLE隔离级别， MySQL 会对所有读加锁，那么 T2 在 B 上读取 b 时会被一直阻塞，直到 T1 在 B 上完成commit，这时 T2 在 B 读取到的就是 b'。 也就是说，SERIALIZABLE隔离级别保证了读到 a' 的事务，不会读到 b ，而是读到 b'，确保了事务ACID的要求。\n更加详细的描述可以参考鹅厂 TDSQL XA 事务隔离级别的奥秘，他们的结论是：\n如果某个并发事务调度机制可以让具有依赖关系的事务构成一个有向无环图(DAG)，那么这个调度就是可串行化的调度。由于每个后端DB都在使用serializable隔离级别，所以每个后端DB上面并发执行的事务分支构成的依赖关系图一定是DAG。\n只要所有连接都是用serializable隔离级别，那么TDSQL XA执行的事务仍然可以达到可串行化隔离级别。\n# SERIALIZABLE性能差，有更好的实现方式吗 如果分布式事务想实现read-committed以上的隔离级别，又不想使用SERIALIZABLE，有什么更好的方式吗？\n当然有，想想看TiDB是怎么做的，底层TiKV是一个整体，有全局的MVCC，所以能够做到分布式事务的Snapshot隔离级别。\nPostgreSQL社区中，有Postgres-XC和Postgres-XL的方案，采用的并发机制是全局MVCC 和本地写锁。 Postgres-XC 维持了全局活跃事务列表，从而提供全局MVCC。\n虽然MySQL也实现了MVCC，但它没有将底层K/V带有时间戳的版本信息暴露出来。也就是说，多个MySQL实例组成的集群没有全局的MVCC，无法得到全局一致性快照，自然就很难做到分布式的Snapshot隔离级别。腾讯的这篇文章也分析了这么做比较困难：\n由于MySQL innodb使用MVCC做select（除了serializable和for update/lock in share mode子句），还需要将这个全局事务id给予innodb做事务id，同时，还需要TDSQL XA集群的多个set的innodb 共享各自的本地事务状态给所有其他innodb（这也是PGXL 所做的），任何一个innodb的本地事务的启动，prepare，commit，abort都需要通知给所有其他innodb实例。只有这样做，集群中的每个innodb实例才能够建立全局完全有一致的、当前集群中正在处理的所有事务的状态，以便做多版本并发控制。 这本身都会造成极大的性能开销，并且导致set之间的严重依赖，降低系统可靠性。这些都是我们要极力避免的。\n# 结论 根据上面的分析，如果使用MySQL 的 XA分布式事务，最安全的方式还是按照官方建议，使用SERIALIZABLE隔离级别。\n如果想基于MySQL做改造，实现全局MVCC，从而实现分布式事务的Snapshot隔离级别，目前还没有看到MySQL社区有这类项目，相信实现难度比较大。\n","date":"2018-06-05T14:44:59+08:00","permalink":"https://mazhen.tech/p/%E5%85%B3%E4%BA%8Emysql-xa%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","title":"关于MySQL XA事务的隔离级别"},{"content":"现在新出现的MySQL中间件除了基本的数据sharding功能，都增加了很多高级特性，我觉得有三个特性比较重要：\n分布式事务的支持 数据的强一致复制，提高了数据的安全性和可用性 支持跨shard join 通过对这些特性的支持，MySQL中间件具备了一些newSQL数据库的能力，不再是个纯粹的中间件，让用户更容易使用。我调研了最近开源的青云RadonDB，希望了解下这方面最新的进展。\n先简单看下RadonDB的整体架构：存储/计算分离。存储节点是MySQL，3个一组用raft实现数据的强一致性，不再是异步/半同步复制，数据的安全性、可用性级别更高。上层是SQL节点，负责客户端连接、SQL语句的解析和执行、分布式事务协调、数据sharding逻辑等。右下脚计算节点的作用，后面会解释。\n在知乎上看到\u0026lt;如何评价青云开源的分布式数据库 radondb\u0026gt;，RandoDB被吐槽的很厉害。我们从这些吐槽可以了解产品宣传之外的一些信息，知道做这种中间件不是那么容易。大家对RandoDB的几个关键特性的实现方式都不太满意。让我们逐一看看。\n分布式事务的实现 对分布式事务的实现大家吐槽的最厉害：\n官方宣传用XA实现了Snapshot Isolation，然而众所周知XA是无法实现SI的。所谓的事务其实只支持单条SQL，BEGIN / SET AUTOCOMMIT=0 都不支持。\n单语句事务，就是不能 begin 开启事务。\n为了达到 SI 隔离级别，在执行用户 SQL 时，会加上一个 commitLock，防止其他事务提交。这决定了加锁必须时间很短，比如一条SQL，如果你从start transaction开始加锁，那其他事务全都无法提交了，系统事实上已经不可用。\n所谓分布式事物快照隔离级别是 radondb 层 query 和 commit 语句串行化实现的。这个应该是串行化隔离级别了。而且是和冲突没关系的串行化，就是说根本不管两个事物之间有没有冲突数据。性能自行脑补。\n没有 XA log 的 XA 事务原子性实现都是耍流氓。\n为什么XA是无法实现SI？ 我的理解是，单个MySQL实例虽然实现了MVCC，但它没有将底层K/V带有时间戳的版本信息暴露出来。也就是说，多个MySQL实例组成的集群没有全局的MVCC，每个实例内部的MVCC是独立的，无法得到全局一致性快照。XA事务跨越了多个节点，所以没办法实现Snapshot隔离级别。可以对比下TiDB的实现，底层TiKV是一个整体，有全局的MVCC，所以能在上层支持分布式事务的Snapshot隔离级别。\nRandoDB的实现能work，但相当于在Proxy层将所有事务串行化，即使两个事务之间没有数据冲突。而且只有单语句事务。\n对于XA log，开发者的解释是：\nproxy xa log只针对xa commit出错，目前通过分析log然后人工介入，这里没有再记log的必要\n我觉得这么做很不严谨。2PC协议有一个问题，一旦事务参与者投票，它必须等待coordinator给出指示，提交或放弃。如果这时coordinator挂了，事务参与者除了等待什么也做不了。事务处于未决状态，事务的最终结果记录在coordinator的事务日志中，只能等它recovery。因此，现在很多改进的做法是用Paxos/raft保证事务日志的高可用，coordinator挂了可以快速切换。即使不用raft，找一个地方可靠持久的保存事务日志是非常必要的。\n使用Raft保证强一致性 现在很多项目都会使用Paxos/Raft来改进MySQL的复制机制，实现数据的强一致性。如果主、备间任何时刻都完全一致，那么任何时刻都可以安全的进行主备切换。如果无法保证主、备间的强一致性，那么当有持续不断的更新时，主备切换就无法保证强一致性，需要在切换时暂停主库的写入，服务会有短暂的中断。\n腾讯的PhxSQL就是建立在Paxos的一致性和MySQL的binlog流水基础上，通过Paxos保证强一致和高可用的MySQL集群。关于PhxSQL的设计理念可以参见：\n谈谈PhxSQL的设计和实现哲学（上） 谈谈PhxSQL的设计和实现哲学（下） 采用类似做法的还有阿里云的MySQL金融版。另外，MySQL官方也从5.7开始推出了Group Replication机制，内部使用Paxos实现了多个副本的一致。\nRadonDB的实现机制和PhxSQL不太一样。它在一组MySQL集群内的复制还是通过Semi-Sync机制（Semi-Sync设置为无限大，不退化为异步复制），保证有一个slave和master完全一致。主备切换时会选择这个slave为主，然后结合MySQL的 Multi-threaded replication 和 GTID机制 进行快速回放，让主备重新一致。Raft用在哪里了？在 RadonDB 只使用 Raft 进行选主，当主挂掉之后，使用 Raft 选出新的主。Raft选主的逻辑是选出一个拥有最多的committed log的成员作为主，那么对于RadonDB来说，哪个MySQL的GTID最大就选哪个。\n我自己还没有使用Raft的经验，不确定RadonDB的实现机制是否合理。但利用Semi-Sync模拟同步复制的方案，我觉得有一个地方不妥。当和主库保持强同步的备库有问题时，这组MySQL整体就不可用，因为它需要至少一个备库和主库完全一致，这就因为单点降低了整个集群的可用性。如果是用Raft做数据复制，就不会有这种单点影响全局可用性的问题。\n另外，RandoDB被吐槽 Raft 的实现业余、不严谨：\n打开用来做HA的Xenon模块，一看又是作业级的肯写代码不肯写测试的raft练手实现。raft测试用例一共1500行go代码 刚才数了下，自己的raft库光election相关的单元测试用例就数千行代码了。做生产环境用的系统不是练手写作业，需要一个go的raft库，既然都不肯写完备的测试了，那就老老实实用etcd或者hashicorp的raft。自己私下撸一个raft库练手，和给自己全职项目选一个可靠的raft实现，两者真的不矛盾。最滑稽，只做选主干嘛自己撸一个raft实现？\njoin等复杂查询的实现 严格说RandoDB是不支持join的。它的做法是让计算节点通过binglog复制了全量数据，SQL节点会把join等复杂查处路由到计算节点执行。\n“计算节点”使用tokudb存储引擎存储全量数据，为了支持复杂查询。。。如果我一个分布式系统的数据总量有20T、100T，也用单个“计算节点”存储全量数据？而且这个数据同步过程是异步的，显然没法用在OLTP场景。\n通过一些实用的方式支持了Join，这种做法可以work，但RandoDB离它宣称的数据库还差很远，缺少全局的执行计划优化。\n总体来说，RandoDB的理想很宏大，用实用的方案解决了一些问题，但要成为真正成熟的数据库产品还差的比较远。RadonDB 的核心代码1万行左右。加上其它类库引入，Radon代码11万+， Xenon代码5万行+ 。\n最后，看到有人推荐腾讯的TDSQL，也顺便了解了一下。从资料看TDSQL很不错，可惜不是开源产品。除了水平扩张、安全增强、自动化运维以外，它具备了我们上面提到的数据库中间件的高级特性：\n支持分布式事务XA 全局事务的隔离级别最高可以达到serializable级别 分布式事务对业务透明，兼容单机事务语法 允许事务中多条语句分别发给多个分片 支持autocommit下单条语句写访问多个分片 默认采用强同步复制，即主从节点数据完全一致 复杂查询方面 允许以流式处理方式运行group by、order by 支持两个Shard使用shardkey（分表键）做等值连接，以及使用shardkey的子查询 支持了部分受限的复杂查询，对于数据库中间件来说已经算比较强大了。关于TDSQL的分布式事务，可以通过这两篇进行更多的了解：\n一文教你迅速解决分布式事务 XA 一致性问题 鹅厂 TDSQL XA 事务隔离级别的奥秘 如果我们做MySQL中间件，可以瞄准TDSQL，对于分布式事务、数据强一致性，以及复杂查询、跨shard join 等特性都要考虑支持。\n","date":"2018-06-03T14:43:12+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8Eradondb%E7%9C%8B%E6%96%B0%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E7%89%B9%E6%80%A7/","title":"从RadonDB看新型数据库中间件的特性"},{"content":"TiDB源码解读系列的《Insert语句概览》讲解了Insert执行的整体流程，并在最后用一幅图描述了整个流程：\n我按照自己的理解对这幅图扩展了一下，在原先数据结构转换流程的基础上，补充了代码的调用流程，个人感觉更加全面，希望对你阅读代码也有帮助。\n","date":"2018-05-15T14:38:05+08:00","permalink":"https://mazhen.tech/p/tidb-insert-%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE/","title":"TiDB Insert 执行流程图"},{"content":" Previous Next \u0026nbsp; \u0026nbsp; Page: / ","date":"2018-05-11T17:01:04+08:00","permalink":"https://mazhen.tech/p/cursor%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0%E6%80%BB%E7%BB%93/","title":"Cursor功能实现总结"},{"content":"PingCAP发布了TiDB的源码阅读系列文章，让我们可以比较系统的去学习了解TiDB的内部实现。最近的一篇《SQL 的一生》，从整体上讲解了一条SQL语句的处理流程，从网络上接收数据，MySQL协议解析和转换，SQL语法解析，查询计划的制定和优化，查询计划执行，到最后返回结果。\n其中，SQL Parser的功能是把SQL语句按照SQL语法规则进行解析，将文本转换成抽象语法树（AST），这部分功能需要些背景知识才能比较容易理解，我尝试做下相关知识的介绍，希望能对读懂这部分代码有点帮助。\nTiDB是使用goyacc根据预定义的SQL语法规则文件parser.y生成SQL语法解析器。我们可以在TiDB的Makefile文件中看到这个过程，先build goyacc工具，然后使用goyacc根据parser.y生成解析器parser.go：\n1 2 3 4 5 6 goyacc: $(GOBUILD) -o bin/goyacc parser/goyacc/main.go parser: goyacc bin/goyacc -o /dev/null parser/parser.y bin/goyacc -o parser/parser.go parser/parser.y 2\u0026gt;\u0026amp;1 ... goyacc是yacc的Golang版，所以要想看懂语法规则定义文件parser.y，了解解析器是如何工作的，先要对Lex \u0026amp; Yacc有些了解。\n# Lex \u0026amp; Yacc 介绍 Lex \u0026amp; Yacc 是用来生成词法分析器和语法分析器的工具，它们的出现简化了编译器的编写。Lex \u0026amp; Yacc 分别是由贝尔实验室的Mike Lesk 和 Stephen C. Johnson在1975年发布。对于Java程序员来说，更熟悉的是ANTLR，ANTLR 4 提供了 Listener+Visitor 组合接口， 不需要在语法定义中嵌入actions，使应用代码和语法定义解耦。Spark的SQL解析就是使用了ANTLR。Lex \u0026amp; Yacc 相对显得有些古老，实现的不是那么优雅，不过我们也不需要非常深入的学习，只要能看懂语法定义文件，了解生成的解析器是如何工作的就够了。我们可以从一个简单的例子开始：\n上图描述了使用Lex \u0026amp; Yacc构建编译器的流程。Lex根据用户定义的patterns生成词法分析器。词法分析器读取源代码，根据patterns将源代码转换成tokens输出。Yacc根据用户定义的语法规则生成语法分析器。语法分析器以词法分析器输出的tokens作为输入，根据语法规则创建出语法树。最后对语法树遍历生成输出结果，结果可以是产生机器代码，或者是边遍历 AST 边解释执行。\n从上面的流程可以看出，用户需要分别为Lex提供patterns的定义，为 Yacc 提供语法规则文件，Lex \u0026amp; Yacc 根据用户提供的输入文件，生成符合他们需求的词法分析器和语法分析器。这两种配置都是文本文件，并且结构相同：\n1 2 3 4 5 ... definitions ... %% ... rules ... %% ... subroutines ... 文件内容由 %% 分割成三部分，我们重点关注中间规则定义部分。对于上面的例子，Lex 的输入文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ... %% /* 变量 */ [a-z] { yylval = *yytext - \u0026#39;a\u0026#39;; return VARIABLE; } /* 整数 */ [0-9]+ { yylval = atoi(yytext); return INTEGER; } /* 操作符 */ [-+()=/*\\n] { return *yytext; } /* 跳过空格 */ [ \\t] ; /* 其他格式报错 */ . yyerror(\u0026#34;invalid character\u0026#34;); %% ... 上面只列出了规则定义部分，可以看出该规则使用正则表达式定义了变量、整数和操作符等几种token。例如整数token的定义如下：\n1 2 3 4 [0-9]+ { yylval = atoi(yytext); return INTEGER; } 当输入字符串匹配这个正则表达式，大括号内的动作会被执行：将整数值存储在变量 yylval 中，并返回 token 类型 INTEGER 给 Yacc。\n再来看看 Yacc 语法规则定义文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 %token INTEGER VARIABLE %left \u0026#39;+\u0026#39; \u0026#39;-\u0026#39; %left \u0026#39;*\u0026#39; \u0026#39;/\u0026#39; ... %% program: program statement \u0026#39;\\n\u0026#39; | ; statement: expr { printf(\u0026#34;%d\\n\u0026#34;, $1); } | VARIABLE \u0026#39;=\u0026#39; expr { sym[$1] = $3; } ; expr: INTEGER | VARIABLE { $$ = sym[$1]; } | expr \u0026#39;+\u0026#39; expr { $$ = $1 + $3; } | expr \u0026#39;-\u0026#39; expr { $$ = $1 - $3; } | expr \u0026#39;*\u0026#39; expr { $$ = $1 * $3; } | expr \u0026#39;/\u0026#39; expr { $$ = $1 / $3; } | \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; { $$ = $2; } ; %% ... 第一部分定义了 token 类型和运算符的结合性。四种运算符都是左结合，同一行的运算符优先级相同，不同行的运算符，后定义的行具有更高的优先级。\n语法规则使用了BNF定义。BNF 可以用来表达上下文无关（context-free）语言，大部分的现代编程语言都可以使用 BNF 表示。上面的规则定义了三个产生式。产生式冒号左边的项（例如 statement）被称为非终结符， INTEGER 和 VARIABLE 被称为终结符,它们是由 Lex 返回的 token 。终结符只能出现在产生式的右侧。可以使用产生式定义的语法生成表达式：\n1 2 3 4 5 expr -\u0026gt; expr * expr -\u0026gt; expr * INTEGER -\u0026gt; expr + expr * INTEGER -\u0026gt; expr + INTEGER * INTEGER -\u0026gt; INTEGER + INTEGER * INTEGER 解析表达式是生成表达式的逆向操作，我们需要归约表达式到一个非终结符。Yacc 生成的语法分析器使用自底向上的归约（shift-reduce）方式进行语法解析，同时使用堆栈保存中间状态。还是看例子，表达式x + y * z的解析过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 1 . x + y * z 2 x . + y * z 3 expr . + y * z 4 expr + . y * z 5 expr + y . * z 6 expr + expr . * z 7 expr + expr * . z 8 expr + expr * z . 9 expr + expr * expr . 10 expr + expr . 11 expr . 12 statement . 13 program . 点（.）表示当前的读取位置，随着 . 从左向右移动，我们将读取的token压入堆栈，当发现堆栈中的内容匹配了某个产生式的右侧，则将匹配的项从堆栈中弹出，将该产生式左侧的非终结符压入堆栈。这个过程持续进行，直到读取完所有的tokens，并且只有启始非终结符（本例为 program）保留在堆栈中。\n产生式右侧的大括号中定义了该规则关联的动作，例如：\n1 expr: expr \u0026#39;*\u0026#39; expr { $$ = $1 * $3; } 我们将堆栈中匹配该产生式右侧的项替换为产生式左侧的非终结符，本例中我们弹出 expr '*' expr，然后把 expr 压回堆栈。 我们可以使用 $position 的形式访问堆栈中的项，$1引用的是第一项，$2引用的是第二项，以此类推。$$ 代表的是归约操作执行后的堆栈顶。本例的动作是将三项从堆栈中弹出，两个表达式相加，结果再压回堆栈顶。\n上面例子中语法规则关联的动作，在完成语法解析的同时，也完成了表达式求值。一般我们希望语法解析的结果是一棵抽象语法树（AST），可以这么定义语法规则关联的动作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ... %% ... expr: INTEGER { $$ = con($1); } | VARIABLE { $$ = id($1); } | expr \u0026#39;+\u0026#39; expr { $$ = opr(\u0026#39;+\u0026#39;, 2, $1, $3); } | expr \u0026#39;-\u0026#39; expr { $$ = opr(\u0026#39;-\u0026#39;, 2, $1, $3); } | expr \u0026#39;*\u0026#39; expr { $$ = opr(\u0026#39;*\u0026#39;, 2, $1, $3); } | expr \u0026#39;/\u0026#39; expr { $$ = opr(\u0026#39;/\u0026#39;, 2, $1, $3); } | \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; { $$ = $2; } ; %% nodeType *con(int value) { ... } nodeType *id(int i) { ... } nodeType *opr(int oper, int nops, ...) { ... } 上面是一个语法规则定义的片段，我们可以看到，每个规则关联的动作不再是求值，而是调用相应的函数，该函数会返回抽象语法树的节点类型 nodeType，然后将这个节点压回堆栈，解析完成时，我们就得到了一颗由 nodeType 构成的抽象语法树。对这个语法树进行遍历访问，可以生成机器代码，也可以解释执行。\n至此，我们大致了解了Lex \u0026amp; Yacc的原理。其实还有非常多的细节，例如如何消除语法的歧义，但我们的目的是读懂TiDB的代码，掌握这些概念已经够用了。\n# goyacc 简介 goyacc 是golang版的 Yacc。和 Yacc的功能一样，goyacc 根据输入的语法规则文件，生成该语法规则的go语言版解析器。goyacc 生成的解析器 yyParse 要求词法分析器符合下面的接口：\n1 2 3 4 type yyLexer interface { Lex(lval *yySymType) int Error(e string) } 或者\n1 2 3 4 5 type yyLexerEx interface { yyLexer // Hook for recording a reduction. Reduced(rule, state int, lval *yySymType) (stop bool) // Client should copy *lval. } TiDB没有使用类似 Lex 的工具生成词法分析器，而是纯手工打造，词法分析器对应的代码是 parser/lexer.go， 它实现了 goyacc 要求的接口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ... // Scanner implements the yyLexer interface. type Scanner struct { r reader buf bytes.Buffer errs []error stmtStartPos int // For scanning such kind of comment: /*! MySQL-specific code */ or /*+ optimizer hint */ specialComment specialCommentScanner sqlMode mysql.SQLMode } // Lex returns a token and store the token value in v. // Scanner satisfies yyLexer interface. // 0 and invalid are special token id this function would return: // return 0 tells parser that scanner meets EOF, // return invalid tells parser that scanner meets illegal character. func (s *Scanner) Lex(v *yySymType) int { tok, pos, lit := s.scan() v.offset = pos.Offset v.ident = lit ... } // Errors returns the errors during a scan. func (s *Scanner) Errors() []error { return s.errs } 另外lexer 使用了字典树技术进行 token 识别，具体的实现代码在parser/misc.go\n# TiDB SQL Parser的实现 终于到了正题。有了上面的背景知识，对TiDB 的 SQL Parser 模块会相对容易理解一些。先看SQL语法规则文件parser.y，goyacc 就是根据这个文件生成SQL语法解析器的。\nparser.y 有6500多行，第一次打开可能会被吓到，其实这个文件仍然符合我们上面介绍过的结构：\n1 2 3 4 5 ... definitions ... %% ... rules ... %% ... subroutines ... parser.y 第三部分 subroutines 是空白没有内容的， 所以我们只需要关注第一部分 definitions 和第二部分 rules。\n第一部分主要是定义token的类型、优先级、结合性等。注意 union 这个联合体结构体：\n1 2 3 4 5 6 7 %union { offset int // offset item interface{} ident string expr ast.ExprNode statement ast.StmtNode } 该联合体结构体定义了在语法解析过程中被压入堆栈的项的属性和类型。\n压入堆栈的项可能是终结符，也就是 token，它的类型可以是item 或 ident；\n这个项也可能是非终结符，即产生式的左侧，它的类型可以是 expr 、 statement 、 item 或 ident。\ngoyacc 根据这个 union 在解析器里生成对应的 struct 是：\n1 2 3 4 5 6 7 8 type yySymType struct { yys int offset int // offset item interface{} ident string expr ast.ExprNode statement ast.StmtNode } 在语法解析过程中，非终结符会被构造成抽象语法树（AST）的节点 ast.ExprNode 或 ast.StmtNode。抽象语法树相关的数据结构都定义在 ast 包中，它们大都实现了 ast.Node 接口：\n1 2 3 4 5 6 7 // Node is the basic element of the AST. // Interfaces embed Node should have \u0026#39;Node\u0026#39; name suffix. type Node interface { Accept(v Visitor) (node Node, ok bool) Text() string SetText(text string) } 这个接口有一个 Accept 方法，接受 Visitor 参数，后续对 AST 的处理，主要依赖这个 Accept 方法，以 Visitor 模式遍历所有的节点以及对 AST 做结构转换。\n1 2 3 4 5 // Visitor visits a Node. type Visitor interface { Enter(n Node) (node Node, skipChildren bool) Leave(n Node) (node Node, ok bool) } 例如 plan.preprocess 是对 AST 做预处理，包括合法性检查以及名字绑定。\nunion 后面是对 token 和 非终结符 按照类型分别定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 /* 这部分的token是 ident类型 */ %token \u0026lt;ident\u0026gt; ... add \u0026#34;ADD\u0026#34; all \u0026#34;ALL\u0026#34; alter \u0026#34;ALTER\u0026#34; analyze \u0026#34;ANALYZE\u0026#34; and \u0026#34;AND\u0026#34; as \u0026#34;AS\u0026#34; asc \u0026#34;ASC\u0026#34; between \u0026#34;BETWEEN\u0026#34; bigIntType \u0026#34;BIGINT\u0026#34; ... /* 这部分的token是 item 类型 */ %token \u0026lt;item\u0026gt; /*yy:token \u0026#34;1.%d\u0026#34; */ floatLit \u0026#34;floating-point literal\u0026#34; /*yy:token \u0026#34;1.%d\u0026#34; */ decLit \u0026#34;decimal literal\u0026#34; /*yy:token \u0026#34;%d\u0026#34; */ intLit \u0026#34;integer literal\u0026#34; /*yy:token \u0026#34;%x\u0026#34; */ hexLit \u0026#34;hexadecimal literal\u0026#34; /*yy:token \u0026#34;%b\u0026#34; */ bitLit \u0026#34;bit literal\u0026#34; andnot \u0026#34;\u0026amp;^\u0026#34; assignmentEq \u0026#34;:=\u0026#34; eq \u0026#34;=\u0026#34; ge \u0026#34;\u0026gt;=\u0026#34; ... /* 非终结符按照类型分别定义 */ %type \u0026lt;expr\u0026gt; Expression \u0026#34;expression\u0026#34; BoolPri \u0026#34;boolean primary expression\u0026#34; ExprOrDefault \u0026#34;expression or default\u0026#34; PredicateExpr \u0026#34;Predicate expression factor\u0026#34; SetExpr \u0026#34;Set variable statement value\u0026#39;s expression\u0026#34; ... %type \u0026lt;statement\u0026gt; AdminStmt \u0026#34;Check table statement or show ddl statement\u0026#34; AlterTableStmt \u0026#34;Alter table statement\u0026#34; AlterUserStmt \u0026#34;Alter user statement\u0026#34; AnalyzeTableStmt \u0026#34;Analyze table statement\u0026#34; BeginTransactionStmt \u0026#34;BEGIN TRANSACTION statement\u0026#34; BinlogStmt \u0026#34;Binlog base64 statement\u0026#34; ... %type \u0026lt;item\u0026gt; AlterTableOptionListOpt \u0026#34;alter table option list opt\u0026#34; AlterTableSpec \u0026#34;Alter table specification\u0026#34; AlterTableSpecList \u0026#34;Alter table specification list\u0026#34; AnyOrAll \u0026#34;Any or All for subquery\u0026#34; Assignment \u0026#34;assignment\u0026#34; ... %type \u0026lt;ident\u0026gt; KeyOrIndex \u0026#34;{KEY|INDEX}\u0026#34; ColumnKeywordOpt \u0026#34;Column keyword or empty\u0026#34; PrimaryOpt \u0026#34;Optional primary keyword\u0026#34; NowSym \u0026#34;CURRENT_TIMESTAMP/LOCALTIME/LOCALTIMESTAMP\u0026#34; NowSymFunc \u0026#34;CURRENT_TIMESTAMP/LOCALTIME/LOCALTIMESTAMP/NOW\u0026#34; ... 第一部分的最后是对优先级和结合性的定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 ... %precedence sqlCache sqlNoCache %precedence lowerThanIntervalKeyword %precedence interval %precedence lowerThanStringLitToken %precedence stringLit ... %right assignmentEq %left pipes or pipesAsOr %left xor %left andand and %left between ... parser.y文件的第二部分是SQL语法的产生式和每个规则对应的 aciton 。SQL语法非常复杂，parser.y 的大部分内容都是产生式的定义。\nSQL 语法可以参照MySQL参考手册的SQL Statement Syntax 部分，例如 SELECT 语法的定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 SELECT [ALL | DISTINCT | DISTINCTROW ] [HIGH_PRIORITY] [STRAIGHT_JOIN] [SQL_SMALL_RESULT] [SQL_BIG_RESULT] [SQL_BUFFER_RESULT] [SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS] select_expr [, select_expr ...] [FROM table_references [PARTITION partition_list] [WHERE where_condition] [GROUP BY {col_name | expr | position} [ASC | DESC], ... [WITH ROLLUP]] [HAVING where_condition] [ORDER BY {col_name | expr | position} [ASC | DESC], ...] [LIMIT {[offset,] row_count | row_count OFFSET offset}] [PROCEDURE procedure_name(argument_list)] [INTO OUTFILE \u0026#39;file_name\u0026#39; [CHARACTER SET charset_name] export_options | INTO DUMPFILE \u0026#39;file_name\u0026#39; | INTO var_name [, var_name]] [FOR UPDATE | LOCK IN SHARE MODE]] 我们可以在 parser.y 中找到 SELECT 语句的产生式：\n1 2 3 4 5 6 7 8 9 SelectStmt: \u0026#34;SELECT\u0026#34; SelectStmtOpts SelectStmtFieldList OrderByOptional SelectStmtLimit SelectLockOpt { ... } | \u0026#34;SELECT\u0026#34; SelectStmtOpts SelectStmtFieldList FromDual WhereClauseOptional SelectStmtLimit SelectLockOpt { ... } | \u0026#34;SELECT\u0026#34; SelectStmtOpts SelectStmtFieldList \u0026#34;FROM\u0026#34; TableRefsClause WhereClauseOptional SelectStmtGroup HavingClause OrderByOptional SelectStmtLimit SelectLockOpt { ... } 产生式 SelectStmt 和 SELECT 语法是对应的。\n我省略了大括号中的 action ，这部分代码会构建出 AST 的 ast.SelectStmt 节点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 type SelectStmt struct { dmlNode resultSetNode // SelectStmtOpts wraps around select hints and switches. *SelectStmtOpts // Distinct represents whether the select has distinct option. Distinct bool // From is the from clause of the query. From *TableRefsClause // Where is the where clause in select statement. Where ExprNode // Fields is the select expression list. Fields *FieldList // GroupBy is the group by expression list. GroupBy *GroupByClause // Having is the having condition. Having *HavingClause // OrderBy is the ordering expression list. OrderBy *OrderByClause // Limit is the limit clause. Limit *Limit // LockTp is the lock type LockTp SelectLockType // TableHints represents the level Optimizer Hint TableHints []*TableOptimizerHint } 可以看出，ast.SelectStmt 结构体内包含的内容和 SELECT 语法也是一一对应的。\n其他的产生式也都是根据对应的 SQL 语法来编写的。从 parser.y 的注释看到，这个文件最初是用工具从 BNF 转化生成的，从头手写这个规则文件，工作量会非常大。\n完成了语法规则文件 parser.y 的定义，就可以使用 goyacc 生成语法解析器：\n1 bin/goyacc -o parser/parser.go parser/parser.y 2\u0026gt;\u0026amp;1 TiDB对 lexer 和 parser.go 进行了封装，对外提供 parser.yy_parser 进行SQL语句的解析：\n1 2 3 4 // Parse parses a query string to raw ast.StmtNode. func (parser *Parser) Parse(sql, charset, collation string) ([]ast.StmtNode, error) { ... } 最后，我写了一个简单的例子，使用TiDB的 SQL Parser 进行SQL语法解析，构建出 AST，然后利用 visitor 遍历 AST ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/pingcap/tidb/parser\u0026#34; \u0026#34;github.com/pingcap/tidb/ast\u0026#34; ) type visitor struct{} func (v *visitor) Enter(in ast.Node) (out ast.Node, skipChildren bool) { fmt.Printf(\u0026#34;%T\\n\u0026#34;, in) return in, false } func (v *visitor) Leave(in ast.Node) (out ast.Node, ok bool) { return in, true } func main() { sql := \u0026#34;SELECT /*+ TIDB_SMJ(employees) */ emp_no, first_name, last_name \u0026#34; + \u0026#34;FROM employees USE INDEX (last_name) \u0026#34; + \u0026#34;where last_name=\u0026#39;Aamodt\u0026#39; and gender=\u0026#39;F\u0026#39; and birth_date \u0026gt; \u0026#39;1960-01-01\u0026#39;\u0026#34; sqlParser := parser.New() stmtNodes, err := sqlParser.Parse(sql, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) if err != nil { fmt.Printf(\u0026#34;parse error:\\n%v\\n%s\u0026#34;, err, sql) return } for _, stmtNode := range stmtNodes { v := visitor{} stmtNode.Accept(\u0026amp;v) } } 我实现的 visitor 什么也没干，只是输出了节点的类型。 这段代码的运行结果如下，依次输出遍历过程中遇到的节点类型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 *ast.SelectStmt *ast.TableOptimizerHint *ast.TableRefsClause *ast.Join *ast.TableSource *ast.TableName *ast.BinaryOperationExpr *ast.BinaryOperationExpr *ast.BinaryOperationExpr *ast.ColumnNameExpr *ast.ColumnName *ast.ValueExpr *ast.BinaryOperationExpr *ast.ColumnNameExpr *ast.ColumnName *ast.ValueExpr *ast.BinaryOperationExpr *ast.ColumnNameExpr *ast.ColumnName *ast.ValueExpr *ast.FieldList *ast.SelectField *ast.ColumnNameExpr *ast.ColumnName *ast.SelectField *ast.ColumnNameExpr *ast.ColumnName *ast.SelectField *ast.ColumnNameExpr *ast.ColumnName 了解了TiDB SQL Parser 的实现，我们就有可能实现TiDB当前不支持的语法，例如添加内置函数，也为我们学习查询计划以及优化打下了基础。希望这篇文章对你能有所帮助。\n","date":"2018-05-09T14:34:58+08:00","permalink":"https://mazhen.tech/p/tidb-sql-parser-%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"TiDB SQL Parser 的实现"},{"content":"配合这篇《基于代价的优化》 阅读。\nCBO的整体思路是：从逻辑查询计划树，自上而下枚举每个逻辑运算符可能的物理算子，从所有可能的执行路径中选择一条评估代价最小的作为物理查询计划。\n一个逻辑运算符受两个因素的影响，导致生成多个候选的物理执行计划：\n逻辑运算符可能有多种候选的物理算子供选择，如下表： 有些物理算子会根据参与运算的属性、属性的顺序等因素，生成多种物理执行计划，例如Join的物理算子会根据参与连接的表的顺序，生成多种可能的执行计划。 CBO核心流程的代码在plan/optimizer.go中的physicalOptimize：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func physicalOptimize(logic LogicalPlan) (PhysicalPlan, error) { logic.preparePossibleProperties() _, err := logic.deriveStats() if err != nil { return nil, errors.Trace(err) } t, err := logic.findBestTask(\u0026amp;requiredProp{taskTp: rootTaskType, expectedCnt: math.MaxFloat64}) if err != nil { return nil, errors.Trace(err) } p := t.plan() p.ResolveIndices() return p, nil } 三行关键的代码：\nlogic.preparePossibleProperties()：裁剪参与运算的属性，从而尽可能早的裁减掉成物理计划搜索路径上的分支 logic.deriveStats()：为每个逻辑计划节点生成统计信息，为评估物理计划的代价做准备 logic.findBestTask：生成执行代价最小的task findBestTask的核心逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 for _, pp := range p.self.exhaustPhysicalPlans(prop) { // find best child tasks firstly. childTasks = childTasks[:0] for i, child := range p.children { childTask, err := child.findBestTask(pp.getChildReqProps(i)) if err != nil { return nil, errors.Trace(err) } childTasks = append(childTasks, childTask) } // combine best child tasks with parent physical plan. curTask := pp.attach2Task(childTasks...) // get the most efficient one. if curTask.cost() \u0026lt; bestTask.cost() { bestTask = curTask } } 首先枚举可能的物理执行计划p.self.exhaustPhysicalPlans，然后遍历每种候选计划，找到代价最小的task。这是个递归的过程，当前节点的代价是由所有子节点的代价组成的，所以在遍历的过程中，又会调用child.findBestTask(pp.getChildReqProps(i))找到子节点的最佳task。\n如何评估物理执行计划的代价呢？根据参与运算的关系（表）的统计信息进行评估。代价评估相关逻辑涉及的代码：\n计算关系的统计信息：plan/stats.go 计算task的代价：plan/task.go中的attach2Task系列方法。 ","date":"2018-05-08T14:40:17+08:00","permalink":"https://mazhen.tech/p/%E5%9F%BA%E4%BA%8E%E4%BB%A3%E4%BB%B7%E4%BC%98%E5%8C%96cbo%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81%E5%AF%BC%E8%AF%BB/","title":"基于代价优化（CBO）实现代码导读"},{"content":"Go的错误处理机制很简洁，使用errors.New(text)创建 error，方法的调用者一般按照如下模式处理：\n1 2 3 if err != nil { return err } 这样做最大的问题是error中没有保存方法调用栈等上下文信息，只能靠创建时传递的string参数来区分error，很难定位错误发生的具体位置。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import ( \u0026#34;fmt\u0026#34; \u0026#34;errors\u0026#34; ) func f1() error { return f2() } func f2() error { return f3() } func f3() error { return errors.New(\u0026#34;std error\u0026#34;) } func main() { if err := f1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) } } 执行的输出为：\n1 std error 在实际的程序中调用关系复杂，仅凭错误信息很难定位错误源头。TiDB 使用了juju/errors来记录调用栈：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import ( \u0026#34;github.com/juju/errors\u0026#34; \u0026#34;fmt\u0026#34; ) func jf1() error { err := jf2() if err != nil { return errors.Trace(err) } return nil } func jf2() error { err := jf3() if err != nil { return errors.Trace(err) } return nil } func jf3() error { return errors.New(\u0026#34;juju error\u0026#34;) } func main() { if err := jf1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) } } 这段代码的输出为：\n1 2 3 github.com/mz1999/error/main.go:25: juju error github.com/mz1999/error/main.go:19: github.com/mz1999/error/main.go:11: 可以看到，如果想记录调用栈，每次都需要调用errors.Trace。这样做比较繁琐，而且每次trace时内部都会调用runtime.Caller，性能不佳。TiDB已经调研了新的第三方包pkg/errors准备替换掉juju/errors。\n使用pkg/errors会简单很多，和标准库的errors一致，但可以记录调用栈信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/pkg/errors\u0026#34; ) func pf1() error { return pf2() } func pf2() error { return pf3() } func pf3() error { return errors.New(\u0026#34;pkg error\u0026#34;) } func main() { if err := pf1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) } } 这段代码的输出为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 pkg error main.pf3 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:17 main.pf2 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:13 main.pf1 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:9 main.main /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:21 runtime.main /usr/local/go/src/runtime/proc.go:198 runtime.goexit /usr/local/go/src/runtime/asm_amd64.s:2361 这样看，使用pkg/errors替代标准库的errors就可以满足我们的需求。\n另外，pkg/errors的作者还给了一些最佳实践的建议：\n在你自己的代码中，在错误的发生点使用errors.New 或 errors.Errorf ： 1 2 3 4 5 6 func parseArgs(args []string) error { if len(args) \u0026lt; 3 { return errors.Errorf(\u0026#34;not enough arguments, expected at least 3, got %d\u0026#34;, len(args)) } // ... } 如果你接收到一个error，一般简单的直接返回： 1 2 3 if err != nil { return err } 如果你是调用第三方的包或标准库时接收到error，使用 errors.Wrap or errors.Wrapf 包装这个error，它会记录在这个点的调用栈： 1 2 3 4 f, err := os.Open(path) if err != nil { return errors.Wrapf(err, \u0026#34;failed to open %q\u0026#34;, path) } Always return errors to their caller rather than logging them throughout your program.\n在程序的top level，或者是worker goroutine，使用 %+v 输出error的详细信息。\n1 2 3 4 5 6 7 func main() { err := app.Run() if err != nil { fmt.Printf(\u0026#34;FATAL: %+v\\n\u0026#34;, err) os.Exit(1) } } 如果需要抛出包含MySQL错误码的内部错误，可以使用errors.Wrap包装，附带上报错位置的调用栈信息： 1 2 3 func pf3() error { return errors.Wrap(mysql.NewErr(mysql.ErrCantCreateTable, \u0026#34;tablename\u0026#34;, 500), \u0026#34;\u0026#34;) } 这样，我们既拿到了完整调用栈，又可以使用errors.Cause获取MySQL的错误码等信息：\n1 2 3 4 5 6 7 8 9 10 11 12 if err := pf1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) var sqlError *mysql.SQLError if m, ok := errors.Cause(err).(*mysql.SQLError); ok { sqlError = m } else { sqlError = mysql.NewErrf(mysql.ErrUnknown, \u0026#34;%s\u0026#34;, err.Error()) } fmt.Printf(\u0026#34;\\nMySQL error code: %d, state: %s\u0026#34;, sqlError.Code, sqlError.State) } ","date":"2018-04-06T14:23:40+08:00","permalink":"https://mazhen.tech/p/golang-error%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/","title":"Golang error处理实践"},{"content":"Go中的引用类型不是指针，而是对指针的包装，在它的内部通过指针引用底层数据结构。每一种引用类型也包含一些其他的field，用来管理底层的数据结构。\n看一个例子比较直观：\n1 2 s := []string{\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;} fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) 简单解释一下这段代码。先初始化一个slice，然后使用unsafe.Pointer(\u0026amp;s)把slice的指针转换为通用指针Pointer。Pointer是可以代表任何数据类型的指针。最后把Pointer强制转换为*reflect.SliceHeader。SliceHeader代表的是slice运行时数据结构，定义如下：\n1 2 3 4 5 type SliceHeader struct { Data uintptr Len int Cap int } 可以看到，SliceHeader内部有一个用来指向底层数组的指针Data，另外还有两个属性Len和Cap用来保存slice的内部状态。\n上面的代码运行结果如下：\n\u0026amp;reflect.SliceHeader{Data:0xc420078180, Len:3, Cap:3}\nslice可以自动扩容，当底层数组容量不够时，会自动创建一个新的数组替换。让我们做个实验：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 s := []string{\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;} fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Println() s = append(s, \u0026#34;d\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Println() s = append(s, \u0026#34;e\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Println() 运行结果如下：\n对于初始化容量为3的slice，在向这个slice append 新元素时，底层会创建一个容量翻倍的新数组，并将原先的内容复制过来，再将新元素append到最后。我们可以看到这个slice内部保存底层数组的指针在第一次append后，指向了新的地址。当再向它append新元素时，由于底层数组还有空间，内部指针保持不变，只是更新Len属性为5。\n在Go中进行函数调用时，参数都是按值传递的。对于引用类型也是按值传递，会复制引用本身，但不会复制引用指向的底层数据结构。还是看代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func foo(s []string) { fmt.Println(\u0026#34;======= func foo =======\u0026#34;) s = append(s, \u0026#34;f\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Printf(\u0026#34;\u0026amp;s: %p \\n\u0026#34;, \u0026amp;s) fmt.Println(\u0026#34;========================\\n\u0026#34;) } func main() { ...... s = append(s, \u0026#34;e\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Printf(\u0026#34;\u0026amp;s: %p \\n\u0026#34;, \u0026amp;s) fmt.Println() foo(s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Printf(\u0026#34;\u0026amp;s: %p \\n\u0026#34;, \u0026amp;s) } 运行结果为：\n在函数调用时，传递给函数的slice进行了复制，函数的参数是一个新的slice，但slice内部指针指向的底层数组还是同一个。\n完整的示例代码在https://play.golang.org/p/qwwSuskLfCa，可以在Playground中直接运行。\nGo语言的引用类型有slice, map, channel, interface和function。技术上，string也是引用类型：\n1 2 3 4 type StringHeader struct { Data uintptr Len int } 有时候为了性能优化，可以利用[]byte和string头部结构的“部分相同”，以非安全的指针类型转换来实现类型变更，避免底层数组的复制。例如 TiDB 中就使用了这个技巧：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // String converts slice to string without copy. // Use at your own risk. func String(b []byte) (s string) { if len(b) == 0 { return \u0026#34;\u0026#34; } pbytes := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;b)) pstring := (*reflect.StringHeader)(unsafe.Pointer(\u0026amp;s)) pstring.Data = pbytes.Data pstring.Len = pbytes.Len return } // Slice converts string to slice without copy. // Use at your own risk. func Slice(s string) (b []byte) { pbytes := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;b)) pstring := (*reflect.StringHeader)(unsafe.Pointer(\u0026amp;s)) pbytes.Data = pstring.Data pbytes.Len = pstring.Len pbytes.Cap = pstring.Len return } 上面两个函数实现了[]byte和string的互相转换，不需要底层数组的copy。\n","date":"2018-03-05T14:20:18+08:00","permalink":"https://mazhen.tech/p/go%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%95%E7%94%A8%E7%B1%BB%E5%9E%8B/","title":"Go语言的引用类型"},{"content":"TiDB提供了docker compose的部署方式，可以很方便的在单机上搭建一个TiDB集群作为开发测试环境。如果修改了TiDB源码，可以使用这样方式，先在本机部署集群做一些验证。\n首先本机要安装docker和docker compose，建议参考官方文档Install Docker 和 Install Docker Compose\n下载tidb-docker-compose项目\n1 git clone https://github.com/pingcap/tidb-docker-compose.git 使用docker compose启动TiDB集群 1 cd tidb-docker-compose \u0026amp;\u0026amp; sudo docker-compose up -d 就这么简单，集群启动成功了。使用docker ps查看：\n可以看到，已经启动了三个tikv实例，一个tidb实例，三个pd实例，还有监控和tidb-vision。\n监控的访问地址是 http://localhost:3000，用户名/密码：admin/admin。\ntidb-vision 的访问地址是 http://localhost:8010\n使用MySQL客户端访问TiDB 如果本机有MySQL客户端，可以直接连接：\n1 mysql -h 127.0.0.1 -P 4000 -u root 如果本机没有MySQL客户端，可以使用docker启动一个MySQL容器，然后登录到容器内，再使用MySQL客户端连接TiDB集群。这种方式比较环保，只要有docker环境就行。先查看TiDB集群的docker网络：\n然后启动MySQL容器，注意要加入TiDB集群的docker网络：\n1 sudo docker run --network=tidbdockercompose_default --rm -it mysql /bin/bash 因为和TiDB集群在同一个docker网络，在MySQL容器内，可以使用tidb名称访问到TiDB：\n1 mysql -h tidb -P 4000 -u root 停止集群 1 sudo docker-compose down 如果自己build了TiDB版本想在本机run集群，文档写的很清楚，告诉你镜像应该放在什么位置。\nHave fun!\n","date":"2018-02-09T14:32:20+08:00","permalink":"https://mazhen.tech/p/%E5%88%A9%E7%94%A8docker-compose%E5%9C%A8%E5%8D%95%E6%9C%BA%E4%B8%8A%E7%8E%A9%E8%BD%ACtidb/","title":"利用docker compose在单机上玩转TiDB"},{"content":"翻了一下TiDB的文档，对TiDB有了个大概的了解。简单说，TiDB的实现架构是：底层是分布式KV引擎TiKV，上层是SQL引擎TiDB Servers。一般传统数据库也是这么分层实现的，只不过TiKV实现了一个分布式、强一致、支持事务的K/V，不像数据库是单机版K/V。在TiKV之上实现SQL引擎就简化了很多，因此TiDB Servers是无状态的。\n简化的抽象架构分层：\nTiDB官方文档里的架构图：\n可以看出，TiDB的基础工作和最突出的创新在TiKV，理论上有了这个KV，可以把单机版的SQl引擎实现方式搬过来，就有了一个可扩展的分布式数据库。\n那就看看TiKV的架构：用RocksDB作为单机存储引擎，然后上层用Raft实现了一个分布式、强一致性的K/V。有了这个很强大的分布式K/V，在上面实现了MVCC层，就是对每个Key加了version，然后基于MVCC层最终实现了分布式事务。\nRocksDB内部用的是LSM-Tree，写入性能肯定比MySQL的B+ tree好。读取性能看实现的优化情况了，不过RocksDB是Facebook做的，应该没啥问题。\nRaft的实现和测试用例是从Etcd完全拷贝过来的，可以认为Raft的实现也是稳定的。 作者的原话：\n我们做了一件比较疯狂的事情，就是我们把 Etcd 的 Raft 状态机的每一行代码，line by line 的翻译成了 Rust。而我们第一个转的就是所有 Etcd 本身的测试用例。我们写一模一样的 test ，保证这个东西我们 port 的过程是没有问题的。\n分布式事务参照的是Percolator。Percolator和Spanner差不多，只不过Spanner引入了专有硬件原子钟，而Percolator依靠单点的授时服务器。两者都是对两阶段提交协议的改进。我们搞过J2EE，对两阶段提交协议应该比较熟悉，2PC的问题是：一旦事务参与者投票，它必须等待coordinator给出指示：提交或放弃。如果这时coordinator挂了，事务参与者除了等待什么也做不了。事务处于未决状态，事务的最终结果记录在coordinator的事务日志中，只能等它recovery（HeuristicCommitException、HeuristicMixedException、HeuristicRollbackException等异常就是遇到了这种情况，只好资源自己做了决定）。这么看在本质上，2PC为了达到一致性，实际上是退化到由coordinator单节点来实现atomic commit. Spanner引入了trueTime api，底下存储是MVCC，每行数据都带一个时间戳做version，TrueTime API就是打时间戳的，用时间戳标识事务顺序，解决2PC依赖单点coordinator的问题。而依赖单点的授时服务器的问题，他们是这样解释的：\n因为 TSO 的逻辑极其简单，只需要保证对于每一个请求返回单调递增的 id 即可，通过一些简单的优化手段（比如 pipeline）性能可以达到每秒生成百万 id 以上，同时 TSO 本身的高可用方案也非常好做，所以整个 Percolator 模型的分布式程度很高。\nTiDB的事务隔离级别实现了Read committed和Repeatable read，没有实现最严格的Serializable。不过串行化的隔离级别在现实中很少使用，性能会很差。oracle 11g也没有实现它。oracle实现的是snapshot isolation，实际上比串行化的保证要弱。TiDB和oracle都用是MVCC保证了Repeatable read，简单说就是每个事务都读取一个一致性的snapshot，这个snapshot肯定就是完整状态。所以叫做snapshot isolation。按照TiDB的文档，TiDB 实现的 snapshot 隔离级别，该隔离级别不会出现幻读，但是会出现写偏斜。\n写偏斜是什么，举个简单的例子：两个事务都先分别查询在线值班的医生总数，发现还有两个在线的医生，然后各自更新不同的记录，分别让不同的医生下线。事务提交后，两个医生都下线了，没有一个医生在线值班，出现错误的业务场景。这种异常情况是两个事务分别更新不同的记录。引起写倾斜的的模式：先查询很多列看是否满足某种条件，然后依赖查询结果写入数据并提交。解决的方法有：真正的串行化隔离级别，或者显示的锁定事务依赖的行。\n从文档看，TiDB利用了成熟的开源项目，自己实现了分布式事务、分布式存储和SQL引擎，整体方案诱人，至于软件成熟程度，还需要经过实际的使用测试。\n","date":"2018-02-09T14:28:29+08:00","permalink":"https://mazhen.tech/p/tidb%E5%88%9D%E6%8E%A2/","title":"TiDB初探"},{"content":"Zion项目我们采用Feature Branch Workflow，即每个特性在branch中开发，master始终保持稳定。特性开发完成，需提交pull request，接受其他成员的code review，同时可以在PR中围绕该特性进行讨论，PR记录了开发过程的细节。\n由于是内部项目，我们没有使用fork机制，代码都维护在Github上的一个仓库：apusic/zion。在看具体的流程前，先有一个全局视图：\n# 基本工作流程 从远程clone respository 1 git clone https://github.com/apusic/zion.git 创建特性分支 首先让本地的master处于最新状态：\n1 2 3 git fetch git checkout master git rebase origin/master 创建分支\n1 git checkout -b myfeature 在分支上进行开发 1 2 3 git status # View the state of the repo git add # Stage a file git commit # Commit a file 进行一个功能特性开发时，可以多次提交到本地仓库Repository，不必每次commit都push到远程仓库Remote。\n开发过程中，保持分支和最新代码同步 1 2 3 # While on your myfeature branch. git fetch git rebase origin/master 关于rebase的详细说明，请参考 The \u0026ldquo;Git Branching - Rebasing\u0026rdquo; chapter from the Pro Git book.\n后面会单独介绍rebase冲突的处理。\n将分支发布到中心仓库 所有改动都提交后，执行：\n1 git push -f origin myfeature 创建pull request 访问项目主页，点击Compare \u0026amp; pull request创建pull request\ncode review \u0026amp; discussions 可以要求一个或两个项目成员进行review，也可以围绕该特性进行讨论。\nMerge pull requests 根据项目的配置，pull requests在merge进master之前要满足一些条件，例如至少两个成员review，通过集成测试等。\n所有的检查都通过后，这个pull request就可以merge了。详细的操作参见 Merging a pull request\n这里有三个merge选型：Merge pull request，Squash and merge，Rebase and merge，关于它们的区别请参考GitHub的帮助。 一般建议选择Squash and merge。\n至此，一个特性就开发完成了。\n删除分支 已经完成merged的 pull requests 关联的分支可以在GitHub删除，详细的操作步骤参见GitHub文档 Deleting and restoring branches in a pull request\n# 冲突处理 上面的流程中，在保持分支和最新代码同步时，最有可能产生冲突。\nrebase提示冲突，会列出冲突文件，执行下列步骤：\n手工解决冲突 git add \u0026lt;some-file\u0026gt; 将发生冲突的文件放回index区 git rebase --continue 继续进行rebase 提示rebase成功 在Merge pull requests过程中也可能产生冲突，可以在GitHub的界面上解决冲突，详细的操作轻参考Addressing merge conflicts。\n如果冲突较多，建议先在客户端执行rebase，按照上面的步骤解决完冲突，再进行Merge pull requests。\n","date":"2018-01-15T14:38:05+08:00","permalink":"https://mazhen.tech/p/git-feature-branch-workflow/","title":"Git Feature Branch Workflow"},{"content":"Wireshark是排查网络问题最常用的工具，它已经内置支持了上百种通用协议，同时它的扩展性也很好，对于自定义的应用层网络协议，你可以使用c或者lua编写协议解析插件，这样你就可以在Wireshark中观察到协议的内容而不是二进制流，为排查问题带来一定的便利性。\n最近在排查一个HSF超时的问题，顺便花了些时间为Wireshark写了一个HSF2协议解析插件，目前支持HSF2的request、response和heart beat协议，支持将多个packet还原为上层PDU。暂不支持HSF原先的TB Remoting协议。先看效果。\n首先在Packet List区域已经能识别HSF2协议：\nHSF的请求和响应\nHSF的心跳协议\n点击某个数据包，可以在Packet details区域查看详细的协议内容： HSF请求\n可以看到很多协议的重要信息，包括序列化方式，超时时间，服务名称、方法及参数\nHSF响应\nHeartBeat请求\n心跳协议比较简单，响应就不看了。\n插件是使用lua开发的，安装比较简单，以OS X平台为例：\n将协议解析脚本copy到/Applications/Wireshark.app/Contents/Resources/share/wireshark/ 目录\n编辑init.lua文件，设置disable_lua = false，确保lua支持打开\n在init.lua文件末尾增加\n1 dofile(\u0026#34;hsf2.lua\u0026#34;) 再次启动Wireshark，会对12200端口的数据流使用脚本解析，已经可以识别HSF协议了。\n备注\n附上hsf2.lua，边翻HSF代码边写的，写完眼已经花了，错误难免，欢迎试用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 -- declare the protocol hsf2_proto = Proto(\u0026#34;hsf2\u0026#34;, \u0026#34;Taobao HSF2 Protocol\u0026#34;) -- declare the value strings local vs_id = { [12] = \u0026#34;HSF2 Heart Beat\u0026#34;, [13] = \u0026#34;HSF2 TB Remoting\u0026#34;, [14] = \u0026#34;HSF2 HSF Remoting\u0026#34; } local vs_version = { [1] = \u0026#34;HSF2\u0026#34; } local vs_op = { [0] = \u0026#34;request\u0026#34;, [1] = \u0026#34;response\u0026#34; } local vs_codectype = { [1] = \u0026#34;HESSIAN_CODEC\u0026#34;, [2] = \u0026#34;JAVA_CODEC\u0026#34;, [3] = \u0026#34;TOP_CODEC\u0026#34;, [4] = \u0026#34;HESSIAN2_CODEC\u0026#34;, [5] = \u0026#34;KRYO_CODEC\u0026#34;, [6] = \u0026#34;JSON_CODEC\u0026#34;, [7] = \u0026#34;CUSTOMIZED_CODEC\u0026#34;, } local vs_responsestatus = { [20] = \u0026#34;OK\u0026#34;, [30] = \u0026#34;client timeout\u0026#34;, [31] = \u0026#34;server timeout\u0026#34;, [40] = \u0026#34;bad request\u0026#34;, [50] = \u0026#34;bad response\u0026#34;, [60] = \u0026#34;service not found\u0026#34;, [70] = \u0026#34;service error\u0026#34;, [80] = \u0026#34;server error\u0026#34;, [90] = \u0026#34;client error\u0026#34;, [91] = \u0026#34;Unknow error\u0026#34;, [81] = \u0026#34;Thread pool is busy\u0026#34;, [82] = \u0026#34;Communication error\u0026#34;, [88] = \u0026#34;server will close soon\u0026#34;, [10] = \u0026#34;server send coders\u0026#34;, [83] = \u0026#34;Unkown code\u0026#34; } -- declare the fields local f_id = ProtoField.uint8(\u0026#34;hsf2.id\u0026#34;, \u0026#34;Identification\u0026#34;, base.Dec, vs_id) local f_version = ProtoField.uint8(\u0026#34;hsf2.version\u0026#34;, \u0026#34;version\u0026#34;, base.Dec, vs_version) local f_op = ProtoField.uint8(\u0026#34;hsf2.op\u0026#34;, \u0026#34;operation\u0026#34;, base.DEC, vs_op) local f_codectype = ProtoField.uint8(\u0026#34;hsf2.codectype\u0026#34;, \u0026#34;codectype\u0026#34;, base.DEC, vs_codectype) local f_reserved = ProtoField.uint8(\u0026#34;hsf2.reserved\u0026#34;, \u0026#34;reserved\u0026#34;, base.DEC) local f_req_id = ProtoField.uint64(\u0026#34;hsf2.req_id\u0026#34;, \u0026#34;RequestID\u0026#34;, base.DEC) local f_timeout = ProtoField.uint32(\u0026#34;hsf2.timeout\u0026#34;, \u0026#34;timeout\u0026#34;, base.DEC) local f_service_name_len = ProtoField.uint32(\u0026#34;hsf2.service_name_len\u0026#34;, \u0026#34;Service Name length\u0026#34;, base.DEC) local f_method_name_len = ProtoField.uint32(\u0026#34;hsf2.method_name_len\u0026#34;, \u0026#34;Method Name length\u0026#34;, base.DEC) local f_arg_count = ProtoField.uint32(\u0026#34;hsf2.arg.count\u0026#34;, \u0026#34;Argument Count\u0026#34;, base.DEC) local f_arg_type_len = ProtoField.uint32(\u0026#34;hsf2.arg.type.len\u0026#34;, \u0026#34;Argument Type length\u0026#34;, base.DEC) local f_arg_obj_len = ProtoField.uint32(\u0026#34;hsf2.arg.obj.len\u0026#34;, \u0026#34;Argument Object length\u0026#34;, base.DEC) local f_req_prop_len = ProtoField.uint32(\u0026#34;hsf2.req.prop.len\u0026#34;, \u0026#34;Request Prop Length\u0026#34;, base.DEC) local f_service_name = ProtoField.string(\u0026#34;hsf2.service.name\u0026#34;, \u0026#34;Service Name\u0026#34;) local f_method_name = ProtoField.string(\u0026#34;hsf2.method.name\u0026#34;, \u0026#34;Method Name\u0026#34;) local f_arg_type = ProtoField.string(\u0026#34;hsf2.arg.type\u0026#34;, \u0026#34;Argument Type\u0026#34;) local f_arg_obj = ProtoField.bytes(\u0026#34;hsf2.arg.obj\u0026#34;, \u0026#34;Argument Object\u0026#34;) local f_req_prop = ProtoField.bytes(\u0026#34;hsf2.req.prop\u0026#34;, \u0026#34;Request Prop\u0026#34;) local f_response_status = ProtoField.uint32(\u0026#34;hsf2.response.status\u0026#34;, \u0026#34;Response Status\u0026#34;, base.DEC, vs_responsestatus) local f_response_body_len = ProtoField.uint32(\u0026#34;hsf2.response.body.len\u0026#34;, \u0026#34;Response Body Length\u0026#34;, base.DEC) local f_response_body = ProtoField.bytes(\u0026#34;hsf2.response.body\u0026#34;, \u0026#34;Response Body\u0026#34;, base.DEC) hsf2_proto.fields = { f_id, f_version, f_op, f_codectype, f_reserved, f_req_id, f_timeout, f_service_name_len, f_method_name_len, f_arg_count, f_arg_type_len, f_arg_obj_len, f_req_prop_len, f_service_name, f_method_name, f_arg_type, f_arg_obj, f_req_prop, f_response_status, f_response_body_len, f_response_body } function get_pdu_length(buffer) local offset = 0 local id = buffer(offset, 1):uint() offset = offset + 1 -- heart beat if id == 12 then return 18 end -- TB REMOTING if id == 13 then -- TODO return 18 end -- HSF REMOTING if id == 14 then local version = buffer(offset, 1):uint() offset = offset + 1 local op = buffer(offset, 1):uint() offset = offset + 1 -- request if op == 0 then local service_name_len = buffer(19, 4):uint() local method_name_len = buffer(23,4):uint() local arg_count = buffer(27,4):uint() offset = 27 + 4 local arg_content_len = 0 for i = 1, arg_count do arg_content_len = arg_content_len + buffer(offset,4):uint() offset = offset + 4 end for i = 1, arg_count do arg_content_len = arg_content_len + buffer(offset,4):uint() offset = offset + 4 end local req_prop_len = buffer(offset,4):uint() local len = 30 + arg_count*4*2 + 5 + service_name_len + method_name_len + arg_content_len + req_prop_len return len end -- response if op == 1 then local body_len = buffer(16, 4):uint() return 20 + body_len end end end -- create the dissection function function hsf2_proto.dissector(buffer, pinfo, tree) -- check the protocol -- TODO support TB Remoting local check_proto = buffer(0, 1):uint() if check_proto \u0026lt; 12 or check_proto \u0026gt; 14 or check_proto == 13 then return end -- Set the protocol column pinfo.cols[\u0026#39;protocol\u0026#39;] = \u0026#34;HSF2\u0026#34; -- Reassembling packets into one PDU local pdu_len = get_pdu_length(buffer) if pdu_len \u0026gt; buffer:len() then pinfo.desegment_len = pdu_len - buffer:len() pinfo.desegment_offset = 0 return end -- create the HSF2 protocol tree item local t_hsf2 = tree:add(hsf2_proto, buffer()) local offset = 0 local id = buffer(offset, 1):uint() offset = offset + 1 t_hsf2:add(f_id, id) -- heart beat if id == 12 then local op = buffer(offset, 1):uint() offset = offset + 1 t_hsf2:add(f_op, op) -- Set the info column to the name of the function local info = vs_id[id]..\u0026#34;:\u0026#34;..vs_op[op] pinfo.cols[\u0026#39;info\u0026#39;] = info t_hsf2:add(f_version, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_req_id, buffer(offset, 8)) offset = offset + 8 t_hsf2:add(f_timeout, buffer(offset, 4)) end -- TB REMOTING if id == 13 then -- TODO end -- HSF REMOTING if id == 14 then t_hsf2:add(f_version, buffer(offset, 1)) offset = offset + 1 local op = buffer(offset, 1):uint() offset = offset + 1 t_hsf2:add(f_op, op) -- Set the info column to the name of the function local info = vs_id[id]..\u0026#34;:\u0026#34;..vs_op[op] pinfo.cols[\u0026#39;info\u0026#39;] = info -- request if op == 0 then t_hsf2:add(f_codectype, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_req_id, buffer(offset, 8)) offset = offset + 8 t_hsf2:add(f_timeout, buffer(offset, 4)) offset = offset + 4 local service_name_len = buffer(offset, 4):uint() t_hsf2:add(f_service_name_len, service_name_len) offset = offset + 4 local method_name_len = buffer(offset,4):uint() t_hsf2:add(f_method_name_len, method_name_len) offset = offset + 4 local arg_count = buffer(offset,4):uint() t_hsf2:add(f_arg_count, arg_count) offset = offset + 4 local arg_type_len_array = {} for i = 1, arg_count do arg_type_len_array[i] = buffer(offset, 4):uint(); offset = offset + 4 t_hsf2:add(f_arg_type_len, arg_type_len_array[i]) end local arg_obj_len_array = {} for i = 1, arg_count do arg_obj_len_array[i] = buffer(offset, 4):uint(); offset = offset + 4 t_hsf2:add(f_arg_obj_len, arg_obj_len_array[i]) end local prop_len = buffer(offset, 4):uint(); offset = offset + 4 t_hsf2:add(f_req_prop_len, prop_len) t_hsf2:add(f_service_name, buffer(offset, service_name_len)) offset = offset + service_name_len t_hsf2:add(f_method_name, buffer(offset, method_name_len)) offset = offset + method_name_len for i = 1, #arg_type_len_array do t_hsf2:add(f_arg_type, buffer(offset, arg_type_len_array[i])) offset = offset + arg_type_len_array[i] end for i = 1, #arg_obj_len_array do t_hsf2:add(f_arg_obj, buffer(offset, arg_obj_len_array[i])) offset = offset + arg_obj_len_array[i] end if prop_len \u0026gt; 0 then t_hsf2:add(f_req_prop, buffer(offset, prop_len)) end end -- response if op == 1 then t_hsf2:add(f_response_status, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_codectype, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_req_id, buffer(offset, 8)) offset = offset + 8 local body_len = buffer(offset, 4):uint() t_hsf2:add(f_response_body_len, body_len) offset = offset + 4 t_hsf2:add(f_response_body, buffer(offset, body_len)) end end end -- load the tcp port table tcp_table = DissectorTable.get(\u0026#34;tcp.port\u0026#34;) -- register the protocol to port 12200 tcp_table:add(12200, hsf2_proto) ","date":"2014-12-21T14:15:17+08:00","permalink":"https://mazhen.tech/p/%E4%B8%BAwireshark%E7%BC%96%E5%86%99hsf2%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E6%8F%92%E4%BB%B6/","title":"为Wireshark编写HSF2协议解析插件"},{"content":"DNS（Domain Name System）简单说就是一个名称到IP地址的映射，使用容易记住的域名代替IP地址。基本原理就不讲了，网上的文章很多。\n了解了基本原理，你就可以使用dig(Domain Information Groper）命令进行探索。当对淘宝的几个域名进行dig时，你发现事情并不像想像的那么简单。\n为了使大家的输出一致，我在dig命令中显示的指定了DNS服务器（@222.172.200.68 云南电信DNS）。\ndig @222.172.200.68 login.alibaba-inc.com dig @222.172.200.68 guang.taobao.com dig @222.172.200.68 item.taobao.com 你会发现对于域名的查询，都不是直接返回IP地址这么简单，而是经过了神奇的CNAME。一般文档在介绍CNAME时只是说可以给一个域名指定别名（alias），其实这是DNS运维非常重要的手段，使得DNS配置具有一定的灵活性和可扩展性。结合上面三个域名的解析说一下。先给一张高大上的图，是按照我自己的理解画的，不一定完全正确:)\n图上分了三层，最上层是常规的DNS解析过程，用户通过local DNS做递归查询，最终定位到taobao.com权威DNS服务器。\n中间层可以称为GSLB（Global Server Load Balancing），作用是提供域名的智能解析，根据一定的策略返回结果。淘系目前有三套GSLB：\nF5 GTM：F5的硬件设备，基本已经被淘汰，全部替换为自研软件。GTM功能强大，但对用户而言是黑盒，性能一般价格昂贵。早期淘宝CDN智能调度就是基于F5 GTM做的。 ADNS：阿里自研权威DNS，替换GTM。ADNS很牛逼，可惜资料太少。 Pharos：阿里CDN的大脑，实现CDN流量精确，稳定，安全的调度。 taobao.com权威DNS服务器会根据不用的域名，CNAME到不同的GSLB做智能调度。CNAME的作用有点类似请求分发，taobao.com权威DNS服务器将域名解析请求转交给下一层域名服务器处理。\n最下层是应用层，提供真正的服务。\n现在再看看这三个域名的解析。\nlogin.alibaba-inc.com 被转交给了GTM做智能解析，GTM通过返回不同机房的VIP做流量调度，用户的请求最终经过LVS到达我们的应用。\nguang.taobao.com的解析过程和login.alibaba-inc.com类似，只不过智能调度换成了ADNS。\nitem.taobao.com有点小复杂。我们都知道CDN是做静态资源加速的，像这样的静态资源域名img04.taobaocdn.com会由Pharos解析调度，为用户返回就近的CDN节点。但什么时候动态内容也经过CDN代理了？这就是高大上的统一接入层。简单说下过程：item.taobao.com通过Pharos的智能调度，返回给用户就近的CDN节点。当用户的请求到达CDN节点时，这个节点会为动态内容的域名选择合适的后端服务，相当于每次都做回源处理。这个CDN节点可以理解为用户请求的代理。CDN在选择后端服务时，会执行单元化、小淘宝等逻辑，将请求发送到正确的机房。请求到达机房后，先进入统一接入层，注意这里的后端应用不需要申请VIP，IP地址列表保存在VIPServer中。统一接入层从VIPServer中拿到后端应用的IP地址列表，进行请求分发。VIPServer的作用类似HSF的ConfigServer，可以大大减少应用VIP的数量。以后做单元化部署的域名都会接入统一接入层，将单元化的逻辑上推到了CDN节点。\n貌似是讲清楚了，不过这个过程已经做了很大的简化，因为我也仅仅是了解个大概。作为业务开发重点关注的是最下面的Java应用，转岗到技术保障后，才发现有机会可以从全局了解网站架构，接触到网络、DNS、CDN、LVS\u0026amp;VIP等等基础设施。\n","date":"2014-09-10T11:04:27+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8E%E5%BC%80%E5%8F%91%E8%A7%92%E5%BA%A6%E7%9C%8Bdns/","title":"从开发角度看DNS"},{"content":"在Linux上做网络应用的性能优化时，一般都会对TCP相关的内核参数进行调节，特别是和缓冲、队列有关的参数。网上搜到的文章会告诉你需要修改哪些参数，但我们经常是知其然而不知其所以然，每次照抄过来后，可能很快就忘记或混淆了它们的含义。本文尝试总结TCP队列缓冲相关的内核参数，从协议栈的角度梳理它们，希望可以更容易的理解和记忆。注意，本文内容均来源于参考文档，没有去读相关的内核源码做验证，不能保证内容严谨正确。作为Java程序员没读过内核源码是硬伤。\n下面我以server端为视角，从连接建立、数据包接收和数据包发送这3条路径对参数进行归类梳理。\n# 一、连接建立 简单看下连接的建立过程，客户端向server发送SYN包，server回复SYN＋ACK，同时将这个处于SYN_RECV状态的连接保存到半连接队列。客户端返回ACK包完成三次握手，server将ESTABLISHED状态的连接移入accept队列，等待应用调用accept()。\n可以看到建立连接涉及两个队列：\n半连接队列，保存SYN_RECV状态的连接。队列长度由net.ipv4.tcp_max_syn_backlog设置\naccept队列，保存ESTABLISHED状态的连接。队列长度为min(net.core.somaxconn, backlog)。其中backlog是我们创建ServerSocket(int port,int backlog)时指定的参数，最终会传递给listen方法：\n1 2 #include \u0026lt;sys/socket.h\u0026gt; int listen(int sockfd, int backlog); 如果我们设置的backlog大于net.core.somaxconn，accept队列的长度将被设置为net.core.somaxconn\n另外，为了应对SYN flooding（即客户端只发送SYN包发起握手而不回应ACK完成连接建立，填满server端的半连接队列，让它无法处理正常的握手请求），Linux实现了一种称为SYN cookie的机制，通过net.ipv4.tcp_syncookies控制，设置为1表示开启。简单说SYN cookie就是将连接信息编码在ISN(initial sequence number)中返回给客户端，这时server不需要将半连接保存在队列中，而是利用客户端随后发来的ACK带回的ISN还原连接信息，以完成连接的建立，避免了半连接队列被攻击SYN包填满。对于一去不复返的客户端握手，不理它就是了。\n# 二、数据包的接收 先看看接收数据包经过的路径：\n数据包的接收，从下往上经过了三层：网卡驱动、系统内核空间，最后到用户态空间的应用。Linux内核使用sk_buff(socket kernel buffers)数据结构描述一个数据包。当一个新的数据包到达，NIC（network interface controller）调用DMA engine，通过Ring Buffer将数据包放置到内核内存区。Ring Buffer的大小固定，它不包含实际的数据包，而是包含了指向sk_buff的描述符。当Ring Buffer满的时候，新来的数据包将给丢弃。一旦数据包被成功接收，NIC发起中断，由内核的中断处理程序将数据包传递给IP层。经过IP层的处理，数据包被放入队列等待TCP层处理。每个数据包经过TCP层一系列复杂的步骤，更新TCP状态机，最终到达recv Buffer，等待被应用接收处理。有一点需要注意，数据包到达recv Buffer，TCP就会回ACK确认，既TCP的ACK表示数据包已经被操作系统内核收到，但并不确保应用层一定收到数据（例如这个时候系统crash），因此一般建议应用协议层也要设计自己的ACK确认机制。\n上面就是一个相当简化的数据包接收流程，让我们逐层看看队列缓冲有关的参数。\n网卡Bonding模式 当主机有1个以上的网卡时，Linux会将多个网卡绑定为一个虚拟的bonded网络接口，对TCP/IP而言只存在一个bonded网卡。多网卡绑定一方面能够提高网络吞吐量，另一方面也可以增强网络高可用。Linux支持7种Bonding模式：\n- `Mode 0 (balance-rr)` Round-robin策略，这个模式具备负载均衡和容错能力 - `Mode 1 (active-backup)` 主备策略，在绑定中只有一个网卡被激活，其他处于备份状态 - `Mode 2 (balance-xor)` XOR策略，通过源MAC地址与目的MAC地址做异或操作选择slave网卡 - `Mode 3 (broadcast)` 广播，在所有的网卡上传送所有的报文 - `Mode 4 (802.3ad)` IEEE 802.3ad 动态链路聚合。创建共享相同的速率和双工模式的聚合组 - `Mode 5 (balance-tlb)` Adaptive transmit load balancing - `Mode 6 (balance-alb)` Adaptive load balancing 详细的说明参考内核文档Linux Ethernet Bonding Driver HOWTO。我们可以通过cat /proc/net/bonding/bond0查看本机的Bonding模式：\n一般很少需要开发去设置网卡Bonding模式，自己实验的话可以参考这篇文档\n网卡多队列及中断绑定 随着网络的带宽的不断提升，单核CPU已经不能满足网卡的需求，这时通过多队列网卡驱动的支持，可以将每个队列通过中断绑定到不同的CPU核上，充分利用多核提升数据包的处理能力。\n首先查看网卡是否支持多队列，使用lspci -vvv命令，找到Ethernet controller项：\n如果有MSI-X， Enable+ 并且Count \u0026gt; 1，则该网卡是多队列网卡。\n然后查看是否打开了网卡多队列。使用命令cat /proc/interrupts，如果看到eth0-TxRx-0表明多队列支持已经打开：\n最后确认每个队列是否绑定到不同的CPU。cat /proc/interrupts查询到每个队列的中断号，对应的文件/proc/irq/${IRQ_NUM}/smp_affinity为中断号IRQ_NUM绑定的CPU核的情况。以十六进制表示，每一位代表一个CPU核：\n``` （00000001）代表CPU0 （00000010）代表CPU1 （00000011）代表CPU0和CPU1 ``` 如果绑定的不均衡，可以手工设置，例如：\n``` echo \u0026quot;1\u0026quot; \u0026gt; /proc/irq/99/smp_affinity echo \u0026quot;2\u0026quot; \u0026gt; /proc/irq/100/smp_affinity echo \u0026quot;4\u0026quot; \u0026gt; /proc/irq/101/smp_affinity echo \u0026quot;8\u0026quot; \u0026gt; /proc/irq/102/smp_affinity echo \u0026quot;10\u0026quot; \u0026gt; /proc/irq/103/smp_affinity echo \u0026quot;20\u0026quot; \u0026gt; /proc/irq/104/smp_affinity echo \u0026quot;40\u0026quot; \u0026gt; /proc/irq/105/smp_affinity echo \u0026quot;80\u0026quot; \u0026gt; /proc/irq/106/smp_affinity ``` Ring Buffer\nRing Buffer位于NIC和IP层之间，是一个典型的FIFO（先进先出）环形队列。Ring Buffer没有包含数据本身，而是包含了指向sk_buff（socket kernel buffers）的描述符。\n可以使用ethtool -g eth0查看当前Ring Buffer的设置：\n上面的例子接收队列为4096，传输队列为256。可以通过ifconfig观察接收和传输队列的运行状况：\nRX errors：收包总的错误数\nRX dropped: 表示数据包已经进入了Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃。\nRX overruns: overruns意味着数据包没到Ring Buffer就被网卡物理层给丢弃了，而CPU无法及时的处理中断是造成Ring Buffer满的原因之一，例如中断分配的不均匀。\n当dropped数量持续增加，建议增大Ring Buffer，使用ethtool -G进行设置。\nInput Packet Queue(数据包接收队列) 当接收数据包的速率大于内核TCP处理包的速率，数据包将会缓冲在TCP层之前的队列中。接收队列的长度由参数net.core.netdev_max_backlog设置。\nrecv Buffer recv buffer是调节TCP性能的关键参数。BDP(Bandwidth-delay product，带宽延迟积) 是网络的带宽和与RTT(round trip time)的乘积，BDP的含义是任意时刻处于在途未确认的最大数据量。RTT使用ping命令可以很容易的得到。为了达到最大的吞吐量，recv Buffer的设置应该大于BDP，即recv Buffer \u0026gt;= bandwidth * RTT。假设带宽是100Mbps，RTT是100ms，那么BDP的计算如下：\n1 BDP = 100Mbps * 100ms = (100 / 8) * (100 / 1000) = 1.25MB Linux在2.6.17以后增加了recv Buffer自动调节机制，recv buffer的实际大小会自动在最小值和最大值之间浮动，以期找到性能和资源的平衡点，因此大多数情况下不建议将recv buffer手工设置成固定值。\n当net.ipv4.tcp_moderate_rcvbuf设置为1时，自动调节机制生效，每个TCP连接的recv Buffer由下面的3元数组指定：\n1 net.ipv4.tcp_rmem = \u0026lt;MIN\u0026gt; \u0026lt;DEFAULT\u0026gt; \u0026lt;MAX\u0026gt; 最初recv buffer被设置为，同时这个缺省值会覆盖net.core.rmem_default的设置。随后recv buffer根据实际情况在最大值和最小值之间动态调节。在缓冲的动态调优机制开启的情况下，我们将net.ipv4.tcp_rmem的最大值设置为BDP。\n当net.ipv4.tcp_moderate_rcvbuf被设置为0，或者设置了socket选项SO_RCVBUF，缓冲的动态调节机制被关闭。recv buffer的缺省值由net.core.rmem_default设置，但如果设置了net.ipv4.tcp_rmem，缺省值则被\u0026lt;DEFAULT\u0026gt;覆盖。可以通过系统调用setsockopt()设置recv buffer的最大值为net.core.rmem_max。在缓冲动态调节机制关闭的情况下，建议把缓冲的缺省值设置为BDP。\n注意这里还有一个细节，缓冲除了保存接收的数据本身，还需要一部分空间保存socket数据结构等额外信息。因此上面讨论的recv buffer最佳值仅仅等于BDP是不够的，还需要考虑保存socket等额外信息的开销。Linux根据参数net.ipv4.tcp_adv_win_scale计算额外开销的大小：\nBuffer / 2tcp_adv_win_scale\n如果net.ipv4.tcp_adv_win_scale的值为1，则二分之一的缓冲空间用来做额外开销，如果为2的话，则四分之一缓冲空间用来做额外开销。因此recv buffer的最佳值应该设置为：\nBDP / (1 – 1 / 2tcp_adv_win_scale)\n# 三、数据包的发送 发送数据包经过的路径：\n和接收数据的路径相反，数据包的发送从上往下也经过了三层：用户态空间的应用、系统内核空间、最后到网卡驱动。应用先将数据写入TCP send buffer，TCP层将send buffer中的数据构建成数据包转交给IP层。IP层会将待发送的数据包放入队列QDisc(queueing discipline)。数据包成功放入QDisc后，指向数据包的描述符sk_buff被放入Ring Buffer输出队列，随后网卡驱动调用DMA engine将数据发送到网络链路上。\n同样我们逐层来梳理队列缓冲有关的参数。\nsend Buffer 同recv Buffer类似，和send Buffer有关的参数如下：\n1 2 3 net.ipv4.tcp_wmem = \u0026lt;MIN\u0026gt; \u0026lt;DEFAULT\u0026gt; \u0026lt;MAX\u0026gt; net.core.wmem_default net.core.wmem_max 发送端缓冲的自动调节机制很早就已经实现，并且是无条件开启，没有参数去设置。如果指定了tcp_wmem，则net.core.wmem_default被tcp_wmem的覆盖。send Buffer在tcp_wmem的最小值和最大值之间自动调节。如果调用setsockopt()设置了socket选项SO_SNDBUF，将关闭发送端缓冲的自动调节机制，tcp_wmem将被忽略，SO_SNDBUF的最大值由net.core.wmem_max限制。\nQDisc QDisc（queueing discipline ）位于IP层和网卡的ring buffer之间。我们已经知道，ring buffer是一个简单的FIFO队列，这种设计使网卡的驱动层保持简单和快速。而QDisc实现了流量管理的高级功能，包括流量分类，优先级和流量整形（rate-shaping）。可以使用tc命令配置QDisc。\nQDisc的队列长度由txqueuelen设置，和接收数据包的队列长度由内核参数net.core.netdev_max_backlog控制所不同，txqueuelen是和网卡关联，可以用ifconfig命令查看当前的大小：\n使用ifconfig调整txqueuelen的大小：\n1 ifconfig eth0 txqueuelen 2000 Ring Buffer 和数据包的接收一样，发送数据包也要经过Ring Buffer，使用ethtool -g eth0查看：\n其中TX项是Ring Buffer的传输队列，也就是发送队列的长度。设置也是使用命令ethtool -G。\nTCP Segmentation和Checksum Offloading 操作系统可以把一些TCP/IP的功能转交给网卡去完成，特别是Segmentation(分片)和checksum的计算，这样可以节省CPU资源，并且由硬件代替OS执行这些操作会带来性能的提升。\n一般以太网的MTU（Maximum Transmission Unit）为1500 bytes，假设应用要发送数据包的大小为7300bytes，MTU1500字节 － IP头部20字节 － TCP头部20字节＝有效负载为1460字节，因此7300字节需要拆分成5个segment：\nSegmentation(分片)操作可以由操作系统移交给网卡完成，虽然最终线路上仍然是传输5个包，但这样节省了CPU资源并带来性能的提升：\n可以使用ethtool -k eth0查看网卡当前的offloading情况：\n上面这个例子checksum和tcp segmentation的offloading都是打开的。如果想设置网卡的offloading开关，可以使用ethtool -K(注意K是大写)命令，例如下面的命令关闭了tcp segmentation offload：\n1 sudo ethtool -K eth0 tso off 网卡多队列和网卡Bonding模式 在数据包的接收过程中已经介绍过了。\n至此，终于梳理完毕。整理TCP队列相关参数的起因是最近在排查一个网络超时问题，原因还没有找到，产生的“副作用”就是这篇文档。再想深入解决这个问题可能需要做TCP协议代码的profile，需要继续学习，希望不久的将来就可以再写文档和大家分享了。\n参考文档\nQueueing in the Linux Network Stack TCP Implementation in Linux: A Brief Tutorial Impact of Bandwidth Delay Product on TCP Throughput Java程序员也应该知道的系统知识系列之网卡 ","date":"2014-08-16T11:11:24+08:00","permalink":"https://mazhen.tech/p/linux-tcp%E9%98%9F%E5%88%97%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E7%9A%84%E6%80%BB%E7%BB%93/","title":"Linux TCP队列相关参数的总结"},{"content":"Java中通过Socket.setSoLinger设置SO_LINGER选项，有三种组合形式：\nSocket.setSoLinger(false, linger) 设置为false，这时linger值被忽略。摘自unix network programming：\nThe default action of close with a TCP socket is to mark the socket as closed and return to the process immediately. The socket descriptor is on longer usable by the process: it can\u0026rsquo;t be used as an argument to read or write. TCP will try to send any data that is already queued to be sent to the other end, and after this occurs, the normal TCP connection termination sequence takes place.\n如果设置为false，socket主动调用close时会立即返回，操作系统会将残留在缓冲区中的数据发送到对端，并按照正常流程关闭(交换FIN-ACK），最后连接进入TIME_WAIT状态。\n我们可以写个演示程序，客户端发送较大的数据包后，立刻调用close，而server端将Receive Buffer设置的很小。close会立即返回，客户端的Java进程结束，但是当我们用tcpdump/Wireshark抓包会发现，操作系统正在帮你发送数据，内核缓冲区中的数据发送完毕后，发送FIN包关闭连接。\nSocket.setSoLinger(true, 0) TCP discards any data still remaining in the socket send buffer and sends an RST to the peer, not the normal four-packet connection termination sequence.\n主动调用close的一方也是立刻返回，但是这时TCP会丢弃发送缓冲中的数据，而且不是按照正常流程关闭连接（不发送FIN包），直接发送RST，对端会收到java.net.SocketException: Connection reset异常。同样使用tcpdump抓包可以很容易观察到。\n另外有些人会用这种方式解决主动关闭放方有大量TIME_WAIT状态连接的问题，因为发送完RST后，连接立即销毁，不会停留在TIME_WAIT状态。一般不建议这么做，除非你有合适的理由：\nIf the a client of your server application misbehaves (times out, returns invalid data, etc.) an abortive close makes sense to avoid being stuck in CLOSE_WAIT or ending up in the TIME_WAIT state. If you must restart your server application which currently has thousands of client connections you might consider setting this socket option to avoid thousands of server sockets in TIME_WAIT (when calling close() from the server end) as this might prevent the server from getting available ports for new client connections after being restarted.\nOn page 202 in the aforementioned book it specifically says: \u0026ldquo;There are certain circumstances which warrant using this feature to send an abortive close. One example is an RS-232 terminal server, which might hang forever in CLOSE_WAIT trying to deliver data to a stuck terminal port, but would properly reset the stuck port if it got an RST to discard the pending data.\u0026rdquo;\nSocket.setSoLinger(true, linger \u0026gt; 0)\nif there is any data still remaining in the socket send buffer, the process will sleep when calling close() until either all the data is sent and acknowledged by the peer or the configured linger timer expires. if the linger time expires before the remaining data is sent and acknowledged, close returns EWOULDBLOCK and any remaining data in the send buffer is discarded.\n如果SO_LINGER选项生效，并且超时设置大于零，调用close的线程被阻塞，TCP会发送缓冲区中的残留数据，这时有两种可能的情况：\n数据发送完毕，收到对方的ACK，然后进行连接的正常关闭（交换FIN-ACK） 超时，未发送完成的数据被丢弃，连接发送RST进行非正常关闭 类似的我们也可以构造demo观察这种场景。客户端发送较大的数据包，server端将Receive Buffer设置的很小。设置linger为1，调用close时等待1秒。注意SO_LINGER的单位为秒，好多人被坑过。假设close后1秒内缓冲区中的数据发送不完，使用tcpdump/Wireshark可以观察到客户端发送RST包，服务端收到java.net.SocketException: Connection reset异常。\n最后，在使用NIO时，最好不设置SO_LINGER，以后会再写一篇文章分析。\n","date":"2014-08-10T11:08:57+08:00","permalink":"https://mazhen.tech/p/tcp-so_linger-%E9%80%89%E9%A1%B9%E5%AF%B9socket.close%E7%9A%84%E5%BD%B1%E5%93%8D/","title":"TCP `SO_LINGER` 选项对Socket.close的影响"},{"content":"早上毕玄转给我一个问题，vsearch在上海机房部署的应用，在应用关闭后，端口释放的时间要比杭州机房的时间长。\nTCP的基本知识，主动关闭连接的一方会处于TIME_WAIT状态，并停留两倍的MSL（Maximum segment lifetime）时长。\n那就检查一下MSL的设置。网上有很多文章说，可以通过设置net.ipv4.tcp_fin_timeout来控制MSL。其实这有点误导人。查看Linux kernel的文档 ，发现tcp_fin_timeout是指停留在FIN_WAIT_2状态的时间：\ntcp_fin_timeout - INTEGER The length of time an orphaned (no longer referenced by any application) connection will remain in the FIN_WAIT_2 state before it is aborted at the local end. While a perfectly valid \u0026ldquo;receive only\u0026rdquo; state for an un-orphaned connection, an orphaned connection in FIN_WAIT_2 state could otherwise wait forever for the remote to close its end of the connection. Default: 60 seconds\n幸好这个问题原先在内部请教过：\nsysctl调节不了，只能调节复用和回收。 以前改小是改下面文件，重新编译内核的。 grep -i timewait_len /usr/src/kernels/2.6.32-220.el6.x86_64/include/net/tcp.h define TCP_TIMEWAIT_LEN (60HZ) / how long to wait to destroy TIME-WAIT define TCP_FIN_TIMEOUT TCP_TIMEWAIT_LEN\n而阿里内核支持修改TIME_WAIT时间：\nnet.ipv4.tcp_tw_timeout 然后找了两台机器做对比，用sysctl命令查看。杭州机房的机器：\nsudo sysctl -a | grep net.ipv4.tcp_tw_timeout net.ipv4.tcp_tw_timeout = 3 上海机房的机器：\n$sudo sysctl -a | grep net.ipv4.tcp_tw_timeout net.ipv4.tcp_tw_timeout = 60 原因很明显，上海机器的设置为60S。\n","date":"2014-07-01T11:00:10+08:00","permalink":"https://mazhen.tech/p/%E5%BA%94%E7%94%A8%E5%85%B3%E9%97%AD%E5%90%8E%E5%8D%A0%E7%94%A8%E7%AB%AF%E5%8F%A3%E6%97%B6%E9%97%B4%E8%BF%87%E9%95%BF%E7%9A%84%E9%97%AE%E9%A2%98/","title":"应用关闭后占用端口时间过长的问题"},{"content":"遇到性能问题怎么分析定位？这个问题太难回答了，各种底层环境、依赖系统、业务场景，怎么可能有统一的答案。于是产生了各种分析性能问题的“流派”。两个典型的 ANTI-METHODOLOGIES：\nblame-someone-else 使用此方法的人遵循下列步骤：\n找到一个不是他负责的系统或环境 假定问题和这个组件有关 将问题转交个负责这个组件的团队 如果证明是错误的，重复步骤1 路灯法 没有系统的方法论，只是使用自己擅长的工具去观察，而不管问题到底出现在哪儿。就像丢了钥匙的人去路灯下寻找，仅仅是因为路灯下比较亮。这种行为被称为路灯效应。\n相信很多同学已经脑补出上述的两个场景，他们的行为模式让人抓狂。于是有聪明人总结出了《The USE Method》。USE是Utilization，Saturation 和 Errors的缩写，简单说USE是一套分析系统性能问题的方法论，具体表现为一个checklist，分析过程就是对照checklist一项项检查，希望能快速定位瓶颈资源或错误。\n初看这个方法感觉有点太简单了吧，这也能称为方法论？不过这确实体现出了老外的做事风格，任何事情都会去做定量分析，力求逻辑完整。而我们往往讳莫高深的一笑，只可意会不可言传。\n简单介绍下USE，详细内容推荐看这篇《The USE Method》。USE的一句话总结：\nFor every resource, check utilization, saturation, and errors.\n术语解释\nresource：CPU，内存，磁盘，网络等一切物理设备资源 utilization：资源利用率。例如CPU的资源利用率90% saturation：当资源繁忙时仍能接收新的任务，这些额外的任务一般都放入了等待队列。saturation就表现为队列的长度，例如CPU的平均运行队列为4（Linux上使用vmstat命令获得）。 errors：系统的错误报告数，例如TCP监听队列overflowed次数。 列出系统中的所有资源，然后逐项检查利用率、等待队列和错误数，就这么简单！下表是一个范例：\nresource type metric CPU utilization CPU utilization (either per-CPU or a system-wide average) CPU saturation run-queue length Memory capacity utilization available free memory (system-wide) Memory capacity saturation anonymous paging or thread swapping Network interface utilization RX/TX throughput / max bandwidth Storage Storage device I/O utilization device busy percent Storage device I/O saturation wait queue length Storage device I/O errors device errors (\u0026ldquo;soft\u0026rdquo;, \u0026ldquo;hard\u0026rdquo;, \u0026hellip;) 对于资源测量数据的解读，作者给了一些建议，例如：资源利用率100%肯定表示该资源是系统瓶颈，70%以上的利用率就要引起足够的重视，一般IO设备利用率高于70%，响应时间将大幅上升。资源等待队列大于0意味着可能存在问题。资源的任何错误计数，都值得仔细调查，特别是当性能变差时，错误计数在上升。\n要使用这个方法，你还需要一份完整的资源列表，一般的系统资源包括：\nCPUs: sockets, cores, hardware threads (virtual CPUs) Memory: capacity Network interfaces Storage devices: I/O, capacity Controllers: storage, network cards Interconnects: CPUs, memory, I/O 作者很厚道的按照每种操作系统给出了checklist，重点关注《USE Method: Linux Performance Checklist》，不仅列出了资源，而且告诉你如何进行测量。例如CPU运行队列的测量：\nsystem-wide: vmstat 1, \u0026ldquo;r\u0026rdquo; \u0026gt; CPU count [2]; sar -q, \u0026ldquo;runq-sz\u0026rdquo; \u0026gt; CPU count; dstat -p, \u0026ldquo;run\u0026rdquo; \u0026gt; CPU count; per-process: /proc/PID/schedstat 2nd field (sched_info.run_delay); perf sched latency (shows \u0026ldquo;Average\u0026rdquo; and \u0026ldquo;Maximum\u0026rdquo; delay per-schedule); dynamic tracing, eg, SystemTap schedtimes.stp \u0026ldquo;queued(us)\u0026rdquo;\n根据作者的实践经验，使用USE方法解决了80%的性能问题，只付出了5%的努力，当考虑了所有的资源，你不太可能忽视任何问题。简单有效！\n","date":"2014-06-07T10:56:00+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8use-method%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98/","title":"使用USE Method分析系统性能问题"},{"content":"/proc是一个伪文件系统，可以像访问普通文件系统一样访问系统内部的数据结构，获取当前运行的进程、统计和硬件等各种信息。例如可以使用cat /proc/cpuinfo获取CPU信息。\n/proc/sys/下的文件和子目录比较特别，它们对应的是系统内核参数，更改文件内容就意味着修改了相应的内核参数，可以简单的使用echo命令来完成修改：\n1 echo 1 \u0026gt; /proc/sys/net/ipv4/tcp_syncookies 上面这个命令启用了TCP SYN Cookie保护。使用echo修改内核参数很方便，但是系统重启后这些修改都会消失，而且不方便配置参数的集中管理。/sbin/sysctl命令就是用来查看和修改内核参数的工具。sysctl -a会列出所有内核参数当前的配置信息，比遍历目录/proc/sys/方便多了。sysctl -w修改单个参数的配置，例如：\n1 sysctl -w net.ipv4.tcp_syncookies=1 和上面echo命令的效果一样。需要注意的是，要把目录分隔符斜杠/替换为点.，并省略proc.sys部分。\n通过sysctl -w修改，还是没有解决重启后修改失效的问题。更常用的方式是，把需要修改的配置集中放在/etc/sysctl.conf文件中，使用sysctl -p重新加载配置使其生效。在系统启动阶段，init程序会运行/etc/rc.d/rc.sysinit脚本，其中包含了执行sysctl命令，并使用了/etc/sysctl.conf中的配置信息。因此放在/etc/sysctl.conf中的系统参数设置在重启后也同样生效，同时也便于集中管理修改过了哪些内核参数。\n最后，哪里有比较完整的内核参数说明文档？我觉得kernel.org的文档比较全。例如我们常会遇到的网络内核参数，net.core 和 net.ipv4 。TCP相关的参数，也可以通过man文档了解。\n","date":"2014-05-30T20:21:08+08:00","permalink":"https://mazhen.tech/p/linux%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%B9%E6%B3%95/","title":"Linux内核参数的配置方法"}]