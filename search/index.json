[{"content":"在前面的文章 追踪定位 Java 进程的 Socket 创建中，介绍了如何使用 async-profiler 追踪 tracepoint 事件 sys_enter_bind，定位 Java 中网络监听端口的创建。但部分系统因内核配置限制可能不支持该事件。本文将详细介绍如何通过手动创建kprobe事件，实现对bind系统调用的精准追踪。\n# 问题背景：系统不支持 syscalls:sys_enter_bind tracepoint 当执行perf list | grep syscalls时，若输出中没有syscalls:sys_enter_bind，说明系统内核未启用CONFIG_SYSCALL_TRACEPOINTS配置（可通过grep CONFIG_SYSCALL_TRACEPOINTS /boot/config-$(uname -r)验证）。此时无法直接使用syscalls系列跟踪点，需通过kprobe动态探测内核函数实现追踪。\n# 解决方案：手动创建 kprobe 事件追踪 bind 系统调用 kprobe是内核提供的动态调试机制，可在任意内核函数的入口/出口插入探测点。bind系统调用的内核实现函数通常为sys_bind或__sys_bind，我们可通过kprobe追踪该函数。\n# 步骤 1：确认 bind 系统调用的内核实现函数 首先需确定当前内核中bind系统调用对应的函数名，执行以下命令（需 root 权限）：\n1 grep -r \u0026#34;bind\u0026#34; /proc/kallsyms | grep -E \u0026#34;sys_bind|__sys_bind\u0026#34; 示例输出（不同系统可能不同）：\n1 ffff000008aabbe0 T __sys_bind 记录函数名（如__sys_bind），后续将基于此创建探测点。\n# 步骤 2：手动创建 kprobe 事件 kprobe事件需通过内核tracefs接口创建，操作如下：\n进入 tracefs 目录（内核跟踪接口目录）：\n1 cd /sys/kernel/debug/tracing 创建 kprobe 事件：\n使用echo命令向kprobe_events文件写入事件定义，格式为：\n1 2 # p:表示在函数入口插入探测点，kprobes/bind_kprobe 为事件名，__sys_bind 为目标内核函数 echo \u0026#39;p:kprobes/bind_kprobe __sys_bind\u0026#39; \u0026gt; kprobe_events 验证事件创建成功：\n查看kprobe_events文件确认事件已添加：\n1 2 cat kprobe_events # 输出应包含：p:kprobes/bind_kprobe __sys_bind # 步骤 3：启用 kprobe 事件 创建事件后需手动启用，否则perf无法捕获：\n1 2 # 启用事件（1表示启用，0表示禁用） echo 1 \u0026gt; events/kprobes/bind_kprobe/enable # 步骤 4：用 perf 追踪 bind 系统调用 事件启用后，perf可直接追踪该kprobe事件，操作如下：\n启动 perf 追踪（在任意目录执行）：\n1 2 # -e 指定追踪 kprobes:bind_kprobe 事件，-g 记录调用栈，-a 追踪所有 CPU perf record -e kprobes:bind_kprobe -g -a 此时perf会持续运行，等待bind系统调用触发。\n触发 bind 操作： 在另一个终端启动目标程序（如需要追踪的 Java 进程），执行可能触发bind的操作（如启动网络服务）。\n停止追踪并分析结果：\n按Ctrl+C停止perf，生成perf.data文件；\n执行perf report查看结果，可看到触发bind调用的进程、调用栈等信息：\n1 perf report # 步骤 5：清理 kprobe 事件 追踪完成后，需清理创建的kprobe事件，避免占用系统资源：\n禁用事件：\n1 2 cd /sys/kernel/debug/tracing echo 0 \u0026gt; events/kprobes/bind_kprobe/enable 删除事件：\n1 2 # 使用相对路径删除（在 tracing 目录下） echo \u0026#39;-:kprobes/bind_kprobe\u0026#39; \u0026gt; kprobe_events 验证清理结果：\n1 cat kprobe_events # 输出为空，说明事件已删除 # 关键注意事项 内核配置要求： 系统需启用CONFIG_KPROBE_EVENTS和CONFIG_DEBUG_FS（可通过grep -E \u0026quot;CONFIG_KPROBE_EVENTS|CONFIG_DEBUG_FS\u0026quot; /boot/config-$(uname -r)验证，需均为y）。\n权限问题： 所有操作需 root 权限，且tracefs需已挂载（默认挂载，若未挂载可执行mount -t tracefs nodev /sys/kernel/debug/tracing）。\n# 总结 当系统不支持syscalls:sys_enter_bind跟踪点时，通过手动创建kprobe事件，可间接实现对bind系统调用的追踪。该方法依赖内核动态探测机制，无需重新编译内核，适用于多数 Linux 发行版，是调试网络程序绑定端口行为的有效手段。\n","date":"2025-09-25T10:24:48+08:00","permalink":"https://mazhen.tech/p/%E6%89%8B%E5%8A%A8%E5%88%9B%E5%BB%BA-kprobe-%E4%BA%8B%E4%BB%B6%E8%BF%BD%E8%B8%AA-bind-%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8/","title":"手动创建 kprobe 事件追踪 bind 系统调用"},{"content":"在上一篇文章追踪定位 Java 进程的 Socket 创建中，我们使用 BCC 脚本成功捕获了 Java 进程在启动阶段对 Unix socket 的创建。其中，对于一个由 JVM 原生代码触发的 socket 调用，我们得到了如下调用堆栈：\n1 2 3 4 5 [12:23:13.184181] [socket(AF_UNIX)] [PPID=177932] [PID=177961] [FD=4] b\u0026#39;socket+0xb [libc.so.6]\u0026#39; b\u0026#39;__nscd_get_mapping+0xbd [libc.so.6]\u0026#39; b\u0026#39;__nscd_get_map_ref+0xcf [libc.so.6]\u0026#39; b\u0026#39;[unknown]\u0026#39; 虽然我们根据堆栈中的 __nscd_get_mapping 等函数名推断出其行为与名称服务查询有关，但这个结果留下了一个明显的问题：调用堆栈在 __nscd_get_map_ref 函数之后就中断了，最后一行显示为 [unknown]。\n__nscd_get_map_ref 这个函数名表明它是一个内部实现函数，而非供外部程序直接调用的公共 API。因此，调用它的函数极有可能仍然位于 glibc 库的内部。这就引出了一个疑问：为什么堆栈会在 glibc 库的内部中断？[unknown] 的背后究竟隐藏着哪个函数？\n本文将深入分析这个现象的底层原因，并提供一个能获取完整原生调用堆栈的方法。\n# 丢失的帧指针 (Frame Pointers) 要理解堆栈为什么会中断，我们首先需要了解 BCC 默认是如何获取调用堆栈的。它使用的是一种名为“帧指针回溯”（Frame Pointer Walking）的技术。\n简单来说，当一个函数被调用时，它会在自己的栈帧（Stack Frame）中保存上一个函数栈帧的基地址。这个基地址通常存储在一个专用的寄存器中（在 x86-64 架构上是 %rbp），这个寄存器就被称为帧指针（Frame Pointer）。通过当前函数的帧指针，可以找到上一个函数的帧指针，如此层层递进，形成一个类似链表的结构。BCC 正是依赖这条“指针链”来快速地回溯出完整的函数调用关系。\n这种方法的优点是速度快、性能开销很低。但它的缺点是，它严重依赖于被追踪的程序在编译时保留了帧指针。如果编译器为了性能优化（例如，使用 -fomit-frame-pointer 标志），决定不使用 %rbp 寄存器作为帧指针，而是将其当作一个通用寄存器来使用，那么这个函数栈帧中就不会包含保存上一个帧指针的操作。当堆栈回溯到这个函数时，指针链就会在此处断裂。\n基于以上原理，我们可以做出一个合理的推测：BCC 的堆栈追踪之所以在 glibc 内部中断，是因为调用链中的某个函数在编译时被优化，省略了帧指针，导致回溯链条在此处断裂。\n# 历史回顾，为何帧指针会“丢失”？ 前面我们推测堆栈中断是由于 glibc 中的某些函数在编译时省略了帧指针。但为什么编译器会做出这样的优化呢？\n知名性能专家 Brendan Gregg 在他的文章 The Return of the Frame Pointers 中详细阐述了这个问题。在过去的 32 位 CPU 架构（如 x86）时代，CPU 可用的通用寄存器数量非常有限。为了尽可能地提升性能，编译器开发者无所不用其极。其中一项广为采用的优化就是通过 -fomit-frame-pointer 编译标志，不再将 %ebp 寄存器固定用作帧指针，而是将其解放出来，作为一个通用的寄存器参与数据计算。\n这个优化的理论收益是：可以减少函数调用时的指令数，并多出一个可用的寄存器，从而生成体积更小、执行速度可能更快的代码。代价则是牺牲了程序的调试性和可分析性，使得基于帧指针的堆栈回溯变得不可靠。在那个年代，这个选择被认为是值得的，并逐渐成为了 GCC 等主流编译器的默认行为。\n在现代的 64 位 CPU (x86-64) 上，寄存器数量已经大大增加，省略帧指针带来的性能优势已经微乎其微，但这个编译默认选项却作为一种“历史惯例”被长期保留了下来。因此，我们当前系统上使用的 glibc 库，其二进制文件很可能就是遵循这一惯例构建的产物。\n业界已经重新认识到帧指针对于系统可观测性的重要性。现在，主流的 Linux 发行版（如 Ubuntu 24.04 和 Fedora 38）已经开始在其软件包的编译选项中重新默认启用帧指针。但在那之前，我们仍然需要处理这些“历史遗留”的二进制文件。\n# 使用 GDB 验证猜想 现在我们使用 GDB (GNU Debugger) 直接检查相关函数的汇编代码，验证 glibc 内部的某些函数可能省略了帧指针的推测。\n加载 libc.so.6 到 GDB\n我们首先启动 GDB，并将 glibc 库作为分析目标加载进来。\n1 $ gdb /usr/lib/x86_64-linux-gnu/libc.so.6 检查导致回溯失败的函数 __nscd_get_map_ref\n根据 BCC 的输出，__nscd_get_map_ref 是堆栈中可见的最后一个函数名。回溯正是在尝试寻找它的调用者时失败的。因此，我们首先来检查这个函数的汇编代码。\n1 (gdb) disassemble __nscd_get_map_ref 输出结果会类似下面这样：\n1 2 3 4 5 6 7 8 9 10 Dump of assembler code for function __nscd_get_map_ref: 0x0000000000171670 \u0026lt;+0\u0026gt;:\tendbr64 0x0000000000171674 \u0026lt;+4\u0026gt;:\tpush %r15 0x0000000000171676 \u0026lt;+6\u0026gt;:\tpush %r14 0x0000000000171678 \u0026lt;+8\u0026gt;:\tpush %r13 0x000000000017167a \u0026lt;+10\u0026gt;:\tpush %r12 0x000000000017167c \u0026lt;+12\u0026gt;:\tpush %rbp 0x000000000017167d \u0026lt;+13\u0026gt;:\tpush %rbx 0x000000000017167e \u0026lt;+14\u0026gt;:\tsub $0x28,%rsp ... 我们看到，这个函数的序言没有 push %rbp 和 mov %rsp,%rbp。它直接通过 sub 指令来操作栈顶指针 %rsp 以分配栈空间。这明确地证明了 __nscd_get_map_ref 这个函数在编译时省略了帧指针。正是因为缺少这个关键的“路标”，BCC 无法找到其调用者的栈帧，导致回溯在此中断，下一行只能显示为 [unknown]。\n检查被成功“穿越”的函数 __nscd_get_mapping\n我们再来检查一下堆栈中的前一个函数 __nscd_get_mapping。BCC 成功地从这个函数回溯到了 __nscd_get_map_ref，这说明 __nscd_get_mapping 自身应该保留了帧指针。\n1 (gdb) disassemble __nscd_get_mapping 这一次，输出结果证实了我们的推断：\n1 2 3 4 5 Dump of assembler code for function __nscd_get_mapping: 0x0000000000171210 \u0026lt;+0\u0026gt;:\tendbr64 0x0000000000171214 \u0026lt;+4\u0026gt;:\tpush %rbp 0x0000000000171215 \u0026lt;+5\u0026gt;:\tmov %rsp,%rbp ... 头两行指令 push %rbp 和 mov %rsp,%rbp 是设置帧指针的标准操作。这证明了 __nscd_get_mapping 确实保留了帧指针，BCC 正是利用了这一点，才得以成功地从它回溯到上一层。\n至此，证据确凿。BCC 的帧指针回溯之旅在成功经过 __nscd_get_mapping 之后，到达了 __nscd_get_map_ref 这一站。由于 __nscd_get_map_ref 函数本身缺少帧指针，导致 BCC 无法定位其调用者的信息，回溯过程被迫中断。我们的猜想得到了验证。\n# 使用 DWARF 堆栈回溯 既然基于帧指针的“快捷方式”因为道路中断而无法走通，我们就需要启用一种更可靠的回溯方法。这个方法就是基于 DWARF 调试信息。\nDWARF 是一种被广泛使用的调试信息格式。当程序以 -g 等标志编译时，编译器会生成丰富的 DWARF 信息，并将其存储在可执行文件中，或者是一个独立的符号文件。这些信息详尽地描述了程序的结构，包括函数、变量、类型，以及如何在任何指令点精确地重建调用堆栈的规则。\n与依赖运行时指针链的帧指针回溯不同，DWARF 回溯更像是拿着一张由编译器绘制的“代码地图”和“堆栈布局说明书”。性能分析工具（如 perf）可以利用这份“地图”，即使在函数省略了帧指针的情况下，也能通过分析当前的指令指针和栈内容，精确地计算出调用者的位置。\n要成功使用 DWARF 回溯，我们必须满足两个前提条件：\n安装必要的依赖：perf 的 DWARF 回溯功能依赖于一些系统库（如 libunwind, libdw）。我们需要确保这些库已经安装，以保证 perf 的核心引擎能够正常工作。\n调试信息必须存在：被追踪的程序库（在我们的案例中是 glibc）必须有对应的调试信息包（在 Ubuntu 上通常是 dbgsym 或 dbg 包）。这些包包含了 perf 解读堆栈所需的 DWARF 地图。\n满足这两个条件后，我们就可以指示 perf 工具使用 DWARF 模式来捕获调用堆栈，从而绕过帧指针缺失的问题。\n# 还原完整的调用堆栈 现在，来捕获之前中断的完整调用堆栈。\n准备环境\n在开始追踪之前，我们需要确保 perf 的 DWARF 引擎及其依赖是完整的，并且 glibc 的调试信息也已经就绪。\n在 Ubuntu 系统上，可以通过以下命令安装所有必要的软件包：\n1 2 3 4 5 # 安装 perf 进行 DWARF 回溯所需的核心依赖库 sudo apt install libunwind-dev libdw-dev # 安装 glibc 的调试符号包 sudo apt install libc6-dbg 使用 perf 捕获数据\n环境准备就绪后，我们就可以使用 perf record 命令来启动 Java 应用并捕获系统调用事件。这次，我们将明确要求 perf 使用 DWARF 模式进行堆栈回溯。\n1 2 3 4 5 # 放宽内核对 perf 的安全限制，允许追踪用户空间 echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid # 使用 --call-graph dwarf 参数启动追踪 sudo perf record -e syscalls:sys_enter_socket --call-graph dwarf -- java Main 参数说明：\n--call-graph dwarf：这是最关键的参数。它指示 perf 在捕获事件时，必须使用 DWARF 信息来回溯并记录完整的调用堆栈。 -- java Main：-- 符号告诉 perf，后面的所有内容都是要执行的命令。请将其替换为您实际的 Java 应用启动命令。 perf 会启动 Java 应用，并从进程开始到结束持续追踪。应用退出后，追踪数据会自动保存在当前目录下的 perf.data 文件中。\n分析结果\n最后，我们使用 perf script 命令来读取 perf.data 文件，并将其中的原始数据翻译成人类可读的调用堆栈。\n1 sudo perf script 这一次，输出的结果将不再有 [unknown]。您会看到一份类似下面这样完整、清晰的调用堆栈：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 java 391724 [003] 952479.264820: syscalls:sys_enter_socket: family: 0x00000001, type: 0x00080801, protocol: 0x00000000 7fb4c96feb3b __GI_socket+0xb (inlined) 7fb4c9747c4f open_socket+0x3f (/usr/lib/x86_64-linux-gnu/libc.so.6) 7fb4c97482cc __nscd_get_mapping+0xbc (/usr/lib/x86_64-linux-gnu/libc.so.6) 7fb4c974873e __nscd_get_map_ref+0xce (/usr/lib/x86_64-linux-gnu/libc.so.6) 7fb4c97449c6 nscd_getpw_r+0x66 (/usr/lib/x86_64-linux-gnu/libc.so.6) 7fb4c9744e45 __nscd_getpwuid_r+0x65 (/usr/lib/x86_64-linux-gnu/libc.so.6) 7fb4c96c0e3f __getpwuid_r+0x1bf (inlined) 7fb4c8aaba99 get_user_name+0x59 (/usr/lib/jvm/java-17-openjdk-amd64/lib/server/libjvm.so) 7fb4c8aad1e3 PerfMemory::create_memory_region+0x93 (/usr/lib/jvm/java-17-openjdk-amd64/lib/server/libjvm.so) 7fb4c8aab773 perfMemory_init+0x73 (/usr/lib/jvm/java-17-openjdk-amd64/lib/server/libjvm.so) 7fb4c863b3c6 vm_init_globals+0x26 (/usr/lib/jvm/java-17-openjdk-amd64/lib/server/libjvm.so) 7fb4c8cfa409 Threads::create_vm+0x249 (/usr/lib/jvm/java-17-openjdk-amd64/lib/server/libjvm.so) 7fb4c8709c6d JNI_CreateJavaVM+0x4d (/usr/lib/jvm/java-17-openjdk-amd64/lib/server/libjvm.so) 7fb4c98050f0 JavaMain+0x90 (/usr/lib/jvm/java-17-openjdk-amd64/lib/libjli.so) 7fb4c9809a38 ThreadJavaMain+0x8 (/usr/lib/jvm/java-17-openjdk-amd64/lib/libjli.so) 7fb4c966bac2 start_thread+0x2f2 (/usr/lib/x86_64-linux-gnu/libc.so.6) 7fb4c96fd84f __clone3+0x2f (inlined) 通过与 BCC 的输出进行对比，我们可以清晰地看到，之前中断的堆栈已经被完美地补充完整了。[unknown] 的真实身份——nscd_getpw_r、__getpwuid_r 等函数——以及更上层的 libjvm.so 内部调用，现在都已一目了然。\n通过 perf 捕获的这份完整调用堆栈，为我们揭示了 Unix socket 创建的完整路径：\n事件的起点：在 JVM 启动的极早期阶段（JNI_CreateJavaVM -\u0026gt; Threads::create_vm），JVM 需要初始化其性能数据子系统（PerfMemory）。\n直接原因：PerfMemory::create_memory_region 函数在初始化过程中，调用了其内部的 get_user_name 函数，目的是为了获取当前运行用户的名称。这通常是为了创建用户专属的性能数据文件（例如 /tmp/hsperfdata_\u0026lt;user\u0026gt;/\u0026lt;pid\u0026gt;）。\n触发系统调用：为了从用户 ID 解析出用户名，get_user_name 函数调用了 glibc 提供的标准 POSIX 函数 __getpwuid_r。\nNSS 介入：系统的 NSS(Name Service Switch) 框架接管了这个请求。glibc 内部的 __nscd_* 系列函数开始工作，准备通过配置好的名称服务守护进程（如 lwsmd 或 nscd）来完成查询。\n最终结果：为了与这个守护进程通信，glibc 的客户端逻辑最终调用了 socket() 系统调用，创建了我们最初观察到的那个 Unix socket。\n这个 Unix socket 的创建，其根本原因在于 JVM 在初始化性能监控模块时，需要获取当前用户信息。\n# 总结 本文深入探讨了上一篇文章 中遗留的一个问题：为何使用 BCC 追踪 Unix socket 创建时，调用堆栈会在 glibc 库内部中断并显示 [unknown]。\n首先分析了 BCC 默认依赖的“帧指针回溯”技术在面对编译优化时的问题。通过 GDB 对 glibc 库相关函数的汇编代码进行现场取证，证实了堆栈中断的确是因为 glibc 内部某个函数在编译时省略了帧指针。\n为了解决这个问题，文章介绍了“DWARF 回溯”方法，最终成功捕获到了从 JVM 内部 C++ 函数到 glibc 再到系统调用的完整调用堆栈，彻底解开了 [unknown] 背后的秘密。\n","date":"2025-09-19T09:47:41+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8-dwarf-%E8%BF%98%E5%8E%9F%E5%AE%8C%E6%95%B4%E7%9A%84-glibc-%E8%B0%83%E7%94%A8%E5%A0%86%E6%A0%88/","title":"使用 DWARF 还原完整的 glibc 调用堆栈"},{"content":"When using CRaC to create a checkpoint image, Java applications need to properly handle the external resources they hold, such as open log files, listening service ports, and external database connection pools.\nFor files opened by a Java application, CRaC can handle them automatically by defining File Descriptor Policies. For listening ports or connection pools enabled by the application, it is generally recommended that the application implements CRaC\u0026rsquo;s Resource interface to close these resources before a checkpoint and reopen them after a restore.\nTo properly handle network resources held by a Java application, one must first know where these resources are created. However, in real-world development, applications usually do not call the JDK\u0026rsquo;s basic network APIs directly. In most cases, network connections are handled by frameworks like Netty or Dubbo, so developers may not even know where listening ports are opened or network connections are created.\nFurthermore, some resources, like Unix sockets, are not typically created directly by Java but are opened by the underlying JVM or dependent C libraries, making it even more difficult to trace their creation.\nThis article will introduce how to use BCC and async-profiler to trace and locate the specific points of Socket creation in a Java application.\n# Using BCC to Trace and Locate Unix Socket Creation Let\u0026rsquo;s start with a concrete example. When creating a checkpoint image for a Java application, the following exception was encountered:\n1 2 3 4 5 Suppressed: jdk.internal.crac.mirror.impl.CheckpointOpenSocketException: FD fd=4 type=socket path=socket:[7504404],port=29295 at java.base/jdk.internal.crac.mirror.Core.translateJVMExceptions(Core.java:116) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore1(Core.java:189) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore(Core.java:315) at java.base/jdk.internal.crac.mirror.Core.checkpointRestoreInternal(Core.java:328) Even though the JVM parameter -Djdk.crac.collect-fd-stacktraces=true was added, the logs still did not show the specific location where file descriptor 4 (FD 4) was opened. This usually means that FD 4 was likely opened in Native Code.\nWe use the lsof command to view the details of FD 4:\n1 2 $ lsof -p 172963 | grep 7504404 exe 172963 mazhen 4u unix 0x00000000bfe2338f 0t0 7504404 type=STREAM The fifth column, unix, indicates that the resource corresponding to this file descriptor is a Unix socket.\nTo further investigate the origin and purpose of this Unix socket, we need to find out which process is connected to the other end:\n1 2 3 4 5 6 7 8 9 # -U: Filter for Unix Sockets. # -a: Logical AND operation, combining the preceding conditions (-p and -U). # +E: Display socket endpoint information. $ sudo lsof -p 172963 -U -a +E COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME lwsmd 1373 root 73u unix 0x000000001b73058d 0t0 7506688 /var/lib/pbis/.lsassd type=STREAM -\u0026gt;INO=7504404 172963,exe,4u exe 172963 mazhen 4u unix 0x00000000bfe2338f 0t0 7504404 type=STREAM -\u0026gt;INO=7506688 1373,lwsmd,73u exe 172963 mazhen 407u unix 0x000000005c8b74c1 0t0 7507634 type=STREAM -\u0026gt;INO=7507635 172963,exe,408u exe 172963 mazhen 408u unix 0x00000000cf9143dc 0t0 7507635 type=STREAM -\u0026gt;INO=7507634 172963,exe,407u From the output, we can see that file descriptor 4 is connected to an external process, lwsmd. This is a security authentication-related service used to integrate Linux into Windows Active Directory. The Java process communicates with lwsmd via a Unix socket to implement user authentication and permission management.\nAdditionally, we found a pair of file descriptors, 407 and 408, which are the two ends of a Unix socket, both created and held by the Java process itself. This resource pair did not throw an exception during the checkpoint process, indicating they were handled properly.\nNow we know the Java process creates these Unix sockets, but the key question is: How do we locate the exact creation point?\nThe most direct way to find the source of Unix socket creation is to trace the system call it triggers at the kernel level. By capturing the complete call stack at the moment the system call occurs, we can trace it back to the specific code location.\nAt the application level, code creates Unix sockets by calling the socket() and socketpair() library functions provided by glibc. These functions prepare the arguments and then execute a special CPU instruction to switch from user mode to kernel mode, initiating the actual system call.\nTo observe this event in the kernel, we can use the kernel\u0026rsquo;s static probe points: Tracepoints. Tracepoints are static probes preset by kernel developers in the kernel code, allowing us to observe key kernel activities. When a system call request enters the kernel, the corresponding tracepoint is triggered, and we can use tools to capture this event and get context information, including the call stack.\nFor creating Unix sockets, we are interested in the following two tracepoints:\nsyscalls:sys_enter_socket: Triggered when the socket() system call enters the kernel. syscalls:sys_enter_socketpair: Triggered when the socketpair() system call enters the kernel. Since the socket() function can create various types of sockets (TCP, UDP, etc.), we must add a filter to our tracing to only capture calls where the first argument, family, is AF_UNIX. This allows us to precisely target the creation of Unix sockets.\nTo achieve this, we will use BCC (BPF Compiler Collection), a powerful toolset.\nBCC comes with a general-purpose trace tool that can trace any function. We can use it for a quick proof of concept:\n1 $ sudo python3 /usr/share/bcc/tools/trace -K -U \u0026#39;t:syscalls:sys_enter_socket (args-\u0026gt;family == 1) \u0026#34;socket(family=%d, type=%d)\u0026#34;, args-\u0026gt;family, args-\u0026gt;type\u0026#39; -K, -U: Capture kernel and user-space stacks, respectively. t:syscalls:sys_enter_socket: Specifies the tracepoint to trace. (args-\u0026gt;family == 1): A filter to only care about AF_UNIX (value 1) type socket creations. The output of this command is as follows:\n1 2 3 4 5 177325 177330 java sys_enter_socket socket(family=1, type=526337) -14 socket+0xb [libc.so.6] __nscd_open_socket+0x3b [libc.so.6] inet_pton+0x2e [libc.so.6] ... As you can see, this command successfully captured the socket call and its stack. However, it has a major drawback: the output does not include the file descriptor returned by the system call. Therefore, we cannot associate this stack with the FD 4 we are interested in.\nTo get the file descriptor returned by socket() or socketpair(), we need to write a custom BCC script:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 #!/usr/bin/env python3 from bcc import BPF from datetime import datetime # BPF C code prog = r\u0026#34;\u0026#34;\u0026#34; #include \u0026lt;uapi/linux/ptrace.h\u0026gt; #include \u0026lt;linux/sched.h\u0026gt; #include \u0026lt;net/sock.h\u0026gt; // for AF_UNIX // Define data structure to send to userspace enum call_type { TYPE_SOCKET = 1, TYPE_SOCKETPAIR = 2, }; struct event_t { u64 ts; u32 pid; u32 tid; u32 ppid; int stack_id; int fds[2]; // fds[0] for socket, fds[0] \u0026amp; fds[1] for socketpair enum call_type call_type; }; BPF_PERF_OUTPUT(events); // Used to pass socketpair parameter pointer between enter and exit struct data_t { int *sv_ptr; }; BPF_HASH(infotmp, u32, struct data_t); BPF_STACK_TRACE(stack_traces, 16384); // --- tracepoint for socket() --- TRACEPOINT_PROBE(syscalls, sys_enter_socket) { if (args-\u0026gt;family != AF_UNIX) { return 0; } u32 tid = (u32)bpf_get_current_pid_tgid(); struct data_t data = {}; data.sv_ptr = NULL; // Mark this as socket() call infotmp.update(\u0026amp;tid, \u0026amp;data); return 0; } TRACEPOINT_PROBE(syscalls, sys_exit_socket) { u32 tid = (u32)bpf_get_current_pid_tgid(); struct data_t *datap = infotmp.lookup(\u0026amp;tid); if (!datap) { return 0; } int retval = args-\u0026gt;ret; if (retval \u0026gt;= 0) { struct event_t event = {}; struct task_struct *task = (struct task_struct *)bpf_get_current_task(); u64 id = bpf_get_current_pid_tgid(); event.ts = bpf_ktime_get_ns(); event.pid = id \u0026gt;\u0026gt; 32; event.tid = (u32)id; bpf_probe_read_kernel(\u0026amp;event.ppid, sizeof(event.ppid), \u0026amp;task-\u0026gt;real_parent-\u0026gt;tgid); event.stack_id = stack_traces.get_stackid(args, BPF_F_USER_STACK); event.call_type = TYPE_SOCKET; event.fds[0] = retval; event.fds[1] = -1; events.perf_submit(args, \u0026amp;event, sizeof(event)); } infotmp.delete(\u0026amp;tid); return 0; } // --- tracepoint for socketpair() --- TRACEPOINT_PROBE(syscalls, sys_enter_socketpair) { if (args-\u0026gt;family != AF_UNIX) { return 0; } u32 tid = (u32)bpf_get_current_pid_tgid(); struct data_t data = {}; data.sv_ptr = (int *)args-\u0026gt;usockvec; infotmp.update(\u0026amp;tid, \u0026amp;data); return 0; } TRACEPOINT_PROBE(syscalls, sys_exit_socketpair) { u32 tid = (u32)bpf_get_current_pid_tgid(); struct data_t *datap = infotmp.lookup(\u0026amp;tid); if (!datap || !datap-\u0026gt;sv_ptr) { if (datap) { infotmp.delete(\u0026amp;tid); } return 0; } int retval = args-\u0026gt;ret; if (retval == 0) { struct event_t event = {}; struct task_struct *task = (struct task_struct *)bpf_get_current_task(); u64 id = bpf_get_current_pid_tgid(); event.ts = bpf_ktime_get_ns(); event.pid = id \u0026gt;\u0026gt; 32; event.tid = (u32)id; bpf_probe_read_kernel(\u0026amp;event.ppid, sizeof(event.ppid), \u0026amp;task-\u0026gt;real_parent-\u0026gt;tgid); event.stack_id = stack_traces.get_stackid(args, BPF_F_USER_STACK); event.call_type = TYPE_SOCKETPAIR; bpf_probe_read_user(\u0026amp;event.fds, sizeof(event.fds), datap-\u0026gt;sv_ptr); events.perf_submit(args, \u0026amp;event, sizeof(event)); } infotmp.delete(\u0026amp;tid); return 0; } \u0026#34;\u0026#34;\u0026#34; # Load BPF program b = BPF(text=prog) print(\u0026#34;Tracing socket(AF_UNIX) and socketpair(AF_UNIX) calls... Ctrl-C to stop.\\n\u0026#34;) # Perf Buffer callback handler def print_event(cpu, data, size): event = b[\u0026#34;events\u0026#34;].event(data) time_str = datetime.fromtimestamp(event.ts / 1e9).strftime(\u0026#39;%H:%M:%S.%f\u0026#39;) tgid = event.pid if event.call_type == 1: # TYPE_SOCKET call_str = \u0026#34;[socket(AF_UNIX)]\u0026#34; fds_str = f\u0026#34;[FD={event.fds[0]}]\u0026#34; elif event.call_type == 2: # TYPE_SOCKETPAIR call_str = \u0026#34;[socketpair(AF_UNIX)]\u0026#34; fds_str = f\u0026#34;[FDs=[{event.fds[0]}, {event.fds[1]}]]\u0026#34; else: call_str = \u0026#34;UNKNOWN\u0026#34; fds_str = \u0026#34;\u0026#34; print(f\u0026#34;[{time_str}] {call_str} [PPID={event.ppid}] [PID={event.pid}] {fds_str}\u0026#34;) try: for addr in b[\u0026#34;stack_traces\u0026#34;].walk(event.stack_id): print(\u0026#34; %s\u0026#34; % b.sym(addr, tgid, show_module=True, show_offset=True)) except KeyError: print(\u0026#34; [Stack trace unavailable for stack_id %d due to process exit]\u0026#34; % event.stack_id) print(\u0026#34;\u0026#34;) # Open perf buffer and set callback function b[\u0026#34;events\u0026#34;].open_perf_buffer(print_event) # Main loop, poll perf buffer while True: try: b.perf_buffer_poll() except KeyboardInterrupt: exit() The idea behind this script is to store information on system call entry (sys_enter_*) and capture the return value (i.e., the file descriptor) on exit (sys_exit_*), then send the file descriptor, stack trace, and other information to the user-space Python program for printing.\nFirst, run this script, and then start the Java application.\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ sudo ./trace_unix_socket.py \u0026gt; unixsocket # Start the Java application in another terminal # Get the PID of the Java process $ jps 178106 Jps 177961 GlassFishMain # Confirm the Unix socket again with lsof $ sudo lsof -p 177961 -U -a +E COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME lwsmd 1373 root 78u unix 0x000000001e5b48ae 0t0 7629677 /var/lib/pbis/.lsassd type=STREAM -\u0026gt;INO=7625335 177961,exe,4u exe 177961 mazhen 4u unix 0x00000000b7b4fa52 0t0 7625335 type=STREAM -\u0026gt;INO=7629677 1373,lwsmd,78u exe 177961 mazhen 407u unix 0x0000000078480cf3 0t0 7630050 type=STREAM -\u0026gt;INO=7630051 177961,exe,408u exe 177961 mazhen 408u unix 0x000000002d0320c8 0t0 7630051 type=STREAM -\u0026gt;INO=7630050 177961,exe,407u Now, in the script\u0026rsquo;s output file unixsocket, we can search by the process PID (177961) and file descriptors (FD 4, 407, 408) to find the corresponding creation stacks:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ... [12:23:13.184181] [socket(AF_UNIX)] [PPID=177932] [PID=177961] [FD=4] b\u0026#39;socket+0xb [libc.so.6]\u0026#39; b\u0026#39;__nscd_get_mapping+0xbd [libc.so.6]\u0026#39; b\u0026#39;__nscd_get_map_ref+0xcf [libc.so.6]\u0026#39; b\u0026#39;[unknown]\u0026#39; ... [12:23:20.370346] [socketpair(AF_UNIX)] [PPID=2583] [PID=177961] [FDs=[407, 408]] b\u0026#39;socketpair+0xe [libc.so.6]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;JavaCalls::call_helper(JavaValue*, methodHandle const\u0026amp;, JavaCallArguments*, JavaThread*)+0x334 [libjvm.so]\u0026#39; b\u0026#39;JavaCalls::call_virtual(JavaValue*, Handle, Klass*, Symbol*, Symbol*, JavaThread*)+0x20c [libjvm.so]\u0026#39; b\u0026#39;thread_entry(JavaThread*, JavaThread*)+0x70 [libjvm.so]\u0026#39; b\u0026#39;JavaThread::run()+0x127 [libjvm.so]\u0026#39; b\u0026#39;Thread::call_run()+0xa1 [libjvm.so]\u0026#39; b\u0026#39;thread_native_entry(Thread*)+0xe3 [libjvm.so]\u0026#39; b\u0026#39;start_thread+0x2f3 [libc.so.6]\u0026#39; For FD 4, the functions __nscd_get_mapping and __nscd_get_map_ref in the call stack are key clues. They indicate that glibc is attempting a name service query (e.g., resolving a username). This explains our earlier observation of why the Java process connects to lwsmd. The __nscd_* functions are the generic client logic used by glibc for name resolution, and the actual service it connects to is determined by the system\u0026rsquo;s NSS (Name Service Switch). In my test scenario, the NSS configuration directs authentication queries to lwsmd, which is why the Java process connects to it via a Unix socket. At this point, we know the purpose and origin of FD 4. How to handle this resource requires further exploration, but at least we have identified the root cause.\nLooking at FDs 407 and 408, we can see function calls from libjvm.so, which clearly indicates that a Java thread called socketpair to create this Unix socket pair. However, the most critical part of the call stack is shown as [unknown].\nThis is because the JIT (Just-In-Time) compiler dynamically compiles hot code into machine code at runtime and stores it in anonymous memory regions. These regions do not have traditional symbol tables, so general-purpose system-level tools like BCC cannot find any symbol information when trying to resolve these memory addresses, and can only display them as [unknown].\nAlthough this Unix socket (FDs 407, 408) was handled properly and did not cause a CRaC exception, if we are curious and want to know exactly which piece of Java code created it, what should we do?\nTo solve the [unknown] problem, we need a tool that understands the Java runtime better: async-profiler.\n# Using async-profiler to Trace Socket Creation async-profiler is a low-overhead, high-precision performance analysis tool for Java that directly utilizes Linux\u0026rsquo;s perf_events subsystem and HotSpot JVM-specific APIs to collect performance data.\nCompared to traditional Java Profilers, it can not only analyze Java code but also monitor the activities of non-Java threads (like GC and JIT compiler threads) and display native and kernel frames in call stacks, giving us a complete view from Java methods down to C/C++ library functions and kernel system calls.\nGeneral-purpose system tracing tools like BCC do not understand the internal workings of the JVM. When they encounter machine code generated by the JIT compiler in anonymous memory regions, they cannot find corresponding symbol tables and thus cannot resolve method names. In contrast, async-profiler uses specific APIs provided by HotSpot (like AsyncGetCallTrace) and can parse JVM internal data structures to obtain symbol information for JIT-compiled code, accurately mapping memory addresses back to the original Java method names.\nIn essence, async-profiler combines the capabilities of system-level tracing tools and traditional Java profilers, providing a complete and accurate performance insight into Java applications.\n# Tracing the creation location of socketpair In the previous section, we used BCC to trace Unix socket-related system calls. This time, we will use async-profiler to do the same, but with the ability to display Java method names in the call stack.\nasync-profiler supports using Linux\u0026rsquo;s perf_events, so it can capture tracepoint events from perf_events, achieving the same system call tracing as BCC. Additionally, async-profiler can be launched as a Java Agent, which means we can load it at the start of the Java process and not miss any early initialization behavior.\nReturning to our previous question, to locate the Java code that created the socketpair for file descriptors 408 and 409, we can attach async-profiler as an agent when starting the Java application and specify that we want to trace the syscalls:sys_enter_socketpair event.\nModify the Java startup command as follows:\n1 java -agentpath:/path/to/libasyncProfiler.so=start,event=syscalls:sys_enter_socketpair,file=/tmp/socketpair-trace.html ... Parameter explanation:\n-agentpath:/path/to/libasyncProfiler.so: Specifies the location of the async-profiler dynamic library. start: Indicates that profiling should start immediately. event=syscalls:sys_enter_socketpair: Specifies the event to trace. Here we are interested in the entry of the socketpair system call. file=/tmp/socketpair-trace.html: Outputs the analysis result as an HTML flame graph. Note that by default, the kernel may prohibit non-root users from tracing certain sensitive kernel events (like system calls), so it\u0026rsquo;s best to start the Java application with sudo.\nAfter the application runs and exits normally, the /tmp/socketpair-trace.html file will be generated according to the configuration.\nThis flame graph clearly reveals the parts that BCC could not resolve ([unknown]), accurately pinpointing the Java code source that created the socketpair.\nFrom the graph above, we can see that this socketpair was created by Java\u0026rsquo;s WatchService API in its Linux implementation. WatchService is a standard interface provided by Java NIO for monitoring directory changes in the file system (such as file creation, modification, or deletion). The call stack shows that the WatchService was called by the FileInstall component of the Apache Felix framework. Its purpose is to monitor a deployment directory to enable hot deployment of OSGi bundles.\nWe observed earlier that the file descriptor pair created by socketpair did not cause an exception during the checkpoint process. The reason is that CRaC already provides built-in support for the Linux implementation of Java\u0026rsquo;s WatchService (see OpenJDK CRaC PR #72). This means CRaC can automatically recognize and properly handle the internal file descriptors used by WatchService.\n# Tracing the Listening Position of a Service Port The same principle applies to locating the listening position of a network service port. For network services created with the NIO framework, the application needs to implement the Resource interface to adapt to CRaC, handling the closing and reopening of the listening port. If not handled properly, an exception similar to the following may occur when creating a checkpoint image:\n1 2 3 4 5 6 7 8 9 10 11 jdk.internal.crac.mirror.CheckpointException Suppressed: sun.nio.ch.EPollSelectorImpl$BusySelectorException: Selector sun.nio.ch.EPollSelectorImpl@476d29ab has registered keys from channels: [sun.nio.ch.ServerSocketChannelImpl[closed]] at java.base/sun.nio.ch.EPollSelectorImpl.beforeCheckpoint(EPollSelectorImpl.java:405) at java.base/jdk.internal.crac.mirror.impl.AbstractContext.invokeBeforeCheckpoint(AbstractContext.java:43) at java.base/jdk.internal.crac.mirror.impl.AbstractContext.beforeCheckpoint(AbstractContext.java:58) at java.base/jdk.internal.crac.mirror.impl.BlockingOrderedContext.beforeCheckpoint(BlockingOrderedContext.java:64) at java.base/jdk.internal.crac.mirror.impl.AbstractContext.invokeBeforeCheckpoint(AbstractContext.java:43) at java.base/jdk.internal.crac.mirror.impl.AbstractContext.beforeCheckpoint(AbstractContext.java:58) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore1(Core.java:154) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore(Core.java:315) at java.base/jdk.internal.crac.mirror.Core.checkpointRestoreInternal(Core.java:328) From the exception message, we know that the direct cause of the checkpoint failure is that the EPollSelectorImpl instance is in a \u0026ldquo;busy\u0026rdquo; state. It appears that even though the ServerSocketChannel itself may have been closed, its registration with the Selector was not fully cancelled.\nHowever, this exception message does not tell us where in the application this port was created and listened on. To properly handle the listening port, we must first find its creation point.\nHow can we do this? We can trace a key system call: bind.\nThe bind system call is a necessary step for any network server program to start. Its function is to associate a created socket file descriptor with a specific IP address and port number. Only after bind is successfully executed can the server listen for and accept client connections on that port. Therefore, by capturing the call stack when the bind system call occurs, we can precisely locate which piece of Java code triggered the port listening.\nUse the following command to start the Java application:\n1 java -agentpath:/path/to/libasyncProfiler.so=start,event=syscalls:sys_enter_bind,file=/tmp/bind-trace.html -jar ... After the application starts successfully and begins serving traffic, stop it and then analyze the generated /tmp/bind-trace.html file.\nFrom the flame graph above, we can see that the port binding operation was performed by the Grizzly NIO framework, which acts as the underlying network engine for the GlassFish application server. The entire process is automatically triggered by the HK2 dependency injection framework during the server startup phase to initialize and start the core network services.\nBy analyzing the flame graph, we can conclude that implementing the Resource interface at the GrizzlyListener level would be a reasonable approach to manage the closing and restarting of the listening port.\n# Summary This article aims to solve the challenge of accurately locating the creation points of network resources (like Unix sockets and service listening ports) when adapting Java applications for CRaC.\nFirst, it introduces the use of the system-level tracing tool BCC. Although BCC can capture the call stacks of native code by tracing system calls like socket and socketpair, it cannot resolve JIT-compiled Java code, leading to key information being displayed as [unknown].\nTo address this issue, the article introduces the more powerful async-profiler. It combines the capability of underlying perf_events tracing with a deep understanding of the JVM internals, allowing it to perfectly resolve symbols for JIT code. Through practical examples of tracing sys_enter_socketpair and sys_enter_bind tracepoints, it demonstrates how to use the flame graphs generated by async-profiler to trace system calls back to specific Java code.\n","date":"2025-09-15T14:47:32+08:00","permalink":"https://mazhen.tech/p/tracing-and-locating-socket-creation-in-java-processes/","title":"Tracing and Locating Socket Creation in Java Processes"},{"content":"在使用 CRaC 创建 checkpoint 镜像时，需要 Java 应用能够恰当处理它持有的外部资源，例如打开的日志文件，监听的服务端口，对外创建的数据库连接池等。\n对于 Java 应用打开的文件，可以通过定义文件描述符策略 让 CRaC 自动处理。对于应用启用的监听端口，或创建的连接池，一般建议应用实现 CRaC 的 Resource 接口，在 checkpoint 前关闭资源，在 restore 后重新打开。\n要妥善的处理 Java 持有的网络资源，首先需要知道这些资源具体是在哪里创建的。然而在实际开发中，应用程序通常不会直接调用 JDK 的基础网络 API，大多数情况下是通过 Netty、Dubbo 等框架来处理网络连接，所以造成开发人员可能自己都不知道在哪里打开了监听端口，或者创建了网络连接。\n还有一些资源，例如 Unix socket，一般不是 Java 直接创建持有的，而是底层的 JVM 或依赖的 C 库打开的，要追踪定位这类资源的创建，就更加困难了。\n本文将介绍如何使用 BCC 和 async-profiler 追踪定位 Java 应用中创建 Socket 的具体位置。\n# 使用 BCC 追踪定位 Unix Socket 的创建 我们从一个具体的的例子开始。在为某个 Java 应用创建 checkpoint 镜像时，遇到了如下异常：\n1 2 3 4 5 Suppressed: jdk.internal.crac.mirror.impl.CheckpointOpenSocketException: FD fd=4 type=socket path=socket:[7504404],port=29295 at java.base/jdk.internal.crac.mirror.Core.translateJVMExceptions(Core.java:116) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore1(Core.java:189) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore(Core.java:315) at java.base/jdk.internal.crac.mirror.Core.checkpointRestoreInternal(Core.java:328) 尽管已经添加了 JVM 参数 -Djdk.crac.collect-fd-stacktraces=true，但日志中仍然没有输出文件描述符 4（FD 4）的具体打开位置。这通常意味着 FD 4 很可能是在 Native Code 中被打开的。\n我们使用 lsof 命令查看 FD 4 的详细信息：\n1 2 $ lsof -p 172963 | grep 7504404 exe 172963 mazhen 4u unix 0x00000000bfe2338f 0t0 7504404 type=STREAM 第五列的 unix 表明，该文件描述符对应的资源是一个 Unix socket。\n为了进一步调研这个 Unix socket 的来源和作用，我们需要知道它的另一端连接着哪个进程：\n1 2 3 4 5 6 7 8 9 # -U ：筛选 Unix Socket。 # -a：逻辑 “与”（AND）操作，将前面的条件（-p 和 -U）组合，表示必须同时满足 # +E：显示套接字的端点信息（Endpoints）。 $ sudo lsof -p 172963 -U -a +E COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME lwsmd 1373 root 73u unix 0x000000001b73058d 0t0 7506688 /var/lib/pbis/.lsassd type=STREAM -\u0026gt;INO=7504404 172963,exe,4u exe 172963 mazhen 4u unix 0x00000000bfe2338f 0t0 7504404 type=STREAM -\u0026gt;INO=7506688 1373,lwsmd,73u exe 172963 mazhen 407u unix 0x000000005c8b74c1 0t0 7507634 type=STREAM -\u0026gt;INO=7507635 172963,exe,408u exe 172963 mazhen 408u unix 0x00000000cf9143dc 0t0 7507635 type=STREAM -\u0026gt;INO=7507634 172963,exe,407u 从输出中可以看出，文件描述符 4 连接到外部进程 lwsmd。这是一个安全认证相关的服务，用于将 Linux 集成到 Windows 的 Active Directory。Java 进程通过 Unix socket 和 lwsmd 通信，以实现用户认证和权限管理。\n此外，我们还发现了一对文件描述符 407 和 408，它们是一个 Unix socket 的两端，并且都是由该 Java 进程自身创建和持有。在创建 checkpoint 镜像时，这对资源并没有抛出异常，说明它们已经被妥善处理。\n现在我们知道了 Java 进程会创建这些 Unix socket，但关键问题是：如何定位到具体的创建位置呢？\n要找到创建 Unix socket 的源头，最直接的方法就是追踪它在内核层面触发的系统调用。通过捕获系统调用发生时的完整调用堆栈，我们就能反向追溯到具体的代码位置。\n在应用程序层面，代码通过调用 glibc 提供的 socket() 和 socketpair() 库函数 创建 Unix socket，它们的会准备好参数，然后执行一条特殊的 CPU 指令，使程序从用户模式切换到内核模式，从而发起真正的系统调用。\n为了在内核中观察到这个事件，我们可以利用内核的静态探测点：Tracepoints。Tracepoint 是内核开发者在内核代码中预设的静态探测点，允许我们观察内核的关键活动。当一个系统调用请求进入内核时，相应的 tracepoint 就会被触发，我们可以使用工具捕获这个事件，获取当时的上下文信息，包括调用堆栈。\n对于创建 Unix socket 的操作，我们关心以下两个 tracepoint：\nsyscalls:sys_enter_socket: 当 socket() 系统调用进入内核时触发。 syscalls:sys_enter_socketpair: 当 socketpair() 系统调用进入内核时触发。 由于 socket() 函数可以创建多种类型的 socket (TCP, UDP 等)，我们必须在追踪时添加一个过滤条件，只捕获其第一个参数 family 为 AF_UNIX 的调用，这样才能精确地锁定 Unix socket 的创建。\n要实现这一点，我们将使用 BCC（BPF Compiler Collection） 这是一个强大的工具集。\nBCC 自带了一个通用的 trace 工具，能够追踪任意函数，我们可以先用它来快速验证一下思路：\n1 $ sudo python3 /usr/share/bcc/tools/trace -K -U \u0026#39;t:syscalls:sys_enter_socket (args-\u0026gt;family == 1) \u0026#34;socket(family=%d, type=%d)\u0026#34;, args-\u0026gt;family, args-\u0026gt;type\u0026#39; -K, -U: 分别表示捕获内核和用户空间的堆栈。 t:syscalls:sys_enter_socket: 指定要追踪的 tracepoint。 (args-\u0026gt;family == 1): 过滤器，只关心 AF_UNIX (值为 1) 类型的 socket 创建。 这个命令的输出如下：\n1 2 3 4 5 177325 177330 java sys_enter_socket socket(family=1, type=526337) -14 socket+0xb [libc.so.6] __nscd_open_socket+0x3b [libc.so.6] inet_pton+0x2e [libc.so.6] ... 可以看出，这个命令成功捕获到了 socket 调用及其堆栈。但它有一个很大的缺点：输出中不包含系统调用返回的文件描述符。因此，我们无法将这个堆栈与我们关心的 FD 4 关联起来。\n为了获取 socket 或 socketpair() 返回的文件描述符，我们需要编写一个自定义的 BCC 脚本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 #!/usr/bin/env python3 from bcc import BPF from datetime import datetime # BPF C code prog = r\u0026#34;\u0026#34;\u0026#34; #include \u0026lt;uapi/linux/ptrace.h\u0026gt; #include \u0026lt;linux/sched.h\u0026gt; #include \u0026lt;net/sock.h\u0026gt; // for AF_UNIX // Define data structure to send to userspace enum call_type { TYPE_SOCKET = 1, TYPE_SOCKETPAIR = 2, }; struct event_t { u64 ts; u32 pid; u32 tid; u32 ppid; int stack_id; int fds[2]; // fds[0] for socket, fds[0] \u0026amp; fds[1] for socketpair enum call_type call_type; }; BPF_PERF_OUTPUT(events); // Used to pass socketpair parameter pointer between enter and exit struct data_t { int *sv_ptr; }; BPF_HASH(infotmp, u32, struct data_t); BPF_STACK_TRACE(stack_traces, 16384); // --- tracepoint for socket() --- TRACEPOINT_PROBE(syscalls, sys_enter_socket) { if (args-\u0026gt;family != AF_UNIX) { return 0; } u32 tid = (u32)bpf_get_current_pid_tgid(); struct data_t data = {}; data.sv_ptr = NULL; // Mark this as socket() call infotmp.update(\u0026amp;tid, \u0026amp;data); return 0; } TRACEPOINT_PROBE(syscalls, sys_exit_socket) { u32 tid = (u32)bpf_get_current_pid_tgid(); struct data_t *datap = infotmp.lookup(\u0026amp;tid); if (!datap) { return 0; } int retval = args-\u0026gt;ret; if (retval \u0026gt;= 0) { struct event_t event = {}; struct task_struct *task = (struct task_struct *)bpf_get_current_task(); u64 id = bpf_get_current_pid_tgid(); event.ts = bpf_ktime_get_ns(); event.pid = id \u0026gt;\u0026gt; 32; event.tid = (u32)id; bpf_probe_read_kernel(\u0026amp;event.ppid, sizeof(event.ppid), \u0026amp;task-\u0026gt;real_parent-\u0026gt;tgid); event.stack_id = stack_traces.get_stackid(args, BPF_F_USER_STACK); event.call_type = TYPE_SOCKET; event.fds[0] = retval; event.fds[1] = -1; events.perf_submit(args, \u0026amp;event, sizeof(event)); } infotmp.delete(\u0026amp;tid); return 0; } // --- tracepoint for socketpair() --- TRACEPOINT_PROBE(syscalls, sys_enter_socketpair) { if (args-\u0026gt;family != AF_UNIX) { return 0; } u32 tid = (u32)bpf_get_current_pid_tgid(); struct data_t data = {}; data.sv_ptr = (int *)args-\u0026gt;usockvec; infotmp.update(\u0026amp;tid, \u0026amp;data); return 0; } TRACEPOINT_PROBE(syscalls, sys_exit_socketpair) { u32 tid = (u32)bpf_get_current_pid_tgid(); struct data_t *datap = infotmp.lookup(\u0026amp;tid); if (!datap || !datap-\u0026gt;sv_ptr) { if (datap) { infotmp.delete(\u0026amp;tid); } return 0; } int retval = args-\u0026gt;ret; if (retval == 0) { struct event_t event = {}; struct task_struct *task = (struct task_struct *)bpf_get_current_task(); u64 id = bpf_get_current_pid_tgid(); event.ts = bpf_ktime_get_ns(); event.pid = id \u0026gt;\u0026gt; 32; event.tid = (u32)id; bpf_probe_read_kernel(\u0026amp;event.ppid, sizeof(event.ppid), \u0026amp;task-\u0026gt;real_parent-\u0026gt;tgid); event.stack_id = stack_traces.get_stackid(args, BPF_F_USER_STACK); event.call_type = TYPE_SOCKETPAIR; bpf_probe_read_user(\u0026amp;event.fds, sizeof(event.fds), datap-\u0026gt;sv_ptr); events.perf_submit(args, \u0026amp;event, sizeof(event)); } infotmp.delete(\u0026amp;tid); return 0; } \u0026#34;\u0026#34;\u0026#34; # Load BPF program b = BPF(text=prog) print(\u0026#34;Tracing socket(AF_UNIX) and socketpair(AF_UNIX) calls... Ctrl-C to stop.\\n\u0026#34;) # Perf Buffer callback handler def print_event(cpu, data, size): event = b[\u0026#34;events\u0026#34;].event(data) time_str = datetime.fromtimestamp(event.ts / 1e9).strftime(\u0026#39;%H:%M:%S.%f\u0026#39;) tgid = event.pid if event.call_type == 1: # TYPE_SOCKET call_str = \u0026#34;[socket(AF_UNIX)]\u0026#34; fds_str = f\u0026#34;[FD={event.fds[0]}]\u0026#34; elif event.call_type == 2: # TYPE_SOCKETPAIR call_str = \u0026#34;[socketpair(AF_UNIX)]\u0026#34; fds_str = f\u0026#34;[FDs=[{event.fds[0]}, {event.fds[1]}]]\u0026#34; else: call_str = \u0026#34;UNKNOWN\u0026#34; fds_str = \u0026#34;\u0026#34; print(f\u0026#34;[{time_str}] {call_str} [PPID={event.ppid}] [PID={event.pid}] {fds_str}\u0026#34;) try: for addr in b[\u0026#34;stack_traces\u0026#34;].walk(event.stack_id): print(\u0026#34; %s\u0026#34; % b.sym(addr, tgid, show_module=True, show_offset=True)) except KeyError: print(\u0026#34; [Stack trace unavailable for stack_id %d due to process exit]\u0026#34; % event.stack_id) print(\u0026#34;\u0026#34;) # Open perf buffer and set callback function b[\u0026#34;events\u0026#34;].open_perf_buffer(print_event) # Main loop, poll perf buffer while True: try: b.perf_buffer_poll() except KeyboardInterrupt: exit() 这个脚本的思路是在系统调用进入 (sys_enter_*) 时暂存信息，在退出 (sys_exit_*) 时捕获返回值（即文件描述符），然后将文件描述符、堆栈信息等一并发送到用户空间的 Python 程序进行打印。\n先运行这个脚本，然后再启动 Java 应用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ sudo ./trace_unix_socket.py \u0026gt; unixsocket # 在另一个终端启动 Java 应用 # 获取 Java 进程的 PID $ jps 178106 Jps 177961 GlassFishMain # 再次使用 lsof 确认 Unix socket $ sudo lsof -p 177961 -U -a +E COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME lwsmd 1373 root 78u unix 0x000000001e5b48ae 0t0 7629677 /var/lib/pbis/.lsassd type=STREAM -\u0026gt;INO=7625335 177961,exe,4u exe 177961 mazhen 4u unix 0x00000000b7b4fa52 0t0 7625335 type=STREAM -\u0026gt;INO=7629677 1373,lwsmd,78u exe 177961 mazhen 407u unix 0x0000000078480cf3 0t0 7630050 type=STREAM -\u0026gt;INO=7630051 177961,exe,408u exe 177961 mazhen 408u unix 0x000000002d0320c8 0t0 7630051 type=STREAM -\u0026gt;INO=7630050 177961,exe,407u 现在，我们可以在脚本的输出文件 unixsocket 中，根据进程 PID (177961) 和文件描述符（FD 4，407，408）进行查找，找到对应的创建堆栈：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ... [12:23:13.184181] [socket(AF_UNIX)] [PPID=177932] [PID=177961] [FD=4] b\u0026#39;socket+0xb [libc.so.6]\u0026#39; b\u0026#39;__nscd_get_mapping+0xbd [libc.so.6]\u0026#39; b\u0026#39;__nscd_get_map_ref+0xcf [libc.so.6]\u0026#39; b\u0026#39;[unknown]\u0026#39; ... [12:23:20.370346] [socketpair(AF_UNIX)] [PPID=2583] [PID=177961] [FDs=[407, 408]] b\u0026#39;socketpair+0xe [libc.so.6]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;[unknown]\u0026#39; b\u0026#39;JavaCalls::call_helper(JavaValue*, methodHandle const\u0026amp;, JavaCallArguments*, JavaThread*)+0x334 [libjvm.so]\u0026#39; b\u0026#39;JavaCalls::call_virtual(JavaValue*, Handle, Klass*, Symbol*, Symbol*, JavaThread*)+0x20c [libjvm.so]\u0026#39; b\u0026#39;thread_entry(JavaThread*, JavaThread*)+0x70 [libjvm.so]\u0026#39; b\u0026#39;JavaThread::run()+0x127 [libjvm.so]\u0026#39; b\u0026#39;Thread::call_run()+0xa1 [libjvm.so]\u0026#39; b\u0026#39;thread_native_entry(Thread*)+0xe3 [libjvm.so]\u0026#39; b\u0026#39;start_thread+0x2f3 [libc.so.6]\u0026#39; 对于 FD 4，调用堆栈中的 __nscd_get_mapping 和 __nscd_get_map_ref 函数是关键线索，它表明 glibc 正在尝试进行一次名称服务查询（例如，解析一个用户名）。这正好解释了前面我们观察到的现象，为何 Java 进程会连接到 lwsmd。__nscd_* 是 glibc 用于名称解析的通用客户端逻辑，而具体解析动作需要连接到哪个服务，则由系统的 NSS(Name Service Switch) 决定。在我的测试场景，NSS 的配置将认证查询指向了 lwsmd，所以 Java 进程才会通过 Unix socket连接 lwsmd。至此，我们知道了 FD 4 的作用和来源，至于如何处理这个资源，还需要后续再探索，但至少现在我们已经明确了问题的根源。\n再看 FD 407 和 408，我们可以看到 libjvm.so 的函数调用，这清晰地表明是 Java 线程调用了 socketpair 创建了这对 Unix socket。然而，最关键的调用栈部分却显示为 [unknown]。\n这是因为 JIT (Just-In-Time) 编译器在运行时会将热点代码动态编译成机器码，并存放在匿名的内存区域。这些区域没有传统的符号表，因此像 BCC 这样的通用系统级工具在试图解析这些内存地址时，找不到任何符号信息，只能无奈地显示为 [unknown]。\n虽然这个 Unix socket (FD 407, 408) 已经被妥善处理，没有引起 CRaC 异常，但如果我们出于好奇，想知道究竟是哪段 Java 代码创建了它，应该如何做呢？\n要解决 [unknown] 的问题，我们需要一个更了解 Java 运行时的工具： async-profiler 。\n# 使用 async-profiler 追踪 Socket 创建 async-profiler 是一个为 Java 设计的低开销、高精度的性能分析工具，它直接利用 Linux 的 perf_events 子系统和 HotSpot JVM 特有的 API 来收集性能数据。\n相比传统 Java Profiler，它不仅能分析 Java 代码，还能监控到非 Java 线程（如 GC、JIT 编译器线程）的活动，并能展示原生代码（Native）和内核（Kernel）的调用栈帧，让我们看到从 Java 方法到 C/C++ 库函数再到内核系统调用的完整链路。\nBCC 等通用系统追踪工具，它们不理解 JVM 的内部工作原理，当遇到 JIT 编译器在匿名内存区域生成的机器码时，找不到对应的符号表，无法解析出方法名。而 async-profiler 是利用 HotSpot 提供的特定 API（如 AsyncGetCallTrace），它能解析 JVM 内部数据结构，从而获取 JIT 编译后代码的符号信息，将内存地址精确地映射回原始的 Java 方法名。\n可以这么说，async-profiler 综合了系统级追踪工具和传统 Java profiler 的能力，提供了对 Java 应用完整、精确性能洞察。\n# 追踪 socketpair 的创建位置 上一节，我们利用 BCC 追踪 Unix socket 相关的系统调用，这次我们用 async-profiler 完成同样的功能，并且能在调用堆栈中展示出 Java 方法名。\nasync-profiler 支持使用 Linux 的 perf_events ，所以它能够捕获 perf_events 中的 tracepoint 事件，实现和 BCC 一样的系统调用追踪。同时，async-profiler 可以作为一个 Java Agent 启动，这意味着我们可以在 Java 进程启动时就加载它，不会错过 Java 应用早期的初始化行为。\n回到之前的问题，为了定位创建文件描述符 408 和 409 的 socketpair 调用究竟源于哪段 Java 代码，我们可以在启动 Java 应用时，通过 -agentpath 参数挂载 async-profiler，并指定追踪 syscalls:sys_enter_socketpair 事件。\n修改 Java 启动命令如下：\n1 java -agentpath:/path/to/libasyncProfiler.so=start,event=syscalls:sys_enter_socketpair,file=/tmp/socketpair-trace.html ... 参数说明：\n-agentpath:/path/to/libasyncProfiler.so: 指定 async-profiler 动态链接库的位置。 start: 表示立即开始分析。 event=syscalls:sys_enter_socketpair: 指定要追踪的事件。这里我们关心 socketpair 系统调用的入口。 file=/tmp/socketpair-trace.html: 将分析结果输出为一个 HTML 格式的火焰图。 需要注意的是，一般内核默认禁止非 root 用户追踪某些敏感的内核事件（比如系统调用），所以最好使用 sudo 启动 Java 应用。\n应用正常运行并结束后，会根据配置生成 /tmp/socketpair-trace.html 文件。\n这张火焰图清晰地揭示了之前 BCC 无法解析的 [unknown] 部分，可以精准地定位到创建 socketpair 的 Java 代码源头。\n从上图可以看出，这个 socketpair 是由 Java 的 WatchService API 在其 Linux 实现中创建的。WatchService 是 Java NIO 提供的一个标准接口，用于监控文件系统的目录变化（如文件的创建、修改或删除）。调用堆栈显示，是 Apache Felix 框架的 FileInstall 组件调用了 WatchService，其目的是为了监控一个部署目录，从而实现 OSGi bundle 的热部署功能。\n我们前面观察到，这对由 socketpair 创建的文件描述符并没有在 checkpoint 过程中引发异常。原因是，CRaC 已经专门为 Java 的 WatchService 在 Linux 上的实现提供了内置支持（见 OpenJDK CRaC PR #72）。这意味着 CRaC 能够自动识别并妥善处理 WatchService 在内部使用的相关文件描述符。\n# 追踪服务端口的监听位置 同样的原理也适用于定位网络服务端口的监听位置。对于使用 NIO 框架创建的网络服务，如果要适配 CRaC，需要应用实现 Resource 接口，处理好监听端口的关闭和重新打开。如果未做适当的处理，那么在创建 checkpoint 镜像时，则会遇到类似下面的异常：\n1 2 3 4 5 6 7 8 9 10 11 jdk.internal.crac.mirror.CheckpointException Suppressed: sun.nio.ch.EPollSelectorImpl$BusySelectorException: Selector sun.nio.ch.EPollSelectorImpl@476d29ab has registered keys from channels: [sun.nio.ch.ServerSocketChannelImpl[closed]] at java.base/sun.nio.ch.EPollSelectorImpl.beforeCheckpoint(EPollSelectorImpl.java:405) at java.base/jdk.internal.crac.mirror.impl.AbstractContext.invokeBeforeCheckpoint(AbstractContext.java:43) at java.base/jdk.internal.crac.mirror.impl.AbstractContext.beforeCheckpoint(AbstractContext.java:58) at java.base/jdk.internal.crac.mirror.impl.BlockingOrderedContext.beforeCheckpoint(BlockingOrderedContext.java:64) at java.base/jdk.internal.crac.mirror.impl.AbstractContext.invokeBeforeCheckpoint(AbstractContext.java:43) at java.base/jdk.internal.crac.mirror.impl.AbstractContext.beforeCheckpoint(AbstractContext.java:58) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore1(Core.java:154) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore(Core.java:315) at java.base/jdk.internal.crac.mirror.Core.checkpointRestoreInternal(Core.java:328) 从异常信息中，我们知道 checkpoint 失败的直接原因是 EPollSelectorImpl 实例正处于“忙碌”状态。可以看出，即使 ServerSocketChannel 本身可能已经关闭，但它与 Selector 的注册关系没有被完全解除。\n但这个异常信息并没有告诉我们这个端口是在应用程序的哪个位置被创建和监听的。要妥善的处理监听端口，必须先找到创建监听端口的具体位置。\n如何做呢？我们可以追踪一个关键的系统调用：bind。\nbind 系统调用是任何网络服务端程序启动的必经之路。它的作用是将一个创建好的 socket 文件描述符与一个具体的 IP 地址和端口号绑定起来。只有执行了 bind 并成功之后，服务器才能在该端口上监听并接受客户端连接。因此，通过捕获 bind 系统调用发生时的调用堆栈，我们就能精确地定位到是哪一段 Java 代码触发了端口监听。\n使用以下命令启动 Java 应用：\n1 java -agentpath:/path/to/libasyncProfiler.so=start,event=syscalls:sys_enter_bind,file=/tmp/bind-trace.html -jar ... 应用成功启动并对外提供服务后，停止应用，然后分析生成的 /tmp/bind-trace.html 文件。\n从上面的火焰图可以看出，端口绑定操作是由 Grizzly NIO 框架执行的，该框架作为 GlassFish 应用服务器的底层网络引擎。整个过程在服务器启动阶段，由 HK2 依赖注入框架自动触发，以初始化和启动核心的网络服务。\n通过分析火焰图，我们可以得出结论：在 GrizzlyListener 处实现 Resource 接口可能比较合理，可以在这里管理监听端口的关闭和重启。\n# 总结 本文旨在解决为 Java 应用适配 CRaC 时，如何精确定位网络资源（如 Unix socket 和服务监听端口）创建位置的难题。\n首先介绍了使用系统级追踪工具 BCC 的方法。虽然 BCC 能通过追踪 socket 和 socketpair 等系统调用，捕获到原生代码的调用堆栈，但它无法解析 JIT 编译的 Java 代码，导致关键信息显示为 [unknown]。\n为解决此问题，文章介绍了更为强大的 async-profiler。它结合了底层 perf_events 追踪能力，以及对 JVM 内部的理解，能够完美解析 JIT 代码的符号。通过追踪 sys_enter_socketpair 和 sys_enter_bind tracepoint 的实际案例，展示了如何利用 async-profiler 生成的火焰图，将系统调用反向追溯到具体的 Java 代码。\n","date":"2025-09-14T14:47:32+08:00","permalink":"https://mazhen.tech/p/%E8%BF%BD%E8%B8%AA%E5%AE%9A%E4%BD%8D-java-%E8%BF%9B%E7%A8%8B%E7%9A%84-socket-%E5%88%9B%E5%BB%BA/","title":"追踪定位 Java 进程的 Socket 创建"},{"content":"在使用 CRaC 创建 checkpoint 镜像时，需要 Java 应用能够恰当处理它持有的外部资源，例如打开的日志文件，监听的服务端口，对外创建的数据库连接池等。\n对于 Java 应用打开的文件，可以通过定义文件描述符策略 让 CRaC 自动处理。对于监听端口或连接池，一般建议应用实现 CRaC 的 Resource 接口，在 checkpoint 前关闭资源，在 restore 后重新打开。\n但有一些资源，不是 Java 应用直接打开持有的，而是底层的 JVM 或依赖的 C 库打开的，这类资源要追踪定位是谁创建的比较困难。\n# 初步排查过程 最近我就碰到了这样一个例子。在对 Java 应用创建 checkpoint 时，报如下异常：\n1 2 3 4 5 6 7 An exception during a checkpoint operation: jdk.internal.crac.mirror.CheckpointException Suppressed: jdk.internal.crac.mirror.impl.CheckpointOpenSocketException: FD fd=6 type=socket path=socket:[2885875383],port=29295 at java.base/jdk.internal.crac.mirror.Core.translateJVMExceptions(Core.java:116) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore1(Core.java:189) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore(Core.java:315) at java.base/jdk.internal.crac.mirror.Core.checkpointRestoreInternal(Core.java:328) 表明 Java 应用持有文件描述 (fd) 6，类型是 socket，inode 为 2885875383.\n于是查看该 Java 进程持有的所有 socket 资源：\n1 2 3 ll /proc/2444401/fd | grep socket ... lrwx------ 1 root root 64 Sep 3 11:19 6 -\u0026gt; socket:[2885875383] 果然 Java 进程持有的 fd 6，类型是 socket，inode 为 2885875383。进一步使用 lsof 命令查看这个 socket 资源的详细信息：\n1 2 lsof -p 2444401 | grep 2885875383 exe 2444401 root 6u unix 0x00000000b8935f1c 0t0 2885875383 type=STREAM 第五列为 unix，表示该文件描述符对应的资源是 unix domain socket。\n想进一步调研这个 domain socket 的来源和作用，它的一端是 Java 进程持有，那么另一端是谁持有呢？\n1 2 3 4 5 6 7 # -U ：筛选 Unix 域套接字（Unix Domain Sockets）。 # -a：逻辑 “与”（AND） 操作，将前面的条件（-p 和 -U）组合，表示必须同时满足 # +E：显示套接字的 端点信息（Endpoints）。 lsof -p 2444401 -U -a +E COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME exe 2444401 root 6u unix 0x00000000b8935f1c 0t0 2885875383 type=STREAM exe 2444401 root 57u unix 0x00000000a4202b14 0t0 2885883519 type=STREAM 输出中并没有发现对端的进程信息。\n一般情况下，加了 +E 参数，输出中会通过 -\u0026gt;INO= 指示出对端进程信息，例如：\n1 2 3 4 5 6 $ sudo lsof -p 1167 -U -a +E ... container 1131 root 8u unix 0xffff9d34cdf28c00 0t0 14525 /run/containerd/containerd.sock type=STREAM -\u0026gt;INO=21711 1167,dockerd,8u (CONNECTED) ... dockerd 1167 root 8u unix 0xffff9d34cdf29400 0t0 21711 type=STREAM -\u0026gt;INO=14525 1131,container,8u (CONNECTED) ... 可以看出，在 containerd 这一端，它使用文件描述符 8（标记为 8u，表示可读可写）来管理 domain socket，这个套接字被绑定到文件系统路径 /run/containerd/containerd.sock 上。行末的 -\u0026gt;INO=21711 1167,dockerd,8u (CONNECTED) 表明，该套接字的对端是进程 ID 为 1167 的 dockerd。dockerd 同样使用文件描述符 8 来管理这个连接，且连接状态为已建立（CONNECTED）。\n再查看 dockerd 进程这端的情况，它也使用文件描述符 8（8u）来维护这个通信通道。与 containerd不同，dockerd 端的套接字没有绑定任何文件系统路径，这是一个匿名套接字。同样，行末的 -\u0026gt;INO=14525 1131,container,8u (CONNECTED) 验证了连接的对称性，对端是进程 ID 为 1131 的 containerd，连接状态同样是已建立。\n而我们上面查看 Java 进程 2444401 持有的 domain socket，并么有发现对端信息。我们现在只知道 Java 进程通过文件描述符 6 持有一个 domain socket，但是 domain socket 的作用是什么，为什么会被创建，完全不知道。如何进一步排查呢？\n# strace 首先想到的是通过 strace 追踪系统调用，看能不能发现什么有用的信息。在 Java 进程的启动命令前加入 strace：\n1 2 3 4 # -f 跟踪子进程（fork/vfork/clone 创建的进程） # -yy 增强网络信息显示，打印 socket 的完整协议信息 # -e trace=desc,network 过滤系统调用类型，跟踪文件描述符和网络相关的调用 strace -f -yy -e trace=desc,network -o /tmp/startup_trace.log java ... 最终在 startup_trace.log 文件中，没有找到文件描述 6 相关的信息，这条路没走通。\n# 安装 BCC 然后想到，可以使用 eBPF 追踪系统中，所有 domain socket 的创建信息。先安装 BCC，当前内核版本较低，从源码编译安装 BCC。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # 安装编译依赖 dnf groupinstall -y \u0026#34;Development Tools\u0026#34; dnf install -y cmake clang llvm python3-devel elfutils-libelf-devel # 安装 LLVM 和 Clang 开发包 dnf install -y llvm-devel clang-devel # 查找 LLVM 安装路径 find /usr -name \u0026#34;LLVMConfig.cmake\u0026#34; 2\u0026gt;/dev/null /usr/lib64/cmake/llvm/LLVMConfig.cmake which llvm-config /usr/bin/llvm-config # 克隆 BCC 源码 git clone https://github.com/iovisor/bcc.git cd bcc # 创建构建目录 mkdir build \u0026amp;\u0026amp; cd build # 配置编译，指定 Python 路径 # 使用找到的 LLVM 路径配置 CMake，`-DLLVM_DIR` 参数应该指向包含 `LLVMConfig.cmake` 文件的目录 cmake .. -DPYTHON_CMD=python3 -DLLVM_DIR=/usr/lib64/cmake/llvm # 编译（使用多核加速） make -j$(nproc) # 安装 make install ldconfig 安装完成后进行验证：\n1 2 python3 -c \u0026#34;from bcc import BPF; print(\u0026#39;BCC 安装成功！\u0026#39;)\u0026#34; BCC 安装成功！ # BCC 脚本 指挥 AI 写一个追踪 domain socket 创建的脚本 trace_unix_socket_creation.py：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 #!/usr/bin/env python3 import re from bcc import BPF # AF_UNIX 的值为 1 AF_UNIX = 1 prog = r\u0026#34;\u0026#34;\u0026#34; #include \u0026lt;uapi/linux/ptrace.h\u0026gt; #include \u0026lt;linux/sched.h\u0026gt; #include \u0026lt;net/sock.h\u0026gt; // For AF_UNIX struct data_t { u32 tid; u32 ppid; int stack_id; }; BPF_HASH(infotmp, u32, struct data_t); BPF_STACK_TRACE(stack_traces, 1024); // Hook for socket() syscall int trace_socket_entry(struct pt_regs *ctx, int domain, int type, int protocol) { if (domain != AF_UNIX) { return 0; } u32 tid = bpf_get_current_pid_tgid() \u0026gt;\u0026gt; 32; struct task_struct *task = (struct task_struct *)bpf_get_current_task(); u32 ppid = 0; bpf_probe_read_kernel(\u0026amp;ppid, sizeof(ppid), \u0026amp;task-\u0026gt;real_parent-\u0026gt;tgid); struct data_t data = {}; data.tid = tid; data.ppid = ppid; data.stack_id = stack_traces.get_stackid(ctx, BPF_F_USER_STACK); infotmp.update(\u0026amp;tid, \u0026amp;data); return 0; } int trace_socket_return(struct pt_regs *ctx) { u32 tid = bpf_get_current_pid_tgid() \u0026gt;\u0026gt; 32; struct data_t *datap = infotmp.lookup(\u0026amp;tid); if (!datap) return 0; int retval = PT_REGS_RC(ctx); if (retval \u0026gt;= 0) { bpf_trace_printk(\u0026#34;socket(AF_UNIX) call: ppid=%d tid=%d new_fd=%d\\\\n\u0026#34;, datap-\u0026gt;ppid, datap-\u0026gt;tid, retval); bpf_trace_printk(\u0026#34;stackid=%d\\\\n\u0026#34;, datap-\u0026gt;stack_id); } infotmp.delete(\u0026amp;tid); return 0; } \u0026#34;\u0026#34;\u0026#34; b = BPF(text=prog) b.attach_kprobe(event=\u0026#34;__sys_socket\u0026#34;, fn_name=\u0026#34;trace_socket_entry\u0026#34;) b.attach_kretprobe(event=\u0026#34;__sys_socket\u0026#34;, fn_name=\u0026#34;trace_socket_return\u0026#34;) print(\u0026#34;Tracing socket(AF_UNIX, ...) calls... Ctrl-C to stop.\\n\u0026#34;) tid_to_tgid_cache = {} def get_tgid(tid): if tid in tid_to_tgid_cache: return tid_to_tgid_cache[tid] # --- 关键修改：处理竞态条件 --- try: with open(f\u0026#34;/proc/{tid}/status\u0026#34;, \u0026#34;r\u0026#34;) as f: for line in f: if line.startswith(\u0026#34;Tgid:\u0026#34;): tgid = int(line.split()[1]) tid_to_tgid_cache[tid] = tgid return tgid # 捕获 FileNotFoundError 或 ProcessLookupError except (FileNotFoundError, ProcessLookupError): # 线程已退出，返回 -1 表示无效 return -1 return -1 while True: try: (task, tid, cpu, flags, ts, msg) = b.trace_fields() msg = msg.decode(\u0026#39;utf-8\u0026#39;, errors=\u0026#39;replace\u0026#39;).strip() print(msg) if \u0026#34;stackid=\u0026#34; in msg: m = re.search(r\u0026#39;stackid=(\\d+)\u0026#39;, msg) if m: sid = int(m.group(1)) tgid = get_tgid(tid) # 只有在成功获取 TGID 时才尝试解析符号 if tgid != -1: for addr in b[\u0026#34;stack_traces\u0026#34;].walk(sid): print(\u0026#34; %s\u0026#34; % b.sym(addr, tgid, show_module=True, show_offset=True)) else: print(f\u0026#34; [Could not get symbols for exited tid {tid}]\u0026#34;) except KeyboardInterrupt: exit() AI 还贴心的给出了脚本执行的流程图： # 最终定位 先停止 Java 进程，然后运行该脚本，在启动 Java 进程。\n1 ./trace_unix_socket_creation.py \u0026gt; socket_trace 在 socket_trace 中根据 Java 进程的 PID 搜索，找到了 文件描述 6 的创建调用堆栈：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 socket(AF_UNIX) call: ppid=1527014 tid=1527479 new_fd=6\\n stackid=523\\n b\u0026#39;socket+0x8 [libc-2.28.so]\u0026#39; b\u0026#39;[unknown] [libnss_sss.so.2]\u0026#39; b\u0026#39;_nss_sss_getpwuid_r+0xe4 [libnss_sss.so.2]\u0026#39; b\u0026#39;getpwuid_r+0x14c [libc-2.28.so]\u0026#39; b\u0026#39;get_user_name(unsigned int)+0x5c [libjvm.so]\u0026#39; b\u0026#39;PerfMemory::create_memory_region(unsigned long)+0x94 [libjvm.so]\u0026#39; b\u0026#39;perfMemory_init()+0xa8 [libjvm.so]\u0026#39; b\u0026#39;vm_init_globals()+0x24 [libjvm.so]\u0026#39; b\u0026#39;Threads::create_vm(JavaVMInitArgs*, bool*)+0x234 [libjvm.so]\u0026#39; b\u0026#39;JNI_CreateJavaVM+0x80 [libjvm.so]\u0026#39; b\u0026#39;JavaMain+0x80 [libjli.so]\u0026#39; b\u0026#39;ThreadJavaMain+0xc [libjli.so]\u0026#39; b\u0026#39;[unknown] [libpthread-2.28.so]\u0026#39; b\u0026#39;[unknown] [libc-2.28.so]\u0026#39; 调用堆栈记录了文件描述符 6 被创建的全过程，让我们从下往上，一步步还原：\nJNI_CreateJavaVM: Java 虚拟机的诞生。当我们执行 java 命令时，libjli.so (Java Launcher Interface) 库会加载核心的 JVM 库 (libjvm.so)，并调用 JNI_CreateJavaVM 这个函数来真正创建和初始化一个 JVM 实例。\nThreads::create_vm -\u0026gt; vm_init_globals: 在 JVM 内部，create_vm 函数开始搭建虚拟机运行所需的基础环境，其中一步就是初始化各种全局组件 (vm_init_globals)。\nperfMemory_init: perfMemory 子系统被初始化。这个子系统是 JVM 性能监控的关键。它负责创建一块特殊的内存区域，用于存放 JVM 内部的性能计数器，例如 JIT 编译统计、垃圾回收次数和耗时等。我们熟知的 jps、jstat、jcmd 等命令行工具，正是通过读取这块内存区域来获取 JVM 实时运行数据的。\nPerfMemory::create_memory_region -\u0026gt; get_user_name: PerfMemory 创建的这块内存区域，实际上是以内存映射文件（memory-mapped file）的形式存在的，这些文件通常位于 /tmp/hsperfdata_\u0026lt;username\u0026gt;/ 目录下（hsperfdata 是 HotSpot Performance Data 的缩写）。为了构建这个目录路径，JVM 需要知道当前运行它的用户名是什么，于是它调用了内部函数 get_user_name。\ngetpwuid_r -\u0026gt; _nss_sss_getpwuid_r: get_user_name 函数通过调用标准的 glibc 函数 getpwuid_r，根据当前用户的 UID (User ID) 来查询其用户名。在现代 Linux 系统中，这个查询过程并非简单地读取 /etc/passwd 文件。它是由 NSS (Name Service Switch) 机制来管理的。NSS 会根据 /etc/nsswitch.conf 的配置，将这类查询请求转发给相应的处理模块。在我们的例子中，请求被转发给了 SSSD (System Security Services Daemon) 的客户端库 libnss_sss.so.2。\n[unknown] [libnss_sss.so.2] -\u0026gt; socket: libnss_sss.so.2 库只是一个“中间人”，它需要和后台运行的 sssd 守护进程进行通信，才能真正完成用户信息的查询。它选择的通信方式正是 Unix Domain Socket。于是，它调用了 socket() 系统调用来创建一个套接字，准备连接 sssd 服务。\nnew_fd=6: 在这个精确的时间点，我们的 Java 进程中最小的可用文件描述符编号恰好是 6。因此，内核将 6 这个编号分配给了这个新创建的套接字。\n所以，文件描述符 6 并非由我们的 Java 应用代码直接创建，而是在 JVM 启动的极早期，由其内部的性能监控子系统为了获取当前用户名，间接触发了系统的 NSS 模块，进而由 SSSD 客户端库创建的、用于和 SSSD 后台服务通信的 Unix Domain Socket。\n# SSSD 是什么 从堆栈分析可知，这个 Domain Socket 是为了连接 SSSD 服务。SSSD (System Security Services Daemon) 是现代 Linux 系统中用于集中管理身份认证、授权和用户信息查询的核心服务。它充当了一个缓存和转发层，可以对接多种后端服务，如本地文件、LDAP、Kerberos 或 Active Directory。\nSSSD 的作用是响应来自 libnss_sss 库的请求，查询并返回当前 UID 对应的用户名。libc 库在第一次查询后，通常会保持这个套接字连接打开，以便后续的查询可以复用这个连接，避免重复创建和连接的开销。这解释了为什么在 Java 进程的整个生命周期中，我们都能看到这个 FD 6 的存在。\n# 查看 SSSD 进程及其配置 我们可以使用 systemctl 来确认 sssd 服务的运行状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 检查 sssd 服务的总体状态 systemctl status sssd ● sssd.service - System Security Services Daemon Loaded: loaded (/usr/lib/systemd/system/sssd.service; enabled; vendor preset: disabled) Active: active (running) since Tue 2025-08-05 11:03:10 CST; 4 weeks 1 days ago Main PID: 916 (sssd) Tasks: 4 Memory: 45.8M CGroup: /system.slice/sssd.service ├─ 916 /usr/sbin/sssd -i --logger=files ├─1027 /usr/libexec/sssd/sssd_be --domain implicit_files --uid 0 --gid 0 --logger=files ├─1054 /usr/libexec/sssd/sssd_nss --uid 0 --gid 0 --logger=files └─1055 /usr/libexec/sssd/sssd_pam --uid 0 --gid 0 --logger=files 查看 /etc/nsswitch.conf 文件，是否在 passwd、group、shadow 等关键数据库的查询源列表中包含了 sss。\n1 2 3 4 5 6 cat /etc/nsswitch.conf ... passwd: sss files systemd shadow: files sss group: sss files systemd ... passwd 这一行表明，当需要查询用户信息时，首先就去问 SSSD 服务 (sss)，然后再查找本地的 /etc/passwd 文件 (files)，如果前两者都找不到，还会查询由 systemd-logind 管理的动态用户。\n这个配置清晰地表明，SSSD 在该系统的用户和组信息查询中处于最高优先级。这解释了，在 JVM 在启动时，为了获取当前用户名，就会立即触发与 SSSD 的通信。\n# 总结 通过 lsof 初步定位问题，再借助强大的 eBPF 工具 BCC 深入追踪系统调用，成功地揭开了 JVM 底层的文件描述符的秘密。它并非由应用代码创建，而是 JVM 初始化性能监控模块时，通过 NSS 机制与系统核心的 SSSD 服务建立的通信通道。\n","date":"2025-09-03T10:37:32+08:00","permalink":"https://mazhen.tech/p/%E8%BF%BD%E8%B8%AA-java-%E8%BF%9B%E7%A8%8B%E5%88%9B%E5%BB%BA%E7%9A%84-domain-socket/","title":"追踪 Java 进程创建的 domain socket"},{"content":"\n提到 Java 的垃圾回收（GC），很多开发者脑海中会立刻浮现出一个词：“Stop-The-World”，也就是我们常说的 STW。它意味着在那一刻，所有的应用线程都会被冻结，等待 GC 完成它的工作。这是 Java 应用的性能“痛点”，我们总希望它越短越好。\n我们知道 STW 会暂停应用，但你是否想过一个更深层次的问题：JVM 究竟是如何做到让成千上万个正在高速运行的线程，能在同一时刻“安全地”停下来呢？\n更进一步，是只有 GC 才会引发这种全局暂停吗？如果 Java 应用出现了一次意料之外的卡顿，但 GC 日志却显示一切正常，那“元凶”又会是谁呢？\n这些问题的答案，都指向了 JVM 运行时中一个至关重要，但又很少有人了解的底层机制：Safepoint。\n实际上，垃圾回收只是使用 Safepoint 机制的众多“客户”之一。Safepoint 本身是 JVM 的一个更为底层的通用设施，是所有需要进行全局性操作（比如代码反优化、线程转储等）时，都必须依赖的坚实地基。\n接下来，就让我们一起揭开 Safepoint 的神秘面纱，看看它是什么、为什么需要它，以及它又是如何在底层运作，并悄无声息地影响着我们的应用程序。\n# 什么是 Safepoint 我们知道，JVM 有时需要执行一些影响全局的操作，最典型的就是垃圾回收（Garbage Collection）。要安全地进行这些操作，一个基本前提就是让所有正在忙碌的线程都停下来，即“Stop-The-World”。\n那么，JVM 可以在任意时刻，暂停一个正在运行的线程吗？答案是：不能。\n原因在于一个核心概念：状态的可解析性（State Parsability）。\n# JIT 优化带来的问题 为了追求极致的性能，JIT（Just-In-Time） 会将热点代码编译成本地机器码，并进行各种优化，比如：\n寄存器分配：为了读写更快，一个 Java 的对象引用，可能不是在线程运行栈的内存中，而是被临时放到了 CPU 寄存器中。 指令重排：为了充分利用 CPU 流水线，实际执行的机器指令顺序可能和我们代码的逻辑顺序并不一致。 方法内联：一些方法可能被内联（inlining），导致其独立的栈帧结构直接消失。 这些优化极大地提升了运行速度，但也带来了一个“副作用”：如果在任意一个时刻暂停线程，JVM 会发现自己面对一个“混乱”的现场，它可能无法分清 CPU 寄存器里的某个值到底是一个整数，还是一个对象引用。这种状态，我们称之为不可解析的（unparsable）。\n如果在这种“不确定”的状态下强行进行垃圾回收，后果是灾难性的。JVM 可能会把一个存活的对象当成垃圾回收掉，或者在移动对象后没能更新所有指向它的引用（包括那些藏在寄存器里的引用），最终导致程序崩溃。\n# OopMap：JIT 为 JVM 运行时生成的“地图” 为了解决这个问题，JIT 编译器和 JVM 达成了一个的协议：JIT 在编译代码时，会为某些特定的指令位置额外生成一份元数据，这份元数据就像一张地图，它被称为 OopMap (Object Pointer Map)。\n这张“地图“精确地记录了：“如果程序恰好在这个指令位置停下，那么它的栈和寄存器应该这样解读：RAX寄存器里的是一个对象引用，栈上-8偏移量处是一个整数，-16偏移量处又是一个对象引用……”\n有了这份地图，垃圾回收器就能准确地找出当前线程中所有的根对象（GC Roots），从而安全地进行后续的存活对象分析。\n为了解决这个问题，JIT 编译器为 JVM 准备好了一份地图（OopMap - Object Pointer Map），这份地图精确地记录了在当前这个点，哪些寄存器、哪些栈位置存放的是对象引用。有了这张地图，垃圾回收器（GC）就能准确地找出所有的根对象（GC Roots），从而安全地进行存活对象分析。\n但是，为每一条机器指令都生成这样一张“地图”的成本太高了，会消耗大量内存并拖慢编译过程。因此，JIT 只会在某些特定的、易于管理的位置生成和保存“地图”。\n这些带有正确“地图”信息，能够让 JVM 安全解析线程状态的特定位置，就是安全点（Safepoint）。\n# Safepoint：一个协作式的“约定地点” 现在我们可以给 安全点（Safepoint） 一个更清晰的定义：\nSafepoint 是代码中预先定义好的特定位置，当一个线程执行到这些位置时，它的底层执行状态（寄存器、栈）是确定的，能够被 JVM 准确的解析，并可以被 JVM 安全地检查和修改。\n因此，当 JVM 需要暂停所有线程时（例如进行 GC），它不会简单粗暴的直接中断它们的执行，而是会设置一个全局标志，然后等待所有线程主动运行到离它们最近的一个 Safepoint，在那里检查到这个标志后，自行决定暂停。\n这就是 Safepoint 机制的核心：它采用 协作式（Cooperative） 的方式工作。JVM 无法强迫线程停下，只能等待线程“自觉”地走到一个双方约定好的、可以安全“交接”的地方。这种机制，保证了所有全局操作的绝对安全和正确。\n# Safepoint 的应用场景 提到“Stop-The-World”，大部分开发者首先想到的就是垃圾回收。GC 的许多关键阶段，都需要在一个全局的 Safepoint 中进行，可以安全的进行对象的标记和清理。\n但是，如果我们将 Safepoint 仅仅与 GC 划上等号，那就大大低估了它的重要性。\n实际上，Safepoint 是 JVM 的一个基础性平台机制。任何需要安全地检查或修改所有线程共享数据（如堆、类元数据、线程栈等）的重量级操作，都必须依赖它来创造一个安全确定的执行环境。GC 只是这个平台上最“知名”的客户而已。\n除了 GC，还有一些常见的 VM 操作，同样需要在全局 Safepoint 中执行：\n代码反优化 (Deoptimization)：当 JIT 编译后的代码因为某些原因（如分支预测失败、加载了未被预期的子类等）需要回退到解释执行状态时，JVM 需要暂停线程，修改其栈帧，这个过程必须在 Safepoint 中进行。\n类的重定义与热部署 (Class Redefinition)：像我们在 IDE 中热更新代码，或者使用某些动态诊断工具时，JVM 需要替换掉已加载的类。这个操作会涉及修改类的元数据和可能存在的实例，显然也需要所有线程暂停。\n线程堆栈转储 (Thread Dump)：当我们使用 jstack 或其他工具来获取所有线程的堆栈快照时，为了得到一个准确、一致的视图，JVM 会触发一次 Safepoint。\n堆转储 (Heap Dump)：与线程转储类似，生成堆快照也需要在一个静态的内存视图下进行。\n偏向锁批量撤销 (Bulk Revocation of Biased Locks)：当某个类的偏向锁被大量撤销时，JVM 需要遍历所有线程栈和堆，找到并修改相关的锁记录。\n其他 JVMTI 操作：许多通过 JVMTI（JVM Tool Interface）提供的调试和监控功能，例如强制修改变量值等，也依赖 Safepoint 来保证操作的原子性和安全性。\n因此，下一次当你的应用出现卡顿，而 GC 日志却“清白无辜”时，不妨思考一下，是否是上述这些原因之一，触发了一次意料之外的 Safepoint 停顿。\n# Safepoint 的工作机制 我们已经知道，JVM 无法在任意时刻强行暂停线程，而是需要线程的“协作”。那么，所有线程的“集体暂停”，是如何发生的呢？\n# 协作式轮询机制 整个机制的核心是一种协作式轮询（Cooperative Polling）。它的工作流程非常直观：\n当 JVM 需要所有线程进入 Safepoint 时，它会设置一个全局的“请暂停”标志。 每个正在运行的应用线程，在执行自己代码的同时，会周期性地检查这个全局标志。 一旦某个线程发现这个全局标志被设置，它立即在这个安全的位置停下，并自行进入阻塞状态。 JVM 会一直等待，直到所有的应用线程都到达了各自的 Safepoint 并进入阻塞状态。 此时，JVM 可以安全地进行它的全局操作（如垃圾回收、代码去优化等），因为它知道所有线程都处于可解析的稳定状态。 全局操作完成后，JVM 会清除那个全局的“请暂停”标志，并唤醒所有正在阻塞的线程。 线程们被唤醒后，它们从之前暂停的 Safepoint 处，继续执行自己的代码。 # 轮询点的位置 一个很自然的问题是：线程在哪里进行轮询检查呢？\n如果检查得太频繁，比如每条指令后都检查，那无疑会带来巨大的性能开销。如果检查得太少，又可能导致线程长时间无法响应暂停请求。因此，JIT 编译器采取了一种折衷策略，在一些关键位置插入轮询指令，这些位置被称为轮询点（Poll Locations）：\n方法的返回处：当一个方法即将返回给调用者之前。 循环的回边（Loop Back-edges）：在一个循环即将进入下一次迭代之前。 值得注意的是，为了避免因编译器优化（比如将多个小循环合并成一个大循环）而导致线程长时间“失联”，现代的 JDK 倾向于在所有循环的回边都插入轮询点，这大大降低了线程无法及时响应 Safepoint 的风险。\n如果你对汇编代码感兴趣，可以参考我以前的文章使用 PrintAssembly 查看 JIT 编译后的汇编代码，为 JDK 安装配置好准备 hsdis 插件，然后使用类似下面的命令行参数运行应用，查看你指定代码的 JIT 反汇编：\n1 2 3 4 java -XX:+UnlockDiagnosticVMOptions \\ -XX:CompileCommand=\u0026#34;print,SafepointTest::*\u0026#34; \\ -XX:PrintAssemblyOptions=intel \\ SafepointTest 随着程序的运行，你可以看到热点代码 C1、C2 编译结果的反汇编输出。输出的反汇编代码包含了丰富的注释，其中的注释 {poll} 和 {poll_return} 就表明是安全点轮询 (Safepoint Poll)。\n1 2 3 4 5 6 7 8 9 ... 0x00007d6f183a075b: test\tdword ptr [r10], eax; {poll} 0x00007d6f183a075e: cmp\trbx, 0xf4240 0x00007d6f183a0765: jl\t0x7d6f183a0750 0x00007d6f183a0767: add\trsp, 0x10 0x00007d6f183a076b: pop\trbp 0x00007d6f183a076c: cmp\trsp, qword ptr [r15 + 0x448] ; {poll_return} ... # 核心性能指标：到达安全点的时间 (Time-To-Safepoint, TTSP) 现在，我们来到了 Safepoint 机制中对性能影响最大，也很容易被忽视的一环。\n想象一下，JVM 设置了全局的“请暂停”标志，此刻：\n线程 A 可能正好执行完一次循环，它立刻看到了标志，并马上在 Safepoint 处暂停了。 线程 B 可能正在执行一段长计算的中间部分，它需要再运行多条指令才能到达下一个循环的末尾，也就是下一个轮询点。 JVM 的全局操作（如 GC）必须等到所有线程都暂停后才能开始。这意味着，即使线程 A 已经早早地“就位”，它也必须原地等待，直到最慢的线程 B 姗姗来迟。\n从 JVM 发起暂停请求，到最后一个线程终于到达 Safepoint 并暂停，这段等待时间，被称为 “到达安全点的时间”（Time-To-Safepoint，简称 TTSP）。\n总停顿时间 ≈ 到达安全点的时间 (TTSP) + 安全点内VM操作的耗时\n这个公式表明，我们通过 GC 日志看到的停顿时间，仅仅是公式的后半部分。而同样会冻结应用的 TTSP，却隐藏在幕后。在某些极端情况下，一个行为异常的线程导致的超长 TTSP，甚至可能超过 GC 本身的时间，成为应用卡顿的真正元凶。后面我会介绍使用 -Xlog:safepoint 参数，查看每次暂停的 TTSP。\n# OpenJDK 底层实现揭秘 理解了 Safepoint 的“是什么”和“为什么”，我们现在深入其内部，看看在 OpenJDK 中，这一切是如何通过代码实现的。\n# VMThread 与 VM_Operation 在 JVM 内部，存在一个特殊的系统级线程，名为 VMThread。它不执行任何 Java 应用代码，其核心职责是管理和执行需要全局协调的内部任务。它是一个单一的、高优先级的后台线程，核心使命是处理那些需要全局一致性的、重量级的内部任务。这些任务如果由普通应用线程来执行，可能会引发复杂的竞态条件和死锁问题。因此，将它们集中交由一个专职的 VMThread 来处理，是 JVM 内部的一种设计模式。\n当 JVM 的某个子系统（比如 GC、JIT 编译器、代码热更新模块等）需要执行一个全局操作时，它不会直接动手，而是把这个操作封装成一个对象，这个对象就是 VM_Operation。VM_Operation 是一个任务的抽象，它清晰地定义了需要执行什么操作（doit() 方法）以及这个操作是否需要在 Safepoint 中进行（evaluate_at_safepoint() 方法）。\n这种设计的好处在于解耦：任务的“请求方”和“执行方”被分开了。请求方只需创建一个 VM_Operation 并将其提交到一个共享的任务队列 VMOperationQueue 中，而无需关心复杂的线程同步和暂停逻辑。\n我们可以在 vmOperation.hpp 中，查看目前 JVM 支持的所有 VM_Operation 列表：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 // The following classes are used for operations // initiated by a Java thread but that must // take place in the VMThread. #define VM_OP_ENUM(type) VMOp_##type, // Note: When new VM_XXX comes up, add \u0026#39;XXX\u0026#39; to the template table. #define VM_OPS_DO(template) \\ template(Halt) \\ template(SafepointALot) \\ template(ThreadDump) \\ template(PrintThreads) \\ ... template(G1CollectForAllocation) \\ template(G1CollectFull) \\ template(G1PauseRemark) \\ template(G1PauseCleanup) \\ template(G1TryInitiateConcMark) \\ ... template(ZMarkEndOld) \\ template(ZMarkEndYoung) \\ template(ZMarkFlushOperation) \\ template(ZMarkStartYoung) \\ template(ZMarkStartYoungAndOld) \\ template(ZRelocateStartOld) \\ template(ZRelocateStartYoung) \\ ... template(ShenandoahFullGC) \\ template(ShenandoahInitMark) \\ template(ShenandoahFinalMarkStartEvac) \\ template(ShenandoahInitUpdateRefs) \\ template(ShenandoahFinalUpdateRefs) \\ template(ShenandoahFinalRoots) \\ template(ShenandoahDegeneratedGC) \\ ... 从上面的列表中可以看到，各 GC 算法需要在 Safepoint 交由 VMThread 执行的 VM 操作。当然也包含一些非 GC 的其他 VM 操作，如 ThreadDump。\n# Safepoint 整体流程 有了 VMThread 和 VM_Operation 这两个基本构件，Safepoint 的整体执行流程就变得非常清晰了。它是一个标准化的、由 VMThread 主导的调度过程。\n提交任务\n当一个需要 Safepoint 的 VM_Operation 被创建并提交到 VMOperationQueue 后，它就进入了等待处理的状态。\n调度与发起\nVMThread 在其主循环中，从队列里取出这个 VM_Operation。在确认该任务需要 Safepoint 后，它便调用 SafepointSynchronize::begin()，正式启动了将所有线程带入 Safepoint 的同步过程。\n武装与轮询\nbegin() 方法会设置一个全局的 Safepoint 状态标志，并遍历所有存活的 JavaThread 实例，修改它们各自线程本地的轮询状态。这个过程被称为 “武装”（Arming）。此后，每个正在运行的线程在执行到代码中的 轮询点（Poll Location） 时，就会检测到这个状态改变。\n阻塞与同步\n一旦检测到暂停请求，线程会主动执行 SafepointSynchronize::block() 方法，使自身进入阻塞状态，并在一个全局的屏障（WaitBarrier）上等待。VMThread 则会持续检查所有线程的状态，直到确认所有线程都已暂停。这个等待所有线程完成同步的耗时，就是 TTSP（Time-To-Safepoint）。\n执行操作\n一旦确认所有线程都已同步，JVM 就进入了一个全局静止状态。此时，VMThread 才安全地调用 VM_Operation 对象的核心 doit() 方法，执行实际的任务逻辑（例如，GC 扫描）。\n恢复执行\n操作完成后，VMThread 调用 SafepointSynchronize::end() 方法。该方法负责重置全局状态，解除对各线程轮询状态的“武装”，并最终通过 WaitBarrier 唤醒所有被阻塞的 Java 线程，使其恢复正常执行。\n这个流程确保了所有全局操作都在一个绝对安全、一致的环境中进行，是 JVM 稳定运行的基石。在接下来的部分，我们将深入探讨第 3 步中的“轮询”是如何在不同执行模式下被高效实现的。\n# JIT 代码的轮询：利用页保护（Page Protection） 对于经过 JIT 编译的高度优化的代码，性能是第一要务。如果在循环中频繁地使用 if-else 条件分支来检查 Safepoint 标志，会对 CPU 的指令流水线造成干扰，带来不可忽视的性能损失。\n为了实现一个近乎“零开销”的轮询，OpenJDK 的工程师们采用了一个极为巧妙的方案，它利用了现代操作系统和硬件提供的一项功能：页保护（Page Protection）。\n# 什么是页保护机制 页保护是现代操作系统与 CPU 硬件协同提供的一项内存管理基础功能。操作系统不直接管理单个字节，而是以 页（Page） 为单位（通常是 4KB）来管理内存。\n这个机制的核心是虚拟内存系统。程序中使用的地址并非真实的物理内存地址，而是虚拟地址。操作系统通过 页表（Page Table） 来维护虚拟地址到物理地址的映射关系。当程序访问一个虚拟地址时，CPU 内的 内存管理单元（MMU） 会查询页表，完成地址转换。\n关键在于，页表的每一项不仅记录了映射关系，还附带了一组保护标志（Protection Flags），用以规定该内存页的访问权限。常见的权限包括：\nPROT_READ：允许读取。 PROT_WRITE：允许写入。 PROT_EXEC：允许执行代码。 PROT_NONE：不允许任何形式的访问。 当程序访问一个内存页时，MMU 会首先检查其权限。如果访问合法，操作顺利完成。但如果程序试图读、写或执行一个被设置为 PROT_NONE 的页，MMU 会立即识别出这是非法操作，并触发一个硬件层面的异常。这个异常会使 CPU 陷入（trap） 操作系统内核。内核的异常处理器随即接管，分析异常原因，并向引发问题的进程发送一个特定的信号（Signal），在 Linux 上通常是 SIGSEGV（段错误）。\n简单来说，页保护机制就像给每个内存页上了一把锁。如果你有正确的权限，就能进入；如果没有，就会引发硬件异常，由操作系统内核接管，并向你发送SIGSEGV 信号。\n# OpenJDK 如何利用这一机制 Safepoint 轮询机制面临的挑战是：检查操作必须在性能关键路径上（例如循环中），所以它必须极度高效。如果每次都用 if (should_stop) { ... } 这样的条件分支来检查，会对 CPU 的指令流水线造成干扰，影响性能。\nOpenJDK 的实现另辟蹊径，将软件层面的轮询检查转换为了一个硬件层面的事件。我们再来看 OpenJDK 的实现：\n初始化：创建特殊的内存页\n在 JVM 启动时，它会通过系统调用分配一个特殊的内存页，我们称之为轮询页（Polling Page）。这个页被设置为没有任何访问权限（PROT_NONE）。\n1 2 3 4 5 6 7 8 9 // 源码片段中的逻辑 (SafepointMechanism.cpp) // 分配内存，一部分作为 bad_page，一部分作为 good_page char* polling_page = os::reserve_memory(allocation_size, !ExecMem, mtSafepoint); char* bad_page = polling_page; char* good_page = polling_page + page_size; os::protect_memory(bad_page, page_size, os::MEM_PROT_NONE); // 关键！设置为无权限 os::protect_memory(good_page, page_size, os::MEM_PROT_READ); // 设置为可读 从上面的代码中，可以看出，JVM 会分配两个相邻的页，一个无权限的 bad_page 和一个有读权限的 good_page，通过切换线程本地指针来指向其中一个。\nJIT 编译：插入轮询指令\n当 JIT 编译热点代码时，它会在需要进行 Safepoint 检查的地方（如循环的回边、方法返回前）插入一条非常简单的内存读取指令。\n在 x86 汇编中，这条指令看起来像这样：\n1 test\tdword ptr [r10], eax; {poll} 这条指令的作用是尝试从某个固定的虚拟地址（轮询页的地址）读取数据。\n正常执行（快速路径）\n在绝大多数时间里，JVM 并不需要线程暂停。\n此时，虽然 JIT 编译的代码中包含了内存读取指令，但 test 指令要访问的内存地址是一个合法的、可读的普通内存地址（good_page）。因此，test 指令每次都能成功执行，速度极快，几乎不产生任何性能开销，也不会引起分支预测失败。这是 Safepoint 机制性能如此之高的关键。\n发起 Safepoint（慢速路径）\n当 VMThread 决定发起一次全局 Safepoint 时，它会执行一个关键动作：将所有线程的轮询指针，都修改为指向那个被保护的、权限为 PROT_NONE 的轮询页地址（bad_page）。\n触发与处理\n此后，任何一个正在执行 JIT 代码的线程，当它再次运行到那条 test 内存读取指令时，就会试图访问一个 PROT_NONE 的地址。MMU 立刻捕获到这次非法访问，触发硬件异常，使系统陷入内核，并最终由操作系统向 JVM 进程发送 SIGSEGV 信号。\nJVM 内部预先注册了一个自定义的 SIGSEGV 信号处理器。当处理器被调用时，它会检查导致段错误的内存地址。如果发现地址正是那个特殊的轮询页，它便知道这并非程序缺陷，而是一个预期的 Safepoint 触发事件。于是，它会引导当前线程执行 SafepointSynchronize::block() 进入阻塞状态，等待 Safepoint 结束。\n通过这种方式，OpenJDK 将一个频繁的软件检查，转换成了一个在绝大多数情况下无开销的硬件操作，只在需要时才通过一个可控的硬件异常进入处理流程。\n# 其他场景下的 Safepoint 处理 页保护机制是为高性能 JIT 代码量身定制的方案。对于仍在解释器中运行的代码，或是已进入 JNI 的线程，JVM 采用了不同的处理方式。\n解释器代码 (Interpreted Code)\n解释器的工作模式是逐条地翻译和执行字节码，其性能无法与 JIT 编译后的代码相比。因此，它采用了更直接的轮询方式。\n当 Safepoint 被“武装”时，解释器会切换到一个备用的派发表（Dispatch Table），这个备用表中的字节码实现，在执行循环跳转等指令时，会额外包含一条直接检查线程本地 Safepoint 标志位的指令。这种方式避免了复杂的信号处理，实现简单明了。\n原生代码 (Native Code / JNI)\n当一个 Java 线程进入 JNI 调用，执行原生 C/C++ 代码时，它已经脱离了 JVM 的直接控制，自然也无法进行轮询。此时，JVM 会将这个线程标记为正在执行原生代码（_thread_in_native）。从 JVM 的视角看，这个线程的 Java 栈是固定的、安全的。\n当这个线程执行完原生代码，准备返回 Java 世界时，在过渡阶段，它必须检查全局的 Safepoint 标志。如果此时 JVM 正处于一个全局 Safepoint 中，该线程会被暂停在返回的关口，直到 Safepoint 结束，才能安全地继续执行 Java 代码。\n已阻塞的线程 (Blocked Threads)\n这是最简单的一种情况。对于那些已经因为锁竞争、执行 Object.wait()、Thread.sleep() 或阻塞式 I/O 而处于阻塞状态的线程，它们本身就已经“静止”了，不会对 JVM 的全局操作构成威胁。\n因此，当 VMThread 在进行同步时，一旦检测到某个线程处于 _thread_blocked 状态，就会直接将其视为已完成同步，无需任何额外的操作。这些线程对 TTSP 的贡献时间为零。\n至此，OpenJDK 通过为不同执行模式量身定制的策略，构建了一套完整而高效的 Safepoint 协作机制。\n# 实战：监控与分析安全点 了解了 Safepoint 的工作原理后，我们最关心的问题是：如何知道我自己的应用程序中 Safepoint 的表现如何？它是否是导致我的应用卡顿的元凶？\n幸运的是，OpenJDK 提供了一个强大的参数，让我们能清晰地看到每一次 Safepoint 操作的细节。\n# -Xlog:safepoint 参数 要开启 Safepoint 日志，你需要在启动 JVM 时添加 -Xlog 参数，并指定 safepoint 标签，例如：\n1 -Xlog:safepoint:safepoint.log 这个配置会 Safepoint 日志输出到 safepoint.log 文件中。在 JDK 9 之前，可以使用 -XX:+PrintSafepoints 和 -XX:+PrintGCApplicationStoppedTime 这两个参数来获取类似的信息。\n# 日志解读 开启日志后，你会看到类似下面这样的输出。我们来逐列解读其中的关键信息：\n1 2 [5.745s][info][safepoint] Safepoint \u0026#34;G1CollectForAllocation\u0026#34;, Time since last: 21216960 ns, Reaching safepoint: 23275 ns, At safepoint: 3836938 ns, Total: 3900312 ns [8.713s][info][safepoint] Safepoint \u0026#34;ThreadDump\u0026#34;, Time since last: 128147238 ns, Reaching safepoint: 145843 ns, At safepoint: 572080 ns, Total: 802856 ns 让我们以第一条日志为例，分解它的含义：\n[5.745s]: 时间戳。表示从 JVM 启动到此次 Safepoint 操作开始的时间点。\nSafepoint \u0026quot;G1CollectForAllocation\u0026quot;: 触发原因。这是最重要的信息之一，它告诉我们这次全局暂停是由什么事件引起的。这里的 G1CollectForAllocation 意味着 G1 垃圾收集器因为堆内存分配请求无法满足而触发了一次 Young GC。在第二条日志中，原因则是 ThreadDump，说明是外部工具请求了线程堆栈转储。\nTime since last: 21216960 ns: 距离上次安全点的间隔。表示从上一个 Safepoint 结束到本次开始，应用程序自由运行的时间（约 21.2 毫秒）。这个值反映了 Safepoint 的发生频率。\nReaching safepoint: 23275 ns: 到达安全点耗时。这就是我们反复强调的 TTSP。这里是约 23.3 微秒，一个非常健康的值。它表示从 JVM 发起请求到所有线程都暂停，总共花了多长时间。\nAt safepoint: 3836938 ns: 在安全点内执行操作的耗时。这是真正的“Stop-The-World”时长，即 VMThread 执行其核心任务所花的时间。这里是约 3.8 毫秒。\nTotal: 3900312 ns: 总停顿时间。约等于 Reaching safepoint + At safepoint 的总和。这是应用线程从被请求暂停到最终被唤醒所经历的全部时间。\n# 如何定位问题 通过分析这份日志，我们可以清晰地诊断出应用的停顿模式：\n停顿的元凶是谁？\n观察 Safepoint \u0026quot;Cause\u0026quot; 字段。如果大部分停顿都由 G1CollectForAllocation 或类似 GC 原因引起，说明性能瓶颈在于对象分配速率过高或堆内存不足。如果看到很多 Deoptimize 或 RevokeBias，则可能需要关注 JIT 编译或锁竞争的问题。\n是 TTSP 过长，还是 VM 操作本身耗时？\n这是分析的关键。对比 Reaching safepoint 和 At safepoint 这两个值。\n如果 At safepoint 很大（通常是毫秒级别），说明是 VM 操作本身很耗时。比如 GC 耗时长，就应该去调优 GC 参数或分析内存使用。\n如果 Reaching safepoint 很大（比如也达到了毫秒级别），这通常是一个更危险的信号，说明有线程花了很长时间才响应暂停请求。此时，你应该去检查代码中是否存在没有 Safepoint 轮询点的长计算，或者是否存在严重的 CPU 资源争抢。\n以文章开头提供的日志为例，我们可以看到绝大多数停顿都是由 GC (G1CollectForAllocation, G1PauseRemark 等) 引起的，并且 At safepoint 时间远大于 Reaching safepoint 时间。这清晰地表明，该应用的停顿瓶颈在于 GC 操作本身，而不是 TTSP。\n掌握了 Safepoint 日志的分析方法，你就拥有了一把强大的武器，能够更全面、更深入地诊断和优化 JVM 的停顿问题。\n# 总结 希望通过本文的介绍，你能够建立起关于 Safepoint 的几个核心认知：\nSafepoint 是所有 STW 停顿的基础。我们常说的 GC STW，其本质就是一次全局 Safepoint 操作。但 GC 只是触发 Safepoint 的众多“客户”之一，代码反优化、线程转储等多种 VM 操作同样依赖它。\n这是一种“协作”，而非“命令”。JVM 无法在任意时刻强行中断一个正在高速运行的线程。它必须依赖线程主动在预设的轮询点检查并自行暂停。这种协作式机制，是保证所有全局操作安全、正确的前提。\n停顿时间由两部分构成。一次完整的 Safepoint 停顿，包含了所有线程到达 Safepoint 的时间（TTSP）和在 Safepoint 内执行 VM 操作的时间。仅仅关注 GC 日志里的操作耗时是不够的，被忽略的 TTSP 常常是导致应用卡顿的“隐形杀手”。\n高效的实现源于对底层的深刻理解。OpenJDK 通过巧妙地利用页保护和信号处理机制，为 JIT 编译代码实现了近乎零开销的轮询，这充分展现了现代虚拟机在性能工程上的智慧。\n最终，理解 Safepoint 并不只是为了满足技术上的好奇心。它为我们提供了一个全新的、更底层的视角来审视应用的性能问题。下一次，当你面对一个棘手的延迟或性能抖动问题，在深入分析业务逻辑和 GC 日志之外，不妨也加上 -Xlog:safepoint，看一看那些隐藏在幕后的“集体暂停”，或许，问题的答案就藏在其中。\n","date":"2025-07-16T11:13:04+08:00","permalink":"https://mazhen.tech/p/%E6%8F%AD%E7%A7%98-jvm-%E5%81%9C%E9%A1%BF%E7%9A%84%E8%83%8C%E5%90%8E%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-safepoint/","title":"揭秘 JVM 停顿的背后：深入理解 Safepoint"},{"content":"\n在上一篇博客中，我们深入探讨了性能分析中两个概念：Wall-Clock 采样与 CPU-Cycles 采样的本质区别。这一次我想使用 perf 这个强大的工具来验证上一篇的内容。\n对于 CPU-Cycles 采样，路径非常明确：perf record -e cycles，或者干脆不带 -e 参数时，默认使用的就是这种基于硬件的 On-CPU 分析。\n但问题来了：如何用 perf 实现 Wall-Clock 采样呢？\n一个看似可能的答案是 cpu-clock 软件事件。然而，官方文档对它的描述只有一句，语焉不详：\nThis reports the CPU clock, a high-resolution per-CPU timer.\n报告提供 CPU 时钟信息，即一种高精度的每 CPU 计时器。\n我尝试用各种 AI DeepResearch 一番，仍然没有找到答案，而且发现自己之前写的那篇《深入探索 perf CPU Profiling 实现原理》竟排在参考引用文档列表中很高的位置。同时，另一位知名开发者 Robert Haas（PostgreSQL 的核心贡献者）也曾抱怨过 cpu-clock 缺乏文档说明。\n既然现有的资料无法给出确切答案，唯一的办法就是到 Linux 内核源码中找答案，搞清楚 cpu-clock 和 cycles 这对采样事件，究竟在底层是如何实现的，也算给 AI 贡献一些原创素材。\n# 深入内核：两种采样事件的实现 # cpu-clock 的实现 通过内核源码 kernel/events/core.c，我们可以分析出 cpu-clock 事件实现的核心逻辑，它的实现依赖于 Linux 的高精度定时器（hrtimer）。\n整个流程可以概括为：\n初始化：当一个 cpu-clock 事件被创建时，cpu_clock_event_init() 会调用 perf_swevent_init_hrtimer()。这会在每个被监控的 CPU 上设置一个周期性的 hrtimer。 定时中断：这个 hrtimer 会以我们指定的频率（例如 -F 99 就是 99Hz）周期性地触发一个硬件中断。 中断处理：中断会激活 perf_swevent_hrtimer() 这个处理函数。在这个函数里，内核会判断事件是否需要记录样本。 样本采集：如果需要，它会调用 __perf_event_overflow()，最终触发 overflow_handler（如 perf_event_output_forward），后者会收集当前上下文的数据（如调用栈、寄存器等），打包成一个 PERF_RECORD_SAMPLE，然后写入 Ring Buffer。 简单来说，cpu-clock 是一个纯粹的软件事件，使用 hrtimer 周期性地触发一个中断，中断处理函数负责生成并记录一个样本。 它就像一个设定的闹钟，每隔固定的时间就响一次，然后记录下那个瞬间 CPU 正在处理的任务。\n# cpu-cycles 的实现 cpu-cycles 是一个硬件事件，它直接利用 CPU 内部的性能监控单元（PMU）来统计 CPU 时钟周期，它的实现与 cpu-clock 软件事件有本质区别。\n以 x86 平台上为例，cpu-cycles 事件的实现在 arch/x86/events/core.c 文件中，我们可以看到它的实现与硬件紧密耦合。整体流程如下：\n初始化与编程：当一个 cpu-cycles 事件被创建时，x86_pmu_event_init() 会将这个通用事件类型映射为一个特定的硬件事件编码。事件被启用时，调度器（x86_schedule_events）会为其分配一个可用的硬件性能计数器（PMC）。内核通过 wrmsr 指令将事件编码写入 PerfEvtSel 寄存器，并设置 PMC 寄存器的初始值为一个负数（例如 -sample_period），然后启动该计数器。\n硬件计数：CPU 硬件开始自动对每个时钟周期进行计数。PMC 的值从初始的负数开始，每个周期加一，向零逼近。这个过程完全由硬件执行，没有软件开销。\n硬件中断 (PMI)：当硬件计数器从 0xFFFF... 溢出到 0 时，PMU 会自动触发一个性能监控中断（PMI）。在 x86 平台上，这通常是一个不可屏蔽中断（NMI）。\n中断处理：NMI 会激活 perf_event_nmi_handler()，它会调用特定于 PMU 的中断处理函数（如 x86_pmu.handle_irq）。此函数会： a. 识别溢出：检查是哪个 PMC 发生了溢出。 b. 重置计数器：调用 x86_perf_event_set_period() 将该 PMC 的值重新设置为初始的负数，为下一次采样做准备。 c. 样本采集：调用 perf_event_overflow()，最终触发 overflow_handler（如 perf_event_output_forward），后者会收集当前上下文的数据（如调用栈、寄存器等），打包成一个 PERF_RECORD_SAMPLE，然后写入 Ring Buffer。\n简而言之，cpu-cycles 是一个硬件事件。 它不像闹钟，更像一个只在工作时才计数的“计步器”。它关心的是 CPU 完成的工作量。\n# 它们采集了哪些数据？ 无论是哪种方式触发了采样，最终内核都会通过 perf_prepare_sample() 和 perf_output_sample() 这一系列的函数，将当时的“现场快照”打包成一个 PERF_RECORD_SAMPLE 记录。这份快照通常包含（取决于你的 -g 和其他 perf 参数）：\n指令指针 (IP): CPU 当时正在执行哪一条指令的地址。 进程 ID 和线程 ID (PID/TID): 是哪个进程/线程在执行。 时间戳 (Time): 采样发生的时间。 CPU 编号: 在哪个 CPU 核心上发生的采样。 调用链 (Callchain): 这是通过 -g 参数启用的。内核会从当前的栈帧指针（Frame Pointer）开始，回溯整个调用栈，记录下从当前函数到顶层函数的完整路径。对于系统调用，它能同时抓取到用户栈和内核栈，拼接成一个完整的调用链。 其他元数据: 如事件周期、地址信息等。 正是这些丰富的数据，让我们能够在事后通过 perf report 或火焰图，重构出程序的运行画像。\n# 两个事件的对比 特性 cpu-clock cpu-cycles 事件源 软件 (内核 hrtimer) 硬件 (CPU PMU) 触发机制 固定时间间隔到达，触发定时器中断 固定工作量完成，触发 PMC 溢出中断 CPU 空闲时 计时器继续运行，会采样到idle任务 计数器暂停，不会产生任何采样 # 动手验证 理论分析之后，我们需要一个实验来直观地感受这种差异。一个好的实验，应该能创造出极端的对比场景，直观的展示出两种采样方式的差异。\n我的思路是构建一个“脉冲式”的负载模型，让程序在两种截然不同的状态间切换：\nCPU 饱和阶段: 计算密集型，消耗系统所有的 CPU 资源。 空闲阶段: 程序进入长时间的休眠，让所有 CPU 核心休息。 # 实验程序 实验程序是一个多线程 C 程序 work_and_wait_mt.c，它会创建与核心数相同的工作线程，所有线程通过屏障（barrier）同步，同时开始计算，同时结束计算，实现“集体计算 100 毫秒，集体休眠 900 毫秒”的脉冲式负载。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;pthread.h\u0026gt; #include \u0026lt;stdbool.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; // --- Configuration --- long burn_time_ms = 100; long idle_time_ms = 900; int total_cycles = 10; long multiplier = 200000; // **重要**: 请先用单线程校准这个值！ // --- Globals for thread coordination --- pthread_barrier_t barrier; volatile bool time_to_exit = false; // CPU 密集型计算函数 void __attribute__((noinline)) burn_cpu(long milliseconds) { volatile unsigned long i; for (i = 0; i \u0026lt; milliseconds * multiplier; i++) { // Burn baby burn } } // 工作线程的执行函数 void* worker_function(void* arg) { int thread_id = *(int*)arg; printf(\u0026#34;Worker thread %d started.\\n\u0026#34;, thread_id); while (!time_to_exit) { // 1. 等待主线程的“开始计算”信号 // 所有工作线程和主线程都会在此集合 pthread_barrier_wait(\u0026amp;barrier); // 如果是退出信号，则跳出循环 if (time_to_exit) { break; } // 2. 执行计算任务 burn_cpu(burn_time_ms); // 3. 等待所有其他工作线程完成计算 // 主线程也在此等待，以确认计算阶段结束 pthread_barrier_wait(\u0026amp;barrier); } printf(\u0026#34;Worker thread %d exiting.\\n\u0026#34;, thread_id); return NULL; } int main() { // 自动检测可用的 CPU 核心数 long num_cores = sysconf(_SC_NPROCESSORS_ONLN); if (num_cores \u0026lt;= 0) { num_cores = 1; // 备用值 } printf(\u0026#34;Detected %ld CPU cores.\\n\u0026#34;, num_cores); // +1 是因为主线程也要参与屏障同步 pthread_barrier_init(\u0026amp;barrier, NULL, num_cores + 1); pthread_t* threads = malloc(num_cores * sizeof(pthread_t)); int* thread_ids = malloc(num_cores * sizeof(int)); // 创建工作线程 for (long i = 0; i \u0026lt; num_cores; i++) { thread_ids[i] = i; pthread_create(\u0026amp;threads[i], NULL, worker_function, \u0026amp;thread_ids[i]); } printf(\u0026#34;\\nStarting multi-threaded workload...\\n\u0026#34;); printf(\u0026#34;----------------------------------------------------------\\n\u0026#34;); for (int i = 0; i \u0026lt; total_cycles; i++) { printf(\u0026#34;Cycle %d/%d: Orchestrating CPU burn...\\n\u0026#34;, i + 1, total_cycles); // 1. 主线程到达屏障，释放所有等待的 worker 线程开始计算 pthread_barrier_wait(\u0026amp;barrier); // 2. 主线程也在此等待，直到所有 worker 线程都完成计算 pthread_barrier_wait(\u0026amp;barrier); printf(\u0026#34;Cycle %d/%d: Orchestrating idle period...\\n\u0026#34;, i + 1, total_cycles); usleep(idle_time_ms * 1000); } printf(\u0026#34;----------------------------------------------------------\\n\u0026#34;); printf(\u0026#34;Workload finished. Signaling threads to exit.\\n\u0026#34;); // 通知所有线程退出 time_to_exit = true; // 最后一次释放屏障，让所有在等待的线程可以检查到退出标志 pthread_barrier_wait(\u0026amp;barrier); // 等待所有线程结束并清理资源 for (long i = 0; i \u0026lt; num_cores; i++) { pthread_join(threads[i], NULL); } pthread_barrier_destroy(\u0026amp;barrier); free(threads); free(thread_ids); return 0; } 在编写和测试这样的程序时，有两个细节需要注意。\n# 防止函数内联 现代编译器非常智能，当你使用 -O2 等优化选项时，它可能会发现 burn_cpu 函数非常简单，从而执行函数内联（Function Inlining）——直接把 burn_cpu 的循环代码“复制 - 粘贴”到调用它的 worker_function 中。这会导致在 perf 报告里，我们看不到 burn_cpu 这个独立的函数，所有的开销都会被算在 worker_function 头上，这会干扰我们的分析。为了避免这种情况，我们必须明确地告诉编译器不要这么做：\n1 2 3 void __attribute__((noinline)) burn_cpu(long milliseconds) { // ... } # 校准计算强度 burn_cpu 函数里的循环次数需要根据你的 CPU 性能进行调整。一个固定的循环次数，在我的电脑上可能运行 100 毫秒，在你的高性能服务器上可能只需要 30 毫秒。我们需要一个合适的“乘数（multiplier）”，让 burn_cpu(100) 能在你的机器上大致运行 100 毫秒。\n我使用了一个简单的校准程序 calibrate.c，它只包含 burn_cpu(1000)，然后用 time 命令来测量其**user时间**（即 CPU 在用户态的实际耗时）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include \u0026lt;stdio.h\u0026gt; void burn_cpu(long milliseconds) { // 这是我们要调整的乘数，先从一个初始值开始 long multiplier = 200000; volatile unsigned long i; for (i = 0; i \u0026lt; milliseconds * multiplier; i++) { // Just burn cycles } } int main() { long test_duration_ms = 1000; // 我们期望它运行 1 秒钟 printf(\u0026#34;Burning CPU for a target of %ldms...\\n\u0026#34;, test_duration_ms); burn_cpu(test_duration_ms); printf(\u0026#34;Calibration burn finished.\\n\u0026#34;); return 0; } 现在，我们编译并使用 time 命令来测量这个程序的实际运行时间。我们使用 -O2 优化，这更接近生产环境的编译设置。\n1 2 $ gcc -O2 -o calibrate calibrate.c $ time ./calibrate time 命令会输出类似下面的结果：\n1 2 3 4 5 6 Burning CPU for a target of 1000ms... Calibration burn finished. real\t0m0.052s user\t0m0.051s sys\t0m0.001s 这里的关键是 user 时间。它表示程序在用户态实际消耗的 CPU 时间。\n目标时间: 1.0 秒 (1000ms) 实际时间: 0.051 秒 这说明我们当前的乘数 200000 太小了，程序运行得太快。我们可以用一个简单的比例来计算新的乘数：\n新乘数 = 旧乘数 * (目标时间 / 实际时间)\n在这个例子中： 新乘数 = 200,000 * (1.0 / 0.051) ≈ 200,000 * 19.6 ≈ 3920000\n我们可以取一个整数，比如 4000000。\n现在，修改 calibrate.c 中的乘数为 4000000，然后再次运行校准测试：\n1 2 3 4 5 6 7 8 $ gcc -O2 -o calibrate calibrate.c $ time ./calibrate Burning CPU for a target of 1000ms... Calibration burn finished. real\t0m0.987s user\t0m0.984s sys\t0m0.002s 这次的输出 user 时间应该会非常接近 1.0 秒了，现在就可以用这个新的乘数修改 work_and_wait_mt.c ，来进行 perf 对比实验了。\n# 编译实验程序 我们使用以下命令进行编译：\n1 gcc -g -O2 -o work_and_wait_mt work_and_wait_mt.c -pthread -g: 生成调试信息，这是 perf 能够将内存地址翻译成函数名的关键。 -O2: 开启二级优化，这更接近生产环境的编译设置，也使得我们防止内联的操作变得必要。 -pthread: 链接 pthread 库，因为我们用到了多线程和屏障。 # 执行采样 现在，我们用两种不同的采样方式，在**系统全局模式（-a）**下，对这个程序进行分析。\n我们使用系统全局（-a）模式，以确保能捕获到 idle 任务。\n# 实验一：cpu-cycles 事件 采集数据 1 $ sudo perf record -a -F 99 -g -- ./work_and_wait_mt 生成报告 1 $ sudo perf report 报告解读 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Samples: 3K of event \u0026#39;cpu_core/cycles/P\u0026#39;, Event count (approx.): 124421182834 Children Self Command Shared Object Symbol + 99.28% 99.12% work_and_wait_m work_and_wait_mt [.] burn_cpu 0.25% 0.00% swapper [kernel.kallsyms] [k] secondary_startup_64_no_verify 0.25% 0.00% swapper [kernel.kallsyms] [k] cpu_startup_entry 0.25% 0.00% swapper [kernel.kallsyms] [k] do_idle 0.20% 0.00% swapper [kernel.kallsyms] [k] start_secondary 0.19% 0.00% work_and_wait_m [kernel.kallsyms] [k] asm_sysvec_apic_timer_interrupt 0.19% 0.00% swapper [kernel.kallsyms] [k] cpuidle_idle_call 0.16% 0.00% work_and_wait_m [kernel.kallsyms] [k] sysvec_apic_timer_interrupt 0.16% 0.00% work_and_wait_m [kernel.kallsyms] [k] __sysvec_apic_timer_interrupt 0.16% 0.00% work_and_wait_m [kernel.kallsyms] [k] hrtimer_interrupt 0.16% 0.00% work_and_wait_m [kernel.kallsyms] [k] __hrtimer_run_queues 0.16% 0.00% work_and_wait_m [kernel.kallsyms] [k] tick_nohz_highres_handler 0.16% 0.00% work_and_wait_m [kernel.kallsyms] [k] tick_sched_handle 0.16% 0.00% work_and_wait_m [kernel.kallsyms] [k] update_process_times 0.16% 0.00% work_and_wait_m [kernel.kallsyms] [k] scheduler_tick 0.16% 0.00% work_and_wait_m [kernel.kallsyms] [k] task_tick_fair 0.16% 0.00% swapper [kernel.kallsyms] [k] call_cpuidle ... 第一行是报告的标题：\nSamples: 3K: 在程序运行期间，perf 总共捕获了 3000 个样本。 event \u0026lsquo;cpu_core/cycles/P\u0026rsquo;: perf 告诉你它采样的是 cycles 事件。这个名字 cpu_core/cycles/P 是一个更具体的硬件事件名称，但本质就是 CPU 时钟周期。 Event count (approx.): 程序在运行时，大约产生了 1 千 2 百亿个 CPU 周期。 接下来是表格的列：\nChildren: 子项开销。表示这个函数以及它调用的所有子函数总共占用的 CPU 时间百分比。可以理解为，当程序执行到这个函数所在的调用链上时，所花费的时间。 Self: 自身开销。表示 CPU 时间只花在这个函数本身内部的百分比，不包括它调用的其他函数。这是定位代码热点的最关键指标。 Command: 运行的命令名 (work_and_wait)。 Shared Object: 该符号所在的库或可执行文件。 Symbol: 函数名或内核符号名。 Self: 99.12%，这是最重要的信息。在所有样本中，有 99.12% 的样本都是在 CPU 执行 burn_cpu 函数内部的指令时捕获的。这清晰地表明，这个函数是程序中唯一的 CPU 性能热点。\nswapper（即 idle 任务）在这里也出现了，但它的占比极小，与 burn_cpu 产生的海量周期相比，这些内核自身的管理开销几乎可以忽略不计。\n总结 cpu-cycles 报告： burn_cpu消耗了 99.12% 的 CPU 算力，perf 准确地抓住了那个让 CPU 发热的元凶。swapper（idle 任务）几乎可以忽略不计，证明了 cpu-cycles 采样在程序休眠时是“暂停”的，它只关心工作，不关心等待。\n# 实验二：cpu-clock 事件 采集数据 1 $ sudo perf record -a -F 99 -e cpu-clock -g -- ./work_and_wait_mt 生成报告: 1 $ sudo perf report 报告解读: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 Samples: 15K of event \u0026#39;cpu-clock\u0026#39;, Event count (approx.): 156434341870 Children Self Command Shared Object Symbol + 63.88% 0.00% swapper [kernel.kallsyms] [k] secondary_startup_64_no_verify + 63.88% 0.00% swapper [kernel.kallsyms] [k] cpu_startup_entry + 63.88% 0.00% swapper [kernel.kallsyms] [k] do_idle + 63.87% 0.00% swapper [kernel.kallsyms] [k] cpuidle_idle_call + 63.87% 0.00% swapper [kernel.kallsyms] [k] call_cpuidle + 63.87% 63.85% swapper [kernel.kallsyms] [k] cpuidle_enter_state + 63.87% 0.00% swapper [kernel.kallsyms] [k] cpuidle_enter + 58.09% 0.00% swapper [kernel.kallsyms] [k] start_secondary + 36.08% 36.08% work_and_wait_m work_and_wait_mt [.] burn_cpu + 5.79% 0.00% swapper [kernel.kallsyms] [k] x86_64_start_kernel + 5.79% 0.00% swapper [kernel.kallsyms] [k] x86_64_start_reservations + 5.79% 0.00% swapper [kernel.kallsyms] [k] start_kernel + 5.79% 0.00% swapper [kernel.kallsyms] [k] arch_call_rest_init + 5.79% 0.00% swapper [kernel.kallsyms] [k] rest_init 0.01% 0.01% Lingma [kernel.kallsyms] [k] finish_task_switch.isra.0 ... 情况发生了戏剧性的反转。在这份报告中，swapper （空闲任务）成为了绝对的主角。有 63.85% 的时间样本，都捕获到 CPU 正处于 cpuidle_enter_state 这个函数内部，这正是内核让 CPU“休眠”的地方。\nburn_cpu 沦为配角 (36.08% Self)，曾经消耗了 99% CPU 算力的 burn_cpu 函数，在时间的维度上，只占了 36.08% 的份额。\n总结 cpu-clock 报告： swapper 成了主角，占据了 63.85% 的时间样本，证明了 CPU 在绝大部分时间里，都处于内核安排的“睡眠”状态。burn_cpu 沦为配角，只占了 36.08% 的时间。\n# 对比总结 采样事件 核心问题 burn_cpu 表现 swapper 表现 分析视角 cpu-cycles “谁消耗了算力？” 绝对主力 (99%) 微不足道 On-CPU (工作量分析) cpu-clock “时间花在哪了？” 少数派 (~36%) 绝对主力 (64%) Wall-Clock (延迟/等待分析) # 一个有用的技巧：进程专属模式下的 cpu-clock 如果不使用 -a 参数，而是运行以下命令：\n1 $ sudo perf record -F 99 -e cpu-clock -g -- ./work_and_wait_mt 得到的报告竟然和 cycles 采样的结果惊人地一致，swapper 消失了，burn_cpu 再次占据了 99% 以上的份额，休息时间（swapper/idle）完全消失了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Samples: 5K of event \u0026#39;cpu-clock\u0026#39;, Event count (approx.): 56252524690 Children Self Command Shared Object Symbol + 99.98% 99.98% work_and_wait_m work_and_wait_mt [.] burn_cpu 0.02% 0.02% work_and_wait_m [kernel.kallsyms] [k] __check_heap_object 0.02% 0.00% work_and_wait_m work_and_wait_mt [.] worker_function 0.02% 0.00% work_and_wait_m libc.so.6 [.] __printf_chk 0.02% 0.00% work_and_wait_m libc.so.6 [.] __vfprintf_internal 0.02% 0.00% work_and_wait_m libc.so.6 [.] __printf_buffer_to_file_done 0.02% 0.00% work_and_wait_m libc.so.6 [.] _IO_file_xsputn@@GLIBC_2.2.5 0.02% 0.00% work_and_wait_m libc.so.6 [.] _IO_do_write@@GLIBC_2.2.5 0.02% 0.00% work_and_wait_m libc.so.6 [.] _IO_file_write@@GLIBC_2.2.5 0.02% 0.00% work_and_wait_m libc.so.6 [.] __GI___libc_write 0.02% 0.00% work_and_wait_m [kernel.kallsyms] [k] entry_SYSCALL_64_after_hwframe 0.02% 0.00% work_and_wait_m [kernel.kallsyms] [k] do_syscall_64 0.02% 0.00% work_and_wait_m [kernel.kallsyms] [k] x64_sys_call ... 原因是这次使用的命令是进程专属（Process-Specific）模式，而不是系统全局（System-Wide）模式。\n-a参数的全称是 --all-cpus，是对全系统范围内所有 CPU 进行收集。如果不加 -a，上面的命令明确告诉 perf，只监控 ./work_and_wait_mt 这个进程以及它创建的所有子进程。对于任何其他进程的活动，请直接忽略。\n当 perf 在进程专属模式下运行时，它内部有一个强大的“过滤器”，当 cpu-clock 的“闹钟”响起时，perf 会检查当前在 CPU 上运行的是哪个进程。如果恰好是我们的 work_and_wait_mt，样本就被记录下来。如果在程序休眠期间，CPU 上运行的是 swapper 任务，perf 会发现这个任务不属于它的监控目标，于是直接丢弃了这个样本。\n通过这个实验我们发现，在进程专属模式下，cpu-clock 的行为会因为进程过滤器的存在而变得非常像 cpu-cycles。这意味着，在不方便使用硬件 cycles 事件，或者想纯粹用软件事件来分析一个特定程序的 On-CPU 热点时，使用“进程专属模式”下的 cpu-clock 是一个完全可行的替代方案。 同时，要真正地用 cpu-clock 来分析等待时间（Off-CPU），必须使用 -a 参数切换到系统全局模式。\n# 总结 通过对内核源码的剖析，以及一系列实验，现在我们可以回答开头那个令人困惑的问题了：cpu-clock 和 cycles 到底有何不同？\ncpu-clock 基于时间，它的闹钟忠实地记录下每个时间点的系统状态。 cpu-cycles 基于工作量，它的计步器精准地衡量了 CPU 的实际消耗。 另外，在进程专属模式下，perf 内置的“过滤器”会将所有非目标进程的样本（包括 swapper）丢弃，使得 cpu-clock 的行为趋近于 cpu-cycles，成为一种纯软件实现的 On-CPU 分析替代方案。\n当面对文档的语焉不详和网络信息的模糊不清时，只有深入源码、亲手验证，才是揭开迷雾正确路径。希望这篇文章对你也有帮助。\n","date":"2025-06-20T11:10:02+08:00","permalink":"https://mazhen.tech/p/perf-%E7%9A%84-cpu-clock-%E4%B8%8E-cpu-cycles-%E9%87%87%E6%A0%B7%E5%AE%9E%E7%8E%B0%E4%B8%8E%E5%AF%B9%E6%AF%94/","title":"Perf 的 cpu-clock 与 cpu-cycles 采样实现与对比"},{"content":"\n我曾写过一篇 深入探索 perf CPU Profiling 实现原理 的文章，希望能帮助大家理解这个强大工具背后的运作机制。在那篇文章里，我尽可能地拆解了从中断到调用栈还原的整个流程。\n然而，在后续的技术学习和思考过程中，我意识到其中一个核心概念的阐述，虽然看似合理，却隐含着一个重要且易被误解的核心概念。今天，我想写下这篇“勘误与续篇”，与大家一同分享这个让我醍醐灌顶的“Aha!”时刻，重新探讨两种采样方式：Wall-Clock 采样与 CPU-Cycles 采样。\n# 最初的误解：CPU 的节拍是恒定的吗？ 在我之前的文章中，关于为何采样 cycles 事件能分析 CPU 性能，我是这样描述的：\n“每经过一个 CPU 周期都会触发一个 cycles 事件。可以认为，cycles 事件是均匀的分布在程序的执行期间。这样，以固定频率去采样的 cycles 事件，也是均匀的分布在程序的执行期间。我们在采样 cycles 事件时，记录 CPU 正在干什么，持续一段时间收集到多个采样后，我们就能基于这些信息分析程序的行为\u0026hellip;”\n这个逻辑听起来是不是无懈可击？一个 4 GHz 的 CPU，每秒不就是产生 40 亿个时钟周期（cycles）吗？那么按 cycles 采样不就等同于按一个极其精确的时间间隔采样吗？\n正是这个看似合理的假设，让我走入了一个误区。 事实是，CPU 远比我们想象的要“聪明”和“懒惰”。\n# 墙上时钟采样（Wall-Clock Sampling） 为了理解 cpu-cycles 采样的精妙之处，我们首先要看它的参照物：墙上时钟采样（Wall-Clock Sampling）。\n你可以把墙上时钟采样想象成一个厨房定时器。你设定它每 1 分钟响一次，它就会忠实地每分钟提醒你一次，完全不管你是在切菜、炒菜，还是在刷手机，等水烧开。\n这种采样方式依赖一个全局的、按真实时间流逝的计时器。比如，你设定每 10 毫秒采样一次，操作系统就会每隔 10 毫秒触发一次中断，然后去看：“嘿，CPU 核心此刻正在忙什么？”\n这种方式的核心是对时间点的快照，它并不关心两次快照之间发生了什么。让我们从 CPU 的视角来看一个场景：\n假设在 100 毫秒的总时间内，一个 CPU 核心的活动如下：\n0-20ms: 运行高强度的计算任务（进程 A）。 20-80ms: 系统无事可做，CPU 进入空闲（Idle）状态。进程 A 正在等待网络数据，而系统中没有其他需要计算的任务。 80-100ms: 计算任务（进程 A）被唤醒，继续在 CPU 上运行。 现在，我们用一个每 10 毫秒一次的墙上时钟采样器来观察这个 CPU 核心。它总共会进行 10 次采样，采样点大约会落在 10ms, 20ms, 30ms, \u0026hellip;, 100ms 的时刻。\n在 10ms 和 20ms 时刻，采样器会看到 CPU 正在执行进程 A。 在 30ms, 40ms, 50ms, 60ms, 70ms 时刻，采样器会看到 CPU 正在执行一个特殊的空闲任务（Idle Task）。 在 80ms, 90ms, 100ms 时刻，采样器又会看到 CPU 在执行进程 A。 最终的分析报告会告诉你，在大约 50% 的采样点上，CPU 在执行进程 A，而在另外 50% 的采样点上，CPU 处于空闲状态。\n这个结果本身是准确的，它忠实地反映了 CPU 在总时间内的状态分布。但 Wall-Clock 采样方式，将“计算”和“等待导致的空闲”混在了一起。如果你想找出是什么消耗了最多的 CPU 计算资源，这种采样方式可能会给你带来困惑，因为它花费了大量的样本去记录“什么都没在发生”的空闲时刻。\n想起一个流传已久的性能分析领域的笑话：一位程序员使用性能分析器后发现，有一个进程占用了大量的 CPU 时间。于是他花了很大力气进行优化，但程序的运行速度并没有变快。原来被他优化这个进程，其实是“idle loop”，也就是没有其他工作可做时才会运行的部分。\n# CPU-Cycles 采样：只关心“干活”的计步器 现在，让我们揭开 cpu-cycles 采样的真正面纱。\n“一个 4 GHz 的 CPU，每秒产生 40 亿个 cycles”这个描述是不准确的。 它描述的是 CPU 的最高性能，而不是恒定状态。\n正确的理解是：现代 CPU 为了节能和控制温度，会动态地调整自己的状态。\n当无事可做时（Idle）：CPU 会进入深度睡眠（C-states），此时它的时钟频率极低，甚至可能完全停摆。在这期间，它几乎不产生任何 cpu-cycles。 当任务清闲时（Light Load）：它会降低自己的工作频率（比如从 4 GHz 降到 1 GHz）来省电。 当任务繁重时（Heavy Load）：它才会火力全开，提升到标称频率甚至超频来快速完成工作。 现在，cpu-cycles 的真正含义就清晰了：它不是时间的度量，而是 CPU 完成工作的度量单位。\ncpu-cycles 采样就像一个只在你跑步时才计数的计步器。当你坐下休息时，计步器是完全静止的。同理，当 CPU 进入空闲状态时，为它计数的 cpu-cycles 计数器也随之暂停了。\n操作系统会与 CPU 的性能监控单元（PMU）协作，设定一个阈值，当累计消耗的 CPU Cycles 达到了预设值（比如 3000 万个周期）时，才会触发一次采样中断。\n这种采样方式保证了每一次采样，都必然命中了一个真正在消耗 CPU 资源、在“干活”的线程。一个线程被采样的次数，与它消耗的 CPU 总量成正比。\n# CPU-Cycles vs. Wall-Clock 在分别了解了 Wall-Clock 和 CPU-Cycles 采样之后，我们可能会有一个疑问：它们到底哪个更好？其实，这是一个关乎视角的问题，而非优劣之分。\n当 CPU 被 100% 占满、满负荷运转时，无论是 Wall-Clock 采样还是 cpu-cycles 采样，它们看到的结果会非常相似，都是 CPU 在忙于执行代码的快照。\n然而，真正的区分出现在 CPU 并非 100% 繁忙的时候。这两种方法最根本的区别，在于它们如何看待 CPU 的空闲（Idle）时间。\nWall-Clock 采样，作为一个时间的忠实记录者，它会一视同仁地记录下所有状态，无论是繁忙的计算，还是无所事事的等待。 CPU-Cycles 采样，则像一个只关心产出的质检员。如果生产线（CPU）停了，它也跟着停下休息，完全不关心停了多久。它只在指令执行时才进行工作。 这种看待问题的不同视角，在性能分析领域有着专门的术语：On-CPU 分析和 Off-CPU 分析。\nOn-CPU 分析关注的是：“是什么让我的 CPU 如此忙碌？”它的目标是找到消耗计算资源最多的代码热点。 Off-CPU 分析关注的是：“我的程序为什么在等待，而不是在运行？”它的目标是找到那些导致程序停滞的瓶颈，如 I/O 等待、锁竞争、或者休眠。 这两种分析方法，也对应着我们最关心的两个核心性能指标：吞吐量（Throughput）和延迟（Latency）。\nCPU-Cycles 采样，通过聚焦 On-CPU 时间，帮助我们提升程序的计算效率。这直接关系到吞吐量，即单位时间内能处理多少工作。优化掉一个 CPU 热点，意味着每个任务消耗的 CPU 时间更少，服务器自然能承载更多的请求。\nWall-Clock 采样，通过完整地展现包括 Off-CPU 在内的全部时间，帮助我们理解和优化程序的响应速度。这直接关系到延迟，即完成单个任务需要多长时间。如果一个请求 99% 的时间都在等待数据库返回结果，那么优化计算逻辑对降低延迟几乎没有帮助。\n我们应该如何选择呢？答案是：取决于你想要解决的问题。\n如果你想优化一个计算密集型服务，降低服务器成本，或者想找出算法中的性能瓶颈，那么 CPU-Cycles 采样是你的不二之选。它是性能优化的“手术刀”，精准且致命。\n但如果你遇到的问题是“我的程序启动为什么这么慢？”，情况就完全不同了。应用程序的启动过程，是一个混合了大量磁盘 I/O（读取配置文件和库）、网络 I/O（连接数据库或服务）以及 CPU 计算的复杂过程。在这种场景下，Wall-Clock 采样会非常有用。它能为你绘制一幅完整的启动时间线，清晰地标出那些漫长的“等待”鸿沟，让你知道时间到底被浪费在了哪里。同理，在调试锁竞争或分析外部服务调用延迟时，它也是一把利器。\n总而言之，这两种采样方式没有绝对的优劣，它们像是性能分析工具箱里两种不同用途的工具，为我们提供了观察系统的不同维度。\n# perf record -F 99 的真正魔法 理解了上述区别，我们才能真正领会 perf record -F 99 命令背后的原理。\n它并不是简单地“每秒采样 99 次”，背后是内核与硬件精巧的协作：\n目标驱动：-F 99 告诉内核：“我的目标是在每个 CPU 核心上，都达到平均每秒 99 次的采样频率。” 动态计算：内核不是设定一个固定的 cpu-cycles 间隔。相反，它会通过 cpufreq 子系统或 MSR 寄存器，实时地查询到 CPU 当前的工作频率。 智能调整： 如果 CPU 正以 4 GHz 的高频率运行，内核会计算出一个较大的 cpu-cycles 间隔（比如 4,000,000,000 / 99），然后设置硬件计数器。 如果 CPU 因为负载低而降频到 1 GHz，内核会立刻感知到，并自动将采样间隔调整为一个较小的 cpu-cycles 数值（比如 1,000,000,000 / 99）。 cpu-cycles 的采样周期是动态变化的，但最终达成“每秒采样 99 次”这个目标。\n这种机制带来了巨大的好处：\n它只在 CPU 忙碌时采样，天然地过滤掉了所有 I/O 等待和空闲时间造成的噪音。 它自适应 CPU 的频率变化，确保了分析结果在不同负载下的一致性。 它在不同硬件上表现一致，你在笔记本和在服务器上使用 -F 99，得到的都是相似密度的有效数据。 # 总结 这次对性能分析的重新探索，始于一个看似简单的问题：“为何采样 cycles 事件能分析 CPU 的性能？”我们最初的假设是，cycles 在时间上是均匀的，采样它就如同按时间采样。\n而经过这趟深入的旅程，我们发现：CPU-Cycles 采样并非时间的度量，而是CPU 完成工作量的度量。\n当 CPU 因为等待 I/O 或无事可做而进入休眠时，它的 cpu-cycles 计数器也随之暂停。只有当代码真正在 CPU 上执行指令时，cycles 才会产生。这意味着，一个函数被 cycles 采样命中的次数，与它流逝了多少时间无关，而是与其消耗了多少实际的 CPU 计算资源成正比。CPU-Cycles 采样的结果反映了程序在 CPU 繁忙时间（Busy Time）中的资源消耗分布，是分析 On-CPU 问题、优化吞吐量（Throughput） 的“手术刀”。\nWall-Clock 采样以恒定的时间间隔为基准，它捕获的是在特定时间点上 CPU 的状态。其结果反映了程序在总流逝时间（Total Time） 中的状态分布，天然地包含了 On-CPU 和 Off-CPU（如 Idle, I/O Wait）两个维度，是分析延迟（Latency） 问题的利器。\n这次“勘误”，不仅修正了我之前的文章，也使我对性能分析的底层原理有了一个更清晰的理解，希望对你也有帮助。\n","date":"2025-06-18T10:09:54+08:00","permalink":"https://mazhen.tech/p/wall-clock-%E4%B8%8E-cpu-cycles-%E9%87%87%E6%A0%B7%E7%9A%84%E5%8C%BA%E5%88%AB/","title":"Wall-Clock 与 CPU-Cycles 采样的区别"},{"content":"-XX:+PrintAssembly 是一个 JVM 诊断参数，用于查看 JIT 编译器为 Java 方法生成的原生汇编代码。\n直接查看汇编代码，让我们能绕开“黑盒”，直接审视 JIT 编译器对代码实际做了哪些优化，这有助于我们验证关于 JIT 行为的各种断言（例如指令重排、循环展开、安全点插入等），而不是依赖于社区中的一些“都市传说”或经验之谈。\n当然，对于大多数 Java 开发者来说，阅读汇编代码并非日常工作，它可能看起来很复杂。但幸运的是，我们无需成为汇编专家。借助一些基础知识，特别是 JVM 在输出中自动插入的丰富注释，理解关键逻辑会比想象中容易。\n本文的目标就是提供一个实战指南，带你从零开始设置并使用 PrintAssembly，并学会如何解读其输出。\n# PrintAssembly 的作用是什么？ 简单来说，PrintAssembly 的作用是将 JIT 编译器生成的本地机器码（Machine Code）反汇编成人类可读的汇编语言（Assembly Language），并输出到控制台。\n这对于以下场景至关重要：\n深度性能分析：查看是否存在低效的指令序列，或者 CPU 是否利用了特定的向量化指令（如 SSE, AVX）。 理解 JIT 优化：验证 JIT 是否进行了我们期望的优化，例如方法内联、循环展开、逃逸分析等。 学习 JVM：直观地了解 Java 字节码是如何被转换成与硬件交互的底层指令的。 # 使用步骤 要成功使用 PrintAssembly，主要分为三步：准备插件、安装插件、运行程序。\n# 准备 hsdis 插件 # hsdis 的作用及原理 当你启用 -XX:+PrintAssembly 参数后，JVM 内部的一个功能就被激活了。这个功能需要将 JIT 编译器刚刚生成的、存储在内存里的一串二进制机器码，翻译成我们能读懂的汇编指令。\n为了完成这个翻译任务，JVM 的开发者们面临一个经典的选择：是自己从零开始为支持的每一种 CPU 架构（x86, ARM 等）都编写一个复杂的反汇编器，还是利用市面上已有的、非常成熟的专业工具？\n答案显而易见，后者是更明智的选择。社区早已有了像 GNU binutils、Capstone 和 LLVM 这样强大且专业的反汇编引擎。\nGNU binutils：经典且功能强大，但其 GPL 许可证使其无法被 Oracle/OpenJDK 直接集成和分发。 Capstone: 一个轻量级、跨平台、多架构的反汇编框架，采用友好的 BSD 许可证，是目前推荐的选择。 LLVM: 强大的编译器基础设施，其内部也包含反汇编功能。 但新的问题来了：JVM 如何与这些五花八门的外部工具对接呢？\n为了解决这个问题，HotSpot JVM 的设计者定义了一个名为 hsdis (HotSpot Disassembler) 的标准接口。hsdis 本身并不复杂，它只做一件事：充当桥梁或适配器。JVM 在 JIT 编译完成后，会通过hsdis，将编译后的机器码，发送给后端真正的反汇编引擎，后者会将机器码转换成人类可读的汇编语言。\n现在，我们可以把整个流程串起来，看看它是如何运作的：\nJIT 编译完成：你的 Java 方法变得足够“热”，JIT 编译器（如 C1 或 C2）将其编译成了一段原生机器码，存放在内存的某个位置。 PrintAssembly 激活翻译任务：JVM 注意到你设置了 -XX:+PrintAssembly，于是它准备调用反汇编功能。 JVM 寻找并调用 hsdis：JVM 在其库路径下寻找名为 hsdis-*.so 的文件。找到后，它通过标准接口调用 hsdis，并告诉它：“嘿，在这块内存地址（比如 0x00007c1ef03a0700），有一段特定长度的机器码，你帮我翻译一下。” hsdis 转发请求给后端：hsdis 插件收到请求后，自己并不会进行翻译。它只是一个“中间人”，它会立刻转身，调用它在编译时就链接好的后端引擎，并将 JVM 给它的信息原封不动地传递过去。 后端引擎执行反汇编：Capstone 这样的专业引擎开始工作，它解析二进制码流，将其转换成一行行人类可读的汇编指令文本，比如 add rbx, 1。 结果返回：Capstone 将翻译好的文本结果返回给 hsdis，hsdis 再将其返回给 JVM。 JVM 输出到控制台：JVM 拿到汇编文本后，还会贴心地附加上它自己掌握的上下文信息（比如代码行号、安全点轮询注释等），最后将这整套完整的信息打印到你的控制台。 所以，hsdis 的核心作用是解耦，它让 JVM 无需关心底层究竟是哪个反汇编工具在工作，从而可以灵活地替换或选择不同的后端，也解决了因许可证问题（如 binutils 的 GPL）导致无法在官方 JDK 中直接集成的难题。\n# 构建 hsdis (Ubuntu + OpenJDK 21u + Capstone) 由于多数 JDK 发行版不自带 hsdis，我们需要自行构建。下面以 Ubuntu 平台为例，使用 Capstone 作为后端引擎，从 OpenJDK 21u 源码构建。\na. 准备构建环境\n1 2 3 # 安装基础构建工具和 Capstone 开发库 sudo apt update sudo apt install -y build-essential libcapstone-dev b. 获取 OpenJDK 源码\n1 2 3 # 克隆 OpenJDK 21 更新版本的源码 git clone https://github.com/openjdk/jdk21u.git cd jdk21u c. 配置并构建\n1 2 3 4 5 6 7 8 # 运行 configure，指定使用 capstone 作为 hsdis 的后端 bash configure --with-hsdis=capstone # 检查配置结果，确保 hsdis (capstone) 被正确识别 # ... # 运行 make 命令构建 hsdis make build-hsdis 构建成功后，你会在 build/linux-x86_64-server-release/support/hsdis/ 目录下找到目标文件 hsdis-amd64.so。\n# 安装 hsdis 插件 将构建好的 hsdis-amd64.so 文件复制到你正在使用的 JDK 的相应目录中，JVM 启动时会自动在该位置查找它。\n1 2 # 将插件复制到 server VM 的库目录下 cp build/linux-x86_64-server-release/support/hsdis/hsdis-amd64.so ${JAVA_HOME}/lib/server/ # 运行程序 插件准备就绪，我们现在可以开始运行程序并查看输出了。首先准备好我们的示例代码：\n示例程序：SafepointTest.java\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 public class SafepointTest { static final double PI = 3.14159; public static void main(String[] args) { for (int i = 0; i \u0026lt; 200_000; i++) { countedLoop(); nonCountedLoop(); } } public static void countedLoop() { long sum = 0; for (int i = 0; i \u0026lt; 1000; i++) { sum += i; } double result = sum * PI; } public static void nonCountedLoop() { long i = 0; while (i \u0026lt; 10000) { i++; } } } 先编译它：javac SafepointTest.java\n# 解锁诊断参数：-XX:+UnlockDiagnosticVMOptions 要让 JVM 输出 JIT 编译后的汇编代码，核心参数是 -XX:+PrintAssembly。不过，这个参数属于 JVM 的诊断选项（Diagnostic Options）。\n诊断选项功能强大，但如果使用不当，可能影响程序的稳定性和性能，甚至导致 JVM 崩溃。因此，JVM 的开发者默认将这些选项“锁定”，以防被误用。要使用它们，必须先通过 -XX:+UnlockDiagnosticVMOptions 这个参数来“解锁”。\n需要特别注意的是，解锁参数必须放在所有诊断参数的前面。\n# 启用汇编打印：-XX:+PrintAssembly 解锁之后，我们就可以使用核心参数 -XX:+PrintAssembly。\n1 java -XX:+UnlockDiagnosticVMOptions -XX:+PrintAssembly SafepointTest 如果你直接运行上面的命令，JVM 会打印出所有被 JIT 编译的方法的汇编代码，其中会包含大量来自 Java 标准库的内部方法。这会产生大量的信息，让你很难找到自己关心的部分。\n# -XX:CompileCommand 精确控制编译和输出 CompileCommand 是一个功能非常强大的诊断参数，它允许我们对 JIT 编译器的行为进行细粒度的控制。它的功能包括：\nexclude: 禁止 JIT 编译某个方法。 inline / dontinline: 强制或禁止对某个方法进行内联。 log: 只将特定方法的编译日志输出到 hotspot.log 文件中。 option: 只对特定方法启用某个 JVM 诊断选项（如 PrintInlining）。 print: 与 PrintAssembly 选项类似，但可以指定方法名或通配符。 当你觉得直接使用 -XX:+PrintAssembly 输出的信息过多，CompileCommand 的 print 指令就派上了用场。\n在我们的例子中，可以这么写：\n-XX:CompileCommand=\u0026quot;print,SafepointTest::*\u0026quot; 这条指令地告诉 JVM：“请只打印出 SafepointTest 这个类里所有方法的 JIT 汇编代码。”\nSafepointTest::*: 是一个匹配模式，:: 是类与方法的分隔符，* 是通配符，代表 SafepointTest 类中的所有方法。\n通过这种方式，我们就能只关注自己编写的代码，大大提高了分析效率。\n# 设置汇编语法：-XX:PrintAssemblyOptions=intel 这是一个可选但很有帮助的参数。主流的 x86 汇编有两种语法格式：\nIntel 语法: 指令 目标, 源 (例如 mov rax, rbx) AT\u0026amp;T 语法: 指令 源, 目标 (例如 movq %rbx, %rax) 对于大多数开发者而言，Intel 语法更直观易读。通过设置此参数，我们可以让输出更符合我们的阅读习惯。\n综合起来，最终的命令如下：\n1 2 3 4 java -XX:+UnlockDiagnosticVMOptions \\ -XX:CompileCommand=\u0026#34;print,SafepointTest::*\u0026#34; \\ -XX:PrintAssemblyOptions=intel \\ SafepointTest 执行它，你就能在控制台看到 SafepointTest.java 中几个方法被 JIT 编译后的汇编代码了。\n# 解读汇编输出 你的控制台会打印出多个方法的汇编代码。一个方法可能被多次编译。JVM 为了平衡启动速度和长期运行性能，采用了**分层编译（Tiered Compilation）**的策略。\nC1 编译器编译 (Client Compiler)：\nC1 编译器会快速地将热点代码（被频繁调用的代码）编译成本地机器码。 它的编译速度快，但优化程度较低。 C1 编译出的代码中会内嵌大量的性能分析探针（Profiling），用于收集代码运行时的信息，比如分支跳转频率、调用的具体类型等。这些信息将用于更高层次的优化。 输出中 ============================= C1-compiled nmethod ============================== 块就是 C1 编译的结果。出现了两次是因为 JVM 在收集到更多信息后，用 C1 对其进行了再次编译。 C2 编译器编译 (Server Compiler)：\n当一个方法变得“非常热”时（即被执行了足够多次，并且 C1 收集到了足够的性能分析数据），C2 编译器会介入。 C2 编译器会进行非常深入和激进的优化，例如方法内联、循环展开、死代码消除等。 它的编译过程更耗时，但生成的代码执行效率极高。 输出中 ============================= C2-compiled nmethod ============================== 块就是 C2 编译的结果。 我们选取其中一段输出来分析其结构，以 C2 编译的 nonCountedLoop 方法为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 ============================= C2-compiled nmethod ============================== \u0026lt;-- 标题 ----------------------------------- Assembly ----------------------------------- \u0026lt;-- 段落标题 Compiled method (c2) 23 7 % 4 SafepointTest::nonCountedLoop @ 2 (18 bytes) \u0026lt;-- 编译摘要 total in heap [0x00007c1ef03a0590,0x00007c1ef03a0810] = 640 \u0026lt;-- 内存布局 relocation [0x00007c1ef03a06e8,0x00007c1ef03a0700] = 24 main code [0x00007c1ef03a0700,0x00007c1ef03a07a0] = 160 ... [Disassembly] \u0026lt;-- 反汇编内容开始标志 -------------------------------------------------------------------------------- [Constant Pool (empty)] \u0026lt;-- 常量池 -------------------------------------------------------------------------------- [Verified Entry Point] \u0026lt;-- 验证入口点 # {method} {0x00007c1e7b400428} \u0026#39;nonCountedLoop\u0026#39; \u0026#39;()V\u0026#39; in \u0026#39;SafepointTest\u0026#39; 0x00007c1ef03a0710: sub rsp, 0x18 ... 0x00007c1ef03a0757: add rbx, 1 ; ImmutableOopMap {} ;*goto {reexecute=1 rethrow=0 return_oop=0} ; - (reexecute) SafepointTest::nonCountedLoop@14 (line 23) 0x00007c1ef03a075b: test dword ptr [r10], eax; {poll} 0x00007c1ef03a075e: cmp rbx, 0x2710 0x00007c1ef03a0765: jl 0x7c1ef03a0750 ... 0x00007c1ef03a0779: ret [Exception Handler] ... [Deopt Handler Code] ... -------------------------------------------------------------------------------- [/Disassembly] 一段完整的汇编输出主要包含以下部分：\n编译摘要 (Compiled method ...):\nc2: 表示由 C2（Server）编译器编译。也可能是 c1。 23: 本次编译的内部 ID。 %: 表示这是一次 OSR (On-Stack Replacement) 编译，即在循环中途进行的编译。 4: 编译层级（Tier 4），代表 C2 编译。 SafepointTest::nonCountedLoop @ 2 (18 bytes): 方法名、OSR 编译的入口字节码索引、以及原始方法的字节码大小。 内存布局 (total in heap ...): 描述这段编译好的代码（nmethod）在内存中的详细分布，包括了代码段、重定位信息、常量、元数据等。\n反汇编主体 ([Disassembly]):\n[Constant Pool]: 如果代码中用到了常量，会在这里列出。 [Verified Entry Point]: 这是编译后方法的入口点。 汇编指令: 核心部分。每一行包含：内存地址、指令（如 add rbx, 1）和 JVM 添加的注释。 add rbx, 1: 对应 Java 代码中的 i++。 cmp rbx, 0x2710: 对应 while (i \u0026lt; 10000) 的条件比较（0x2710 = 10000）。 jl 0x7c1ef03a0750: 条件跳转指令，如果小于则跳回循环开始处，形成循环。 test dword ptr [r10], eax; {poll}: 安全点轮询 (Safepoint Poll)。这是 JVM 在循环中插入的检查点，用于判断是否需要暂停线程执行 GC 或其他 VM 操作。nonCountedLoop 因为循环次数不确定，所以必须在循环体内插入安全点检查。而 countedLoop 是一个可数循环（Counted Loop），JVM 知道其执行次数，通常会将安全点检查放在循环外部，从而提高循环性能。 丰富的注释: JVM 会在指令旁添加非常有价值的注释，如 {poll}（安全点轮询）、{runtime_call}（调用 JVM 运行时）、以及与源代码的映射关系 SafepointTest::nonCountedLoop@14 (line 23)，极大地帮助了我们理解代码。 处理器代码 ([Exception Handler], [Deopt Handler Code]): 定义了当发生异常或需要去优化（Deoptimization）时，代码应该跳转到的地址。\n# 总结 PrintAssembly 是一个揭示 JVM JIT 编译器工作奥秘的强大工具。虽然初看之下纷繁复杂，但通过掌握其使用方法和输出结构，你可以：\n定位性能瓶颈：通过分析热点方法的汇编代码。 验证优化效果：确认 JIT 是否按预期工作。 深化对 JVM 的理解：连接上层 Java 代码和底层硬件执行的桥梁。 下次当你对一段代码的性能感到困惑时，不妨卷起袖子，用 PrintAssembly 深入其境，看看它到底在 CPU 上是如何运行的。\n","date":"2025-06-15T19:39:55+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8-printassembly-%E6%9F%A5%E7%9C%8B-jit-%E7%BC%96%E8%AF%91%E5%90%8E%E7%9A%84%E6%B1%87%E7%BC%96%E4%BB%A3%E7%A0%81/","title":"使用 PrintAssembly 查看 JIT 编译后的汇编代码"},{"content":" # CRaC 的性能飞跃与 Linux 内核的基石 Java 应用的启动速度，尤其是在微服务和 Serverless 场景下的“冷启动”，一直是其性能优化的重点。传统的启动过程涉及 JVM 初始化、类加载和 JIT 预热，耗时较长。CRaC (Coordinated Restore at Checkpoint) 技术为此提供了一种创新方案：它在应用达到理想状态时捕获其完整运行时快照（Checkpoint），并在需要时快速恢复（Restore），从而实现毫秒级启动和即时峰值性能。\n这种强大的进程“冻结”与“复苏”能力并非凭空而来，它深度依赖于 Linux 操作系统提供的底层机制，并通过 CRIU (Checkpoint/Restore In Userspace) 这个工具集得以实现。因此，要真正理解 CRaC 的工作原理，探究其背后的 Linux 系统编程知识至关重要。本文将为熟悉编程但可能不熟悉 Linux 底层的开发者，解析 CRaC 实现所依赖的关键 Linux 概念（如进程/线程、/proc 文件系统、ptrace 系统调用等），揭示 CRaC 性能飞跃背后的 Linux“魔法”。\n# 进程与线程及其生命周期 要理解 CRaC 如何操作运行中的 Java 应用，我们首先需要了解 Linux 是如何组织和管理程序执行的。其核心概念是进程 (Process)。你可以将进程看作是一个正在运行的程序的实例，它是操作系统分配资源（如内存、文件句柄）和进行调度的基本单位。每个进程都仿佛生活在自己的独立世界里，拥有独立的地址空间，这保证了进程间的隔离性。\n然而，在一个进程内部，往往需要同时执行多个任务流。这时线程 (Thread) 就登场了。线程是进程内部的实际执行单元，有时也被称为轻量级进程（LWP）。与进程不同，同一进程内的所有线程共享该进程的地址空间和大部分资源，这使得线程间的通信和切换更为高效。但每个线程仍然保有自己独立的执行上下文，如程序计数器、寄存器和栈。\n有趣的是，从 Linux 内核的视角来看，它并不严格区分进程和线程。两者都被视为可调度的“任务 (Task)”，并由统一的数据结构 task_struct 来描述。线程仅仅是与其他任务共享了更多资源（特别是内存地址空间）的任务而已。因此，内核调度器可以对它们一视同仁。为了管理这些任务，系统为它们分配了唯一的身份标识：PID (Process ID) 用于标识整个进程（一组共享资源的线程），而 TID (Thread ID) 则用于标识每一个单独的线程（任务）。对于单线程进程，PID 和 TID 是相同的。\n新进程的诞生，最经典的方式是通过 fork() 系统调用。当一个进程调用 fork()，内核会创建出它的一个几乎完全相同的副本——子进程。子进程继承了父进程大部分状态，包括内存内容的副本（通过写时复制优化）、文件描述符等。fork() 的奇妙之处在于它在父进程中返回子进程的 PID，而在子进程中返回 0，使得程序可以根据返回值区分父子，执行不同的逻辑。CRIU 在恢复进程状态时，正是利用 fork() 来重建 Checkpoint 时刻的进程树结构。\nfork() 示例代码 (fork_example.c):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; int main() { pid_t pid = fork(); // 创建子进程 if (pid \u0026lt; 0) { // fork 失败 perror(\u0026#34;fork failed\u0026#34;); exit(EXIT_FAILURE); } else if (pid == 0) { // 子进程执行的代码 printf(\u0026#34;I am the child process, PID: %d\\n\u0026#34;, getpid()); sleep(2); // 模拟工作 printf(\u0026#34;Child process exiting.\\n\u0026#34;); exit(EXIT\\_SUCCESS); // 子进程正常退出 } else { // 父进程执行的代码 printf(\u0026#34;I am the parent process, PID: %d, Child PID: %d\\n\u0026#34;, getpid(), pid); printf(\u0026#34;Parent waiting for child...\\n\u0026#34;); wait(NULL); // 简单等待任意子进程 printf(\u0026#34;Parent process: Child finished.\\n\u0026#34;); } printf(\u0026#34;Process %d exiting.\\n\u0026#34;, getpid()); return 0; } // 编译运行：gcc fork_example.c -o fork_example \u0026amp;\u0026amp; ./fork_example 除了 fork()，还有一个更底层的系统调用 clone()，它提供了更细粒度的控制，允许指定新创建的任务与父任务共享哪些资源，因此 clone() 既可以用来创建进程，也可以用来创建线程。\nfork() 创建的子进程默认执行的是和父进程相同的代码。如果希望子进程去执行一个全新的程序，就需要 exec 系列系统调用（如 execv(), execlp() 等）的帮助。exec 调用会用新程序的映像完全替换当前进程的内存空间（代码、数据、堆栈），然后从新程序的入口点开始执行。一旦 exec 成功，原来的程序就不复存在了，这个调用本身也不会返回。CRaC 在恢复过程中，criuengine 这个辅助程序就利用了一连串的 execv 调用，不断“变身”，最终成为负责等待恢复后 JVM 退出的 restorewait 进程。\nexecv() 示例代码 (exec_example.c):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; int main() { pid_t pid = fork(); if (pid \u0026lt; 0) { perror(\u0026#34;fork failed\u0026#34;); exit(EXIT_FAILURE); } else if (pid == 0) { // 子进程 printf(\u0026#34;Child process (PID: %d) will execute \u0026#39;ls -l\u0026#39;\\n\u0026#34;, getpid()); char *args[] = {\u0026#34;/bin/ls\u0026#34;, \u0026#34;-l\u0026#34;, NULL}; // 准备参数列表 execv(args[0], args); // 执行新程序 // 如果 execv 返回，说明出错了 perror(\u0026#34;execv failed\u0026#34;); exit(EXIT_FAILURE); } else { // 父进程 printf(\u0026#34;Parent process (PID: %d) waiting for child (PID: %d)...\\n\u0026#34;, getpid(), pid); wait(NULL); // 等待子进程结束 printf(\u0026#34;Parent process: Child finished executing \u0026#39;ls -l\u0026#39;.\\n\u0026#34;); } return 0; } // 编译运行：gcc exec_example.c -o exec_example \u0026amp;\u0026amp; ./exec_example 进程有生就有灭。当子进程结束时，它并不会立即消失。内核会保留它的退出状态等信息，等待父进程来“认领”。父进程通过调用 wait() 或 waitpid() 系统调用来获取子进程的终止信息，并告知内核可以彻底清理该子进程了。waitpid() 提供了更多控制，比如可以等待指定的子进程，或者以非阻塞的方式检查子进程状态。\nwaitpid() 示例代码 (waitpid_example.c):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; int main() { pid_t pid1, pid2; int status; pid1 = fork(); // 创建第一个子进程 if (pid1 == 0) { /* Child 1 */ printf(\u0026#34;Child 1 (PID: %d) running...\\n\u0026#34;, getpid()); sleep(2); printf(\u0026#34;Child 1 exiting.\\n\u0026#34;); exit(5); } pid2 = fork(); // 创建第二个子进程 if (pid2 == 0) { /* Child 2 */ printf(\u0026#34;Child 2 (PID: %d) running...\\n\u0026#34;, getpid()); sleep(4); printf(\u0026#34;Child 2 exiting.\\n\u0026#34;); exit(10); } // 父进程等待特定的子进程 pid1 printf(\u0026#34;Parent waiting for Child 1 (PID: %d)...\\n\u0026#34;, pid1); pid_t terminated_pid = waitpid(pid1, \u0026amp;status, 0); // 阻塞等待 pid1 if (terminated_pid == pid1 \u0026amp;\u0026amp; WIFEXITED(status)) { printf(\u0026#34;Parent: Child 1 terminated normally with status: %d\\n\u0026#34;, WEXITSTATUS(status)); } else { /* Handle error or abnormal termination */ } // 父进程等待另一个子进程 pid2 printf(\u0026#34;Parent waiting for Child 2 (PID: %d)...\\n\u0026#34;, pid2); terminated_pid = waitpid(pid2, \u0026amp;status, 0); // 阻塞等待 pid2 if (terminated_pid == pid2 \u0026amp;\u0026amp; WIFEXITED(status)) { printf(\u0026#34;Parent: Child 2 terminated normally with status: %d\\n\u0026#34;, WEXITSTATUS(status)); } else { /* Handle error or abnormal termination */ } printf(\u0026#34;Parent exiting.\\n\u0026#34;); return 0; } // 编译运行：gcc waitpid_example.c -o waitpid_example \u0026amp;\u0026amp; ./waitpid_example 如果在子进程终止后，父进程没有及时调用 wait() 或 waitpid()，那么这个子进程就会变成僵尸进程 (Zombie Process)。它虽然不再运行，但仍在进程表中占据一个位置，等待父进程回收。如果父进程先于子进程退出，子进程就成了孤儿进程 (Orphan Process)。为了避免孤儿进程变成无人认领的僵尸，Linux 会自动将它们的父进程设置为 init 进程（PID 1），由 init 进程负责回收它们。\n理解了孤儿进程和父进程回收机制后，就能明白一种常见的编程技巧——Double Fork。它的目的是创建一个与原始父进程完全脱离关系的后台进程（守护进程）。其步骤是：进程 A fork 出子进程 B，然后进程 A 立刻 waitpid() 等待 B 结束并退出；子进程 B 再次 fork 出孙子进程 C，然后 B 自己立刻退出；此时，孙子进程 C 成为孤儿，被 init 进程接管，从而与 A 彻底解耦。CRIU 在执行 Checkpoint 时，就运用了类似 double fork 的方法，让执行 criu dump 命令的进程脱离被操作的 JVM 进程树，避免了“自己冻结自己”的尴尬局面。\ndouble_fork 示例代码 (double_fork_example.c):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; int main() { pid_t pid1 = fork(); // 第一次 fork if (pid1 \u0026lt; 0) { perror(\u0026#34;First fork failed\u0026#34;); exit(EXIT_FAILURE); } else if (pid1 == 0) { // 第一个子进程 (进程 B) pid_t pid2 = fork(); // 第二次 fork if (pid2 \u0026lt; 0) { perror(\u0026#34;Second fork failed\u0026#34;); exit(EXIT_FAILURE); } else if (pid2 == 0) { // 孙子进程 (进程 C) printf(\u0026#34;Grandchild process (PID: %d, Parent PID: %d) starting.\\n\u0026#34;, getpid(), getppid()); // 通常在这里执行 setsid() 等守护进程化操作 sleep(10); // 模拟后台任务 printf(\u0026#34;Grandchild process finished.\\n\u0026#34;); exit(EXIT_SUCCESS); } else { // 第一个子进程 (进程 B) 立即退出 printf(\u0026#34;First child process (PID: %d) exiting immediately.\\n\u0026#34;, getpid()); exit(EXIT_SUCCESS); } } else { // 原始父进程 (进程 A) printf(\u0026#34;Original parent process (PID: %d) waiting for first child (PID: %d).\\n\u0026#34;, getpid(), pid1); waitpid(pid1, NULL, 0); // 等待第一个子进程退出 printf(\u0026#34;Original parent exiting.\\n\u0026#34;); // 孙子进程已成为孤儿并由 init 接管 } return 0; } // 编译运行：gcc double_fork_example.c -o double_fork_example \u0026amp;\u0026amp; ./double_fork_example 掌握了 Linux 进程和线程的生命周期管理，我们就能更好地理解 CRaC 是如何在这些基础上进行精确的状态捕获与恢复。接下来，我们将目光投向 Linux 提供的一个强大工具——/proc 文件系统，看看它如何帮助我们“透视”运行中进程的内部状态。\n# /proc 文件系统：内核状态的“透视镜” 我们已经了解了 Linux 如何创建和管理进程，但要实现像 CRaC 那样的“冻结”与“复苏”，就必须有办法深入探查一个正在运行的进程内部的详细状态。Linux 提供了一个非常强大的机制来做到这一点，那就是 /proc 文件系统。\n初看起来，/proc 像是一个普通的目录，你可以用 cd 进入，用 ls 查看。但它实际上是一个虚拟文件系统。这意味着它里面的文件和目录并不真正存储在磁盘上，而是由 Linux 内核在运行时动态生成的。/proc 是内核向用户空间（运行中的程序和用户）暴露其内部数据结构和系统信息的主要接口之一。你可以把它想象成一扇窗户，透过它，我们可以直接观察到内核管理下的系统状态，特别是运行中进程的实时信息。\n这对于 CRIU 和 CRaC 来说简直是无价之宝。当 CRIU 需要对一个进程执行 Checkpoint 时，它的大部分信息来源就是 /proc 文件系统。通过读取 /proc 下的特定文件，CRIU 能够获取到目标进程几乎所有的关键状态数据，从而构建出完整的进程快照。\n让我们来看看 /proc 下与进程相关的几个关键“情报站”，它们通常位于以进程 PID 命名的目录下，即 /proc/\u0026lt;pid\u0026gt;/：\n内存布局图 (/proc/\u0026lt;pid\u0026gt;/maps 和 smaps): 这两个文件揭示了进程的虚拟内存是如何组织的。maps 文件列出了进程的所有内存区域（称为 VMA，Virtual Memory Area），包括代码段、数据段、堆、栈以及内存映射的文件和共享库，标明了每个区域的起止地址和权限。smaps 文件则提供了更详细的信息，包括每个 VMA 实际占用的物理内存（RSS - Resident Set Size）、共享/私有内存量、脏页（Dirty pages）数量等。CRIU 通过解析它们来精确了解进程的内存结构，这是 Checkpoint 和 Restore 内存状态的基础。 打开的文件和连接 (/proc/\u0026lt;pid\u0026gt;/fd/ 和 fdinfo/): fd 是一个目录，里面包含了指向该进程当前打开的所有文件描述符的符号链接。链接的名称就是文件描述符的数字（如 0, 1, 2 分别代表标准输入、输出、错误），链接的目标则指明了它实际代表的文件、管道或套接字。而 fdinfo 目录下则包含了与 fd 中每个描述符对应的文件，记录了更详细的状态信息，比如文件的当前读写位置（offset）、打开时的标志位（flags）等。CRIU 需要读取这些信息来保存和恢复进程打开的文件状态。 进程状态报告 (/proc/\u0026lt;pid\u0026gt;/stat): 这个文件以一行文本的形式，提供了关于进程的大量状态信息，由空格分隔。包括进程名、状态（运行、睡眠、僵尸等）、父进程 PID、进程组 ID、使用的内存量、CPU 时间统计等等。CRIU 用它来获取进程的基本属性和运行统计。 线程成员列表 (/proc/\u0026lt;pid\u0026gt;/task/): 对于多线程进程，这个目录非常重要。它下面包含了以该进程下所有线程的 TID 命名的子目录。通过遍历这个目录，CRIU 可以识别出一个进程包含的所有线程（任务），并对每个线程进行单独的状态捕获。 子进程记录 (/proc/\u0026lt;pid\u0026gt;/task/\u0026lt;tid\u0026gt;/children): 这个文件（位于特定线程目录下）记录了由该线程直接创建的所有子进程的 PID 列表。通过递归地读取这个文件，CRIU 能够准确地构建出完整的进程树或进程家族。 内存映射文件访问 (/proc/\u0026lt;pid\u0026gt;/map_files/): 这个目录包含了指向进程内存中通过文件映射（mmap）方式加载的实际文件的符号链接。链接的名称对应 maps 文件中的地址范围。这为 CRIU 提供了一种可靠的方式来访问和读取这些映射文件的内容。 通过组合利用 /proc 文件系统提供的这些（以及其他未列出的）信息源，CRIU 能够像侦探一样，细致入微地收集目标进程及其所有线程、子进程在 Checkpoint 时刻的完整状态。没有 /proc 这个强大的“透视镜”，实现用户空间的 Checkpoint/Restore 将会困难得多。\n了解了如何通过 /proc 获取进程状态后，我们接下来将关注进程运行的“舞台”——内存管理，以及进程如何与外部世界交互的“管道”——文件描述符。\n# ptrace 系统调用：掌控进程的“遥控器” 通过 /proc 文件系统，我们获得了观察运行中进程内部状态的强大能力，就像有了一副“透视镜”。但这还不够，要实现像 CRaC/CRIU 的 Checkpoint/Restore，我们不仅需要观察，还需要能够控制和操纵目标进程——在合适的时机让它暂停，读取甚至修改它的内存和寄存器状态，甚至强制它执行某些操作。这时，Linux 提供了一个终极武器，也是一个颇具争议但极其强大的系统调用：ptrace (process trace)。\nptrace 提供了一种机制，允许一个进程（称为 tracer，追踪者）去观察和控制另一个进程（称为 tracee，被追踪者）的执行，检查和改变 tracee 的内存和寄存器。你可以把它想象成一个赋予 tracer 进程的“远程控制器”，可以用来遥控 tracee 进程的一举一动。\n这个系统调用是许多底层工具的基石：\n调试器 (Debuggers)：像 GDB 这样的调试器，其核心功能（设置断点、单步执行、检查变量值、查看调用栈等）几乎完全依赖于 ptrace。调试器就是一个 tracer 进程，而被调试的程序就是 tracee。 系统调用追踪工具 (System Call Tracers)：strace 命令能够显示一个进程执行的所有系统调用及其参数和返回值，这也是通过 ptrace 实现的。 CRIU (Checkpoint/Restore In Userspace)：正如我们之前提到的，CRIU 大量使用 ptrace 来完成那些仅靠 /proc 无法完成的任务，比如精确地暂停进程、获取和恢复寄存器状态、注入“寄生代码”等。 ptrace 本身是一个非常复杂的系统调用，它的行为由传递给它的第一个参数 request 决定。下面我们介绍一些它提供的关键能力，这些能力对于理解 CRIU 的工作至关重要：\n建立追踪关系 (Attaching)： PTRACE_ATTACH 或 PTRACE_SEIZE：这是 tracer 控制 tracee 的第一步。Tracer 使用这两个请求之一来“附着”到目标 tracee 进程上。一旦附着成功，tracee 就会暂停下来，并且其状态变化（如收到信号、执行系统调用）都会通知 tracer。PTRACE_SEIZE 是一个较新的、更推荐的方式，它避免了 PTRACE_ATTACH 中使用 SIGSTOP 信号可能带来的副作用，控制更为精确。 读写寄存器 (Reading/Writing Registers)： PTRACE_GETREGS, PTRACE_GETFPREGS 等：允许 tracer 读取 tracee 当前的通用寄存器（如指令指针 EIP/RIP、栈指针 ESP/RSP 等）、浮点寄存器等 CPU 状态。 PTRACE_SETREGS, PTRACE_SETFPREGS 等：允许 tracer 修改 tracee 的寄存器状态。这对于恢复进程到某个精确的执行点至关重要，CRIU 在 Restore 阶段就需要用它来设置好恢复后进程的 CPU 上下文。 读写内存 (Reading/Writing Memory)： PTRACE_PEEKDATA, PTRACE_PEEKTEXT：允许 tracer 读取 tracee 进程地址空间中任意位置的数据（通常以字长为单位）。 PTRACE_POKEDATA, PTRACE_POKETEXT：允许 tracer 向 tracee 进程地址空间中任意位置写入数据。CRIU 正是利用这个能力，在 Checkpoint 阶段向目标进程注入“寄生代码”（一段帮助收集内部信息的二进制代码），并在 Restore 阶段将快照中的内存数据写回进程空间。 控制执行 (Controlling Execution)： PTRACE_CONT：让暂停的 tracee 继续执行。可以选择是否传递一个信号给 tracee。 PTRACE_SYSCALL：让 tracee 继续执行，直到它进入或退出下一个系统调用时再次暂停，并通知 tracer。strace 就是基于此工作的。CRIU 也用它来精确控制目标进程执行特定的系统调用（如 mmap, munmap）来辅助内存的 Checkpoint 和 Restore。 PTRACE_SINGLESTEP：让 tracee 执行一条机器指令，然后再次暂停。这是调试器实现单步执行的基础。 解除追踪关系 (Detaching)： PTRACE_DETACH：Tracer 结束对 tracee 的追踪。Tracee 会恢复正常执行，就像从未被追踪过一样（除非 tracer 修改了它的状态）。 通过这些强大的（甚至可以说是危险的）能力，ptrace 赋予了 CRIU 超越普通进程权限的操作能力。它不仅能通过 /proc 看到进程的状态，更能像外科医生一样，精确地暂停进程、检查和修改其内部状态（内存和寄存器），甚至“借用”目标进程的上下文来执行特定操作（如注入代码、强制执行系统调用）。正是 ptrace 的存在，使得在用户空间实现复杂且精确的进程 Checkpoint/Restore 成为可能，也间接支撑了 CRaC 技术的实现。\n当然，ptrace 的强大也意味着潜在的风险，操作系统通常会对其使用施加一些安全限制（例如，一个普通用户进程不能随意 ptrace 其他用户的进程或特权进程）。\n理解了 ptrace 的核心能力后，我们对 CRIU 如何完成那些看似不可能的任务，应该有了更深的体会。接下来，我们将把前面介绍的知识点串联起来，看看 CRIU 具体是如何一步步实现 Checkpoint 和 Restore 的。\n# CRIU 如何实现 Checkpoint 和 Restore？ 现在，我们已经了解了 Linux 的进程线程模型、强大的 /proc 文件系统以及拥有“遥控”能力的 ptrace 系统调用。是时候将这些知识点串联起来，看看 CRIU (Checkpoint/Restore In Userspace) 是如何利用它们来施展“冻结” (Checkpoint) 和“复苏” (Restore) 进程的魔法了。\n# Checkpoint (冻结过程)：为进程拍下精确快照 当 CRIU 被要求对一个进程（及其后代）进行 Checkpoint 时，它会执行一系列精心设计的步骤：\n识别目标家族: 首先，CRIU 需要确定要冻结的完整目标。它从用户指定的根进程 PID 开始，通过递归地读取 /proc/\u0026lt;pid\u0026gt;/task/\u0026lt;tid\u0026gt;/children 文件，像剥洋葱一样，找出所有相关的子进程和线程，构建出完整的进程树。 全体“立正”: 接下来，CRIU 需要让这个庞大家族的所有成员都暂停下来。它使用 ptrace(PTRACE_SEIZE, \u0026hellip;) 附着到进程树中的每一个任务（进程/线程）上。PTRACE_SEIZE 会让这些任务在下一次内核有机会介入时（比如系统调用或中断）进入暂停状态，并且这种暂停方式比老的 PTRACE_ATTACH 更为干净，不依赖 SIGSTOP 信号。 信息大搜集: 进程树被冻结后，CRIU 开始扮演“情报员”的角色，通过 /proc 文件系统和 ptrace 收集每个任务的详细状态： 内存布局: 解析 /proc/\u0026lt;pid\u0026gt;/maps 和 smaps 获取虚拟内存区域（VMA）的地址、大小、权限、映射来源（文件或匿名）等信息。 文件描述符: 读取 /proc/\u0026lt;pid\u0026gt;/fd/ 和 fdinfo/ 目录，记录下所有打开的文件、管道、套接字及其类型、路径、当前读写位置、标志位等状态。 进程/线程核心状态: 通过 /proc/\u0026lt;pid\u0026gt;/stat 获取进程的基本属性，更重要的是，使用 ptrace(PTRACE_GETREGS, \u0026hellip;) 和 PTRACE_GETFPREGS 等命令，直接读取每个任务暂停时的CPU 寄存器内容（包括指令指针、栈指针、通用寄存器等）。这是确保恢复后能从正确位置继续执行的关键。 其他资源: 收集如信号处理器设置、定时器、凭证（UID/GID）、命名空间隶属关系等信息。 内存内容转储 (可能需要“寄生虫”帮忙): 获取内存布局只是第一步，还需要把这些内存区域里的实际数据保存下来。对于大部分内存区域，CRIU 可以通过 /proc/\u0026lt;pid\u0026gt;/mem 文件或者 process_vm_readv 系统调用来读取。但对于某些特殊或私有的内存区域，或者为了获取某些无法从外部探测的内部状态（如精确的文件描述符状态），直接读取可能受限或效率不高。这时，CRIU 会祭出它的“杀手锏”——寄生代码 (Parasite Code)。 CRIU 使用 ptrace 在目标进程的地址空间中分配一小块内存（通过强制目标进程执行 mmap 系统调用）。 然后，使用 ptrace(PTRACE_POKEDATA, \u0026hellip;) 将一段预先编译好的、与位置无关的（PIE）二进制代码（寄生代码）写入这块内存。 最后，通过 ptrace 修改目标进程的指令指针，让它跳转执行这段寄生代码。 寄生代码运行在目标进程的上下文中，拥有访问其所有资源的权限，可以高效地完成内存转储、收集内部信息等任务，并将结果传递给 CRIU。任务完成后，寄生代码通过 rt_sigreturn 系统调用恢复目标进程之前的寄存器状态，CRIU 再强制目标进程执行 munmap 清理掉寄生代码占用的内存，最后 ptrace(PTRACE_DETACH, \u0026hellip;) 脱离，整个过程对目标进程来说几乎是“无痕”的。 写入镜像文件: CRIU 将收集到的所有状态信息（内存布局、寄存器、文件描述符状态、内存数据等）组织起来，写入到磁盘上的一系列镜像文件中。这些文件共同构成了进程在 Checkpoint 时刻的完整快照。 (可选) 终止原进程: 在某些场景下（比如 CRaC 的默认行为），Checkpoint 完成后，原始的 JVM 进程会被 CRIU 终止。 # Restore (复苏过程)：从快照重建鲜活进程 Restore 过程可以看作是 Checkpoint 的逆操作，它更加复杂，需要精确地重建进程状态：\n解析镜像，规划蓝图: CRIU 首先读取 Checkpoint 生成的镜像文件，分析进程间的关系（父子、共享资源等），制定恢复计划。 搭建骨架: CRIU 严格按照镜像中记录的进程树结构，通过多次调用 fork() 来创建新的进程。在这个阶段，通常只创建进程的主线程。 恢复基本资源: 对于每个新创建的进程，CRIU 开始恢复大部分状态： 文件描述符: 根据镜像信息重新打开文件（可能需要验证路径有效性）、创建管道和套接字，并设置好它们的状态（如文件偏移量）。对于共享的文件描述符，需要确保它们指向同一个内核对象（可能用到 SCM_RIGHTS 等技术）。 内存映射 (初步): 使用 mmap() 根据镜像中的 VMA 信息创建内存区域。对于私有内存，会先映射匿名内存，稍后再填充数据；对于文件映射，会重新映射相应的文件。此时映射的虚拟地址可能还不是最终的目标地址。 命名空间: 如果进程使用了非默认的命名空间，CRIU 会负责创建或加入这些命名空间。 其他: 恢复工作目录、根目录、信号处理器等。 关键步骤：切换上下文与精细恢复: 这是 Restore 中最精妙也最困难的部分。因为执行 Restore 操作的 CRIU 代码本身可能就位于需要被恢复内容覆盖的内存区域。为了解决这个问题： CRIU 会找到一块临时的、安全的内存“空地”，加载一小段自包含的、位置无关的恢复器代码 (Restorer Context)。 然后，通过一次跳转，将 CPU 的控制权交给这段恢复器代码。 在恢复器代码的控制下，进行最后也是最关键的恢复步骤： 精确内存布局: 使用 mremap() 将之前映射在临时地址的内存移动到镜像中记录的最终虚拟地址。使用 mmap() 在正确的位置创建文件映射和共享内存映射。至此，进程的内存布局与 Checkpoint 时完全一致。 填充内存数据: 将镜像文件中保存的内存页数据，通过 read() 或类似方式写回到相应的内存区域。 恢复线程: 在最终的内存布局中，根据保存的状态创建并恢复进程的所有其他线程。 恢复寄存器: 使用 ptrace(PTRACE_SETREGS, \u0026hellip;)（或者在恢复器代码内部通过特定机制）将每个线程的 CPU 寄存器（特别是指令指针 IP/PC 和栈指针 SP）精确地设置为 Checkpoint 时保存的值。 恢复其他细节: 恢复定时器、凭证等。 “点火”启动: 当所有状态都恢复完毕，恢复器代码会执行最后一步——通常是一个特殊的返回或跳转指令，将 CPU 的控制权彻底交还给恢复后的进程（主线程）。由于指令指针已经被精确设置，进程会从 Checkpoint 时被中断的那条指令无缝地继续执行，仿佛从未被打断过。 管理 Restore 流程 (execv 链): 前面提到，CRaC 的 java -XX:CRaCRestoreFrom=\u0026hellip; 命令启动后，会通过 criuengine 这个辅助程序来协调 Restore。这个过程涉及多次 execv 调用：初始 Java 命令 execv 变成 criuengine restore，后者再 execv 变成 criu restore 来执行真正的恢复操作。当 criu restore 成功恢复目标 JVM 进程后，它会再次 execv 变成 criuengine restorewait，这个最终的进程负责等待恢复后的 JVM 进程结束，并将 JVM 的退出状态传递回去。 通过这一系列复杂而精密的步骤，结合对 /proc 的读取和对 ptrace 的深度运用，CRIU 实现了在用户空间对运行中进程进行快照和恢复的强大能力，为 CRaC 技术的实现奠定了坚实的基础。\n理解了 CRIU 的基本工作原理后，我们就能更好地理解 CRaC 为何还需要一个“协调”层。接下来，我们将探讨 CRaC 的 Resource API 存在的意义。\n# CRaC 如何指挥 CRIU 我们已经了解了 CRIU 如何利用 Linux 的底层机制来实现进程的 Checkpoint 和 Restore。但 CRaC (Coordinated Restore at Checkpoint) 本身并不直接执行这些复杂的底层操作。相反，CRaC 更像是一个指挥官，它通过协调 JVM 内部状态和外部资源，并在恰当的时机调用 CRIU 来完成实际的“冻结”与“复苏”工作。这种调用通常是通过一个辅助程序（在 OpenJDK CRaC 实现中称为 criuengine）来间接完成的。这个过程中，Linux 的进程创建、替换和管理技术，特别是 fork、execv 和 double fork，扮演了至关重要的角色。\n# Checkpoint 流程中的进程之舞 (double fork) 当用户通过 jcmd \u0026lt;pid\u0026gt; JDK.checkpoint 命令触发 CRaC 的 Checkpoint 时，一场精心编排的进程交互就开始了：\nJVM 内部准备: JVM 接收到命令后，会执行一些准备工作，比如触发一次 Full GC 来减小镜像体积，然后进入一个全局安全点（Safepoint），暂停所有 Java 线程。 启动外部引擎: JVM 调用 fork() 创建一个子进程 (我们称之为 P1)，这个子进程 P1 的任务是执行 criuengine checkpoint 命令。JVM 主进程则会暂停，等待 P1 的某种形式的完成信号或退出。 double fork 登场: 这里的关键在于 criuengine (P1) 如何调用 criu dump 来冻结原始的 JVM 进程。如果 P1 直接调用 criu dump，那么 criu 进程就会是 JVM 的孙子进程，仍然属于同一个进程组，这在某些情况下可能导致问题（比如尝试冻结自己所在的进程组）。为了彻底解耦，criuengine 使用了 double fork 技巧： P1 (criuengine checkpoint) 调用 fork() 创建子进程 P2。然后 P1 会等待 P2 退出。 P2 再次调用 fork() 创建孙子进程 P3。 P2 立即退出 (exit())。 P1 检测到 P2 退出后，P1 也退出。 此时，P3 成为了孤儿进程，其父进程被系统 init 进程（PID 1）接管。P3 现在与原始的 JVM 进程（它的“曾祖父”）在进程树上已经没有直接关系了。 执行冻结: 成为孤儿的 P3 进程现在可以安全地执行 criu dump -t \u0026lt;jvm_pid\u0026gt; \u0026hellip; 命令，目标直指原始的、正在等待的 JVM 进程。criu 利用我们之前讨论的 /proc 和 ptrace 技术，将 JVM 的完整状态保存到镜像文件中。 终结与等待: criu dump 在成功创建镜像后，通常会杀死 (kill) 被冻结的原始 JVM 进程。而原始 JVM 进程在 fork 出 P1 后，实际上并没有完全阻塞，它会继续执行一小段代码，通常是进入一个 sigwaitinfo() 调用，等待一个特定的信号（RESTORE_SIGNAL），这个信号只有在未来的 Restore 过程中才会被发送。但在此之前，它就被 criu dump 结束了生命。 通过 double fork，CRaC 巧妙地确保了执行冻结操作的 criu 进程独立于被冻结的 JVM 进程树之外，保证了 Checkpoint 操作的干净和可靠。\n# Restore 流程中的进程“变身” (execv 链) Restore 过程则展示了 execv 系统调用的威力，它允许一个进程用一个全新的程序映像替换自己，实现“原地变身”：\n启动 Restore 命令: 用户执行 java -XX:CRaCRestoreFrom=\u0026lt;checkpoint_dir\u0026gt;。 JVM 的“改道”: 这个 Java 命令启动的 JVM 进程（我们称之为 P1）在非常早期的初始化阶段，就会检测到 -XX:CRaCRestoreFrom 参数。它不会继续执行标准的 JVM 启动流程，而是立即“改道”。 第一次变身 (execv): P1 调用 execv()，将其自身替换为 criuengine restore 程序。此时，原来的 java 进程 P1 不复存在，取而代之的是运行着 criuengine restore 代码的进程（我们称之为 P2，尽管 PID 可能与 P1 相同）。 第二次变身 (execv): P2 (criuengine restore) 负责解析参数，准备好调用 criu 所需的环境，然后再次调用 execv()，将自身替换为 criu restore 程序（我们称之为 P3）。 CRIU 执行恢复: P3 (criu restore) 读取镜像文件，利用 fork、mmap、ptrace 等技术，在内存中逐步重建 JVM 进程的状态。这个恢复过程可能相当复杂，涉及创建新的进程（恢复后的 JVM），设置内存，恢复文件描述符，恢复线程等。 唤醒与交接: 在恢复的目标 JVM 进程状态基本就绪，但尚未开始执行用户代码时，P3 (criu restore) 会通过其配置的动作脚本（通常是 criuengine 自身）向恢复后的 JVM 进程发送一个特定的信号（如 RESTORE_SIGNAL），这个信号会唤醒 JVM 内部等待的代码（还记得 Checkpoint 最后 JVM 等待的 sigwaitinfo 吗？恢复后的 JVM 就从这里“醒来”）。 第三次变身 (execv): 在成功恢复 JVM 并发送唤醒信号后，P3 (criu restore) 进程的任务也即将结束。根据启动时通过 --exec-cmd 参数的指示，它会执行最后一次 execv()，将自身替换为 criuengine restorewait 程序（我们称之为 P4）。 守望者 (waitpid): P4 (criuengine restorewait) 的唯一使命就是扮演一个“守望者”。它知道刚刚恢复的 JVM 进程的 PID，然后调用 waitpid() 等待这个 JVM 进程结束。当 JVM 最终退出时，P4 会获取其退出状态，并以相同的状态退出。这样，最初启动 java -XX:CRaCRestoreFrom=\u0026hellip; 命令的用户就能得到恢复后 JVM 的最终执行结果。 在这个 execv 调用链中，控制权被平滑地从初始的 Java 命令传递给 criuengine，再到 criu 本身，最后交接给负责等待的 criuengine restorewait。整个过程中，进程的身份（执行的程序）不断变化，但通常是在同一个进程 ID 下完成（除了 criu restore 内部重建 JVM 时会创建新进程），高效地利用了现有的进程上下文来执行不同的任务阶段。\n总结来说，CRaC 并非魔法，而是建立在对 Linux 进程生命周期管理的深刻理解和巧妙运用之上。它通过辅助程序，精确地编排 fork、execv、waitpid 等系统调用，指挥 CRIU 这位“底层大师”完成复杂的 Checkpoint 和 Restore 操作，最终实现了 Java 应用启动性能的巨大飞跃。\n# 总结：Linux 系统编程是 CRaC 的基石 回顾全文，我们一起探索了 CRaC 技术背后所依赖的关键 Linux 系统编程概念。从基本的进程与线程模型、生命周期管理（fork, execv, waitpid, double fork），到强大的进程状态“透视镜” /proc 文件系统，再到能够精细控制进程的“遥控器”ptrace 系统调用，这些都是 Linux 提供的底层能力。\n我们看到，CRIU 正是巧妙地组合运用了这些机制，才得以在用户空间实现对运行中进程进行精确 Checkpoint 和 Restore 的复杂操作。而 CRaC 则更进一步，通过协调 JVM 内部状态和外部资源，并指挥 CRIU 完成核心的冻结与复苏任务，最终达成了大幅优化 Java 应用启动性能的目标。\n因此，理解这些 Linux 系统编程的知识，不仅能帮助我们揭开 CRaC 实现原理的神秘面纱，更能让我们体会到现代软件技术创新往往是建立在对底层系统深刻理解和创造性应用的基础之上。希望本文能为您打开一扇通往 Linux 系统编程世界的小窗，激发您进一步探索的兴趣。\n","date":"2025-04-30T10:18:47+08:00","permalink":"https://mazhen.tech/p/%E7%90%86%E8%A7%A3-crac-%E8%83%8C%E5%90%8E%E7%9A%84-linux-%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B/","title":"理解 CRaC 背后的 Linux 系统编程"},{"content":" # 引言 在现代软件架构中，尤其是在微服务和 Serverless 计算领域，应用的快速启动和高效资源利用变得至关重要。Java 作为企业级应用开发的首选语言，尽管拥有庞大的生态和丰富的开发工具，但在启动速度方面始终面临挑战。与原生编译语言和部分脚本语言相比，Java 应用在启动过程中存在明显的性能差距。“冷启动”，即应用实例首次启动，或是从休眠状态唤醒时，系统需要经历复杂的初始化过程，这不仅会导致显著的时间延迟，还会引发资源消耗的峰值。\n那么，为什么 Java 应用的冷启动会比较慢呢？这并不仅仅是一个单一的步骤，而是涉及多个阶段，通常可以分为“JVM 启动”、“应用启动”和“应用预热”三个主要过程：\nJVM 启动 (JVM Start - 相对较快): 这个阶段主要是 Java 虚拟机（JVM）自身的初始化，包括加载核心库、初始化内存管理（如堆、垃圾收集器）、设置内部数据结构以及进行一些早期的基础优化。 应用启动 (Application Start - 需要一些时间): 在 JVM 准备好之后，开始加载应用程序自身的类和所有依赖库。这是一个密集的过程，涉及查找、验证和解析大量的类。紧接着，应用框架（如 Spring Boot）会执行其初始化逻辑：扫描组件、解析配置、构建依赖注入容器、初始化线程池、建立数据库连接等。这个阶段完成后，应用通常可以处理第一个请求，因此这个阶段的耗时常被称为“首次响应时间（Time to first response）”。 应用预热 (Application Warmup - 需要较长时间): 即使应用能够响应第一个请求，它通常还远未达到最佳性能。Java 的高性能依赖于即时编译器 JIT（Just-In-Time Compiler）。JIT 会在运行时监控代码执行情况，识别“热点（hotspot）”代码（频繁执行的方法），并将其编译成本地机器码以提升效率。这个编译过程是分层的（例如，从解释执行到 C1 编译，再到更深层次优化的 C2 编译），需要时间和实际的业务负载来触发和完成。此外，还可能涉及缓存的填充、连接池的预热等。只有在 JIT 完成了关键代码的编译优化，并且应用处理了一定量的请求后，才能达到其峰值性能。这个过程被称为“应用预热”。期间还可能伴随着编译/反优化（Deoptimisations）和垃圾回收暂停（Garbage Collector pauses）带来的性能波动。 这三个阶段叠加起来，特别是“应用启动”和“应用预热”阶段的耗时，导致了 Java 应用的冷启动时间较长，并且需要一段时间才能达到理想的处理能力。对于需要快速响应和弹性伸缩的微服务和 Serverless 场景，这种延迟是亟待解决的痛点。缓慢的启动影响扩容效率，而漫长的预热则意味着在达到峰值性能前，应用的处理能力受限且响应时间不稳定。\n正是为了解决这一痛点，CRaC (Coordinated Restore at Checkpoint) 技术应运而生。它并非逐一优化上述启动和预热的各个环节，而是另辟蹊径：通过在应用程序完成初始化并充分“预热”达到接近峰值性能状态后，创建一个包含整个 JVM 进程状态的“快照”（Checkpoint），并在需要新实例时直接从这个快照快速“恢复”（Restore）。这种方式几乎完全绕过了耗时的“应用启动”和“应用预热”过程，有望将启动并达到高性能状态的时间缩短到毫秒级别。\n本文旨在深入探讨 CRaC 技术，从其核心原理、与 AOT 等技术的对比、具体实现机制、实际使用方法，到其在不同场景下的应用和生态发展，全面解析这一旨在革新 Java 启动与预热性能的前沿技术。\n# 一、CRaC 概述：告别漫长等待 # 什么是 CRaC？ CRaC（Coordinated Restore at Checkpoint）是 OpenJDK 的一个项目，旨在通过创新的方式显著缩短 Java 应用的启动时间。CRaC 的核心思想是：在应用程序运行到某个理想状态（通常是初始化完成并经过预热后）时，创建一个包含整个 JVM 进程内存状态（包括堆内存、已加载类、已编译代码、线程状态等）的快照，即“检查点”（Checkpoint），并将其持久化。当需要启动新实例时，不再执行传统的启动流程，而是直接从该快照“恢复”（Restore）JVM 状态。这个恢复过程跳过了大部分 JVM 初始化、类加载和应用初始化步骤，从而极大地加快了启动速度。其中，“协调”（Coordinated）是关键，意味着 JVM 需要与应用程序通过特定 API 进行交互，以确保在创建检查点和恢复时，外部资源（如文件句柄、网络连接）能够被妥善地关闭和重新建立。\n# CRaC 的核心优势 极速启动： 这是 CRaC 最显著的优势。传统 Java 应用启动可能需要数秒甚至数十秒，而使用 CRaC 从快照恢复，可以将启动时间缩短到数百毫秒甚至更短，接近原生应用的启动速度。因为它直接跳过了 JVM 初始化、类加载、应用初始化和大部分 JIT 预热等耗时环节。 即时峰值性能： 由于 Checkpoint 通常是在应用已经完成 JIT 编译优化和预热后创建的，因此恢复后的实例几乎可以立即达到其最佳性能状态，避免了传统启动后漫长的“预热”等待期。这对于需要快速响应请求的场景（如 Serverless）尤其重要。 潜在的资源节约： 传统的启动和预热过程通常是 CPU 密集型的。通过 CRaC，这些密集的计算被转移到了 Checkpoint 创建阶段（通常在非高峰时段或构建过程中完成），而在实际需要启动新实例时（如服务扩容或函数调用时），资源消耗显著降低，有助于提高资源利用率和降低成本。 # CRaC 的关键概念 Checkpoint (检查点): 指创建 JVM 进程状态快照的操作，以及生成的包含该状态的持久化文件或镜像。 Restore (恢复): 指从一个已存在的 Checkpoint 快速加载 JVM 状态，启动一个新 JVM 实例的过程。 Coordination (协调): 指 JVM 与应用程序之间通过特定 API（jdk.crac 包）进行的交互。应用程序需要实现接口来管理其资源（如关闭网络连接、文件句柄等）以确保 Checkpoint 的一致性，并在 Restore 后重新建立这些资源。这是保证恢复后的应用能正常工作的关键。 # 二、CRaC vs. AOT：启动优化的两条路径 为了解决 Java 启动慢和预热长的问题，业界探索了不同的优化路径。除了 CRaC，另一种广受关注的技术是 AOT（Ahead-of-Time）编译，特别是以 GraalVM Native Image 为代表的实现。两者都旨在缩短启动时间，但它们的原理和特性却大相径庭。\n# AOT 技术简介 AOT 编译的核心思想是，在应用程序运行之前，就将其 Java 字节码直接编译成本地机器码，生成一个独立的可执行文件（例如 GraalVM Native Image）。这样做带来的主要优势是：\n无需解释字节码 (No interpreting bytecodes): 启动时直接执行本地代码，跳过了 JVM 解释执行字节码的阶段。 无需运行时编译 (No runtime compilation of code): 消除了 JIT 在运行时编译代码带来的 CPU 开销。 启动即全速 (Start at \u0026lsquo;full speed\u0026rsquo;, straight away): 应用启动后几乎立刻就能达到其稳定性能状态（尽管这个稳定状态可能不是最高峰值），大大缩短了“首次响应时间”。 更小的内存占用： 生成的本地可执行文件不包含 JVM 和 JIT 编译器，运行时内存占用通常显著低于标准 JVM。 # AOT 的挑战与局限 然而，AOT 并非完美无缺，它也面临一些固有的挑战：\n静态编译的本质 (AOT is, by definition, static) AOT 的本质是代码在运行之前就被编译。这意味着编译器无法获知代码在运行时的实际行为 。AOT 不能像 JIT 那样根据运行时的真实负载和代码路径进行深度优化。\nProfile Guided Optimization (PGO) 的引入与局限 为了缓解静态编译缺乏运行时信息的缺点，AOT 可以结合 Profile Guided Optimization (PGO)。\nPGO 的基本思路是：先通过插桩（Instrumentation）或者采样的方式运行一次程序，收集代码执行频率、分支跳转等信息，生成 Profile 数据（例如 GCC 中使用 -fprofile-generate 编译运行以生成 profile 文件）；然后，在最终编译时将这些 Profile 数据提供给编译器（例如 GCC 使用 -fprofile-use），让编译器根据这些“先验知识”进行更针对性的优化，比如更好地安排代码布局、更准确地进行分支预测、更有效地进行函数内联等。\n然而，对于 AOT 编译来说，PGO 只能部分缓解问题 (can partially help)。因为收集到的 Profile 数据可能只代表了某一次或某几次运行的特征，无法完全覆盖所有可能的运行时场景和输入数据。因此，基于 PGO 的 AOT 优化效果通常仍难以媲美 JIT 的动态优化能力。\n兼容性问题 AOT 对 Java 的动态特性（如反射、动态代理、运行时字节码生成）支持有限，通常需要额外的配置或代码调整，并非所有 Java 库都能直接兼容。 # JIT 的优势：运行时动态优化的威力 与 AOT 的静态编译不同，JIT 编译是在程序运行期间进行的。这赋予了 JIT 编译器独特的优势，使其能够进行比 AOT 更深层次、更精准的优化。\n基于真实运行情况的优化： JIT 编译器可以观察到代码实际的运行路径、热点方法、分支跳转频率、数据类型分布等信息。基于这些动态收集的信息，JIT 可以做出更明智的优化决策。 激进的优化策略： JIT 可以采用更激进的优化手段，例如： 方法内联 (Method Inlining): 将调用频繁的小方法直接嵌入到调用处，消除方法调用的开销。JIT 可以根据实际调用情况决定是否内联以及内联的深度。 逃逸分析 (Escape Analysis): 分析对象的作用域，如果一个对象只在方法内部使用，不会“逃逸”出去，JIT 可以将其分配在栈上而不是堆上，减轻 GC 压力，甚至进行锁消除。 投机性优化 (Speculative Optimizations): JIT 可以根据观察到的高概率事件进行优化（例如，假设某个类型检查总是成功），并准备好在假设失败时回退到较慢的代码路径（Deoptimization）。AOT 通常无法承担这种风险。 针对特定环境的优化： JIT 编译器知道程序当前运行的 CPU 架构（例如 Haswell, Skylake, Ice Lake 等），可以生成针对该特定 CPU 指令集优化的机器码，最大化硬件性能。AOT 为了通用性，通常只能编译为“最小公分母”的指令集。 支持 Java 动态特性： JIT 天然与 Java 的动态特性（如反射、运行时字节码生成）协同工作，这些特性对于 AOT 来说往往是难点。 正是由于这些基于运行时信息的动态优化能力，经过充分预热的 JIT 代码通常能够达到比 AOT 代码更高的峰值性能。\n# JIT 的主要缺点 尽管 JIT 在峰值性能上有优势，但其缺点也显而易见，这正是 AOT 试图解决的问题。\n启动时间长 (Requires more time to start up): JIT 需要经历 JVM 启动、类加载、解释执行、热点分析、代码编译等多个慢速操作后，才能达到较快的执行速度。 运行时编译开销 (CPU overhead to compile code at runtime): JIT 编译本身需要消耗 CPU 资源。 内存占用大 (Larger memory footprint): JVM、JIT 编译器、性能分析数据等都需要占用额外的内存。 # AOT vs. JIT 对比总结 特性 AOT (Ahead-of-Time) JIT (Just-In-Time) 编译时机 运行前 (静态) 运行时 (动态) 启动速度 快 (Time to first response 短) 慢 (需要 JVM 初始化、类加载、解释执行) 预热时间 短 (几乎无预热) 长 (需要识别热点、分层编译优化) 峰值性能 通常较低 (缺乏运行时信息和动态优化) 通常较高 (可进行激进优化、针对性优化) 内存占用 小 大 (包含 JVM、JIT 编译器、分析数据等) 动态特性支持 有限 (需配置或改造) 良好 (Java 核心优势之一) 运行时开销 低 (无编译开销) 有 (编译 CPU 开销) 兼容性 挑战较大 好 优化依据 静态分析 (+ 有限的 PGO) 运行时真实行为 # CRaC 的切入点 理解了 AOT 和 JIT 各自的优劣后，CRaC 的价值就更加清晰了。\nCRaC 试图结合两者的优点，规避其缺点：它保留了 JIT 带来的峰值性能优势（因为 Checkpoint 是在 JIT 充分预热后创建的），同时通过状态恢复的方式，避免了 JIT 漫长的启动和预热过程，实现了类似 AOT 的快速启动（特别是达到峰值性能的速度）。与 AOT 相比，CRaC 对 Java 动态特性的兼容性更好。\n因此，CRaC、AOT 和传统的 JIT 代表了 Java 性能优化的不同策略，适用于不同的场景和需求。\n# 三、实现原理：深入 CRaC 的心脏 CRaC 技术并非空中楼阁，它的实现依赖于一个强大的 Linux 工具：CRIU。虽然 CRIU 是一个运行在用户空间的程序，但它的核心能力建立在 Linux 内核提供的丰富特性和接口之上。理解 CRIU 的工作原理对于深入掌握 CRaC 至关重要。\n# 基石：CRIU CRIU (Checkpoint/Restore In Userspace) 是一个 Linux 用户空间的工具，它允许你“冻结”（Checkpoint）一个正在运行的应用程序（或一组应用程序），将其状态保存到磁盘文件中，然后在未来的某个时刻从这些文件中“解冻”（Restore）它。被恢复的应用程序将从被冻结的那个精确时刻继续运行，仿佛什么都没有发生过一样。CRIU 的核心能力是保存和恢复进程的各种资源状态。\n# Checkpoint 过程详解 Checkpoint 阶段会执行以下步骤。\n收集进程树并冻结 CRIU 首先需要确定要 Checkpoint 的目标进程及其所有子进程和线程，构成一个完整的进程树。\n/proc 是一个虚拟文件系统，它并不存在于磁盘上，而是由 Linux 内核动态生成，用来提供有关系统状态和正在运行的进程的信息。对于每个正在运行的进程，/proc 下都有一个以该进程的 PID（Process ID）命名的目录。\n在一个进程内部，可能有一个或多个线程。在 Linux 内核看来，线程本质上也是一种“任务”（Task），它们共享同一个地址空间和其他资源，但有自己独立的执行流和调度标识符（TID, Thread ID）。/proc/$pid/task/ 这个目录就包含了该进程（PID 为 $pid）下的所有线程（任务）的信息。该目录下会为每个线程创建一个子目录，目录名就是该线程的 TID。\n通过读取 /proc/$pid/task/ 目录的内容，CRIU 可以识别出属于进程 $pid 的所有线程（包括主线程和其他子线程），获取它们的 TID 列表。\n/proc/$pid/task/$tid/children 文件位于特定线程的目录下，记录了由 这个特定线程（PID 为 $pid，TID 为 $tid）直接创建 的所有子进程的 PID 列表。子进程是由 fork() 或 clone() 系统调用创建的。这个 children 文件告诉我们，从这个线程出发，诞生了哪些新的进程。\nCRIU 从 --tree 选项指定的那个初始 PID 开始，首先通过 /proc/$pid/task/ 找到一个进程的所有线程，然后通过 /proc/$pid/task/$tid/children 找到每个线程创建的子进程，再对这些子进程重复同样的操作，一层层深入下去，最终像剥洋葱一样把整个进程家族（包括所有进程和线程）都识别出来。\n在遍历过程中，CRIU 使用 ptrace 系统调用，命令为 PTRACE_SEIZE，来附加（attach）到目标进程树中的每个任务（进程/线程）上，并将它们暂停下来。\n传统的 PTRACE_ATTACH 依赖信号机制，PTRACE_ATTACH 会向目标进程发送 SIGSTOP，SIGSTOP 信号需要被目标进程的信号处理程序处理，然后进程才停止。\nPTRACE_SEIZE 则不同，它不依赖用户空间的信号传递来让目标进程停止，是内核层面的一个直接操作。当调用 ptrace(PTRACE_SEIZE, tid, ...) 时，内核会标记目标任务（线程 tid）进入 ptrace-stop 状态，这个任务会在下一次内核有机会介入任务执行流的时候暂停。\n收集任务资源并转储 进程树被冻结后，CRIU 开始收集每个任务的详细信息，并将这些信息写入镜像文件（dump files）。这些信息主要来源于 /proc 文件系统。\n内存映射 (Memory maps) CRIU 通过解析 /proc/$pid/maps 和 /proc/$pid/smaps 获取虚拟内存区域（VMA）的布局信息。 /proc/$pid/maps 列出了当前进程 ($pid) 所有内存映射区域（Virtual Memory Areas - VMA） 的详细信息，每一行代表一个连续的虚拟内存区域，通常包含以下字段，用空格分隔：\n1 起始地址-结束地址 权限 偏移量 设备号(主:次) inode 路径名 如果是文件映射，设备号表示文件所在的设备，inode 表示文件的 inode 编号，路径名会显示被映射文件的路径。\n对于匿名映射，即没有关联具体文件，如 malloc 分配的内存、进程的堆、栈等，inode 值为 0，路径名通常为空，或者显示一些特殊标记，如 [heap] 表示进程的堆内存区域，[stack]表示进程的主线程栈区域。\n例如查看应用服务器进程的 maps 文件内容：\n1 2 3 4 5 6 7 8 9 $cat /proc/10854/maps ... 5b565de16000-5b565de17000 rw-p 00003000 103:02 34101432 /home/mazhen/works/jdk-21.0.6/jdk/bin/java 5b5683910000-5b5683958000 rw-p 00000000 00:00 0 [heap] ... 7f95a4000000-7f95a4028000 r--p 00000000 103:02 58869401 /usr/lib/x86_64-linux-gnu/libc.so.6 ... 7fff8da95000-7fff8dab7000 rw-p 00000000 00:00 0 [stack] ... /proc/$pid/maps 是 CRIU 理解进程内存布局的核心依据，而 /proc/$pid/smaps 是 /proc/$pid/maps 的一个扩展版本。\n/proc/$pid/smaps为每一个内存映射区域（VMA）提供了更详细的内存占用统计信息（物理内存占用、共享/私有、干净/脏、匿名、交换、锁定等），以及重要的内核内部标志 (VmFlags)。smaps 由多个块 (block) 组成，每个块对应 /proc/$pid/maps 文件中的一行（即一个 VMA）。\n还是以应用服务器进程为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 $cat /proc/10854/smaps ... 5b5683910000-5b5683958000 rw-p 00000000 00:00 0 [heap] Size: 288 kB KernelPageSize: 4 kB MMUPageSize: 4 kB Rss: 96 kB Pss: 96 kB Pss_Dirty: 96 kB Shared_Clean: 0 kB Shared_Dirty: 0 kB Private_Clean: 0 kB Private_Dirty: 96 kB Referenced: 96 kB Anonymous: 96 kB KSM: 0 kB LazyFree: 0 kB AnonHugePages: 0 kB ShmemPmdMapped: 0 kB FilePmdMapped: 0 kB Shared_Hugetlb: 0 kB Private_Hugetlb: 0 kB Swap: 0 kB SwapPss: 0 kB Locked: 0 kB THPeligible: 0 ProtectionKey: 0 VmFlags: rd wr mr mw me ac sd ... 内存映射文件（mapped files） /proc/$pid/map_files/ 是一个目录，这个目录包含了指向实际被映射文件的符号链接 (symbolic links)。目录中的每个符号链接的名称对应于 /proc/$pid/maps 文件中列出的一个内存区域的地址范围 (格式为 起始地址 - 结束地址)。\nCRIU 通过 /proc/$pid/map_files/ 获取文件映射区域底层文件对象的直接链接，主要用于可靠地访问和读取这些文件映射区域的内容。\n1 2 3 4 5 6 $ ls -l /proc/10854/map_files total 0 lr-------- 1 mazhen mazhen 64 Apr 25 06:58 5b565de12000-5b565de13000 -\u0026gt; /home/mazhen/works/jdk-21.0.6/jdk/bin/java ... lr-------- 1 mazhen mazhen 64 Apr 25 06:58 7f95a4000000-7f95a4028000 -\u0026gt; /usr/lib/x86_64-linux-gnu/libc.so.6 ... 文件描述符 (File descriptors) CRIU 通过读取 /proc/$pid/fd 和 /proc/$pid/fdinfo 获取进程打开的文件、管道、套接字等信息。CRIU 能够处理各种类型的文件描述符，包括常规文件、管道、Unix 套接字、TCP 套接字（甚至包括处于 ESTABLISHED 状态的连接）。\n/proc/$pid/fd是一个目录，它包含符号链接，每个符号链接的名称对应一个已打开的文件描述符编号。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ ls -l /proc/10854/fd ... l-wx------ 1 mazhen mazhen 64 Apr 25 02:00 2 -\u0026gt; \u0026#39;pipe:[195498]\u0026#39; ... lr-x------ 1 mazhen mazhen 64 Apr 25 02:00 3 -\u0026gt; /home/mazhen/works/jdk-21.0.6/jdk/lib/modules ... lrwx------ 1 mazhen mazhen 64 Apr 25 07:21 379 -\u0026gt; \u0026#39;socket:[2629]\u0026#39; lrwx------ 1 mazhen mazhen 64 Apr 25 07:21 380 -\u0026gt; \u0026#39;socket:[2636]\u0026#39; ... lrwx------ 1 mazhen mazhen 64 Apr 25 07:21 477 -\u0026gt; \u0026#39;anon_inode:[eventpoll]\u0026#39; lrwx------ 1 mazhen mazhen 64 Apr 25 07:21 478 -\u0026gt; \u0026#39;anon_inode:[eventfd]\u0026#39; lr-x------ 1 mazhen mazhen 64 Apr 25 07:21 479 -\u0026gt; anon_inode:inotify ... /proc/$pid/fdinfo也是一个目录，它包含普通文件（不是符号链接），每个文件的名称对应一个已打开的文件描述符编号。每个文件（例如/proc/$pid/fdinfo/1）包含关于相应文件描述符的元数据和状态信息。例如：\n1 2 3 4 5 6 7 8 9 10 11 $ ls -l /proc/10854/fdinfo total 0 -r--r--r-- 1 mazhen mazhen 0 Apr 25 07:31 0 -r--r--r-- 1 mazhen mazhen 0 Apr 25 07:31 1 -r--r--r-- 1 mazhen mazhen 0 Apr 25 07:31 10 ... $ cat /proc/10854/fdinfo/20 pos:\t381 flags:\t0100000 mnt_id:\t31 ino:\t32775204 /proc/$pid/fd告诉 CRIU 通过哪些描述符编号打开了哪些资源，/proc/$pid/fdinfo告诉 CRIU 每个已打开描述符的状态和元数据（比如位置和标志）。CRIU 使用来自这两个位置的信息来完整保存进程已打开文件及其状态的情况，以便之后能够准确地恢复它们。\n核心运行参数 (Core parameters) 为了保存一个任务（进程/线程）的核心运行状态以便后续恢复，CRIU 主要结合使用了两种方法。\n使用 ptrace 系统调用的特定命令（例如PTRACE_GETREGS 或 PTRACE_GETFPREGS ）来直接读取任务暂停时的CPU 寄存器 (registers) 内容，包括通用寄存器、指令指针、标志寄存器、浮点寄存器等），以及其他密切相关的底层执行状态信息。\n通过读取和解析 /proc/$pid/stat 文件，获取关于任务的各种状态参数和统计数据。/proc/$pid/stat 以单行文本的形式提供了关于进程的大量状态信息 (status information)，其中的信息由空格分隔，每个字段代表一个特定的进程属性或统计值。下面列出一些最核心和常用的字段：\n可执行文件名 (comm) 进程状态 (state) 父进程 ID (ppid) 进程组 ID (pgrp) 会话 ID (session) 调度优先级和 nice 值 (priority, nice) 虚拟内存大小 (vsize) 常驻集大小 (rss) 进程启动时间 (starttime) 等待子进程的 CPU 时间 (cutime, cstime) 注入寄生代码（Parasite Code）并转储内存 为了获取某些无法从外部直接探测的信息（例如进程凭证、精确的内存布局和内容），CRIU 必须在目标进程的地址空间内部执行特定的代码。这正是通过 寄生代码 (Parasite Code) 技术实现的。\n寄生代码是一段精心构造的小型二进制程序，它以位置无关可执行文件 (PIE, Position-Independent Executable) 格式编译。这一特性至关重要，因为它允许 CRIU 将这段代码加载到目标进程地址空间中的任何可用位置，而无需担心因硬编码地址引发的冲突。该代码通常包含两部分：一小段依赖于具体处理器架构（如 x86, ARM）的汇编引导程序 (bootstrap)，以及一段用 C 语言编写、负责处理命令的通用核心逻辑 (daemon)。\n要在目标进程中运行，寄生代码需要自己的内存空间来存放其代码、运行栈以及用于和 CRIU 进行通信的参数区域。由于 CRIU 不能直接操作目标进程的内存分配，它巧妙地利用了 ptrace 机制：\n准备内存空间 CRIU 首先使用 ptrace 控制目标进程，并保存其当前的寄存器状态（尤其是指令指针 CS:IP 和栈指针）。 接着，CRIU 修改目标进程的寄存器，填入执行 mmap 系统调用所需的编号和参数。 通过 ptrace(PTRACE_SYSCALL,...)，强制目标进程执行这个 mmap 调用。这会在目标进程的地址空间中分配一块共享内存区域。 注入并执行寄生代码 CRIU 使用 ptrace(PTRACE_POKEDATA, ...) ，将预先编译好的完整寄生代码二进制数据写入到刚刚分配的共享内存区域中。 CRIU 再次使用 ptrace 修改目标进程的寄存器，将指令指针 (IP/PC) 指向共享内存中寄生代码的入口点。 CRIU 命令目标进程恢复执行 ptrace(PTRACE_CONT, ...)。此时，目标进程便开始执行被注入的寄生代码。 寄生代码运行在目标进程的上下文中，因此拥有访问该进程所有资源的权限。它可以执行 CRIU 指派的各种任务，例如读取和转储私有内存页、收集文件描述符的详细状态等。\n清理 (Cleanup) 当所有需要通过寄生代码完成的任务结束后，必须将其彻底移除，并将目标进程恢复到之前的状态，仿佛从未被打扰过：\n寄生代码退出 CRIU 通过共享内存或专用通信通道向寄生代码发送一个结束命令 (PARASITE_CMD_FINI)。 寄生代码收到命令后，执行必要的清理操作，然后调用 rt_sigreturn() 系统调用。此系统调用会利用 CRIU 事先准备好的信息，恢复目标进程在寄生代码注入前一刻的寄存器状态。 CRIU 清理环境 CRIU 通过 ptrace 监视系统调用，并拦截 rt_sigreturn() 的退出。 在目标进程寄存器已恢复、但寄生代码的内存区域还在的短暂时刻，CRIU 再次利用 ptrace 强制目标进程执行 munmap 系统调用，将之前为寄生代码分配的共享内存区域解除映射，彻底抹除其痕迹。 恢复正常运行 最后，CRIU 调用 ptrace(PTRACE_DETACH, ...) 从目标进程分离。 目标进程从其原始被中断的指令处（由恢复的寄存器状态决定）继续执行，整个进程树恢复运行，Checkpoint 操作完成。 # Restore 过程详解 Restore (恢复) 过程可以看作是 Checkpoint (检查点) 的逆向操作。在这个过程中，执行恢复命令的 CRIU 进程会经历一系列精心设计的步骤，最终“变形”成为检查点时刻被冻结的目标进程（或进程树），并从那一刻继续运行。整个过程大致分为以下四个主要阶段：\n解析共享资源 CRIU 首先读取检查点生成的镜像文件，分析进程间的依赖关系。它会找出哪些资源实例（例如：同一个会话 ID、同一个打开的文件描述符指向的内核文件对象、同一块共享内存区域等）是被多个进程共同使用的。\n识别出这些共享资源后，CRIU 会标记它们，并确定恢复策略。某些资源会通过继承（如会话 ID，在 fork() 时由子进程自然获得），其他的则需要更复杂的机制，比如利用 Unix domain socket 和 SCM_RIGHTS 消息 在进程间传递文件描述符，或者使用 memfd 等技术来重建共享内存区域。这一步是为了确保在后续阶段，这些共享资源能被正确地创建一次，并被所有相关的进程共享，而不是各自创建独立的实例。\n创建进程树 CRIU 严格按照镜像文件中记录的父子关系，通过多次调用 fork() 系统调用来重新创建原始的进程树。每个 fork() 都会产生一个新的进程，其父进程是之前已恢复的对应父进程。\n注意，在这个阶段，只创建进程的主线程。目标进程的所有其他线程的恢复会被推迟到最后一个阶段，主要是为了简化后续内存布局调整时的同步问题。\n恢复基本任务资源 在这个阶段，CRIU 为进程树中的每个进程恢复除了少数几类特殊资源之外的大部分状态。此时恢复的资源包括：\n文件描述符： 打开检查点时记录的文件（使用保存的路径、访问模式、标志位），并根据需要设置到确切的文件偏移量。对于管道、套接字等也会进行创建。 命名空间 (Namespaces): 如果进程使用了非默认的命名空间（如 PID、Mount、Network、IPC、User、UTS），CRIU 会创建或加入相应的命名空间，隔离进程环境。 私有内存映射： 映射进程的私有内存区域（如代码段、数据段、堆、匿名映射等），并从镜像文件中读取检查点时保存的数据，填充到这些内存区域中。 套接字 (Sockets): 创建套接字，并恢复其状态（如 TCP 连接的状态，如果检查点时保存了相关信息并配置了 TCP 修复）。 工作目录与根目录： 调用 chdir() 和 chroot() 恢复进程检查点时刻的当前工作目录和根目录。 其他： 还可能包括恢复信号处理器、进程的 umask 等。 有四类关键资源在此阶段不会被完全恢复，它们的恢复被特意推迟到了最后阶段：\n内存映射的确切虚拟地址（此阶段可能映射在临时地址）。 定时器 (Timers)。 凭证 (Credentials) (如 UID, GID, Capabilities)。 线程 (Threads) (除了主线程)。 这几类资源之所以延迟恢复，主要是因为它们要么依赖于最终的内存布局，要么涉及特权操作，要么在最终执行前恢复可能导致状态不一致或复杂化处理。\n切换到 Restorer Context，恢复剩余资源并继续执行 这是最关键的一步。因为执行恢复操作的 CRIU 代码本身就位于需要被替换掉的内存区域中。直接执行 munmap() 卸载旧内存或 mmap() 映射新内存到当前地址，都会导致 CRIU 自身崩溃。\n为了解决这个问题，CRIU 引入了一个 Restorer Context (恢复器上下文)，这是一小段自包含的、位置无关的 (PIE) 代码，不依赖外部库，并且被加载到一个临时的、既不属于 CRIU 主体也不属于目标进程最终内存布局的“安全地带 (safe zone)”。\nCRIU 准备好恢复所需的数据（如最终内存映射信息、线程状态、凭证等），找到合适的内存“空洞”加载恢复器代码和数据，然后通过一次跳转，将 CPU 的执行控制权转移给这段恢复器代码。\n在 Restorer Context 中，完成最后几项资源的恢复：\n内存映射 (Memory): 使用 mremap() 将之前映射在临时地址的私有匿名内存移动到最终的目标虚拟地址。使用 mmap() 在正确的地址创建文件映射和共享内存映射（可能通过之前准备好的 memfd 文件描述符来实现共享）。此时，完整的、精确的进程内存布局被建立起来。 定时器 (Timers): 恢复并启动所有的定时器。因为此时环境已稳定，可以避免定时器过早触发或计时偏差。 凭证 (Credentials): 设置进程最终的用户 ID、组 ID、能力集等。这通常在需要特权的操作（如 fork() 指定 PID）完成后，但在彻底放弃特权之前进行。 线程 (Threads): 在最终的内存布局中，根据保存的状态创建并恢复目标进程的所有其他线程。 最后，Restorer Context 完成所有设置后，它会精确地恢复目标进程主线程的寄存器状态（包括最重要的指令指针 IP/PC，指向检查点时刻被中断的那条指令），然后将 CPU 的控制权彻底交还给目标进程。至此，目标进程就像从未被打断过一样，从检查点时刻的状态无缝地继续执行。\n# CRIU 小结 CRIU 通过 ptrace 和精心设计的寄生代码机制，以及对 /proc 文件系统的深度利用，实现了在用户空间对运行中进程进行 Checkpoint 和 Restore 的强大能力，为 CRaC 技术的实现奠定了坚实的基础。\n# CRaC 的设计理念 理解了 CRIU 的强大能力后，一个自然的问题是：既然 CRIU 能够处理打开的文件描述符和网络连接，甚至可以透明地恢复它们，为什么 CRaC 却要求开发者通过 API（jdk.crac.Resource）来手动管理这些外部资源，通常需要在 beforeCheckpoint 中关闭它们，在 afterRestore 中重新建立它们呢？\n对于这个问题，我查阅了 CRaC 所有相关的文档，阅读了 CRaC 原型实现的源码，都没有获得满意的答案。于是在 CRaC 的开发者邮件列表中询问，最终从核心 Committer 的回复中得到解答。\n根据 CRaC 开发者的阐述，这并非技术上的限制，而是一个深思熟虑的架构选择 (architectural choice)。其核心设计理念可以概括为 协调与适应。\nCRIU 的主要动机之一是实现运行中容器的透明迁移。在容器迁移场景下，环境（文件系统、网络）通常是被精心管理的。容器运行时（如 Docker、Kubernetes CRI）可以配合 CRIU 工作，确保恢复后，外部环境（比如网络连接的对端、挂载的文件系统）仍然有效或被正确地重新建立，从而对容器内的进程做到“透明”。比如，网络连接恢复时，容器运行时会处理好 IP 地址、路由等问题。\n如果你追求的是 CRIU 那种“透明恢复”，理论上可以直接在 Java 进程上使用 CRIU。但这有风险，可能会破坏应用程序的内部逻辑。因为 Java 应用可能依赖外部资源的状态，如果环境变化而应用没有感知和调整，就会出问题（比如数据库连接指向了旧的、不存在的 IP，或者文件句柄指向了一个在恢复环境中已变化或不存在的文件）。\nCRaC 的目标不是追求完全透明的恢复。它想要的是：保留 JVM 和应用程序内部计算的有价值的状态（比如 JIT 编译结果、缓存数据、业务逻辑状态），但要让应用程序有机会主动适应恢复时可能已经变化的新环境。\nCRaC（Coordinated Restore at Checkpoint） 的名字强调了“协调”（Coordinated）。它要求应用程序通过实现 Resource 接口来参与 Checkpoint 和 Restore 过程。CRaC 希望开发者对每一个外部资源（文件、网络连接、数据库连接等）在恢复时如何处理，做出有意识的决定：这个资源在 Checkpoint 前应该如何处理（通常是关闭）？在 Restore 后应该如何处理（通常是重新建立或验证）？\n这种强制性的协调机制被 CRaC 视为一个特性 (feature)，而不是一个缺陷。这确保了应用程序能够优雅地适应 (gracefully adapt) 恢复后的新环境，而不是盲目地假设外部世界一成不变。通过显式地关闭和重新建立连接、验证文件句柄等操作，可以大大提高应用程序在 Restore 后的健壮性和正确性。\n简而言之，CRaC 的设计哲学是，牺牲一定的透明度，换取应用在恢复后对环境变化的健壮适应能力。它要求开发者更加明确地思考和管理应用的外部依赖。虽然这在初期可能带来一些额外的工作（比如处理日志文件句柄），但其目的是为了确保应用在 CRaC 恢复后能够稳定、正确地运行在一个可能已经发生变化的新环境中。\n# OpenJDK CRaC 实现概览 了解了 CRaC 的设计理念和底层依赖 CRIU 后，我们来看看 CRaC 功能在 OpenJDK 内部的大致实现流程。这个过程涉及 Java API 层、JVM 内部实现、外部引擎（默认是包装了 CRIU 的 criuengine）以及操作系统层面的交互。\n# Checkpoint 流程概览 Checkpoint 过程的目标是安全地停止 JVM，通知所有已注册的资源进行准备，然后调用外部引擎来创建进程镜像。\n触发 Checkpoint 用户执行 jcmd \u0026lt;pid\u0026gt; JDK.checkpoint，CheckpointDCmd::execute 被调用，它解析 jdk.crac.Core 类，并调用其静态方法 checkpointRestoreInternal(long jcmdStream)。jcmdStream 是用于输出 jcmd 结果的流。\nJava 层准备 遍历所有注册到全局上下文 Context的 Resource 实现，并调用它们的 beforeCheckpoint 方法。Resource 在此方法中执行必要的清理或准备工作（例如，关闭不需要的网络连接、刷新缓冲区等）。\n进入 JVM/Native 层 调用 native 方法 checkpointRestore0(int[] fdArr, Object[] objArr, boolean dryRun, long jcmdStream)。\n定义在 CracCore.c 中的 JNI 实现 Java_jdk_crac_Core_checkpointRestore0 调用 JVM_Checkpoint。\nJVM_Checkpoint 是一个 JVM 标准入口点，它调用 crac::checkpoint，正式进入 CRaC 的 checkpoint 阶段。\nJVM 内部 Checkpoint 准备 crac::checkpoint 是 CRaC 的主入口，首先执行一次强制 Full GC (GCCause::_full_gc_alot) ，清理未使用的堆区域，以减小镜像体积。\n然后通过 VMThread::execute() 进入 JVM 的 Safepoint，这是所有的 Java 线程都已暂停，准备好执行接下来的 checkpoint 操作。\ncheckpoint 操作执行 遍历 /proc/self/fd 下的所有文件描述符，如果有应用程序打开但未声明的资源，会导致 Checkpoint 失败，操作会提前返回，最终导致 Java 层抛出 CheckpointException。\n如果一切顺利，调用 report_ok_to_jcmd_if_any()。这会向 jcmd 客户端发送一个初步的成功响应，然后才调用外部引擎。这样做是因为外部引擎（如 CRIU）通常会杀死原始 JVM 进程，所以响应必须在此之前发送。\n接着在 call_crengine fork 新的进程，加载外部引擎 criuengine，执行 criuengine的 checkpoint方法。\n在checkpoint方法中使用 double fork 技巧，让孙子进程执行 CRIU，这样 CRIU 进程就不再是 JVM 的子进程。\n孙子进程执行的具体命令是 criu dump -t \u0026lt;jvm_pid\u0026gt; -D \u0026lt;checkpoint_dir\u0026gt; --shell-job [options...]，冻结 JVM 进程，将其状态保存到 \u0026lt;checkpoint_dir\u0026gt; 下的镜像文件中，然后 杀死 原始的 JVM 进程。\nJVM 暂停点 JVM 进程从 call_crengine 快速返回，继续执行至 sigwaitinfo() 阻塞。\n1 2 3 4 5 6 7 8 9 sigset_t waitmask; sigemptyset(\u0026amp;waitmask); // 初始化空信号集 sigaddset(\u0026amp;waitmask, RESTORE_SIGNAL); // 添加自定义的恢复信号 siginfo_t info; int sig; do { sig = sigwaitinfo(\u0026amp;waitmask, \u0026amp;info); // \u0026lt;--- JVM 在这里阻塞，阻塞等待指定信号 } while (sig == -1 \u0026amp;\u0026amp; errno == EINTR); 当 JVM 完成检查点（checkpoint）后，会进入等待循环，后续恢复进程会通过RESTORE_SIGNAL信号唤醒 JVM。\n# Checkpoint 的进程交互 在通过外部引擎 criuengine执行criu dump的过程中，使用了 Linux 常见的编程技巧double fork，主要原因是为了解耦：通过让中间进程快速退出，使得执行 criu dump 的孙子进程成为孤儿进程，被 init 进程收养，从而“逃离”了原始 JVM 的进程树。\n最终执行 criu dump 的进程不属于原始 JVM 进程的进程树，这避免了 CRIU 在执行 Checkpoint 时尝试冻结其自身的问题，保证了 Checkpoint 操作的正确性。\n上图总结了 Checkpoint 过程中涉及的多个进程的创建。\nJVM fork -\u0026gt; P1 (criuengine checkpoint): JVM 创建子进程 P1 运行 criuengine checkpoint，JVM 进程等待 P1。 P1 fork -\u0026gt; P2: P1 创建子进程 P2，P1 等待 P2。 P2 fork -\u0026gt; P3 \u0026amp; P2 exit: P2 创建孙子进程 P3，然后 P2 立即退出。 P1 exit: P1 检测到 P2 退出，于是 P1 也退出。 JVM 继续: JVM 检测到 P1 退出，call_crengine 返回，JVM 继续执行直到 sigwaitinfo 阻塞。 P3 fork -\u0026gt; criu dump: P3 成为孤儿进程（被 init/systemd 接管），创建 criu 进程，最终执行 criu dump，冻结并杀死阻塞中的 JVM。 # Restore 流程概览 Restore 过程的目标是从 Checkpoint 镜像启动一个新的 JVM 实例，使其恢复到 Checkpoint 时刻的状态，然后继续执行。\n触发 Restore 用户启动 JVM，并指定 Restore 相关的参数 -XX:CRaCRestoreFrom=\u0026lt;checkpoint_dir\u0026gt;。\nJVM 初始化 在创建 JVM 的过程中，检测到 Restore 请求，调用 crac::restore()。\n准备并切换引擎 进入 crac::restore() ，首先调用compute_crengine() ，确定外部引擎的路径和参数。\n然后使用当前进程 ID (os::current_process_id()) 创建一个唯一的 共享内存 (SHM) 路径，打开 SHM 文件，并将 Restore 参数写入到 SHM（CracRestoreParameters::write_to）。SHM 用于在 crac::restore（初始 JVM）和恢复后的 JVM 之间传递新的启动参数、属性和时间戳。\n最后调用 os::execv(_crengine, _crengine_args)。 execv 会用新的程序（外部引擎 criuengine）替换当前的 JVM 进程。初始启动的 JVM 到此结束。\n外部引擎执行 Restore (criuengine restore) 外部引擎 criuengine 执行 restore 方法，构建 criu restore 命令参数。关键参数包括：\n-D \u0026lt;checkpoint_dir\u0026gt;: 指定镜像目录。 --action-script self: 指定 criuengine 自身作为 CRIU 的动作脚本。 --exec-cmd -- self restorewait: 指定 CRIU 成功恢复进程后，应该执行 criuengine restorewait 命令。这个命令会等待恢复后的 JVM 进程结束。 执行 execv 运行 criu restore 命令，再次替换当前进程。\nCRIU 执行恢复 CRIU 读取镜像文件，在内存中重建 JVM 进程的状态（内存映射、线程状态、寄存器等）。\n在进程状态基本恢复但尚未完全运行时，CRIU 会调用 --action-script 指定的脚本（即 criuengine），进入 post_resume 方法。\n在post_resume 方法中，获取恢复后的 JVM PID 和之前存入的 SHM ID，然后使用 sigqueue 向恢复的 JVM 进程发送 RESTORE_SIGNAL 信号。\n外部等待进程 (criuengine restorewait) 根据参数 --exec-cmd 指定的命令，再次执行 execv，将执行恢复的 CRIU 进程替换为 criuengine restorewait。\ncriuengine 使用 waitpid 等待刚刚恢复并继续运行的 JVM 进程，它会捕获发给它自己的信号，并尝试将这些信号转发给 JVM 进程。\n当 JVM 进程最终退出时，waitpid 返回，criuengine 进程也以相同的退出码或基于信号的状态退出。\n恢复的 JVM 继续执行 恢复的 JVM 进程收到 CRIU 进程发送的 RESTORE_SIGNAL 信号，从 sigwaitinfo() 醒来，执行流回到 checkpoint_restore 函数（在 crac.cpp 中），正好在 call_crengine() 之后等待信号的地方。\n执行一些 JVM 恢复动作，包括根据 SHM ID 从共享内存中读取新的命令行参数，进行时间校准，唤醒可能在 Checkpoint 时处于 sleep 或 park 状态的线程。\n返回 Java 层 JVM 将新参数返回给 jdk.crac.Core，Core 应用新属性，遍历所有注册的 Resource，调用其 afterRestore 方法，执行恢复后的初始化工作（例如，重新建立连接、重新加载配置等）。\n完成 如果没有异常，从 Core.checkpointRestoreInternal 正常退出，Restore 成功，JVM 继续运行。\n# Restore 的进程交互 Restore 过程巧妙地利用了 execv 系统调用来替换当前进程的映像，从而将控制权逐步交给下一个阶段所需的工具，最终恢复目标 JVM 进程。需要注意的是，在这个流程中，fork 并不像 Checkpoint 流程那样显式地用于创建等待子进程的父进程，而是由 CRIU 内部管理，但 execv 是贯穿始终的关键。\n启动 Restore 命令 (用户 -\u0026gt; P1) 用户执行 java -XX:CRaCRestoreFrom=\u0026lt;checkpoint_dir\u0026gt; 命令，启动了一个初始的 JVM 进程，我们称之为 P1。\nP1 执行第一次 execv (P1 -\u0026gt; P2: criuengine restore) 在 crac::restore()中，P1 准备必要的参数，调用 os::execv(_crengine, _crengine_args)，这里的 _crengine 是 criuengine 的路径，_crengine_args 包含了 \u0026ldquo;restore\u0026rdquo; 和 Checkpoint 目录等参数。\n结果 execv 用 criuengine restore 程序替换了 P1 进程。原来的 Java 进程 P1 不复存在。现在的进程我们称之为 P2，虽然 PID 可能与 P1 相同，但运行的程序已改变，P2 正在执行 criuengine restore 的代码。\nP2 执行第二次 execv (P2 -\u0026gt; P3: criu restore) P2 运行 criuengine restore 代码，进行一些准备工作，构建 criu 命令行的参数，调用 execv(criu, const_cast\u0026lt;char **\u0026gt;(args.data())) ，用 criu restore 程序替换了 P2 进程。criuengine restore 进程 P2 不复存在。现在的进程（我们称之为 P3）正在执行 criu restore。\nP3 (CRIU) 恢复 JVM 进程，执行第三次 execv (P3 -\u0026gt; P4: criuengine restorewait) P3 （运行 criu restore）读取 Checkpoint 镜像文件，恢复（fork）目标 JVM 进程。\n在成功恢复 JVM 之后，CRIU 本身需要结束。由于指定了 --exec-cmd -- self restorewait，CRIU 会执行最后一次 execv，用 criuengine restorewait 程序替换了 P3 进程。criu restore 进程 P3 不复存在，现在的进程（我们称之为 P4）正在执行 criuengine restorewait。与此同时，JVM 已经独立运行起来，并完成了 Restore 的 Java 层逻辑。\nP4 (criuengine restorewait) 等待 JVM P4 （运行 criuengine restorewait 代码）获取 JVM 的 PID，设置信号处理程序，尝试将接收到的信号转发给 JVM。最后调用 waitpid(pid_P_JVM, \u0026amp;status, 0)，等待 JVM 进程终止。当 JVM 退出时，P4 获取其退出状态，然后 P4 也以相同的状态退出。 简单来说，当 JVM 恢复时，通过 java -XX:CRaCRestoreFrom=... 启动的进程并不会启动 JVM，而是通过 criuengine 执行 CRIU，后者将恢复的进程作为其子进程启动。当 CRIU 完成进程重建后，它会执行 criuengine restorewait ，该程序的唯一任务是等待其唯一子进程（恢复的 JVM）退出并传递其状态。这意味着现在有两个进程，恢复的 JVM 进程是 criuengine 的子进程。\n# 四、CRaC 使用指南 本章将详细介绍如何在你的 Java 应用程序中使用 CRaC 技术，包括理解其协调机制、使用 API、文件描述符策略以及通过一个 Jetty 示例进行实战演练。\n# 为何需要 org.crac 包 CRaC 的核心 API（如 Resource 接口）最初存在于不同的包路径下（例如早期的 javax.crac 或 JDK 内部的 jdk.crac）。为了提供一个稳定且兼容的编程接口，社区引入了 org.crac 这个独立的库。\n使用 org.crac 库的好处在于：\n平滑采用： 开发者可以依赖这个库来编写 CRaC 相关的代码。 跨运行时兼容： 应用程序可以在不同的 Java 运行时上编译和运行，无论该运行时是否内置了 CRaC 支持（如标准的 OpenJDK、带有 jdk.crac 的 CRaC 构建版本，或者更早期的 javax.crac 实现）。 未来适应性： 即便未来 CRaC API 的包路径发生变化，应用程序代码也无需修改，只需更新 org.crac 库版本即可。 org.crac 库的核心功能是作为 CRaC API 的一个适配器。\n编译时： 它提供了与 jdk.crac（以及历史上的 javax.crac）完全镜像的 API 接口，供开发者编译时依赖。 运行时： org.crac 使用反射机制来检测当前运行的 JVM 是否包含实际的 CRaC 实现（检查是否存在 jdk.crac.Core 或 javax.crac.Core）。 如果检测到 CRaC 实现，所有对 org.crac API 的调用都会被转发给底层的实际实现。 如果未检测到 CRaC 实现（例如在标准 OpenJDK 上运行），请求会被转发到一个虚拟（dummy）实现。这个虚拟实现允许应用程序正常运行，Resource 也可以注册，但任何尝试创建 Checkpoint 的请求（如调用 Core.checkpointRestore()）都会失败并抛出异常。 通过这种方式，org.crac 库确保了应用程序即使在不支持 CRaC 的环境中也能运行，同时在支持 CRaC 的环境中能够无缝对接。\n# 添加 org.crac API 依赖 可以通过 Maven 或 Gradle 将 org.crac 库添加到你的项目中：\nMaven 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.crac\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;crac\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${crac.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Gradle 1 implementation \u0026#39;org.crac:crac:1.4.0\u0026#39; # CRaC 的协调机制 如前所述，CRaC 的核心设计理念是“协调与适应”。虽然底层的 CRIU 能够冻结和恢复进程的大部分状态，但对于外部资源（External Resources），如打开的文件、网络连接（Socket）、数据库连接等，简单的透明恢复可能会导致问题。原因在于：\n环境变化： Restore 发生时，运行环境可能已经改变（例如，IP 地址、主机名、挂载的文件系统内容）。直接恢复旧的资源句柄可能指向无效或错误的目标。 状态失效： 某些外部资源的状态可能具有时效性（例如，数据库连接超时、文件被其他进程修改）。 资源冲突： 恢复的进程可能尝试使用已被新环境占用的资源（例如，端口号）。 因此，CRaC 不选择让 CRIU 默认透明地处理这些外部资源，而是要求应用程序必须参与到 Checkpoint 和 Restore 的过程中，主动管理这些资源。这就是“协调”的含义。应用程序需要明确告知 CRaC 如何安全地处理这些外部连接和状态，以确保在 Restore 后能够正确地适应新环境。\n为了实现这种协调，org.crac 包提供了核心的 Resource 接口：\n1 2 3 4 5 6 package org.crac; // ... imports ... public interface Resource { void beforeCheckpoint(Context\u0026lt;? extends Resource\u0026gt; context) throws Exception; void afterRestore(Context\u0026lt;? extends Resource\u0026gt; context) throws Exception; } 需要管理外部资源的类可以实现 Resource 接口：\n在 Checkpoint 前调用 beforeCheckpoint(Context\u0026lt;? extends Resource\u0026gt; context)，用于释放/关闭外部资源，确保状态一致。如果无法准备好，抛出异常阻止 Checkpoint。 在 Restore 后调用 afterRestore(Context\u0026lt;? extends Resource\u0026gt; context)，用于重新建立/验证外部资源，恢复状态。如果恢复失败，抛出异常。 通过 org.crac.Core.getGlobalContext().register(this) ，将实现了 Resource 的对象注册给 CRaC 运行时。注册顺序决定了 beforeCheckpoint 的调用顺序，而 afterRestore 则以相反顺序调用。\n# 文件描述符策略 (File Descriptor Policies) 虽然 CRaC 推荐通过实现 Resource 接口来主动管理外部资源，但也提供了一种基于配置的备选方案，称为文件描述符策略 (File Descriptor Policies)。这主要用于处理那些难以修改以添加 Resource 回调的代码，例如第三方库或 JDK 内部代码（注意：此策略仅适用于通过 JDK API 打开的文件描述符，不适用于 Native 代码打开的 FD）。\n# 配置方式 通过设置系统属性 jdk.crac.resource-policies 指向一个策略文件来启用。该文件采用类似 YAML 的格式，包含一个或多个规则，规则之间用 --- 分隔。以 # 开头的行是注释。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 规则示例 1: 关闭特定文件 type: file path: /path/to/my/file action: close --- # 规则示例 2: 忽略所有 .log 文件 (交给 CRIU 处理) type: FILE # 类型不区分大小写 path: \u0026#34;**\\*.log\u0026#34; # 支持 glob 模式 action: ignore warn: false # 禁止对此规则匹配的 FD 打印警告 --- # 规则示例 3: 重新打开 Unix socket type: socket family: unix localPath: /var/run/app.sock action: reopen # 注意：socket 的 reopen 目前可能不完善 CRaC 在 Checkpoint 时会检查所有打开的文件描述符。对于每个 FD，它会按顺序查找策略文件中的规则，第一个匹配的规则将被应用，后续规则会被忽略。\n每个规则必须包含 type 和 action 两个属性（值不区分大小写）。\n可用类型 (type):\nfile: 本地文件系统上的文件或目录。 pipe: 匿名管道（命名管道使用 file 类型）。 socket: 网络套接字（TCP, UDP 等）或 Unix 域套接字。 filedescriptor: 无法通过以上类型识别的原始文件描述符（例如，由 Native 代码打开但通过 JDK API 暴露的）。 # 文件 (file) 规则 通过 path 属性匹配，支持 glob 模式。\n**可用操作 (action)**支持：\nerror: (默认) 打印错误并导致 Checkpoint 失败。 ignore: 忽略此 FD，将其处理完全委托给底层的 Checkpoint/Restore 引擎（如 CRIU）。CRIU 通常会尝试验证并在 Restore 时重新打开文件。这是将处理责任交给 CRIU 的方式。 close: 在 Checkpoint 前关闭文件。如果在 Restore 后尝试使用该 FD，会导致运行时异常。 reopen: 在 Checkpoint 前关闭文件，并在 Restore 后尝试在相同位置重新打开它。 # 管道 (pipe) 规则 匿名管道无法通过名称识别，因此通常最多只有一个 pipe 规则。\n可用操作 (action) 支持 error，ignore 和 close，含义和文件规则相同。\n# 套接字 (socket) 规则 可以通过以下属性细化匹配：\nfamily: ipv4/inet4, ipv6/inet6, ip/inet (任意 IP), unix。 localAddress, remoteAddress: IP 地址或 * (任意地址)。 localPort, remotePort: 端口号或 * (任意端口)。 localPath, remotePath: Unix 套接字路径，支持 glob 模式。 可用操作 (action) 支持 error，ignore 和 close。reopen 也可以使用，它会在 Checkpoint 前关闭套接字，但目前重新打开的逻辑（特别是对于监听套接字）可能尚未完全实现。\n# 原始文件描述符 (filedescriptor) 规则 用于匹配那些没有对应 Java 对象（如 FileOutputStream）的文件描述符。\n可以通过数值 value: 123 ，或原生描述的正则表达式 regex: .*something.* (Java 正则语法) 来匹配。\n可用操作 (action) 支持 error， ignore 和 close。\n# 重要提示 文件描述符策略被认为是权宜之计，用于处理无法直接修改代码的情况。首选且更健壮的方式仍然是实现 Resource 接口，因为应用程序最了解如何正确、安全地处理其外部资源，尤其是在面对环境变化时。过度依赖 ignore 策略可能隐藏潜在的 Restore 后问题。\n# CRaC 实战 下面我们通过一个简单的 Jetty Web 服务器示例，演示如何使用 org.crac API 来支持 CRaC。完整代码可在 example-jetty 仓库找到。\n初始 Jetty 应用 假设我们有一个简单的 Jetty 应用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class ServerManager { Server server; public ServerManager(int port, Handler handler) throws Exception { server = new Server(8080); server.setHandler(handler); server.start(); } } public class App extends AbstractHandler { static ServerManager serverManager; public void handle(...) { response.getWriter().println(\u0026#34;Hello World\u0026#34;); } public static void main(String[] args) throws Exception { serverManager = new ServerManager(8080, new App()); } } 添加 org.crac 依赖\n在 pom.xml (Maven) 或 build.gradle (Gradle) 中添加 org.crac 依赖。 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.crac\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;crac\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.1.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 尝试 Checkpoint (预期失败) 编译并运行应用，启用 CRaC 并指定 Checkpoint 目录：\n1 2 3 4 5 # 编译 $ mvn clean package # 运行 $ java -XX:CRaCCheckpointTo=cr -Djdk.crac.collect-fd-stacktraces=true -jar target/example-jetty-1.0-SNAPSHOT.jar 应用启动后，尝试访问 http://localhost:8080，应该能看到“Hello World”。\n1 2 $ curl localhost:8080 Hello World 然后尝试触发 Checkpoint：\n1 2 3 4 5 6 7 # 查找 PID $ jps 35297 example-jetty-1.0-SNAPSHOT.jar 36051 Jps # 触发 Checkpoint jcmd 35297 JDK.checkpoint 此时，应用控制台会打印类似以下的异常并退出，因为 Jetty 打开了监听端口（一个 Socket 文件描述符），而我们没有处理它：\n1 2 3 4 5 6 7 An exception during a checkpoint operation: jdk.internal.crac.mirror.CheckpointException Suppressed: jdk.internal.crac.mirror.impl.CheckpointOpenSocketException: sun.nio.ch.ServerSocketChannelImpl[/[0:0:0:0:0:0:0:0]:8080] at java.base/jdk.internal.crac.JDKSocketResourceBase.lambda$beforeCheckpoint$0(JDKSocketResourceBase.java:68) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore1(Core.java:170) at java.base/jdk.internal.crac.mirror.Core.checkpointRestore(Core.java:315) at java.base/jdk.internal.crac.mirror.Core.checkpointRestoreInternal(Core.java:328) 实现 Resource 接口 我们需要让 ServerManager 实现 Resource 接口，在 Checkpoint 前停止 Jetty 服务器（关闭 Socket），在 Restore 后重新启动它。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import org.crac.Context; import org.crac.Core; import org.crac.Resource; class ServerManager implements Resource { ... @Override public void beforeCheckpoint(Context\u0026lt;? extends Resource\u0026gt; context) throws Exception { // Stop the connectors only and keep the expensive application running Arrays.asList(server.getConnectors()).forEach(c -\u0026gt; LifeCycle.stop(c)); } @Override public void afterRestore(Context\u0026lt;? extends Resource\u0026gt; context) throws Exception { Arrays.asList(server.getConnectors()).forEach(c -\u0026gt; LifeCycle.start(c)); } } 将 Resource 注册到一个 Context 中，该 Context 将调用 Resource 的方法作为通知。有一个全局的 Context 可以作为默认选择。\n1 2 3 4 5 6 7 8 class ServerManager implements Resource { public ServerManager(int port, Handler handler) throws Exception { ... Core.getGlobalContext().register(this); } ... } 再次尝试 Checkpoint (预期成功)\n重新编译并运行应用 1 2 $ mvn clean package $ java -XX:CRaCCheckpointTo=cr -jar target/example-jetty-1.0-SNAPSHOT.jar 访问 http://localhost:8080 进行预热。然后再次触发 Checkpoint：\n1 $ jcmd \u0026lt;pid\u0026gt; JDK.checkpoint 这次，你应该在应用控制台看到类似输出，表明 Jetty 被停止，然后 Checkpoint 被创建，最后原始 JVM 被杀死：\n1 2 3 INFO: Starting checkpoint 2025-04-28 02:42:16.653:INFO:oejs.AbstractConnector:Attach Listener: Stopped ServerConnector@270421f5{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} [1] 42035 killed java -XX:CRaCCheckpointTo=cr -jar target/example-jetty-1.0-SNAPSHOT.jar 同时，在 cr 目录下会生成 Checkpoint 镜像文件。\n1 2 3 4 5 6 7 8 9 10 11 $ ls cr core-42035.img core-42045.img core-42081.img core-42092.img core-42426.img core-42436.img core-42446.img pstree.img core-42036.img core-42046.img core-42082.img core-42093.img core-42427.img core-42437.img dump4.log seccomp.img core-42037.img core-42047.img core-42083.img core-42094.img core-42428.img core-42438.img fdinfo-2.img stats-dump core-42038.img core-42048.img core-42084.img core-42116.img core-42429.img core-42439.img files.img timens-0.img core-42039.img core-42049.img core-42085.img core-42119.img core-42430.img core-42440.img fs-42035.img tty-info.img core-42040.img core-42050.img core-42086.img core-42121.img core-42431.img core-42441.img ids-42035.img core-42041.img core-42051.img core-42088.img core-42422.img core-42432.img core-42442.img inventory.img core-42042.img core-42052.img core-42089.img core-42423.img core-42433.img core-42443.img mm-42035.img core-42043.img core-42079.img core-42090.img core-42424.img core-42434.img core-42444.img pagemap-42035.img core-42044.img core-42080.img core-42091.img core-42425.img core-42435.img core-42445.img pages-1.img 从 Checkpoint 恢复 使用 -XX:CRaCRestoreFrom 参数启动一个新的 JVM 实例：\n1 2 $ java -XX:CRaCRestoreFrom=cr 2025-04-28 02:46:57.664:INFO:oejs.AbstractConnector:Attach Listener: Started ServerConnector@270421f5{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} 现在，你可以再次访问 http://localhost:8080，应用应该能够正常响应。\n查看进程的父子关系：\n1 2 3 4 5 6 7 8 $ ps axfo pid,ppid,command ... 31809 31807 \\_ sshd: mazhen [priv] 31830 31809 \\_ sshd: mazhen@pts/0,pts/1 31831 31830 \\_ -zsh 44224 31831 | \\_ /home/mazhen/21.0.5.crac/lib/criuengine restorewait 42035 44224 | \\_ java -XX:CRaCCheckpointTo=cr -jar target/example-jetty-1.0-SNAPSHOT.jar ... 和前面 CRaC 实现原理分析一致，恢复的 JVM 进程是 criuengine 的子进程。\n# 注意事项 架构与环境限制 CRaC 的 Checkpoint 和 Restore 必须在相同的 CPU 架构（例如，都是 x64 或都是 ARM64）上进行。 此外，它目前主要依赖 Linux 操作系统和特定的支持 CRaC 的 JDK 构建版本（如 Azul Zulu CRaC builds, Apusic JDK with CRaC Support 等）。\n系统时钟变化 应用程序需要注意，从 Checkpoint 到 Restore 之间可能存在显著的系统时钟跳跃。对于依赖时间的逻辑（如缓存过期、定时任务、同步机制），可能需要在 afterRestore 回调中进行校准或特殊处理，以避免因时间差导致的行为异常。虽然 OpenJDK CRaC 内部会尝试校准 System.nanoTime()，但应用层面的时间敏感逻辑仍需开发者关注。\n幂等性 beforeCheckpoint 和 afterRestore 的实现应该是幂等的，即多次调用也应该产生相同的结果或无副作用。\n安全性 Checkpoint 镜像包含了 JVM 进程的完整内存状态，可能包含敏感数据（如密码、密钥、用户数据等）。必须像对待生产数据库备份一样，妥善保管 Checkpoint 文件，控制访问权限。\n# 五、CRaC 的应用场景与生态 CRaC 技术以其显著缩短启动时间和实现即时峰值性能的优势，在多个领域展现出巨大的应用潜力，并且其生态系统正在逐步发展壮大。\n# 理想应用场景 Serverless Functions (FaaS) 这是 CRaC 最典型的应用场景之一。Serverless 函数的冷启动延迟是影响用户体验和成本的关键因素。CRaC 可以将函数的启动时间从秒级降低到毫秒级，极大地改善冷启动性能，使得 Java 在 Serverless 领域更具竞争力。AWS Lambda SnapStart 就是基于类似 CRaC 的技术实现的。\n微服务 在微服务架构中，服务实例需要频繁地启动、停止和水平扩展。CRaC 可以显著加快新服务实例的启动速度，提高自动伸缩（Auto-scaling）的响应能力和效率，尤其是在应对突发流量时。\n批处理作业 对于需要快速启动、执行任务然后退出的批处理作业，CRaC 可以消除大部分启动开销，提高作业执行效率。\n资源受限环境 在内存或 CPU 资源受限的环境中，CRaC 通过避免启动和预热阶段的高资源消耗，有助于更高效地利用资源。\n# 框架与平台支持 随着 CRaC 技术的发展，越来越多的 Java 框架和平台开始提供对其的支持，以简化开发者的使用。\nSpring Framework / Spring Boot 从 Spring Framework 6.1 和 Spring Boot 3.2 开始，提供了对 CRaC 的官方支持。开发者可以通过简单的配置（例如 -Dspring.context.checkpoint=onRefresh）实现应用启动时的自动 Checkpoint，或者手动触发 Checkpoint 以包含更完整的应用状态。Spring 会自动处理内部管理的资源（如数据库连接池、消息监听器等）的 CRaC 回调。\nMicronaut Micronaut 框架提供了专门的 micronaut-crac 模块，可以方便地集成 CRaC 支持。它内置了对常见资源（如 Hikari 数据源、Redis 连接）的协调处理。Micronaut 的构建插件（如 Gradle 插件）甚至可以一键生成包含 CRaC 镜像的 Docker 镜像。\nQuarkus Quarkus 从 2.10.0 版本开始内置了对 CRaC 的基本支持。利用 Quarkus 的构建时优化和 CRaC 的运行时恢复能力，可以进一步提升应用的启动性能。\nAWS Lambda SnapStart 虽然底层实现细节未完全公开，但 AWS Lambda 的 SnapStart 功能在原理和效果上与 CRaC 非常相似，它允许用户为 Lambda 函数创建快照，并在调用时快速恢复，显著降低 Java Lambda 函数的冷启动延迟。这表明 CRaC 的理念已经在主流云平台上得到了应用和验证。\nAzul Zulu Builds of OpenJDK Azul 作为 CRaC 技术的主要推动者之一，提供了包含 CRaC 功能的 OpenJDK 发行版（Zulu），支持 Linux/x64 和 Linux/ARM64 平台，并为 Windows 和 macOS 提供用于开发和测试的模拟版本。\n# CRaC 部署方案 CRaC 的部署方案旨在收集 Java 应用程序初始化和预热所需的数据。\n在金丝雀环境中部署并预热应用 将 Java 应用程序部署到金丝雀（canary）测试环境中。 应用程序处理金丝雀请求，这会触发类加载和 JIT 编译，从而完成预热。 创建 Checkpoint 对正在运行的应用程序进行 Checkpoint 操作。 这将创建 JVM 和应用程序的镜像（image），该镜像被视为新部署包的一部分。 在生产环境中部署和恢复 将带有镜像的 Java 应用程序部署到生产环境中。 通过 -XX:CRaCRestoreFrom=PATH 选项从镜像恢复 Java 进程。 恢复后的 Java 进程将直接使用镜像中已加载的类和 JIT 代码，从而实现快速启动和即时达到最佳性能。 # 性能基准 由 Java 社区及主流框架开发者进行的广泛性能基准测试一致表明，CRaC (Coordinated Restore at Checkpoint) 技术能够为 Java 应用带来显著的性能提升，尤其在启动速度方面表现突出。\n对于常见的 Web 应用程序，例如基于 Spring Boot、Micronaut 或 Quarkus 构建的应用，采用 CRaC 的恢复机制可以将原先需要数秒的启动过程，缩短至几十毫秒级别。这意味着应用能够更快地进入服务状态，提升用户体验和资源利用率。\n为了具体展示 CRaC 的效果，我对 Glassfish 7 进行了 CRaC 的改造适配。在部署了标准 Spring PetClinic 应用的场景下：\n常规启动耗时： 通过标准 ./bin/asadmin start-domain 命令启动，完成整个启动过程需要 8.813 秒 (8813 毫秒)。 1 2 3 4 5 6 7 8 ./bin/asadmin start-domain Waiting for domain1 to start ........ Waiting finished after 8,813 ms. Successfully started the domain : domain1 domain Location: /home/mazhen/works/glassfish7/glassfish/domains/domain1 Log File: /home/mazhen/works/glassfish7/glassfish/domains/domain1/logs/server.log Admin Port: 4,848 Command start-domain executed successfully. CRaC 恢复耗时： 从预先生成的 CRaC 快照 (checkpoint) 文件恢复，使用 java -XX:CRaCRestoreFrom=cr 命令，启动过程仅需约 36.88 毫秒。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ java -XX:CRaCRestoreFrom=cr [#|2025-04-28T06:42:11.559868Z|INFO|GF 7.0.23-SNAPSHOT|jakarta.enterprise.logging.stdout|_ThreadID=323;_ThreadName=Attach Listener;_LevelValue=800;| STARTUPTIME 519895593501838 restore|#] [#|2025-04-28T06:42:11.571942Z|INFO|GF 7.0.23-SNAPSHOT|org.glassfish.grizzly.config.GenericGrizzlyListener|_ThreadID=323;_ThreadName=Attach Listener;_LevelValue=800;| After restore, start transport, address=/0.0.0.0, port=8080|#] [#|2025-04-28T06:42:11.572851Z|INFO|GF 7.0.23-SNAPSHOT|org.glassfish.grizzly.config.GenericGrizzlyListener|_ThreadID=323;_ThreadName=Attach Listener;_LevelValue=800;| After restore, start transport, address=/0.0.0.0, port=8181|#] [#|2025-04-28T06:42:11.573165Z|INFO|GF 7.0.23-SNAPSHOT|org.glassfish.grizzly.config.GenericGrizzlyListener|_ThreadID=323;_ThreadName=Attach Listener;_LevelValue=800;| After restore, start transport, address=/0.0.0.0, port=4848|#] [#|2025-04-28T06:42:11.573409Z|INFO|GF 7.0.23-SNAPSHOT|org.glassfish.grizzly.config.GenericGrizzlyListener|_ThreadID=323;_ThreadName=Attach Listener;_LevelValue=800;| After restore, start transport, address=/0.0.0.0, port=3700|#] ... [#|2025-04-28T06:42:11.607504Z|INFO|GF 7.0.23-SNAPSHOT|jakarta.enterprise.logging.stdout|_ThreadID=323;_ThreadName=Attach Listener;_LevelValue=800;| STARTUPTIME 519895630386924 restore-finish|#] 恢复时间计算：restore-finish - restore，恢复过程耗时为 519895630386924 - 519895593501838 = 36,885,086 纳秒，即 36.88 毫秒。\n对比结果清晰显示，使用 CRaC 恢复，启动时间缩短了约 239 倍 (8813 ms / 36.88 ms)，实现了数量级的性能飞跃。\n除了惊人的启动速度提升，CRaC 更为核心的优势在于实现了“即时峰值性能”。与传统启动方式不同，恢复后的应用程序几乎可以瞬间达到其完全预热 (warmed-up) 后的最佳运行性能。这是因为它跳过了耗时的类加载、初始化以及 JIT (Just-In-Time) 编译器的早期编译和优化阶段。对于需要快速响应负载变化、频繁弹性伸缩或要求低延迟的场景 (如 Serverless、微服务快速扩容)，这一特性具有极其重要的价值。\n# 未来展望：生态持续完善 随着 OpenJDK 对 CRaC 项目的持续推进和标准化，以及越来越多第三方库、框架（如 Spring、Micronaut、Quarkus、Open Liberty 等）的积极适配与集成，CRaC 的生态系统正逐步成熟和完善。这预示着未来在 Java 应用中利用 Checkpoint/Restore 技术将变得更加便捷和普遍，有望成为提升 Java 应用启动性能和运行时效率的标准实践之一。\n# 六、Apusic JDK with CRaC Support Apusic JDK 是金蝶天燕（Apusic）公司基于 OpenJDK 项目构建和维护的 Java 开发工具包（JDK）发行版。为了满足用户对高性能和快速启动的需求，Apusic JDK 团队积极跟进社区前沿技术，并提供了对主流 LTS 版本的广泛支持。\n基于 BiSheng JDK Apusic JDK 的上游是华为公司开源的 BiSheng JDK。BiSheng JDK 本身在 OpenJDK 的基础上进行了性能优化和特性增强，Apusic JDK 继承了这些优势，并结合自身在中间件领域的深厚积累，为企业级应用提供了稳定、高效的 Java 运行时环境。\n支持多 LTS 版本 Apusic JDK 致力于提供稳定可靠的 Java 环境，目前为 Java 8, 11, 17, 21 等多个长期支持（LTS）版本提供构建和支持。\n为 JDK 17 和 21 引入 CRaC 支持 Apusic 团队认识到 CRaC（Coordinated Restore at Checkpoint）技术在解决 Java 应用冷启动慢和提升运行时效率方面具有巨大潜力。然而，由于 CRaC 项目尚未正式合并到 OpenJDK 主线，Apusic 采用了与 Azul 等厂商类似的方式，主动将其核心功能移植（Port）并集成到了 Apusic JDK 17 和 Apusic JDK 21 发行版中。\n提供特定版本的双重发行版 为了方便用户根据实际需求进行选择，针对集成了 CRaC 功能的 JDK 17 和 JDK 21，Apusic 提供了两种发行版：\n标准的 Apusic JDK (17 / 21) ：不包含 CRaC 功能，适用于不需要 Checkpoint/Restore 特性的标准 Java 应用场景。 Apusic JDK with CRaC Support (17 / 21) ：内置了 CRaC 功能的特殊版本。用户可以使用这个版本来开发、测试和部署需要利用 CRaC 进行启动优化的 Java 应用程序。 通过提供带有 CRaC 支持的 JDK 版本（目前为 JDK 17 和 21），Apusic 使得其用户能够在其熟悉的 JDK 发行版上，提前体验和应用 CRaC 技术带来的显著优势，特别是在微服务、Serverless 等对启动速度有严苛要求的场景下，能够获得明显的性能提升。用户在使用 Apusic JDK with CRaC Support 时，可以遵循 CRaC 的标准使用方法和最佳实践。\n","date":"2025-04-28T16:56:24+08:00","permalink":"https://mazhen.tech/p/crac-%E6%8A%80%E6%9C%AF%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/","title":"CRaC 技术深度解析"},{"content":" # 引言 在 Java 8 之前，方法无法直接作为值传递，开发者需要通过匿名类或冗长的接口实现来实现行为参数化。Java 8 引入的 Lambda 表达式和方法引用彻底改变了这一局面，让函数式编程范式在 Java 中落地生根。本文将从行为参数化的设计思想出发，系统讲解 Lambda 的核心概念、语法特性及其在实践中的应用。\n# 一、行为参数化：函数式编程的基石 # 1.1 什么是行为参数化？ 行为参数化（Behavior Parameterization） 是指将代码逻辑（即“行为”）作为参数传递给其他方法的能力。这种设计允许方法的执行逻辑动态变化，从而提高代码的灵活性和复用性。\n例如，一个筛选苹果的方法 filterApples，其筛选条件可以是颜色、重量或其他属性。通过行为参数化，我们无需为每种条件编写独立的方法，而是将条件逻辑抽象为接口传递：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 public interface ApplePredicate { boolean test(Apple apple); } // 筛选逻辑由调用者决定 List\u0026lt;Apple\u0026gt; filterApples(List\u0026lt;Apple\u0026gt; inventory, ApplePredicate predicate) { List\u0026lt;Apple\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(); for (Apple apple : inventory) { if (predicate.test(apple)) { result.add(apple); } } return result; } # 1.2 从匿名类到 Lambda 在 Java 8 之前，行为参数化需要通过匿名类实现，但代码臃肿且不够直观：\n1 2 3 4 5 6 filterApples(inventory, new ApplePredicate() { @Override public boolean test(Apple apple) { return \u0026#34;red\u0026#34;.equals(apple.getColor()); } }); Lambda 表达式简化了这一过程，直接传递逻辑：\n1 filterApples(inventory, (Apple apple) -\u0026gt; \u0026#34;red\u0026#34;.equals(apple.getColor())); # 二、Lambda 表达式：语法与核心概念 # 2.1 Lambda 的语法结构 Lambda 表达式由三部分组成：参数列表、箭头符号 -\u0026gt; 和 函数主体。\n1 2 3 4 5 6 // 基本语法 (参数列表) -\u0026gt; { 函数主体 } // 示例：比较两个苹果的重量 Comparator\u0026lt;Apple\u0026gt; byWeight = (Apple a1, Apple a2) -\u0026gt; a1.getWeight().compareTo(a2.getWeight()); # 2.2 函数式接口 函数式接口（Functional Interface） 是只包含一个抽象方法的接口。Lambda 表达式本质上是函数式接口的实例。\nJava 8 提供了 @FunctionalInterface 注解标识这类接口，例如：\n1 2 3 4 @FunctionalInterface public interface Predicate\u0026lt;T\u0026gt; { boolean test(T t); } 常见的函数式接口：\nPredicate\u0026lt;T\u0026gt;：接收 T 类型参数，返回 boolean。 Consumer\u0026lt;T\u0026gt;：接收 T 类型参数，无返回值。 Function\u0026lt;T, R\u0026gt;：接收 T 类型参数，返回 R 类型结果。 # 2.3 类型推断与上下文 Lambda 的类型由上下文自动推断：\n1 2 3 4 5 // 显式指定类型 Comparator\u0026lt;Apple\u0026gt; c1 = (Apple a1, Apple a2) -\u0026gt; a1.getWeight().compareTo(a2.getWeight()); // 类型推断（编译器根据目标类型推断参数类型） Comparator\u0026lt;Apple\u0026gt; c2 = (a1, a2) -\u0026gt; a1.getWeight().compareTo(a2.getWeight()); # 三、方法引用：简化 Lambda 的利器 方法引用允许直接通过方法名替代完整的 Lambda 表达式，主要分为四类：\n# 3.1 静态方法引用 语法：类名::静态方法名\n1 Function\u0026lt;Integer, String\u0026gt; intToString = String::valueOf; # 3.2 实例方法引用 语法：对象::实例方法名\n1 2 String str = \u0026#34;Hello\u0026#34;; Supplier\u0026lt;Integer\u0026gt; lengthSupplier = str::length; # 3.3 任意对象的实例方法引用 语法：类名::实例方法名（适用于 Lambda 参数作为方法调用者）\n1 BiFunction\u0026lt;String, String, Integer\u0026gt; compareIgnoreCase = String::compareToIgnoreCase; # 3.4 构造函数引用 语法：类名::new\n1 Supplier\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; listSupplier = ArrayList::new; # 四、复合 Lambda 表达式 # 4.1 比较器链 通过 Comparator 的链式调用实现多级排序：\n1 2 3 4 5 inventory.sort( comparing(Apple::getWeight) .reversed() .thenComparing(Apple::getCountry) ); # 4.2 谓词组合 使用 and、or、negate 组合多个条件：\n1 2 3 Predicate\u0026lt;Apple\u0026gt; redAndHeavy = apple -\u0026gt; \u0026#34;red\u0026#34;.equals(apple.getColor()) .and(apple -\u0026gt; apple.getWeight() \u0026gt; 150); # 4.3 函数组合 通过 andThen 和 compose 实现函数串联：\n1 2 3 4 5 6 7 8 9 10 Function\u0026lt;Integer, Integer\u0026gt; addOne = x -\u0026gt; x + 1; Function\u0026lt;Integer, Integer\u0026gt; multiplyByTwo = x -\u0026gt; x * 2; // 先加 1，再乘以 2 Function\u0026lt;Integer, Integer\u0026gt; combined1 = addOne.andThen(multiplyByTwo); combined1.apply(3); // 结果为 8 // 先乘以 2，再加 1 Function\u0026lt;Integer, Integer\u0026gt; combined2 = addOne.compose(multiplyByTwo); combined2.apply(3); // 结果为 7 # 五、Lambda 的实践与注意事项 # 5.1 异常处理 Lambda 表达式无法直接抛出受检异常（Checked Exception），需通过以下方式解决：\n在函数式接口中声明异常： 1 2 3 4 @FunctionalInterface public interface BufferedReaderProcessor { String process(BufferedReader br) throws IOException; } 在 Lambda 中捕获异常： 1 2 3 4 5 6 7 Function\u0026lt;String, Integer\u0026gt; safeParse = s -\u0026gt; { try { return Integer.parseInt(s); } catch (NumberFormatException e) { return 0; } }; # 5.2 局部变量限制 Lambda 可以捕获实例变量和静态变量，但局部变量必须为 final 或“等效 final”：\n1 2 3 int localVar = 10; Runnable r = () -\u0026gt; System.out.println(localVar); // 合法 localVar = 20; // 编译错误：localVar 必须是 final 或等效 final # 六、总结 Lambda 表达式是 Java 函数式编程的核心工具，通过行为参数化显著提升了代码的简洁性和灵活性。结合方法引用、函数式接口及复合操作，开发者可以构建出高度抽象且易于维护的代码结构。理解并掌握这些特性，是迈向现代 Java 开发的关键一步。\n","date":"2025-01-29T20:03:12+08:00","permalink":"https://mazhen.tech/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-java-lambda-%E8%A1%A8%E8%BE%BE%E5%BC%8F/","title":"深入理解 Java Lambda 表达式"},{"content":"对于长期在 macOS 或 Linux 终端环境下工作的人来说，Windows 的终端体验可能会显得有些格格不入。尤其是 PowerShell，它的默认交互方式与熟悉的 bash 或 zsh 存在显著差异。本文将从配置 Emacs 模式入手，深入了解 Windows 终端的生态，并逐步打造一个更符合你习惯的 PowerShell 环境。\n# Windows 终端：不止 cmd.exe 和 PowerShell 在 Windows 中，我们经常会接触到以下几个概念：\n命令提示符 (cmd.exe)： 这是 Windows 最早的命令行解释器，可以追溯到 DOS 时代。它执行简单的命令，但功能相对有限，交互体验也较为落后。我们可以将其视为 Windows 系统的“元老”，用于执行一些基本的命令操作。 PowerShell： 这是一个更加现代和强大的命令行外壳，基于 .NET 框架构建，以对象而非文本处理数据，更适合自动化、脚本编写和系统管理。它可以被视为 Windows 系统的“新贵”，旨在替代 cmd.exe，成为系统管理的主力工具。 Windows Terminal (Windows 终端)： 这是一个独立的应用程序，用于承载各种命令行 shell（如 cmd.exe, PowerShell, WSL 等）。它提供了标签页、分屏、自定义主题等高级特性，使得在不同外壳之间切换更加便捷。你可以将它视为一个更强大、更现代的“终端模拟器”，用于统一管理各种命令行环境。 它们之间的关系： Windows Terminal 是一个终端应用程序，它像一个容器，可以运行 cmd.exe 和 PowerShell 等命令行 shell。 cmd.exe 和 PowerShell 是不同的命令行解释器，负责解释用户输入的命令并执行操作。也就是说，cmd.exe 和 PowerShell 是 Windows Terminal 中的两种“引擎”。\n# PSReadLine：PowerShell 的交互核心 PSReadLine 是 PowerShell 的一个核心模块，负责处理终端中的文本输入、历史记录、命令补全、语法高亮等关键功能。它直接影响着你在 PowerShell 中的操作效率和流畅度。如果没有 PSReadLine，PowerShell 的交互体验会大打折扣。\n# 版本与功能 Windows 11 预装的 Windows PowerShell 5.1 虽然自带 PSReadLine 模块，但其功能较为基础。为了获得更流畅的体验和更强大的功能，例如预测性 IntelliSense 和更强大的多行编辑，强烈建议升级到 PowerShell 7.2 或更高版本。\n# 升级 PowerShell (PowerShell Core) 下载： 从 Microsoft 官方 GitHub 仓库 下载最新版本的 PowerShell 安装包，并按照安装向导完成安装。 配置终端： 在终端应用程序 Windows Terminal 中，增加一个新的配置文件，将命令行选项设置为你最新安装的 PowerShell 可执行文件路径，例如 \u0026quot;C:\\Program Files\\PowerShell\\7\\pwsh.exe\u0026quot;。 验证版本： 在 PowerShell 中运行 Get-Module PSReadLine -ListAvailable，检查 PSReadLine 版本。如果没有任何输出或版本低于 2.0，则需要升级，详见下文的 PSReadLine 的升级方法。 # PSReadLine 的升级 如果你的 PSReadLine 版本过旧，请运行以下命令更新：\n1 Install-Module -Name PSReadLine -Force # Emacs 模式：让 PowerShell 更顺手 在深入配置 Emacs 模式之前，我们先来了解一下 PowerShell 的 profile 文件。\n# 什么是 PowerShell Profile PowerShell profile 相当于 PowerShell 的启动脚本。每次启动 PowerShell 时，它都会自动执行 profile 文件中的命令。这使得你可以自定义 PowerShell 环境，例如设置别名、函数、环境变量，以及在本例中，配置 PSReadLine 的编辑模式等。可以把它看作是 PowerShell 的“初始化文件”。\nPowerShell 支持多个 profile 文件，它们根据不同的作用域加载。最常用的 profile 文件路径通常是 C:\\Users\\\u0026lt;用户名\u0026gt;\\Documents\\PowerShell\\Microsoft.PowerShell_profile.ps1，它针对当前用户的所有 PowerShell 会话加载。\n理解了 profile 文件的作用后，我们就可以开始配置 Emacs 模式了。\n# 如何在 PowerShell profile 中设置 Emacs 模式 获取 profile 文件路径 在 PowerShell 中运行 $PROFILE，获取 profile 文件路径。通常这个路径类似于C:\\Users\\\u0026lt;用户名\u0026gt;\\Documents\\PowerShell\\Microsoft.PowerShell_profile.ps1。\n创建配置文件 在默认情况下，PowerShell 的 $PROFILE 文件可能并不存在。可以使用以下命令检查文件是否存在：\n1 Test-Path $PROFILE 如果返回 False，说明该文件还没有创建。可以使用以下命令来创建该文件：\n1 New-Item -Path $PROFILE -ItemType File -Force 编辑 profile 文件 一旦文件创建完成，您可以使用任意文本编辑器打开并编辑 $PROFILE 文件。例如使用 notepad 打开配置文件：notepad $PROFILE\n添加配置 在配置文件中，添加以下行来启用 Emacs 模式：\n1 set-PSReadLineOption -EditMode Emacs 这行代码会在每次启动 PowerShell 时自动将编辑模式设置为 Emacs。\n保存并重启 PowerShell 编辑并保存 $PROFILE 文件后，关闭并重新启动 PowerShell。重新打开后，PowerShell 将自动加载您的配置文件，并启用 Emacs 编辑模式。\n另外，$PROFILE 配置文件可以用于保存各种个性化设置，包括自定义函数、别名、环境变量等。在 PowerShell 7+ 中，PowerShell 使用的 $PROFILE 文件路径会有细微变化，确保在编辑前检查 $PROFILE 的具体路径。\n# 其他 PSReadLine 配置 在配置了 Emacs 模式后，PSReadLine 还有许多其他有用的配置选项，可以进一步提升你的使用体验。以下是一些常用的配置示例：\n自定义历史记录保存路径： 1 Set-PSReadLineOption -HistorySavePath \u0026#34;path/to/your/history.txt\u0026#34; 通过此设置，你可以将 PowerShell 的命令历史记录保存到指定的文件中。\n设置历史记录的最大条数： 1 Set-PSReadLineOption -MaximumHistoryCount 1000 此设置可以限制 PowerShell 保存的历史记录条数，防止历史记录文件过大。\n设置预测命令的来源为历史记录： 1 Set-PSReadLineOption -PredictionSource History 设置命令预测的来源，可以使用历史记录或插件。\n查看和自定义快捷键： 1 Get-PSReadLineKeyHandler 使用此命令可以查看当前 PSReadLine 定义的快捷键。你还可以自定义快捷键，例如：\n1 Set-PSReadLineKeyHandler -Key Ctrl+l -Function ClearScreen 这个命令将 Ctrl + L 键绑定到 ClearScreen 函数，实现清屏的功能。\n# 常用快捷键 掌握一些常用的快捷键可以大大提高在 PowerShell 中的工作效率。以下是一些常用的快捷键：\nPowerShell 快捷键：\nCtrl + C: 中断当前正在执行的命令。 Tab: 自动补全命令、路径或变量名。 Ctrl + R: 搜索历史命令。 ↑ 或 ↓: 浏览历史命令。 Ctrl + Shift + T: 在 Windows Terminal 中打开新的 tab Emacs 模式快捷键：\nCtrl + A: 将光标移动到行首。 Ctrl + E: 将光标移动到行尾。 Ctrl + B: 光标左移一个字符。 Ctrl + F: 光标右移一个字符。 Alt + B: 将光标移动到上一个单词的开头。 Alt + F: 将光标移动到下一个单词的开头。 Alt + D: 删除光标后的单词。 Ctrl + K: 删除光标到行尾的所有内容。 Ctrl + U: 删除光标到行首的所有内容。 通过这个设置和快捷键的掌握，您可以在 PowerShell 中体验更接近 Emacs 的编辑体验，并提高命令行的使用效率。\n","date":"2024-12-28T21:24:25+08:00","permalink":"https://mazhen.tech/p/%E4%B8%BA-powershell-%E9%85%8D%E7%BD%AE-emacs-%E6%A8%A1%E5%BC%8F/","title":"为 PowerShell 配置 Emacs 模式"},{"content":"\n对于希望深入了解 Linux 内核的开发者来说，亲自编译和安装内核是一次有趣的实践。在这篇文章中，我将带你从环境准备开始，逐步完成下载内核源代码、配置、编译和构建自定义内核，最终安装并使用它。让我们开始吧。\n# 构建环境准备 在准备构建环境之前，我们需要先理解 native toolchain (本地工具链) 和 cross-toolchain (交叉工具链) 这两个重要的概念。\n工具链 (toolchain) 是编译软件所需的一系列工具集合，包括 make 构建工具、gcc 编译器、标准 GNU C 库 (glibc)、binutils (包含链接器和汇编器)、gdb 调试器等。\nnative toolchain (本地工具链) 指的是运行在特定架构上，用于为相同架构编译代码的工具集合，例如在 x86_64 架构的 Linux 系统上，使用系统自带的 gcc 编译出的程序，可以直接本地运行。\ncross-toolchain (交叉工具链) 则是在一个架构上运行，用于为另一个不同架构的系统构建软件的工具链。例如在 x86_64 架构的 Linux 系统上，如果需要为 ARM 架构的设备构建程序，就需要使用一个针对 ARM 架构的 cross-toolchain。\n本文将讨论在 x86_64 架构上为本机构建内核的情况，没有涉及交叉编译。在本机为另一个目标平台构建内核在嵌入式开发领域很常见，感兴趣的读者可以参考这篇文档：如何交叉编译为树莓派构建内核。\n我选择常用的 Linux 发行版 Ubuntu 作为示例进行说明。针对其他发行版，以下步骤和原理基本相同，只是具体的命令可能略有差异。\n# 安装本地工具链和开发工具 执行下面的命令安装构建内核所需本地工具链和开发工具：\n1 $ sudo apt install git vim build-essential perl gdb dwarves linux-headers-$(uname -r) 其中，build-essential 是一个用于构建 Debian 软件包的基础工具集，它包含了编译软件包所需的关键工具和库，例如编译器、构建工具以及开发头文件。我们可以使用 apt-cache depends 命令来查看 build-essential 包所包含的内容：\n1 2 3 4 5 6 7 8 9 10 $ apt-cache depends build-essential build-essential |Depends: libc6-dev Depends: \u0026lt;libc-dev\u0026gt; libc6-dev Depends: gcc Depends: g++ Depends: make make-guile Depends: dpkg-dev dwarves 包包含了一组高级的 DWARF 工具，主要用于处理由编译器插入到 ELF 二进制文件中的 DWARF 调试信息。这些调试信息对于调试内核模块至关重要。\n$(uname -r) 命令用于动态获取当前正在运行的内核的版本号。 linux-headers-$(uname -r) 则代表与当前运行的 Linux 内核版本相对应的内核头文件。这些头文件是编译与内核模块相关的软件所必需的。\n# 安装构建内核需要的依赖 执行下面的命令安装构建内核需要的依赖包：\n1 2 3 $ sudo apt install \\ bison flex libncurses5-dev ncurses-dev \\ libelf-dev libssl-dev bc zstd 下面详细解释这些软件包的作用：\nbison：是一个语法分析器生成器。它能将编程语言的语法规则转换为可解析这些规则的程序，即生成语法分析器。从 Linux 4.16 版本开始，构建系统会在构建过程中自动生成解析器，这需要 Bison 2.0 或更高版本。 flex：是一个快速的词法分析器生成器。它用于生成对文本进行模式匹配的程序，即生成词法分析器。词法分析器的作用是对输入文本进行扫描，识别出符合特定规则的字符序列。自 Linux 4.16 版本起，构建系统也会在构建过程中自动生成词法分析器，因此需要 Flex 2.5.35 或更高版本。 ncurses： (全称 \u0026ldquo;new curses\u0026rdquo;) 是一个编程库，提供应用程序编程接口（API），允许开发者在终端中创建与终端类型无关的基于文本的用户界面（TUI）。简而言之，它是开发“类 GUI”应用程序的软件工具包，这些应用程序在终端模拟器中运行，不依赖图形界面。libncurses5-dev 和 ncurses-dev 这两个软件包都是开发基于 ncurses 的文本用户界面程序的开发包，它们包含了 ncurses 库的头文件和开发文件，用于编译和链接基于 ncurses 的程序。 libelf-dev：提供了用于处理 ELF (Executable and Linkable Format，可执行与可链接格式) 文件的开发库。ELF 是一种常见的文件格式，用于可执行文件、目标代码、共享库和核心转储文件。libelf 库使开发者能够以与架构无关的方式读取、修改或创建 ELF 文件，同时处理文件大小和字节序（Endian）等问题。Linux 内核和内核模块都使用 ELF 文件格式，因此在编译和调试内核时，libelf 库是必不可少的。 libssl-dev：是 OpenSSL 项目实现 SSL（Secure Sockets Layer）和 TLS（Transport Layer Security）加密协议的开发包。libssl-dev 包含了 OpenSSL 的头文件和开发库，供开发者在程序中使用这些加密协议来进行加密、解密、证书验证、密钥交换等操作。文档指出，从 Linux 内核 v4.3 版本（如果启用模块签名，则从 v3.7 版本开始）及更高版本，需要安装 OpenSSL 开发包。 bc：是一种任意精度的数字处理语言。它支持交互式执行语句，常用于执行精确的数学计算。在内核构建过程中，bc 用于在头文件中生成时间常数。 zstd：是一种高效的压缩算法工具，具有更好的压缩率和压缩速度。在内核构建过程中，构建成功的未压缩的内核镜像 vmlinux 会使用 ZSTD 算法压缩为 bzImage，这是一种可启动的内核镜像格式。 可以参考内核文档 Minimal requirements to compile the Kernel，列出了编译当前版本 Linux 内核所需的最低软件要求。\n# 获取内核源码 获取 Linux 内核源码主要有两种方式：\n从 www.kernel.org 下载并解压特定版本的内核源码压缩包。 使用 git clone 命令从 Git 仓库克隆内核源码。 # 下载特定版本的内核源码 访问 www.kernel.org，该网站首页会展示多个类型的内核源码，包括：\n主线版（mainline）， 开发阶段的版本，包含最新的功能和改动。mainline 由 Linus Torvalds 亲自维护。 稳定版（stable），在 mainline 的基础上进行修复和测试，更加稳定可靠，适合一般用户使用。 长期支持版（longterm）会长期维护，修复 bug 并保持安全，适合一些有长期支持需求的场景。 linux-next 版本用于测试和集成即将进入主线的特性。 关于这些内核版本的具体说明，可以参考官方文档 Active kernel releases，\n这里我们选择当前最新的稳定版 6.12.6 为例。点击相应的 tarball 链接下载：\n浏览器会将 .tar.xz 格式的压缩文件下载到本地。下载完成后，可以使用 tar 命令解压：\n1 2 3 $ tar -Jxvf linux-6.12.6.tar.xz $ cd linux-6.12.6 $ ls -l # 从 git 仓库 clone 源码 除了下载压缩包，还可以使用 Git 从内核仓库克隆源码。如前所述，Linux 内核有多个类型的源码仓库。\n如果想获取包含最新功能的 mainline 源码仓库，执行下面的命令：\n1 $ git clone https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git 这个命令会克隆 Linus Torvalds 亲自维护的 Linux 内核 mainline 仓库。\n我们的需求是获取最新稳定版本的源码仓库，则需要 clone 稳定版的源码仓库：\n1 2 3 $ git clone git://git.kernel.org/pub/scm/linux/kernel/git/stable/linux-stable.git $ cd linux-stable $ git checkout v6.12.6 这里，我们先克隆稳定版仓库，然后使用 git checkout 命令切换到 v6.12.6 标签。\n克隆完整的 Linux 内核源码通常非常耗时，特别是在网络环境不佳的情况下。为了加快克隆速度，可以使用 depth 参数来限制历史提交的深度，从而减少下载的数据量。例如：\n1 git clone --depth 1 \u0026lt;...\u0026gt; 其中 --depth 1 表示只下载最新的提交记录，可以显著减少下载时间。\n如果只是想基于最新稳定版的源码构建内核，那么还是推荐第一种方法，直接从 kernel.org 下载内核源码的压缩包。\n# 内核源码结构概览 内核源码按照子系统和功能，被组织成目录及其子目录，这种结构化的设计使得内核的维护和开发更加高效。下面我们以鸟瞰的方式，对内核源码的整体结构进行一个粗略的认识：\n文件或目录 描述 README 项目的 README 文件，提供了关于内核的简要说明，以及如何访问最新的官方内核文档的链接：https://www.kernel.org/doc/html/latest/。 COPYING 内核源代码的许可条款 MAINTAINERS 内核子系统的维护者列表，提供了维护者的联系方式，是贡献代码的重要参考。 Makefile 内核顶层的 Makefile kernel/ 核心子系统，包含了进程和线程的生命周期管理、CPU 任务调度、锁机制、cgroups (控制组)、定时器、中断处理、信号、内核模块机制、tracing (跟踪)、eBPF 等核心功能。 mm/ 内存管理子系统，负责内核的内存分配、页面管理、交换等功能。 fs/ 内核虚拟文件系统 (VFS) 和各个具体文件系统的实现，如 ext4, btrfs, overlayfs 等等。 block/ 块设备 I/O 实现，包含了页面缓存、通用块设备 I/O 层、I/O 调度等，负责管理磁盘和其他块设备的数据读写。 net/ 网络协议栈的实现，包含了 TCP, UDP, IP 等网络协议的实现。 ipc/ 进程间通信 (IPC) 子系统，提供了进程间交换数据的机制，例如信号量、共享内存、消息队列等。 sound/ 音频子系统，提供了对音频设备的支持。 virt/ 虚拟化子系统，包含了 KVM (Kernel-based Virtual Machine) 的实现，是 Linux 系统支持虚拟化的关键组件。 Documentation/ 内核官方文档，包含了关于内核各个子系统的详细说明 LICENSES/ 内核代码遵循的所有许可证。 arch/ 体系结构相关的代码，例如 arm, x86，riscv 等，包含了特定于不同 CPU 架构的内核实现。 certs/ 用于生成签名模块的代码，用于确保内核模块的安全性。 crypto/ 内核实现的加密和解密算法，为内核提供安全加密功能。 drivers/ 设备驱动程序代码，包含了各种硬件设备的驱动程序，例如显卡、网卡、USB 设备等。 include/ 架构无关的内核头文件，包含了内核编程所需的各种数据结构和函数声明。特定架构的头文件位于 arch/\u0026lt;cpu\u0026gt;/include/ 目录下。 init/ 内核初始化代码，包含了内核启动时的核心代码。内核主函数 start_kernel() 定义在 init/main.c 中，是内核启动的入口点。 io_uring/ io_uring 的实现，是一个高性能的异步 I/O 框架，用于提高 I/O 性能。 lib/ 类似于用户态应用的共享库 glibc，但这是内核代码所使用的库，提供了一些常用的内核数据结构和辅助函数。 rust/ 支持 Rust 编程语言的内核基础设施，用于在内核中使用 Rust 语言编写模块。 samples/ 各种内核特性的示例代码，可以帮助开发者理解和使用内核 API。 scripts/ 各种有用的脚本，其中一些用于内核构建过程中，例如配置工具、编译脚本等。 security/ Linux 安全模块 (LSM) 的实现，包括 SELinux, AppArmor 等，提供了内核安全机制。 tools/ 用户态工具的源代码，例如 perf 和 eBPF 的用户态工具 usr/ 用于生成和加载 initramfs 镜像，initramfs 是一个小的文件系统，内核在初始化阶段利用 initramfs 执行用户空间代码。 # 关于 MAINTAINERS 文件 在内核源码的根目录，有一个名为 MAINTAINERS 的重要文件。该文件详细列出了所有内核子系统的维护者，以及他们的联系方式，包括邮件列表、代码仓库位置、网站等信息。\n随着内核的不断发展，源代码行数已经非常庞大，目前估计接近 3000 万行。即使是 Linus Torvalds 本人，也无法完全掌握所有细节。因此，如果想要向内核上游贡献补丁，通常不会直接提交到 mainline。Linus 不可能对每个补丁进行详细的审查和合并。MAINTAINERS 文件是内核贡献者了解内核组织结构、找到对应维护者以及提交补丁的关键入口。\n实际上，大多数内核子系统都有自己独立的源码仓库。补丁的讨论、提交和合并过程通常发生在这些子系统的源码仓库中，由该子系统的维护者负责把关。子系统的维护者会定期将已经合并的补丁向 mainline 提交。Linus 信任这些维护者，通常会直接合并他们提交的补丁。这就是内核社区的“信任链”模式。由此可见，维护者在内核开发中扮演着至关重要的角色，他们是内核质量的守护者，也是社区活跃的重要力量。\n今年 10 月发生了一件引人注目的事件，即移除俄罗斯维护者事件。实际上，该事件是将邮件后缀为 ru 的维护者从 MAINTAINERS 文件中移除。我们可以在 mainline 源码仓库中看到当时的提交记录：MAINTAINERS: Remove some entries due to various compliance requirements.。\n关于内核的开发流程，以及如何向上游提交补丁，可以参考内核文档 A guide to the Kernel Development Process。该文档详细介绍了内核贡献的各个环节，包括代码风格、补丁格式、提交方式等等，是每个想参与内核贡献的开发者必读资料。\n# 配置内核 配置是构建内核的关键一步。通过配置，我们可以基于统一的源码，构建出适用于服务器、桌面或嵌入式等不同场景的内核。内核的强大之处在于其高度的可定制性，可以根据不同的硬件平台和应用需求进行裁剪和优化。\n# Kconfig 内核的可配置项定义在一系列 Kconfig 文件中，每个 Kconfig 文件中定义了多个 config 项，表示内核编译选项。这些选项决定了内核最终编译包含哪些功能和特性。通过配置这些选项，可以灵活定制内核以满足不同的需求。\nKconfig 文件分布在源码的各个子目录。如前所述，内核源码是按照子系统和功能组织的，因此每个目录下的 Kconfig 文件通常定义了该目录所实现功能的相关配置选项。例如，mm/Kconfig 文件定义了与内存管理子系统相关的配置选项，而 drivers/net/ethernet/Kconfig 则定义了以太网驱动相关的配置选项。源码根目录下的 Kconfig 文件通过 source 指令引用各个子系统的 Kconfig 文件，而各个子系统的 Kconfig 文件也会通过 source 引用其子目录中的 Kconfig 文件。这种层层引用的方式，使得内核的所有可配置项按照层次结构组织起来，方便管理和维护。\nKconfig 文件使用 Kconfig 语法 来定义配置项的名称、类型、依赖关系等。我们以一个具体的配置项为例，简单了解一下Kconfig 语法。打开 mm/Kconfig文件，找到配置项 ZSWAP：\n1 2 3 4 5 6 7 8 9 10 11 12 config ZSWAP bool \u0026#34;Compressed cache for swap pages\u0026#34; depends on SWAP select CRYPTO select ZPOOL help A lightweight compressed cache for swap pages. It takes pages that are in the process of being swapped out and attempts to compress them into a dynamically allocated RAM-based memory pool. This can result in a significant I/O reduction on swap device and, in the case where decompressing from RAM is faster than swap device reads, can also improve workload performance. 下面的表格解释了配置项 ZSWAP 每一行的含义：\nItem 描述 config ZSWAP 定义了配置项的名称为 ZSWAP bool \u0026ldquo;Compressed cache for swap pages\u0026rdquo; 指定配置项为布尔类型 depends on SWAP 定义了此配置依赖的配置项，即只有当 SWAP 配置项被启用时， ZSWAP 配置项才会被显示出来并可以被配置。 select CRYPTOselect ZPOOL 定义了反向依赖关系，即依赖此配置项的其他配置项 help 配置项的帮助信息，描述了 ZSWAP 的功能和使用方法。 内核构建系统可以读取并解析所有 Kconfig 文件，并以可视化的方式展示出来。\n对内核的配置过程，本质上就是从所有 Kconfig 文件中选择想要设置的配置项，并将最终的配置结果记录在源码根目录下的 .config 文件中。\n内核提供了多种配置方式。可以在源码根目录运行 make help 查看配置目标，它们位于 Configuration targets 标题下：\n接下来，我们会选择其中几个常用的目标进行介绍。\n# 默认配置 最新的 Linux 内核源代码已经接近 3000 万行，其配置项之庞大复杂可想而知。我们可以在源码的根目录下执行以下脚本，统计当前内核的可配置项数量：\n1 2 3 4 $ find . -name \u0026#34;Kconfig*\u0026#34; -print0 \\ | xargs -0 grep -P \u0026#39;^\\s*config\\s+\\w+\u0026#39; \\ | wc -l 20961 从零开始配置两万多个配置项简直是噩梦。好在内核已经准备了默认配置。在前面列出的配置目标中，有一个 defconfig 目标：\n1 defconfig\t- New config with default from ARCH supplied defconfig 其含义是：基于 CPU 架构提供的默认配置，创建一个新的配置。\n内核为每一种 CPU 架构都提供了一个默认配置，这些配置存放在 arch/\u0026lt;cpu\u0026gt;/configs 目录下。运行 defconfig 目标：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ make defconfig HOSTCC scripts/basic/fixdep HOSTCC scripts/kconfig/conf.o HOSTCC scripts/kconfig/confdata.o HOSTCC scripts/kconfig/expr.o LEX scripts/kconfig/lexer.lex.c YACC scripts/kconfig/parser.tab.[ch] HOSTCC scripts/kconfig/lexer.lex.o HOSTCC scripts/kconfig/menu.o HOSTCC scripts/kconfig/parser.tab.o HOSTCC scripts/kconfig/preprocess.o HOSTCC scripts/kconfig/symbol.o HOSTCC scripts/kconfig/util.o HOSTLD scripts/kconfig/conf *** Default configuration is based on \u0026#39;x86_64_defconfig\u0026#39; # # configuration written to .config # 可以看到，make defconfig 会基于当前 x86_64 架构的默认配置 x86_64_defconfig，生成最终的配置结果 .config。你可以使用 vi 打开 .config 查看详细的配置信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # # Automatically generated file; DO NOT EDIT. # Linux/x86 6.12.6 Kernel Configuration # CONFIG_CC_VERSION_TEXT=\u0026#34;gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\u0026#34; CONFIG_CC_IS_GCC=y CONFIG_GCC_VERSION=130300 CONFIG_CLANG_VERSION=0 CONFIG_AS_IS_GNU=y CONFIG_AS_VERSION=24200 CONFIG_LD_IS_BFD=y ... # CONFIG_KERNEL_BZIP2 is not set ... CONFIG_NF_LOG_SYSLOG=m ... .config 文件中的每一行都表示一个配置项，有以下几种形式：\nCONFIG_NAME=y: 表示该配置项被启用，并直接编译进内核。 CONFIG_NAME=m: 表示该配置项被启用，但会编译为可加载的内核模块。 # CONFIG_NAME is not set: 表示该配置项未被启用。 CONFIG_NAME=\u0026quot;text\u0026quot;: 表示该配置项的值是一个文本字符串，通常用于设置内核版本、模块名称等文本信息，例如CONFIG_LOCALVERSION=\u0026quot;-my-kernel\u0026quot;。 CONFIG_NAME=number: 表示该配置项的值是一个数字，通常用于设置内核参数、缓冲区大小等数值信息，例如 CONFIG_NR_CPUS=8 表示 CPU 的核数。 注意，正如 .config 文件开头所强调的，该文件是自动生成的，切勿手动修改。\n# 基于现有发行版的配置 另一种简便的配置方法是：基于现有发行版的内核配置。这种配置方式与默认配置一样简单，但通常比直接使用默认配置更好。因为发行版的内核配置通常由专业的工程师进行裁剪和优化，并经过了厂商的充分测试，更能适应实际的应用场景。\n以我当前的构建环境为例，我使用的是 Ubuntu 24 server 版：\n1 2 3 4 5 $ cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=24.04 DISTRIB_CODENAME=noble DISTRIB_DESCRIPTION=\u0026#34;Ubuntu 24.04.1 LTS\u0026#34; 在重新配置之前，运行 make mrproper 清理所有构建过程生成的内容，包括已存在的 .config 文件。这确保我们从一个干净的状态开始：\n1 2 3 4 $ make mrproper CLEAN scripts/basic CLEAN scripts/kconfig CLEAN include/config include/generated .config 然后运行 oldconfig 目标：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 make oldconfig HOSTCC scripts/basic/fixdep HOSTCC scripts/kconfig/conf.o ... # # using defaults found in /boot/config-6.8.0-51-generic # .config:965:warning: symbol value \u0026#39;0\u0026#39; invalid for BASE_SMALL .config:10817:warning: symbol value \u0026#39;m\u0026#39; invalid for ANDROID_BINDER_IPC .config:10818:warning: symbol value \u0026#39;m\u0026#39; invalid for ANDROID_BINDERFS * * Restart config... * * * General setup * Compile also drivers which will not load (COMPILE_TEST) [N/y/?] n Compile the kernel with warnings as errors (WERROR) [N/y/?] n Local version - append to kernel release (LOCALVERSION) [] Automatically append version information to the version string (LOCALVERSION_AUTO) [N/y/?] n ... 可以看到，make oldconfig 会基于 /boot/config-6.8.0-51-generic 文件进行配置。在安装和升级内核时，内核镜像 vmlinuz-version-EXTRAVERSION 默认存储在 /boot 目录下，同时该内核对应的配置文件也会存放在 /boot 目录。make oldconfig 正是使用了当前运行内核所对应的配置。\n由于正在构建的最新内核可能新增或修改了配置项，与当前内核的配置存在差异，因此接下来需要用户逐条设置必要的配置项。通常情况下，一路回车，接受默认选项即可，最终会生成 .config 文件。\n你可能会觉得，对于差异的配置项，逐条确认默认选项比较繁琐。有没有办法一次性确认使用默认选项呢？当然可以，使用 olddefconfig 目标。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ make mrproper $ make olddefconfig HOSTCC scripts/basic/fixdep HOSTCC scripts/kconfig/conf.o HOSTCC scripts/kconfig/confdata.o ... # # using defaults found in /boot/config-6.8.0-51-generic # .config:965:warning: symbol value \u0026#39;0\u0026#39; invalid for BASE_SMALL .config:10817:warning: symbol value \u0026#39;m\u0026#39; invalid for ANDROID_BINDER_IPC .config:10818:warning: symbol value \u0026#39;m\u0026#39; invalid for ANDROID_BINDERFS # # configuration written to .config # 与 oldconfig 类似，olddefconfig 也会基于当前内核的配置生成 .config 文件。不同之处在于，使用 olddefconfig 时，对于新增的配置项，会直接设置为默认值，无需用户逐个确认。\n# 基于当前加载的内核模块 基于现有发行版的内核配置生成 .config 文件虽然非常方便，但如果你觉得发行版预置的配置有些臃肿，启用了过多的模块或内置功能，那么你可以考虑使用另一种方式：基于当前加载到内存中的内核模块，来生成自定义的配置。这种配置方式通常比发行版的默认配置更为紧凑，因为它只包含你当前系统实际使用的模块和功能。\nlsmod 命令可以列出当前驻留在内存中的所有内核模块。我们可以将 lsmod 命令的输出提供给内核构建系统，让构建系统基于当前系统正在运行的内核模块生成配置：\n1 2 $ lsmod \u0026gt; /tmp/lsmod.now $ make LSMOD=/tmp/lsmod.now localmodconfig 这里，我们将 lsmod 命令的输出保存到一个临时文件 /tmp/lsmod.now 中，然后通过 LSMOD 环境变量传递给 Makefile 的 localmodconfig 目标。这样，内核构建系统就能基于内存中实际加载的内核模块，生成最终的配置 .config 文件。\n由于这种配置方式比较常用，内核还提供了类似于 localmodconfig 目标的辅助脚本 scripts/kconfig/streamline_config.pl。该脚本会自动检查系统当前加载的模块，然后基于这些模块生成精简的内核配置。你可以使用以下方式来生成配置：\n1 2 $ scripts/kconfig/streamline_config.pl $ make olddefconfig 首先运行脚本 streamline_config.pl 生成 .config 文件，然后再运行 make olddefconfig，为新版本的内核可能新增的配置项设置默认值。\n# 使用图形界面配置 内核构建系统还提供了图形界面的配置方式。通常，我们会先使用前述的方法生成一个基础配置，然后使用图形界面进行一些微调。在命令行运行 make menuconfig，构建系统会编译并执行 scripts/kconfig/mconf 程序，从而启动一个图形化的配置界面：\n让我们简单介绍一下这个界面的元素：\n方括号 [] 表示布尔类型的选项 [*] 表示启用该特性，编译到内核镜像中 [] 表示关闭该特性 尖括号 \u0026lt;\u0026gt; 具有三种状态 \u0026lt;*\u0026gt; 表示启用该特性，编译到内核镜像中 \u0026lt;M\u0026gt; 表示启用该特性，编译为内核模块 \u0026lt;\u0026gt; 表示关闭该特性 -*- 表示由于依赖要求，此特性必须被启用 {M|*} 表示由于依赖要求，此特性必须编译到内核镜像中（*），或者编译为内核模块（M） (...) 表示需要输入字符或数字。在此选项上按下回车键，将弹出一个输入提示框。 \u0026lt;...\u0026gt; ---\u0026gt; 表示子菜单，按下回车键可进入该子菜单。 在界面上，使用上、下方向键在不同的选项之间进行导航，使用左、右方向键在屏幕下方的菜单 \u0026lt;Select\u0026gt;、\u0026lt;Exit\u0026gt; 等之间进行导航。在当前选中的选项上，导航到 \u0026lt;Help\u0026gt; 菜单并按下回车键，会显示该配置项的帮助信息：\n帮助信息界面会显示当前配置项的名称、类型、依赖关系等，其中还包括了该配置项具体定义在哪个 Kconfig 文件中。这些信息与我们在 Kconfig 文件中看到的内容是一致的。\n在当前高亮的配置项上按空格键可以修改其值。\n下面我们尝试使用图形界面设置几个配置项：\nCONFIG_LOCALVERSION 配置项 CONFIG_LOCALVERSION 的作用是在内核版本号的末尾附加一个自定义的字符串。\n我们先简单了解一下 Linux 内核的版本号命名规则：\n1 major.minor[.patchlevel][-EXTRAVERSION] major：主版本号 minor：次版本号，隶属于主版本号 patchlevel：修订版本号，通常用于修复重大错误和安全问题 EXTRAVERSION：由内核发行版指定，用于跟踪内部修改 可以使用 uname -r 命令查看当前内核的版本信息。以我当前的系统为例：\n1 2 $ uname -r 6.8.0-51-generic 其中，前面三段 6.8.0 分别为主版本号、次版本号和修订版，最后一部分 -51-generic 是 EXTRAVERSION。\n配置项 CONFIG_LOCALVERSION 的导航路径为 General setup -\u0026gt; Local version - append to kernel release。选中该选项并按下回车键，会出现输入提示框，将值修改为你想要设置的内容，例如 -apusic-kernel，可以将公司名称嵌入到版本号中，以标识该内核发行版的厂商。\nCONFIG_IKCONFIG 和 CONFIG_IKCONFIG_PROC 配置项 CONFIG_IKCONFIG 的作用是控制在构建内核时，是否将 .config 文件保存到内核映像中。如果启用此选项，那么可以使用脚本 scripts/extract-ikconfig 从内核镜像文件中提取到所有的配置信息。\n配置项 CONFIG_IKCONFIG_PROC 的作用是，如果启用此选项，则可以通过 /proc/config.gz 访问内核的配置文件。\nCONFIG_IKCONFIG 的导航路径为 General setup -\u0026gt; Kernel .config support。选中该选项并使用空格键将其值修改为 \u0026lt;*\u0026gt;。这时下方会出现新的选项 Enable access to .config through /proc/config.gz (NEW)，再次使用空格键激活此选项。\nCONFIG_HZ_250 当前内核使用四个不同的配置项，分别代表不同的定时器中断频率：\nCONFIG_HZ_100：100Hz，即每秒 100 次中断。较低的频率可能更适合服务器、SMP 和 NUMA 系统。服务器通常不需要像桌面计算机那样快速响应用户交互，而是需要高效地处理大量后台任务。 CONFIG_HZ_250：250Hz，即每秒 250 次中断。这是一种折衷选择，既能保证服务器性能，又能在 SMP 和 NUMA 系统上表现出良好的交互响应。 CONFIG_HZ_300：300Hz，即每秒 300 次中断。与 250Hz 类似，也是一种折衷选择，并且能精确适配 PAL 制和 NTSC 制的帧率，因此非常适合视频和多媒体工作。因为 PAL 制的帧率为 50Hz，NTSC 制的帧率为 60Hz，300Hz 可以被这两种制式的帧率整除，有利于视频播放和同步，减少视频处理过程中的误差和抖动。 CONFIG_HZ_1000：1000Hz，即每秒 1000 次中断。这种高频率通常用于需要快速响应用户交互的系统，如桌面计算机。 首先进入子菜单 Processor type and features，找到 Timer frequency 选项并按下回车键进入。然后在弹出的选项中选择你想要设置的频率。这里我选择设置为 250Hz。\n完成设置后，使用左右方向键导航到 \u0026lt;Exit\u0026gt;，一路退出，最后选择保存设置。这样，我们就完成了使用图形界面对内核进行配置。\n# 使用脚本配置 除了使用图形界面进行配置，内核构建系统还提供了一个 Bash 脚本 scripts/config，允许我们以非交互的方式完成配置。这在需要自动化配置，或者需要在脚本中批量设置配置项时非常有用。\n例如，在 Ubuntu 系统上构建内核时，可能会遇到证书问题。这时，我们可以使用 scripts/config 脚本，将以下两个配置项设置为空字符串：\n1 2 scripts/config --set-str SYSTEM_REVOCATION_KEYS \u0026#34;\u0026#34; scripts/config --set-str SYSTEM_TRUSTED_KEYS \u0026#34;\u0026#34; scripts/config 脚本提供了多种选项，可以用于设置不同类型的配置值。通过 --set-str 选项，我们可以将配置项的值设置为指定的字符串。除了设置字符串类型的值，scripts/config 还支持其他操作，例如：\n--enable \u0026lt;CONFIG_NAME\u0026gt;: 启用一个配置项。 --disable \u0026lt;CONFIG_NAME\u0026gt;: 禁用一个配置项。 例如前面我们通过图形界面启用的CONFIG_IKCONFIG 和 CONFIG_IKCONFIG_PROC 配置项，等效于使用下面的命令：\n1 $ scripts/config --enable IKCONFIG --enable IKCONFIG_PROC 至此，我们已经学习了内核的多种配置方式，接下来就可以开始构建内核了。\n# 构建内核镜像和内核模块 Linux 内核采用了递归式的 make 构建方式。在内核源码的根目录下，有一个顶层的 Makefile 文件，该文件会递归地解析嵌入在各个子目录中的 Makefile 文件。通过运行 make help 命令，我们可以查看默认情况下 all 目标会构建哪些内容：\n1 2 3 4 5 6 7 8 9 $ make help ... Other generic targets: all\t- Build all targets marked with [*] * vmlinux\t- Build the bare kernel * modules\t- Build all modules ... Architecture-specific targets (x86): * bzImage\t- Compressed kernel image (arch/x86/boot/bzImage) 从输出中可以看到，执行 make all 命令会构建前面标记为 * 的目标，包括：\nvmlinux 目标：构建出未压缩的内核镜像文件 vmlinux。 modules 目标：在内核配置中被标记为 M 的选项，都会被构建为内核模块（.ko 文件）。 bzImage 目标：构建出被压缩过的内核镜像文件 bzImage 。 在系统启动过程中，真正被使用的是压缩过的内核镜像。引导程序会将它加载到内存中，并在内存中解压，然后引导系统进入内核。vmlinux 是未压缩的内核映像，其中包含了所有内核符号等额外的调试信息，虽然它不会被直接用于系统启动，但在进行内核调试时会用到，所以它仍然非常重要。\n在内核源码的根目录下执行 make 命令，默认会执行 all 目标，即等同于执行 make all 命令，这样就可以构建出内核镜像和内核模块。\n由于 Linux 内核源码库非常庞大，构建内核是一项非常消耗内存和 CPU 资源的任务。为了加快构建速度，make 工具支持多进程并行处理。我们可以使用 -jn 选项，生成多个进程，并行处理构建过程中相互独立的任务。其中，n 表示可以并行生成的任务数量的上限，通常可以根据下面的经验公式确定 n 的值：\n1 n = number-of-CPU-cores * factor; factor 是一个系数，一般选择为 2。\n那么，如何知道当前系统的 CPU 核数呢？可以使用 nproc 命令：\n1 2 $ nproc 12 lscpu 命令也可以显示 CPU 核数，并且提供了更详细的 CPU 信息：\n1 2 3 4 5 6 7 8 $ lscpu Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Address sizes: 39 bits physical, 48 bits virtual Byte Order: Little Endian CPU(s): 12 On-line CPU(s) list: 0-11 .... 我当前系统的 CPU 核数为 12，因此 n 可以设置为 24。使用下面的命令构建内核：\n1 $ make -j24 构建过程的输出信息非常多，我们可以使用 tee 命令，将标准输出和标准错误信息显示在控制台，并将所有的输出信息保存到 out.log 文件中：\n1 $ make -j24 2\u0026gt;\u0026amp;1 | tee out.log 如果想查看更详细的构建信息，例如 gcc 编译选项，可以使用 V=1 详细模式选项：\n1 $ make -j24 V=1 2\u0026gt;\u0026amp;1 | tee out.log 构建内核是一个耗时的过程。如果你想了解整个构建过程具体花费了多少时间，可以使用 time 命令，它会在 make 命令执行完成后显示执行时间：\n1 $ time make -j24 2\u0026gt;\u0026amp;1 | tee out.log 一切准备就绪，让我们开始构建吧！\n如果没有意外，make 命令将成功执行完成，并生成以下关键文件：\n未压缩的内核映像文件 vmlinux，位于内核源码根目录下 符号地址映射文件 System.map，位于内核源码根目录下 压缩的内核映像文件 bzImage，位于 arch/\u0026lt;cpu\u0026gt;/boot/ 目录下。对于 x86 架构，它的实际位置是 arch/x86/boot/bzImage。 .ko 文件，内核配置选项中被标记为 M 的内核模块，它们会散布在内核源码的各个子目录中。 获得这些构建成果后，下一步就是安装和使用它们了。\n# 安装内核模块 在上一步构建完成后，我们生成了许多内核模块（.ko 文件）。可以使用 find 命令找到这些模块文件：\n1 2 3 4 5 6 $ find . -name \u0026#34;*.ko\u0026#34; ./arch/x86/platform/atom/punit_atom_debug.ko ./arch/x86/crypto/aria-aesni-avx-x86_64.ko ./arch/x86/crypto/sm3-avx-x86_64.ko ./arch/x86/crypto/curve25519-x86_64.ko ... 内核模块以模块化的方式提供内核功能，这使得我们能够根据需要加载或从内核内存中移除特定的功能模块，从而提高内核的灵活性和可维护性。\n构建完成的内核模块需要被安装到指定的位置，这样在系统启动时，才能被正确找到并加载到内核内存中。执行以下命令来安装内核模块：\n1 2 3 4 5 6 7 $ sudo make modules_install [sudo] password for mazhen: SYMLINK /lib/modules/6.12.6-apusic-kernel/build INSTALL /lib/modules/6.12.6-apusic-kernel/modules.order INSTALL /lib/modules/6.12.6-apusic-kernel/modules.builtin INSTALL /lib/modules/6.12.6-apusic-kernel/modules.builtin.modinfo ... 从输出中可以看出，内核模块被安装到了 /lib/modules/6.12.6-apusic-kernel 目录下。注意，目录名 6.12.6-apusic-kernel 实际上就是我们构建的内核版本号。在系统中，每个已安装的内核都会在 /lib/modules/ 目录下有一个对应的目录，并以内核版本号命名，用于存放对应版本的内核模块。这样，系统在启动时就可以加载正确版本的内核模块。\n安装完内核模块后，下一步就要安装内核本身了。\n# 安装内核 执行以下命令来安装新构建的内核：\n1 sudo make install install 目标实际上完成了以下三项关键任务：\n生成 initramfs (以前称为 initrd) 镜像。 将内核镜像及相关文件安装到 /boot 目录。 为新内核镜像配置 GRUB 引导程序。 下面将分别详细介绍这些步骤。\n# initramfs 镜像 install 目标首先会生成 initramfs 镜像。那么，initramfs 是什么，它又有什么作用呢？\ninitramfs 的全称是 initial RAM filesystem，即初始 RAM 文件系统。它是一个使用 cpio 工具创建的归档文件包。tar 工具内部也使用了 cpio，所以可以认为 initramfs 就是一个经过压缩的归档文件包。\n那么，initramfs 包含了什么内容呢？简单来说，initramfs 打包了一个精简版的 root 文件系统，其中包含了在系统初始化阶段需要用到的内核模块、设备驱动和用户态工具。\n为什么需要 initramfs 呢？可以想象一下这个场景：当引导程序加载了内核镜像后，内核开始初始化工作，准备挂载真正的 root 文件系统。这时，内核需要加载文件系统对应的内核模块，但是这些内核模块此时还在磁盘上，在 root 文件系统完成挂载之前，内核还不能访问它们。这就产生了一个“先有鸡还是先有蛋”的问题：为了挂载 root 文件系统，需要从磁盘加载内核模块；而为了能加载内核模块，又需要先挂载 root 文件系统。\ninitramfs 就是为了解决这个问题而存在的。它包含了系统初始化阶段必须用到的内容。在内核挂载真正的 root 文件系统之前，initramfs 作为临时的 root 文件系统被挂载，为内核提供了一个基本的运行环境，使得内核能够执行一些必要的初始化操作。\n# 安装内核镜像 install 目标在生成 initramfs 镜像后，会将内核镜像及相关文件复制到 /boot 目录，包括以下文件：\nconfig-6.12.6-apusic-kernel：内核配置文件。 System.map-6.12.6-apusic-kernel：内核符号地址映射文件。 initrd.img-6.12.6-apusic-kernel：上一步生成的 initramfs 镜像文件。initrd 是 initramfs 的旧称，现在仍被沿用。 vmlinuz-6.12.6-apusic-kernel：arch/x86/boot/bzImage 文件的一个副本，也就是压缩过的内核镜像文件。 vmlinuz 中的 z 表示使用了 gzip 压缩，但实际上现在默认使用的是更优秀的 ZSTD 压缩算法，但文件命名依然保留。这也是为什么在最开始准备构建环境安装依赖时，需要安装 zstd 包。 另外，Ubuntu 提供了 mkinitramfs 和 unmkinitramfs 脚本，用于打包和解包 initramfs 镜像。我们可以使用 unmkinitramfs 脚本来查看 initramfs 镜像内部的结构：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 $ TMPDIR=$(mktemp -d) $ unmkinitramfs /boot/initrd.img-6.12.6-apusic-kernel $ tree ${TMPDIR} |-- early | `-- kernel | `-- x86 | `-- microcode | `-- AuthenticAMD.bin ... `-- main |-- bin -\u0026gt; usr/bin |-- conf | |-- arch.conf | |-- conf.d | |-- initramfs.conf | `-- modules |-- cryptroot | `-- crypttab |-- etc | |-- console-setup ... |-- init |-- lib -\u0026gt; usr/lib |-- lib.usr-is-merged -\u0026gt; usr/lib.usr-is-merged ... |-- usr | |-- bin | | |-- [ | | |-- [[ | | |-- acpid | | |-- arch | | |-- ascii | | |-- ash | | |-- awk | | |-- base32 ... | | |-- modules | | | `-- 6.12.6-apusic-kernel | | | |-- kernel | | | | |-- arch | | | | | `-- x86 | | | | | `-- crypto | | | | | |-- aegis128-aesni.ko ... | | | | |-- drivers | | | | | |-- acpi | | | | | | |-- nfit | | | | | | | `-- nfit.ko | | | | | | |-- platform_profile.ko | | | | | | `-- video.ko | | | | | |-- ata | | | | | | |-- acard-ahci.ko ... | |-- sbin | | |-- blkid | | |-- cache_check -\u0026gt; pdata_tools | | |-- cryptsetup | | |-- dhcpcd ... 447 directories, 1540 files # 更新 GRUB 配置 install 目标的最后一步是为新构建的内核镜像配置 GRUB 引导程序。我们可以查看 GRUB 的配置文件 /boot/grub/grub.cfg，会发现它已经被更新，配置了我们最新构建的内核镜像和 initramfs 镜像。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ... menuentry \u0026#39;Ubuntu, with Linux 6.12.6-apusic-kernel\u0026#39; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option \u0026#39;gnulinux-6.12.6-apusic-kernel-advanced-c3b02810-62dc-430b-b030-79c44c7a231f\u0026#39; { recordfail load_video gfxmode $linux_gfx_mode insmod gzio if [ x$grub_platform = xxen ]; then insmod xzio; insmod lzopio; fi insmod part_gpt insmod ext2 search --no-floppy --fs-uuid --set=root c3b02810-62dc-430b-b030-79c44c7a231f echo \u0026#39;Loading Linux 6.12.6-apusic-kernel ...\u0026#39; linux /boot/vmlinuz-6.12.6-apusic-kernel root=UUID=c3b02810-62dc-430b-b030-79c44c7a231f ro quiet splash quiet splash $vt_handoff echo \u0026#39;Loading initial ramdisk ...\u0026#39; initrd /boot/initrd.img-6.12.6-apusic-kernel } ... 现在，重启系统，默认就会使用我们新构建的内核了。\n# 定制 GRUB GRUB 默认会使用最新构建和安装的内核进行引导。然而，这种默认行为有时可能不符合我们的需求。例如，我们自己构建的内核可能包含实验性的代码和配置，其稳定性可能不如发行版提供的内核。因此，我们可能希望在系统引导阶段看到 GRUB 菜单，以便选择使用哪个内核启动，并将默认启动选项设置为一个稳定的发行版内核。\n定制 GRUB 非常简单，我们只需要以 root 用户身份编辑 GRUB 的主配置文件/etc/default/grub。\n1 2 3 4 5 6 GRUB_DEFAULT=\u0026#34;Advanced options for Ubuntu\u0026gt;Ubuntu, with Linux 6.8.0-51-generic\u0026#34; GRUB_TIMEOUT_STYLE=menu GRUB_TIMEOUT=3 GRUB_DISTRIBUTOR=`( . /etc/os-release; echo ${NAME:-Ubuntu} ) 2\u0026gt;/dev/null || echo Ubuntu` GRUB_CMDLINE_LINUX_DEFAULT=\u0026#34;quiet splash\u0026#34; GRUB_CMDLINE_LINUX=\u0026#34;quiet splash\u0026#34; 首先，我们需要设置默认启动选项 GRUB_DEFAULT。GRUB_DEFAULT 的默认值为 0，表示 GRUB 菜单中的第一个启动项。这样的设置会导致 GRUB 总是使用最新安装的内核作为默认选项。为了避免这种情况，我们将 GRUB_DEFAULT 的值设置为一个具体的菜单项，这样就可以固定默认启动选项，即使安装了新内核也不会改变。注意，子菜单之间需要使用 \u0026gt; 连接，并且文本内容要与 GRUB 菜单项严格一致。\nGRUB_TIMEOUT_STYLE=menu 设置了在引导阶段显示 GRUB 菜单选项。菜单的超时时间由 GRUB_TIMEOUT 设置，本例中设置为 3 秒。如果在 3 秒内没有用户操作，系统将会使用默认选项引导。\n在编辑并保存 /etc/default/grub 文件后，我们需要以 root 用户身份运行 update-grub 命令，以使修改生效：\n1 sudo update-grub 此时，当我们重启系统时，就可以看到 GRUB 菜单。在 Advanced options for Ubuntu 子菜单下，可以选择使用我们新构建的内核 Ubuntu, with Linux 6.12.6-apusic-kernel 进行启动。\n# 验证新内核的配置 现在，我们已经成功地使用自己构建的内核启动了系统！接下来，让我们检查一下之前在配置内核时设置的配置项是否已经生效。\n首先，我们可以检查内核版本：\n1 2 $ uname -r 6.12.6-apusic-kernel 没错，正是我们为 CONFIG_LOCALVERSION 配置项设置的值。\n启用 CONFIG_IKCONFIG 配置项后，内核会包含自身的配置信息。我们可以使用脚本 scripts/extract-ikconfig 从内核镜像中提取出配置信息：\n1 2 3 4 5 6 7 8 9 10 $ scripts/extract-ikconfig /boot/vmlinuz-6.12.6-apusic-kernel # # Automatically generated file; DO NOT EDIT. # Linux/x86 6.12.6 Kernel Configuration # CONFIG_CC_VERSION_TEXT=\u0026#34;gcc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\u0026#34; CONFIG_CC_IS_GCC=y CONFIG_GCC_VERSION=130300 CONFIG_CLANG_VERSION=0 ... 如果启用了 CONFIG_IKCONFIG_PROC 选项，内核配置信息会通过 proc 文件系统的文件 /proc/config.gz 以压缩格式暴露给用户。我们可以使用 gunzip 和 grep 命令来查找配置项 CONFIG_HZ_250：\n1 2 $ gunzip -c /proc/config.gz | grep CONFIG_HZ_250 CONFIG_HZ_250=y 输出结果显示 CONFIG_HZ_250=y，说明我们配置的定时器中断频率也已生效。一切都是那么 perfect！\n# 总结 从准备构建环境开始，我们了解了如何获取内核源码，掌握了多种配置内核的方法，然后成功地构建并安装了内核，定制了 GRUB 启动选项，最终验证了我们自己构建的内核。希望通过这一系列的实践，你能有所收获。\n","date":"2024-12-27T14:50:10+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8E%E6%BA%90%E7%A0%81%E6%9E%84%E5%BB%BA-linux-%E5%86%85%E6%A0%B8/","title":"从源码构建 Linux 内核"},{"content":"指针是 C 语言公认的难点，要不然也不会有那么多 C 语言的书籍专门将“指针”放在标题中进行强调了：\n《C 和指针（Pointers on C）》 《征服 C 指针》 《深入理解 C 指针（Understanding and using C pointers）》 Pointers in C: A Hands on Approach \u0026hellip; 同时，指针也是 C 语言最重要的特性，你不可能在不掌握指针的情况下用好 C。本文试图带你彻底攻克这个难点，让你可以像 Neo 看透 Matrix 一样，看破“指针”。\n本文的主要内容来自《征服 C 指针》和《C 专家编程》，这不是两本通常意义的 C 语言入门书，没有罗列式的讲解 C 的语法，而是能将知识点融会贯通，带有作者鲜明的个人风格，体现出他们丰富的实践经验。《C 专家编程》的作者 Peter van der Linden 曾在 Sun 和 Apple 工作，是 Sun 编译器、SunOS kernel 和 Solaris kernel 的核心开发成员。\n# 什么是指针 指针概念本身并不难，稍有编程经验的人都很容易理解：和 int，double 等类型一样，指针也是一种类型，它的值是内存地址。我们用指针来保存其他类型变量的地址。\n如上图，变量 num1 的类型是 int，它的值是 5；变量 num_p 的类型是指向 int 的指针类型，它的值是变量 num1 的内存地址。\n指针变量 num_p 在初始化时不指向任何变量：\n1 2 3 int num1 = 5; int num2 = 10; int *num_p = NULL; 可以对 int 类型的变量 num1 使用取地址运算符 \u0026amp;，获取变量 num1 的地址，然后赋值给指针变量 num_p ，这时指针 num_p 中的值就是变量 num1 的地址：\n1 num_p = \u0026amp;num1; 对指针使用间接运算符 *，表示该指针指向的变量。可以通过指针 num_p 修改变量 num1 的值：\n1 2 3 4 5 // 通过 num_p 输出 num1 的值 printf(\u0026#34;*num_p..%d\\n\u0026#34;, *num_p); // 通过 num_p 更改 num1 的值 *num_p = 10; 补充\n变量名本质上是内存地址的别名，只是为了引用地址方便。在编译后生成的机器代码中，变量名被替换为相应的内存地址，变量名本身不会出现在最终的机器代码中。为了调试方便，编译器在生成可执行文件时可以选择保留符号信息（使用 -g 选项）。调试符号信息包含了变量名、行号、文件名等信息。\n# 为什么 C 的指针很难 了解了指针的基本概念，你会觉得：就这，一点也不难呀。实际上，C 语言指针的难点并不是指针概念本身，而是在于下面两个原因：\n指针和 C 语言混乱的声明语法纠缠在一起 指针和数组的微妙关系 先看第一个原因，下面这个函数原型声明来自 telnet 程序：\n1 char* const *(*next)(); 初次看到这样的声明让人头大，这到底是什么指针呀！不能简单的用“指针就是地址”来理解这里的指针。\n第二个原因，数组和指针在很多情况下可以“互换”使用。例如声明 int 数组和指针，并将数组赋值给指针：\n1 2 3 int array[5]; int *p; p = array; 我们可以使用指针运算的方式访问数组元素：\n1 *(p + i) = i; 也可以把指针当做数组，使用下标访问数组元素：\n1 p[i] = i; 甚至可以对数组变量进行指针运算：\n1 *(array + i) = i; 给人的感觉是，数组和指针完全等价，可以互换使用，但这个说法并不完全正确。\n数组和指针本来就是不同的东西，虽然在很多场景下它们可以互换使用，但有些场景却必须严格区分。对指针和数组的混乱认知，是造成指针难以理解的另一个重要原因。\n因此，我们要攻克 C 语言的指针，只了解指针的基本概念是完全不够的，必须彻底搞懂 C 语言的声明，以及指针和数组的关系。下面我就分别进行介绍。\n# 解读 C 语言的声明 # 混乱的声明语法 C 的声明语法有些奇怪，原因是 C 语言最初的设计哲学：对象的声明形式与它的使用形式尽可能相似。例如声明一个 int 类型的指针数组：\n1 int *p[3]; 然后以 *p[i] 这样的形式使用指针所指向的 int 数据，声明和使用的形式非常相似。然而这并不是一个确定的规则，例如当我们在指针类型上使用下标运算符时：\n1 2 3 4 int *p; p = array; ... p[0] = 0; 对指针的声明和使用形式就完全不同。在引入 volatile 和 const 关键字后，就出现了更大的破绽：这些关键字只能出现在声明中​，并不会出现在使用中，声明和使用完全相同的情况越来越少。看来当初决定让声明和使用形式上相同不是一个好主意。\n回到前面 int 指针数组的声明 int *p[3]，只从声明的表面上看，你不能确定 int *p[3] 是一个 int 类型的指针数组，还是一个指向 int 数组的指针。你必须记住操作符的优先级，下标操作符 [] 的优先级最高，才能确定 p 首先是一个数组，然后它的元素是 int 类型的指针。\n这就是 C 语言声明最大的问题，无法以一种自然方式从左向右解读一个声明。遇到下面这样的声明，即使是经验丰富的 C 程序员也会觉得麻烦：\n1 char * const *(*next)(); 《C 专家编程》给出了一种方法，让解读复杂声明变得轻松。\n# C 声明的优先级规则 我们可以按照下面表格分步拆解声明，将 C 的声明解读为自然语言。\nC 声明的优先级规则 A 声明从名称开始，然后按照优先级顺序阅读。 B 优先级从高到低的顺序为： B.1 声明中被括号括起来部分 B.2 后缀运算符： 括号 () 表示函数 方括号 [] 表示数组 B.3 前缀运算符：星号表示“指向\u0026hellip;的指针”。 C 如果 const 和/或 volatile 关键字紧邻类型说明符（例如 int、long 等），则它们作用于类型说明符。\n否则，const 和/或 volatile 关键字作用于其左边的指针星号。 现在用上面表格定义的优先级规则，来解读这个原型声明：\n1 char * const *(*next)(); 应用规则 解释 A 首先，找到变量名 \u0026ldquo;next\u0026rdquo;，并注意它直接被括号包围 B.1 然后，我们将括号中的内容作为一个整体 B.3 进入括号内，注意到前缀操作符 *，得出“next 是一个指针，它指向\u0026hellip;” B 然后，我们走出括号，可以选择前缀操作符 * 或者后缀操作符 () B.2 规则 B.2 告诉我们优先级较高的是右边的函数括号 ()，因此我们得到“next 是指针，它指向一个函数，这个函数返回\u0026hellip; ”。 B.3 接着处理前缀星号 *，得出“这个函数返回一个指针，它指向\u0026hellip;” C 最后，将 \u0026ldquo;char * const\u0026rdquo; 解释为“指向 char 的只读指针” 将所有部分结合在一起可以解读为：\n“next 是一个指针，它指向一个函数，这个函数返回一个指针，该指针指向另一个指针，它是一个指向 char 的只读指针。”\n对照上面的优先级规则表格，我们很容易的将复杂的声明，转换为了易于理解的自然语言。\n再试试另外一个例子：\n1 char *(* c[10])(int **p); 按照声明的优先级规则解读：\n应用规则 解释 A 从变量名 c 开始 B 可以选择前缀操作符 * ，或者后缀操作符 [] B.2 选择优先级较高的右边方括号[]，因此我们得到“c 是一个包含了 10 个元素数组，元素类型是\u0026hellip;” B.3 接着处理前缀星号 *，得出“元素类型是指针，它指向\u0026hellip;” B.1 将括号中的内容作为一个整体 B 可以选择前缀操作符 * 或者后缀操作符 () B.2 选择优先级较高的右边函数括号 ()，因此我们得到“**\u0026hellip; 是一个函数，这个函数接收的参数是 int **p，返回值是\u0026hellip; **”。 B.3 接着处理前缀星号 *，得出“返回值是一个指向 char 的指针” 最后将所有部分合在一起：\n“c 是一个包含了 10 个元素数组，元素类型是指针，它指向是一个函数，这个函数接收的参数是 int **p，返回值是一个指向 char 的指针。”\n完美！按照这个规则，再复杂的声明都能读懂了。\n我们可以按照上面表格的规则，可以写一个解析程序，将声明翻译文自然语言。其实这样的解析程序已经被写了无数遍，通常被称为 Cdecl（C declaration）。现在还有这样的在线服务 cdecl.org，输入前面的例子 char * const *(*next)();，会得到下面的结果：\ndeclare next as pointer to function returning pointer to const pointer to char\n妈妈再也不用担心我看不懂 C 的声明了！\n# 关于 const const 并不一定代表常量，它表示被它修饰的对象为“只读”。\n涉及指针和 const 的声明有几种可能的顺序：\n1 2 3 const int * grape; int const * grape; int * const grape_jelly; 根据优先级规则 C，前面两种情况，const 作用于类型说明符 int，表示指针所指向的对象为只读：\n1 2 3 4 5 int x = 10; int y = 20; const int * grape = \u0026amp;x; *grape = 30; // ✘ 不能修改 grape 指向的值 grape = \u0026amp;y; // ✓ 可以修改指针 grape 自身的地址 最后一种情况，const 作用于左边的指针星号，表示指针本身是只读的：\n1 2 3 4 5 int x = 10; int y = 20; int *const grape_jelly = \u0026amp;x; *grape_jelly = 30; // ✓ 可以修改 grape_jelly 指向的值 grape_jelly = \u0026amp;y; // ✘ 不能修改 grape_jelly 的地址 如果想指针和指针所指向的对象都为只读，可以使用下面的声明：\n1 2 const int * const grape; int const * const grape; 实际上，const 最常见的使用场景是，当函数的参数为指针时，将指针指向的对象设置为只读。例如 strcpy 的原型声明如下：\n1 char *strcpy (char *dest, const char *src); const 表明了 strcpy 设计者的意图，src 作为输入，它指向的对象是只读的，不会被 strcpy 修改。\n搞懂了 C 语言的声明，下面再看看指针和数组的关系。\n# 指针和数组 # 数组与指针截然不同 在 C 语言中，数组和指针是截然不同的两种东西：\n数组是相同类型的对象排列而成的集合，而指针的值是地址，表示指向某处。\n但在很多情况下，数组和指针又可以互换使用，这让初学者感到困惑，到底它们什么时候是相同的，什么时候必须严格区分不能混淆？数组和指针这种微妙的关系，是造成指针成为难点的另一个重要原因。\n我们分别从“声明”和“使用”这两种情况考虑：\n数组和指针可以在下面两种情况下互换使用：\n在表达式中使用时 在声明函数形参时 其他情况下，两者不能混淆。\n# 什么时候数组与指针相同 # 在表达式中 根据 ANSI C 标准，在表达式中，数组名会被编译器解释为指向数组第一个元素的指针。因此，代码可以像这样写：\n1 2 3 int array[10]; int *p; p = array; 在表达式中，数组名 array 被视为“指向数组第一个元素的指针”，将它赋值给指针 p 后，p 也指向数组的第一个元素。\n要访问数组的第二个元素，可以使用下标运算符 []：\n1 array[1] = i; 其实在表达式中，下标运算符 [] 与数组无关，它的含义是“从指针指向的地址开始，加上偏移量，取出该位置的值”。编译器会将 array[i] 转换为 *(array + i)，这两种形式是等价的，array[i] 只是 *(array + i) 的简化写法。例如，以下两种方式都能访问数组的第二个元素：\n1 2 array[1] = i; *(array + 1) = i; 由于在表达式中，数组名 array 相当于指向数组首元素的指针，所以同样可以通过指针 p 来访问数组元素：\n1 2 *(p + 1) = i; p[1] = i; 因此，可以说在表达式中，数组和指针是等价的，可以互换使用。\n另外，下标运算符[] 具备交换性，就像加法操作符一样，可以交换两个操作数的位置，保持意义不变，下面两种形式都是正确的，访问数组的第二个元素：\n1 2 p[1] = i; 1[p] = i; # 左值 虽然在表达式中，array 确实被解释为指针，但它本身不能被赋值修改。下面的代码会导致错误：\n1 array = p; // expression must be a modifiable lvalue 提示的错误信息为：“expression must be a modifiable lvalue”。那么，lvalue 是什么意思呢？\n首先，我们来理解错误信息中的“expression（表达式）”。表达式由运算符和操作数组成，能够通过计算返回一个结果，且可能会产生副作用（如修改变量值）。标识符、常量等属于基本表达式，多个表达式通过运算符连接仍然构成表达式。表达式的关键特征是它会产生一个结果。\n相比之下，C 语言中有许多不产生值的语句，比如控制语句和声明语句。而在 Rust 中，几乎所有东西都是表达式，包括控制结构和赋值操作。\n接下来，我们来看看什么是左值。变量名在不同上下文中有两种含义：既可以表示地址，也可以表示该地址中存储的内容。例如：\n1 p = array; 在这条赋值语句中，p 表示一个内存地址，array 表示存储在该地址中的值。当变量名出现在赋值语句的左边时，它代表一个内存地址，称为左值；出现在右边时，它代表内存地址中的内容，称为右值。\n编译器在编译时会为每个变量分配地址，这个地址在编译时是已知的，而变量中的值只有在运行时才能确定。因此，左值在编译时是已知的，而右值要到运行时才可得知。\n左值可以被修改，意味着它能出现在赋值语句的左边，可以向该位置存入数据。右值则不能被修改，不能直接赋值给它：\n1 2 i = 5; // ✓ 5 = 10; // ✘ 数组名在表达式中被解释为指针，即表示内存中的位置，但它是不可修改的左值。这是因为数组名实际上是一个常量指针，表示数组起始位置的地址，而不是一个可以修改的内存位置。因此，不能对数组名进行赋值操作。不过，虽然数组名本身不可修改，数组的元素却是左值，可以通过数组名和索引来访问和修改这些元素。\n最后，回到那个错误信息：“expression must be a modifiable lvalue”，它的意思是，表达式必须是一个可修改的左值。而数组名是不可修改的左值，因此会导致编译错误。\n# 例外情况 从前面的介绍我们已经知道，在表达式中，数组和指针是等价的，可以互换使用，但该规则有下面三种例外情况。\n当数组名作为 sizeof 运算符的操作数时 在数组名作为 sizeof 运算符的操作数的情况下，将数组名解读为指针这一规则是无效的，在这种情况下返回的是数组整体的长度。\n当数组名作为 \u0026amp; 运算符的操作数时 在数组名前加上 \u0026amp; 之后，返回的就是指向数组整体的指针，而不是初始元素指针的指针。\n注意 \u0026amp;array 和 array 的区别。array 表示“指向数组初始元素的指针”，并 \u0026amp;array 表示“指向数组的指针”。实际声明一个指向数组的指针：\n1 2 3 int array[3]; int (*array_p)[3]; // array_p 是指向 int 的数组的指针，这个数组有 3 个元素。 array_p = \u0026amp;array; // 在数组前加上\u0026amp;，获取指向数组的指针 如果执行下面这样的赋值，编译器会报出警告。\n1 array_p = array; 这是因为，“指向 int 的指针”与“指向 int 的数组的指针”是不同的类型。\narray 和 \u0026amp;array 指向的是相同的地址。那么它们到底有何不同呢？那就是在使用它们进行指针运算时，结果不同。array + 1 前进 4 个字节（假设 int 类型的长度是 4 个字节），而 array_p + 1 则前进 4 * 3 个字节。\n初始化数组时的字符串字面量 用双引号括起来的字符串被称为 字符串字面量（string literal）。字符串字面量的类型实际上是“char 数组”，因此在表达式中，它会被解读为指向 char 类型的指针。\n1 2 char *str; str = \u0026#34;abc\u0026#34;; // 将指向\u0026#34;abc\u0026#34;初始元素的指针赋值给 str 当用字符串字面量来初始化 char 数组时，编译器会进行特殊处理。它会将字符串字面量视为一个省略了花括号的字符列表。也就是说，编译器会将 \u0026quot;abc\u0026quot; 这样的字符串字面量视为 { 'a', 'b', 'c', '\\0' } 这样的字符数组。以下两种初始化方式是等价的：\n1 2 char str[] = \u0026#34;abc\u0026#34;; //等价于下面的形式 char str[] = {\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;\\0\u0026#39;}; 需要注意的是，这种处理方式只在数组初始化时适用，所以以下写法是不合法的：\n1 2 char str[4]; str = \u0026#34;abc\u0026#34;; // 错误：expression must be a modifiable lvalue 如果初始化的不是 char 数组，就不会触发这种特殊处理：\n1 char *str = \u0026#34;abc\u0026#34;; 在这种情况下，\u0026quot;abc\u0026quot; 是一个 char 数组，因为它出现在表达式中，所以会被解释为指向该数组首元素的指针，进而被赋值给 str。\n# 函数的形参声明 在 C 语言中，数组不能被整体操作，也就是说，不能直接将一个数组赋值给另一个数组，或者将数组作为参数传递给其他函数。那么，如果我们想把一个数组传递给函数，该怎么做呢？可以通过传递指向数组首元素的指针来实现。\n在函数形参的声明中，编译器会将数组的形式自动改写为指向数组第一个元素的指针。编译器实际上只会将数组的地址传递给函数，而不是传递整个数组的副本。\n下面几种形参声明方式都是等价的：\n1 2 3 int func(int *a) /* 模式 1 */ int func(int a[]) /* 模式 2 */ int func(int a[10]) /* 模式 3 */ 模式 2 与模式 3 是模式 1 的语法糖。即使像模式 3 写上元素个数，编译器也会无视。不管选择哪种方法，在函数内部事实上获得的都是一个指针。\n也就是说，在函数的形参声明中，无法声明一个真正的数组，只能声明指针，即使写成数组的形式，编译器也当做指针对待。\n前面介绍过，在表达式中，数组名会被解读为指向数组初始元素的指针。在调用函数的时候，传递给函数的实参是表达式，因此数组名会被解读为指向数组初始元素的指针。同时，在函数的形参声明中，声明的数组都会被改写为指向数组初始元素的指针。这两个规则完美的契合，让我们在把数组作为实参传递给函数时，实际上传递的是数组首元素指针的副本，符合了函数的形参声明。在函数内部，可以通过指针访问数组的元素，因为在表达式中，指针和数组是等价的，可以互换使用，a[i] 只是 *(a + i) 的简化写法。\n# 什么时候数组和指针不能混淆 除了函数形参的声明以外，数组的声明就是数组，指针的声明就是指针，两者不能混淆。\n# 数组的定义 定义是声明的一种特殊情况，它分配内存空间，并可能提供一个初始值。\n数组定义分配了一块连续的内存空间，而指针定义只分配了存储一个地址的空间。它们在定义时各有其用途，不能混为一谈。例如：\n1 2 int array[10]; // 数组定义 int *ptr; // 指针定义 上面的 array 是实际分配了空间的数组，而 ptr 只是一个指针，它可以指向某块内存但本身并不分配任何数据存储空间。\n# 外部数组的声明 在多个编译单元（如多个 .c 文件）中使用 extern 声明全局变量时，数组和指针必须严格区分。如果在一个文件中定义了一个数组，但在另一个文件中错误地将它声明为指针，程序可能会产生不可预期的错误，甚至崩溃。例如在 file_1.c 中：\n1 int a[100]; // 定义了一个数组 在 file_2.c 中：\n1 extern int *a; // 声明为指针 file_1.c 定义了数组 a，但在 file_2.c 声明它为指针。即使用链接器将它们结合起来，程序也还是不能运行。因为 file_2.c 把原本是 int 数组的 a 的前 8 个字节解释成了指针，并引用了它指向的内容，这样的程序当然会崩溃。\n# 多维数组 理论上，多维数组（Multidimensional Array）和数组的数组（Array of Arrays）是完全不同的概念。多维数组是一个真正的、在内存中连续存储的矩阵式结构，而数组的数组本质上是多个一维数组的嵌套。\n在某些编程语言中（例如 C#），两者之间有着严格的区分。然而，在 C 语言中并不存在真正意义上的“多维数组”。我们看到的类似多维数组的结构，其实是“数组的数组”。例如，以下声明：\n1 int arr[3][4]; arr 是一个包含 3 个元素的数组，其中每个元素都是一个长度为 4 的一维数组。换句话说，arr 是一个数组，里面的每个元素又是一个数组，形成了二维结构。\n那么，arr[i][j] 是如何访问数组元素的呢？\n由于在表达式中，数组名会被解读为指针，因此 arr 被看作是“指向包含 4 个元素的 int 数组的指针”（即 int (*)[4]）。\narr[i] 实际上是 *(arr + i) 的简化形式。表达式 arr + i 代表指针前进了 sizeof(int[4]) * i 个字节的距离。\n*(arr + i) 的类型是一个 int 数组，但在表达式中，数组名又会被解读为指针。因此，*(arr + i) 会转换为“指向第 i 行的 int 数组的首元素的指针”。这意味着 (*(arr + i))[j] 实际上等价于 *((*(arr + i)) + j)，也就是对该行的首地址偏移 j 个位置，最后得到的元素内容，类型为 int。\n# 关于空的下标运算符 [] 前面已经介绍过，当数组作为函数的形参时，可以省略下标运算符 [] 中的元素个数。除此之外，还有几个特殊情况也允许使用空的下标运算符 []。\n函数形参的声明 在函数的形参中，只有最外层的数组会被解读为指针。即使在声明中写了元素个数，编译器也会忽略它。\n通过初始化列表确定数组长度的情况 编译器可以根据初始化列表推导出数组的长度，因此在这种情况下，最外层数组的元素个数可以省略。\n1 2 3 4 5 6 7 8 9 10 11 12 13 int a[] = {1, 2, 3, 4, 5}; char str[] = \u0026#34;abc\u0026#34;; double matrix[][2] = {{1, 0}, {0, 1}}; char *color_name[] = { \u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;blue\u0026#34;, }; char color_name[][6] = { \u0026#34;red\u0026#34;, \u0026#34;green\u0026#34;, \u0026#34;blue\u0026#34;, }; 使用 extern 声明全局变量 当全局变量只在一个编译单元（.c 文件）中定义，而在其他代码文件中通过 extern 声明时，最外层数组的元素个数可以省略。因为数组的实际长度要在链接时才能确定，所以在 extern 声明时，省略数组长度是合法的。\n结构体的柔性数组成员 从 C99 开始，结构体的最后一个成员可以使用柔性数组，即可以用空的 [] 表示其长度。这种数组在实际使用时由运行时的分配情况决定。\n1 2 3 4 typedef struct { int npoints; Point point[]; // 这里没有写元素个数 } Polyline; # 指向函数的指针 在 C 语言中，函数名在表达式中会自动转换为指向该函数的指针，但有两个例外：当它作为地址运算符 \u0026amp; 或 sizeof 的操作数时，函数名保持为函数本身。\n函数调用运算符 () 的操作数实际上并不是函数名本身，而是指向函数的指针。这意味着函数名和指向函数的指针是可以互换使用的。\n一个值得注意的点是，尽管你可以对指向函数的指针使用间接运算符 *，它并不会改变实际的行为。即使将 * 应用于指向函数的指针，它仍会立刻被转换回指向函数的指针。因此，即便你通过多层间接访问函数指针，代码依然能够正常运行。\n例如，以下代码使用了多层间接访问，但它依然等同于直接调用 printf：\n1 (**********printf)(\u0026#34;hello, world\\n\u0026#34;); 这揭示了一个有趣的现象：指向函数的指针在 C 中相当灵活，也使得函数指针的语法有些独特，但其行为仍然遵循 C 语言中表达式的规则。\n# 指针的基本用法 现在我们已经能解读复杂的 C 声明，并理解了数组和指针之间的微妙关系。你可能会问：为什么一定要使用指针？或者说，指针到底有什么用处？接下来，我们将介绍 C 语言中指针的几个基本用法。要用 C 语言编写实用的程序，指针的使用是不可避免的。\n# 从函数返回多个值 C 语言的函数只能返回一个值，我们可以通过使用指针突破这个限制，实现从函数返回多个值的效果。具体做法是将指针作为参数传递给函数，让函数修改指针所指向的对象的值。\n假设我们需要返回的数据的类型为 T ，则参数类型为 T *，即\u0026quot;指向 T 的指针\u0026quot;。下面是一个简单的示例，展示了如何通过指针从函数返回两个整数值：\n1 2 3 4 5 6 7 8 9 10 11 12 void get_values(int *a, int *b) { *a = 10; *b = 20; } int main(void) { int a, b; get_values(\u0026amp;a, \u0026amp;b); printf(\u0026#34;%d %d\\n\u0026#34;, a, b); return 0; } 在 C 语言中进行函数调用时，参数是作为值传递的，被称为值传递（call by value），也就是说，实参的值会被复制调用方函数到形参中，然后形参就可以像普通的局部变量一样使用了。\n在上面示例中，get_values 的参数的类型是“指向 int 的指针”，调用时仍然是值传递，只不过复制到形参的值，是 a 和 b 的地址。这样，在 get_values 内部就可以通过指针修改 a 和 b 的值了。\n# 将数组作为参数传递 在 C 语言中其实是不可以将数组作为参数传递的，但是通过传递指向数组初始元素的指针，可以达到与传递数组相同的效果。\n如下面的示例程序：\n1 2 3 4 5 6 7 8 9 10 11 12 13 void print_array(int *array, int size) { for (int i = 0; i \u0026lt; size; i++) { printf(\u0026#34;%d \u0026#34;, array[i]); } } int main(void) { int array[] = {1, 2, 3, 4, 5}; print_array(array, 5); return 0; } 前面说过，在 C 语言中，参数全部都是通过值传递的，传递给函数的都是它的副本。对于实参是数组的情况也是一样的，只不过传递的是数组首元素指针的副本。\n在 main 中调用 print_array 时，由于函数的实参 array 在表达式中，所以 array 会被解读为指向数组初始元素的指针，然后这个指针的副本会被传递给 print_array。在函数内部，指针可以像数组一样，使用 array[i] 这样的形式访问数组的元素，因为 array[i] 只不过是 *(array + i) 的语法糖。\n另外，print_array 还需要通过参数 size 来接收数组的长度。因为对于 print_array 来说，array 只是一个指针，它无法知道调用方传递的数组的长度。\n回想上一节介绍的，将指针作为参数传递给函数，在函数内部通过指针修改指向的值，达到从函数返回多个值的效果。当数组作为参数传递给函数时，默认传递的就是指向数组的指针，所以在函数内部通过指针修改的和调用方是同一个数组。\n# 多维数组作为参数传递 正如前面介绍的那样，当数组被用作函数形参时，数组的声明会自动被解释为指针的声明。例如：\n1 void func(int a[]) 实际上等价于：\n1 void func(int *a) 即使在声明中显式写上元素个数，比如 void func(int a[10])，编译器仍会忽略数组的大小信息，只会将其视为指向 int 类型的指针。\n那么如果形参是多维数组呢？来看下面的函数声明：\n1 void func(int a[][4]) 在这里，a 的类型是“int 的数组的数组”，但由于它是函数形参，编译器会将其解读为“指向一个长度为 4 的 int 数组的指针”：\n1 void func(int (*a)[4]) 这与一维数组的情况类似。即便你在声明中指定了最外层数组的大小，比如：\n1 void func(int a[3][4]) //元素个数 3 会被忽略 编译器同样会忽略外层数组的长度信息，将其视为“指向包含 4 个元素的 int 数组的指针”。\n在多维数组（即“数组的数组”）中，只有最外层的数组会被解读为指针。这意味着在函数形参中，除了最左边的一维数组外，所有内层数组的维度大小必须显式指定。这是因为在进行地址运算时，编译器需要知道每一维数组的长度，以便正确计算内存中的偏移量。\n# 动态数组 C 语言中的数组，在编译时必须知道数组的长度。虽然 C99 中引入了变长数组（VLA），但它只能用于自动变量，函数结束后数组自动释放。如果想让数组的生命周期跨域多个函数调用，就需要使用动态内存分配。\n通过 malloc 可以在运行时分配所需大小的数组。例如下面的示例程序，根据用户输入的长度动态分配数组：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main(void) { int *array; int size; char input[100]; printf(\u0026#34;请输入数组的大小：\u0026#34;); fgets(input, sizeof(input), stdin); sscanf(input, \u0026#34;%d\u0026#34;, \u0026amp;size); array = (int *)malloc(size * sizeof(int)); for (int i = 0; i \u0026lt; size; i++) { array[i] = i; } for (int i = 0; i \u0026lt; size; i++) { printf(\u0026#34;%d \u0026#34;, array[i]); } return 0; } 需要注意的是，使用 malloc 分配的动态数组，程序员必须自己管理数组元素的个数。使用完毕后，需要通过 free 函数释放内存。\n相对比在 Java 中，数组都是分配在堆上的，new int[10];就相当于 C 中的 malloc(10 * sizeof(int));，并且 Java 有垃圾回收机制自动管理内存。\n# 动态数组的数组 前面介绍过，C 语言中的多维数组其实是“数组的数组”。假设某部门有 10 名员工，我们可以使用一个二维数组来存储每位员工的住址：\n1 char addresses[10][100]; 这里，addresses 是一个 10x100 的字符数组，其中第一维代表员工人数，第二维代表每个住址的最大长度。由于住址的长度不固定，这种预设的最大长度会导致潜在的内存浪费。因此，使用“动态数组的数组”会是更优的选择。\n如果员工人数固定为 10，可以使用“动态数组的数组”来高效地存储每位员工的住址，每行的存储空间根据实际输入动态分配：\n1 char *addresses[10]; 在这种情况下，addresses 是一个包含 10 个指针的数组，每个指针都指向一个动态分配的字符数组，用来存储具体的住址。\n完整的示例程序如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; void read_addresses(char **addresses) { char buffer[256]; for (int i = 0; i \u0026lt; 10; i++) { printf(\u0026#34;请输入住址：\u0026#34;); fgets(buffer, sizeof(buffer), stdin); // 去掉换行符 buffer[strcspn(buffer, \u0026#34;\\n\u0026#34;)] = 0; // 动态分配每个住址的内存并复制数据 addresses[i] = malloc(strlen(buffer) + 1); strcpy(addresses[i], buffer); } } int main() { char *addresses[10]; read_addresses(addresses); printf(\u0026#34;\\n存储的住址:\\n\u0026#34;); for (int i = 0; i \u0026lt; 10; i++) { printf(\u0026#34;%s\\n\u0026#34;, addresses[i]); } return 0; } 注意到 read_addresses 函数的参数是 char **addresses。我们想传递给 read_addresses 的是一个 char * 数组，即 char *addresses[10]，但在函数形参的声明中，数组都会被改写为指针，因此即使函数声明为 void read_addresses(char *addresses[10])，编译器也会改写为 void read_addresses(char **addresses)。\n动态数组的数组 addresses 的内存布局如下：\n# 动态数组的动态数组 在上一节中，我们使用 char 的动态数组来存储每个员工的住址，但假设员工人数是固定的。如果员工人数不固定，我们就需要使用“动态数组的动态数组”。\n1 char **addresses; “类型 T 的动态数组”可以通过“指向 T 的指针”来实现。因此，要获取“T 的动态数组的动态数组”，只需使用“指向 T 的指针的指针”。\n下面是一个完整的示例程序，要求用户先输入员工人数，再逐个输入每位员工的住址：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; void read_addresses(char **addresses, int num_employees) { char buffer[256]; // 假设每个住址不超过 256 个字符 for (int i = 0; i \u0026lt; num_employees; i++) { printf(\u0026#34;请输入员工的住址：\u0026#34;); fgets(buffer, sizeof(buffer), stdin); // 去掉换行符 buffer[strcspn(buffer, \u0026#34;\\n\u0026#34;)] = 0; // 动态分配每个住址的内存并复制数据 addresses[i] = malloc(strlen(buffer) + 1); strcpy(addresses[i], buffer); } } int main() { int num_employees; printf(\u0026#34;请输入员工人数：\u0026#34;); scanf(\u0026#34;%d\u0026#34;, \u0026amp;num_employees); getchar(); // 清除换行符 // 动态分配用于存储指针的空间 char **addresses = malloc(num_employees * sizeof(char *)); read_addresses(addresses, num_employees); printf(\u0026#34;\\n存储的员工住址:\\n\u0026#34;); for (int i = 0; i \u0026lt; num_employees; i++) { printf(\u0026#34;%s\\n\u0026#34;, addresses[i]); } return 0; } 动态数组的动态数组 addresses 的内存布局如下：\n对于上面的示例程序，我们注意到一个细节，用于存储员工住址的 addresses，是在调用 read_addresses 函数之前分配的，我们稍加修改，就可以在 read_addresses 函数内部分配 addresses 的内存：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;string.h\u0026gt; void read_addresses(char ***addresses, int num_employees) { *addresses = malloc(num_employees * sizeof(char *)); char buffer[256]; // 假设每个住址不超过 256 个字符 for (int i = 0; i \u0026lt; num_employees; i++) { printf(\u0026#34;请输入员工的住址：\u0026#34;); fgets(buffer, sizeof(buffer), stdin); // 去掉换行符 buffer[strcspn(buffer, \u0026#34;\\n\u0026#34;)] = 0; // 动态分配每个住址的内存并复制数据 (*addresses)[i] = malloc(strlen(buffer) + 1); strcpy((*addresses)[i], buffer); } } int main() { int num_employees; printf(\u0026#34;请输入员工人数：\u0026#34;); scanf(\u0026#34;%d\u0026#34;, \u0026amp;num_employees); getchar(); // 清除换行符 // 动态分配用于存储指针的空间 char **addresses; read_addresses(\u0026amp;addresses, num_employees); printf(\u0026#34;\\n存储的员工住址:\\n\u0026#34;); for (int i = 0; i \u0026lt; num_employees; i++) { printf(\u0026#34;%s\\n\u0026#34;, addresses[i]); } return 0; } 这时 read_addresses 函数的参数变成 char ***addresses。下面是这两种情况的内存布局对比：\nC 语言中参数都是通过值传递的。对于第一种情况，addresses 指向的空间在 main 中分配，调用 read_addresses 函数时，传递给 read_addresses 的是 addresses 的副本，它们都指向 heap 中同一块内存的指针，因此在 read_addresses 函数内部通过 addresses 对这块空间的修改，在 main 中同样能看到。\n对于第二种情况，动态数组的内存是在 read_addresses 函数中分配，在 main 中调用 read_addresses 函数时，传递给 read_addresses 的是 addresses 指针的副本，即 \u0026amp;addresses，类型为 char ***。在 read_addresses 函数内部通过 *addresses 访问的是 main 中 addresses 变量，不管是为 *addresses 分配内存空间，还是通过 *addresses 修改内存中的值，都是间接修改 main 中 addresses 指向的值。\n# 命令行参数 实际上命令行参数就是一个“char 的动态数组的动态数组”。在 main 函数的定义中：\n1 int main(int argc, char *argv[]) argc 表示命令行参数的个数。 argv 是一个 char * 类型的数组，其中每个元素指向一个命令行参数字符串。 由于在函数的参数列表中，数组会被视作指针，所以写成下面这样也是一样的：\n1 int main(int argc, char **argv) argv[i] 指向每个命令行参数字符串（长度不固定），而 argv 的大小会随着实际参数数量动态调整。\n# 通过参数返回指针 前面介绍过，如果想通过参数返回类型 T，则参数类型为 T *，即“指向 T 的指针”。那么如果想通过参数返回“指向 T 的指针”，则参数类型为 T **，即“指向 T 的指针的指针”。\n什么情况下会用到通过参数返回“指向 T 的指针的指针”呢？一种场景是，如果函数需要改变调用者传入的指针本身，使其指向新的内存区域，可以使用“指向 T 的指针的指针”，即（T **）作为参数。\n例如我想实现一个 read_line 函数，通过返回值来表示处理状态，例如正常读取、文件结尾，或因内存不足而失败等情况。由于函数只能有一个返回值，因此需要通过参数返回读取的结果指针。此时，如果我们希望返回的是 char 类型的指针，那么参数的类型就应是“指向 char 的指针的指针”（即 char **）。下面是完整的示例程序：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; typedef enum { READ_LINE_SUCCESS, // 正常地读取了一行 READ_LINE_EOF, // 读到了文件末尾 READ_LINE_OUT_OF_MEMORY // 因内存不足而失败 } ReadLineStatus; ReadLineStatus read_line(FILE *fp, char **line) { size_t bufsize = 128; size_t position = 0; char *buffer = malloc(bufsize); if (!buffer) return READ_LINE_OUT_OF_MEMORY; int c; while ((c = fgetc(fp)) != \u0026#39;\\n\u0026#39; \u0026amp;\u0026amp; c != EOF) { if (position \u0026gt;= bufsize - 1) { bufsize *= 2; char *new_buffer = realloc(buffer, bufsize); if (!new_buffer) { free(buffer); return READ_LINE_OUT_OF_MEMORY; } buffer = new_buffer; } buffer[position++] = (char)c; } if (position == 0 \u0026amp;\u0026amp; c == EOF) { free(buffer); return READ_LINE_EOF; } buffer[position] = \u0026#39;\\0\u0026#39;; *line = buffer; return READ_LINE_SUCCESS; } int main() { printf(\u0026#34;Please enter a line of text:\\n\u0026#34;); char *line = NULL; ReadLineStatus status = read_line(stdin, \u0026amp;line); if (status == READ_LINE_SUCCESS) { printf(\u0026#34;You entered: %s\\n\u0026#34;, line); free(line); } else if (status == READ_LINE_EOF) { printf(\u0026#34;End of file reached.\\n\u0026#34;); } else if (status == READ_LINE_OUT_OF_MEMORY) { printf(\u0026#34;Error: Out of memory.\\n\u0026#34;); } return 0; } 在 main 中定义的变量 line 类型为“指向 char 的指针（char *）”， read_line 函数内部需要修改 line 本身，使其指向新分配的内存，因此参数类型应该为“指向 char 的指针的指针（char **）”。下面是运行时的内存布局：\n# 双指针 双指针”并不是一个严格的标准术语，所谓双指针，其实就是指向指针的指针。\n从前面的例子可以看到，双指针主要出现在以下两种场景：\n动态数组的动态数组，即在多级数据结构中使用动态内存分配 通过参数返回指针，需要在函数内部修改指针本身 双指针的多层间接引用可能让代码显得复杂难懂，但只要理解为什么要这样做，其实并不难。一开始可以在纸上画出堆栈和堆的内存布局，这将有助于你更直观地理解双指针的工作原理。\n# 纵横可变的二维数组 我们知道，在 C 语言中没有真正的二维数组，只有数组的数组。如何我们需要一个二维数组，两个维度都是在运行时才确定，应该如何做？\n前面介绍过“动态数组的数组”和“动态数组的动态数组”，它们的第二维的长度不固定，像锯齿一样，有一个专门的名称叫 Iliffe 向量。\n现在我们想要的是，像标准二维数组那样，第二维的长度一致的“数组的数组”，尝试像下面这样做：\n1 int board[size][size]; 对于 ANSI C 标准，size 必须是一个整型常量表达式，不能运行时再确定。从 C99 开始，size 可以是变量，这称为可变长数组（Variable Length Array，VLA）​，但 board 必须是自动变量，也就是说只在函数内部有效，函数退出后自动释放。如果希望 board 一直保持到程序退出，则需要使用 malloc() 动态内存分配。\n在 C99 中，通过以下写法就可以得到 size×size 的二维数组。\n1 int (*board)[size] = malloc(sizeof(int) * size * size); 然后就可以通过 board[i]​[j] 访问到数组的各个元素。完整的示例程序如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; int main(void) { int size; printf(\u0026#34;board size?\u0026#34;); scanf(\u0026#34;%d\u0026#34;, \u0026amp;size); /* 分配 size × size 的二维数组 */ int(*board)[size] = malloc(sizeof(int) * size * size); /* 对二维数组赋予适当的值 */ for (int i = 0; i \u0026lt; size; i++) { for (int j = 0; j \u0026lt; size; j++) { board[i][j] = i * size + j; } } /* 显示所赋的值 */ for (int i = 0; i \u0026lt; size; i++) { for (int j = 0; j \u0026lt; size; j++) { printf(\u0026#34;%2d, \u0026#34;, board[i][j]); } printf(\u0026#34;\\n\u0026#34;); } } # 数组的动态数组 假设我们需要实现一个画板来记录用户绘制的折线。如何表示这些点呢？我们可以使用 double[2] 来表示画板上的一个点，其中 double[0] 代表 X 坐标，double[1] 代表 Y 坐标。折线由多个点组成，因此我们需要一个 double[2] 的动态数组，实际上就是一个“数组的动态数组”。\n1 2 3 double (*polyline)[2]; polyline = malloc(sizeof(double[2]) * npoints); 解读一下声明 double (*polyline)[2]，polyline 是一个指针，指向一个 double[2] 的数组。\n如果我们要获取第 i 个点的坐标，可以通过 polyline[i][0] 和 polyline[i][1] 来访问 X 和 Y 坐标。\n使用 double (*polyline)[2] 表示折线可能不够直观，更好的做法是使用结构体：\n1 2 3 4 5 6 7 8 9 10 typedef struct { double x; double y; } Point; typedef struct { int npoints; Point point[]; } Polyline; 在 C99 标准中，引入了“柔性数组成员”（Flexible Array Member, FAM）的特性，这允许结构体的最后一个成员是一个长度可变的数组。上面的结构体 Polyline 就利用了这一特性来定义了一个变长的点数组。\n在给 Polyline 分配内存时，需要如下写法：\n1 2 Polyline *polyline; polyline = malloc(sizeof(Polyline) + sizeof(Point) * npoints); 通过这种方式，我们可以灵活地管理折线中的点数，使代码在处理动态数组时更具简洁性和可维护性。\n# 通用数据结构 前面介绍的指针基本用法，主要是和数组相关，利用了数组和指针之间的微妙关系。这部分是 C 语言的特有的内容，也是指针的难点所在。\n大多数编程语言都会用指针（引用）来构造链表、树等通用数据结构，C 语言也不例外。但数据结构知识不是 C 语言特有的，也不是造成指针难懂的原因，因此本文不再赘述，感兴趣的读者可以参考相关数据结构方面的书籍。\n# 总结 指针的概念本身并不复杂，但让它成为 C 语言中的难点，主要是由于指针与 C 语言复杂的声明语法交织，以及指针与数组之间的微妙关系。\n本文介绍了通过“C 声明优先级规则”表格解析 C 语言声明的方法，现在再复杂的声明对你来说都不是问题。\n接着，本文深入剖析了数组与指针的关系，从声明与使用两个方面进行了阐述。只需记住以下两个场景中，数组和指针可以互换使用：\n在表达式中，数组名会被解释为指向数组第一个元素的指针。因此，array[i] 与 *(array + i) 是等价的，array[i] 只是 *(array + i) 的简化写法。 在函数形参声明中，数组声明会被自动转换为指向数组首元素的指针。 除此之外，数组与指针在其他情况下不可混淆。\n此外，本文还介绍了指针的基本用法，包括从函数返回多个值、动态数组、动态数组的数组、动态数组的动态数组，数组的动态数组，纵横可变的二维数组等。这些用法几乎都与数组相关，属于 C 语言特有的内容，也是指针的难点所在。至于指针在链表、树等通用数据结构中的应用，则不属于本文讨论的范围。\n希望读完本文后，你已经“看破”指针，对你来说不再是难点。\n","date":"2024-10-30T14:13:24+08:00","permalink":"https://mazhen.tech/p/%E4%B8%80%E7%AF%87%E8%AF%BB%E6%87%82-c-%E6%8C%87%E9%92%88/","title":"一篇读懂 C 指针"},{"content":" 最近在处理一个 EJB 调用的问题，和底层的 CORBA 通信有关，都是很古老的技术名词。\n二十多年前我刚参加工作的时候，EJB 带着神秘和时髦的色彩横空出世，可后来没几年就被 Spring Framework 祛魅，很少有人再使用 EJB 开发应用。\nCORBA 则更加古老，估计现在很多程序员都没听说过，更别说开发过 CORBA 组件了。实际上 CORBA 是最早的分布式服务规范，早在 1991 年就发布了 1.0。可以说后来的 EJB，Web Services，甚至微服务，service mesh 都有 CORBA 的影子。\nCORBA 定义了 IDL（Interface Definition Language），用它来描述对象的接口、方法、参数和返回类型等信息，根据 IDL 可以生成各种语言的实现，不同语言编写的对象可以进行交互。 CORBA 定义了一系列服务，如Naming Service，Transaction Service，Security Service等，作为分布式系统的基础服务。事务、安全等服务会随着远程调用进行传播。 CORBA 的 ORB（Object Request Broker） 负责分布式系统中对象之间的通信。用户可以像调用本地对象一样调用远程对象上的方法，ORB 会处理网络通信和远程调用的细节。ORB 之间通过 IIOP（Internet Inter-ORB Protocol）协议进行通信。 就这样，IDL、一系列服务，再加上ORB，构成了 CORBA 的完整体系。其实 CORBA 的理念很好，面向对象，跨语言跨平台，服务传播和网络通信对用户透明。\nCORBA 作为一套成熟的工业规范，后来者自然会想办法吸收和兼容。1998年发布的 JDK 1.2，内置了 Java IDL ，以及全面兼容 ORB 规范的 Java ORB 实现。这时 Java 已经准备在企业端开发领域大展拳脚，JDK 内置了对 CORBA 的支持，为 J2EE （也就是后来的 Java EE，现在的 Jakarta EE）做好了准备。\nEJB 全面继承了 CORBA，Java Transaction Service (JTS) 是 CORBA 事务服务 OTS 的 Java 映射，EJB 之间的远程调用走 RMI/IIOP 协议，事务、安全上下文会通过 IIOP 进行传播。理论上，部署在不同品牌应用服务器上的 EJB 之间可以互相调用，EJB 也可以和任何语言开发的 CORBA 对象进行交互，并且所有 EJB 和 CORBA 对象，可以运行在同一个事务、安全上下文中。\nEJB 的目标是做真正的中间件，连接不同厂商的 J2EE 应用服务器，连接不同语言开发、运行在不同平台上的 CORBA 对象，并且它们可以加入到同一个分布式事务中，受到同样的安全策略保护。\n理想很丰满，现实是 EJB 的理想从来没有被实现过。Rod Johnson 在总结了 J2EE 的优缺点后，干脆抛弃了 EJB(without EJB) ，开发了轻量级 Spring Framework。Spring 太成功了，以至于对很多人来说，Java 开发 ≈ 使用 Spring 进行开发。\n后来的 Web Services/SOA 又把 CORBA、EJB 的路重新走了一遍， 定义了和 IDL 类似的 WSDL，以及一系列的事务规范 WS-Transaction，WS-Coordination，WS-Atomic Transaction。然后开发者又觉得大公司定义的规范太复杂，才有了轻量级的 REST，微服务。\nJava 的 CORBA 实现在 JDK 9 中被标记为 deprecated， 并最终在2018年发布的 JDK 11中被正式移除。EJB 和 CORBA 都没有成功，Java 宣告和 CORBA 分手，一段历史结束。\n在2024年的今天，有着30多年历史的 CORBA 和20多年历史的 EJB 已经是遗留系统，不会再有大批聪明的年轻人愿意投入到这个技术领域。不过对于像我这样还在一线搬砖的大龄程序员，遗留系统也是一种选择。它们和自身的境况很像：激情已过，一天天的老去。我们互相扶持着，每天对它们进行修修补补，打着补丁，它们也回报勉强够养家的报酬，然后一起等待着被淘汰的一天。在 AI 革命，号称要取代码农的今天，竟然还能靠着 20 多年前学到的技能挣到工资，也算一个小小的奇迹吧。\n在翻阅 Java ORB 的源代码时，注意到了很多源文件上都标记了作者的名字，于是顺手在网上一搜，还真找到了作者的信息。\nHarold Carr，84年至94年在惠普实验室从事分布式 C++ 工作，94年加入 Sun，设计了 Sun 的 CORBA ORB，JAX-WS 2.0，负责过 GlassFish 中的 RMI-IIOP 负载平衡和故障切换。Sun 被收购后他一直留在 Oracle，目前仍在 Oracle 实验室从事技术工作。同时他组过乐队录过专辑，还出版过诗集。有意思的人，有意思的经历。\n回到开头的 EJB 问题，仍然没有头绪，继续在代码里找线索。生活充满了眼前的苟且，还能有诗和远方吗？\n","date":"2024-05-14T15:04:43+08:00","permalink":"https://mazhen.tech/p/%E9%81%97%E7%95%99%E7%B3%BB%E7%BB%9F/","title":"遗留系统"},{"content":"\n# perf 是什么 perf 是由 Linux 官方提供的系统性能分析工具 。我们通常说的 perf 实际上包含两部分：\nperf 命令，用户空间的应用程序 perf_events ，Linux 内核中的一个子系统 内核子系统 perf_events 提供了性能计数器（hardware performance counters）和性能事件的支持，它以事件驱动型的方式工作，通过收集特定事件（如 CPU 时钟周期，缓存未命中等）来跟踪和分析系统性能。perf_events是在 2009 年合并到 Linux 内核源代码中，成为内核一个新的子系统。\nperf 命令是一个用户空间工具，具备 profiling、tracing 和脚本编写等多种功能，是内核子系统 perf_events 的前端工具。通过perf 命令可以设置和操作内核子系统 perf_events，完成系统性能数据的收集和分析。\n虽然 perf 命令是一个用户空间的应用程序，但它却位于 Linux 内核源代码树中，在 tools/perf 目录下，它可能是唯一一个被包含在 Linux 内核源码中的复杂用户软件。\nperf 和 perf_events 最初支持硬件计数器（performance monitoring counters，PMC），后来逐步扩展到支持多种事件源，包括：tracepoints、kernel 软件事件、kprobes、uprobes 和 USDT（User-level statically-defined tracing）。\n下图显示了 perf 命令和 perf_events 的关系，以及 perf_events 支持的事件源。\n# perf 事件源 perf 支持来自硬件和软件方面的各种事件。硬件事件来自芯片组中的硬件性能计数器（hardware performance counters），而软件事件则由tracepoints、kprobe 和 uprobe 等调试设施提供。\n可以使用 perf 的子命令 list 列出当前可用的所有事件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ sudo perf list List of pre-defined events (to be used in -e or -M): branch-instructions OR branches [Hardware event] branch-misses [Hardware event] bus-cycles [Hardware event] cache-misses [Hardware event] cache-references [Hardware event] cpu-cycles OR cycles [Hardware event] [...] cgroup-switches [Software event] context-switches OR cs [Software event] cpu-clock [Software event] [...] L1-dcache-load-misses [Hardware cache event] L1-dcache-loads [Hardware cache event] [...] branch-instructions OR cpu/branch-instructions/ [Kernel PMU event] branch-misses OR cpu/branch-misses/ [Kernel PMU event] [...] sched:sched_process_exec [Tracepoint event] sched:sched_process_exit [Tracepoint event] sched:sched_process_fork [Tracepoint event] [...] 事件类型如下：\nHardware Event：CPU 性能计数器（performance monitoring counters） Software event：内核计数器事件 Hardware cache event：CPU Cache 事件 Kernel PMU event：Performance Monitoring Unit (PMU) 事件 Tracepoint event：包含了静态和动态代码追踪事件 Kernel tracepoints：在 kernel 中关键位置的静态追踪代码 kprobes：在内核中的任意位置动态地被插入追踪代码 uprobes：与kprobes类似，但用于用户空间。动态地在应用程序和库中的任意位置插入追踪代码 USDT：是 tracepoints 在用户空间的对应技术，是应用程序和库在它们的代码中提前加入的静态追踪代码 perf 的 “Tracepoint event” 事件源很容易引起混淆，因为除了内核的 tracepoints，基于 kprobe、uprobe 和 USDT 的跟踪事件也被标记为了“Tracepoint event”。默认情况下它们不会出现在 perf list 的输出中， 必须先初始化才会作为 \u0026ldquo;Tracepoint event\u0026rdquo; 中的事件。\n内核 tracepoints 是由 TRACE_EVENT 宏定义。TRACE_EVENT 自动生成静态追踪代码，定义并格式化其参数，并将跟踪事件放入 tracefs （/sys/kernel/debug/tracing）和 perf_event_open 接口。\n与其他性能分析工具相比，perf 特别适合 CPU 分析，它能对运行在 CPU 上代码调用栈（stack traces）进行采样，以确定程序在 CPU 上的运行情况，识别和优化代码中的热点。这种 CPU Profiling 能力是基于硬件计数器 (performance monitoring counters，PMC) 实现的，而 PMC 被内核子系统 perf_events 包装成了 Hardware Event，下面重点介绍。\n# Hardware Event CPU 和其他硬件设备通常提供用于观测性能数据的 PMC。简单来说，PMC 就是 CPU 上的可编程寄存器，可通过编程对特定硬件事件进行计数。通过 PMC 可以监控和计算 CPU 内部各种事件，比如 CPU 指令的执行效率、CPU caches 的命中率、分支预测的成功率等 micro-architectural 级的性能信息。利用这些数据分析性能，可以实现各种性能优化。\nperf 命令通过 perf_event_open(2) 系统调用访问 PMC，配置想要捕获的硬件事件。PMC 可以在两种模式下使用：\nCounting（计数模式），只计算和报告硬件事件的总数，开销几乎为零。 Sampling（采样模式），当发生一定数量的事件后，会触发一个中断，以便捕获系统的状态信息。可用于采集代码路径。 glibc 没有提供对系统调用 perf_event_open 的包装，perf 在 tools/perf/perf-sys.h 定义了自己的包装函数：\n1 2 3 4 5 6 7 8 static inline int sys_perf_event_open(struct perf_event_attr *attr, pid_t pid, int cpu, int group_fd, unsigned long flags) { return syscall(__NR_perf_event_open, attr, pid, cpu, group_fd, flags); } 第一个参数 perf_event_attr 结构体定义了想要监控的事件的类型和行为。例如想要统计 CPU 的总指令数，可以这样初始化 perf_event_attr 结构体：\n1 2 3 4 5 6 7 8 9 struct perf_event_attr pe; memset(\u0026amp;pe, 0, sizeof(pe)); pe.type = PERF_TYPE_HARDWARE; pe.size = sizeof(pe); pe.config = PERF_COUNT_HW_INSTRUCTIONS; pe.disabled = 1; pe.exclude_kernel = 1; pe.exclude_hv = 1; perf 在 perf_event.h 中定义了适用于各类处理器的通用事件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 enum perf_hw_id { PERF_COUNT_HW_CPU_CYCLES = 0, PERF_COUNT_HW_INSTRUCTIONS = 1, PERF_COUNT_HW_CACHE_REFERENCES = 2, ... } enum perf_hw_cache_id { PERF_COUNT_HW_CACHE_L1D = 0, PERF_COUNT_HW_CACHE_L1I = 1, PERF_COUNT_HW_CACHE_LL = 2, ... } ... 针对每种 CPU，需要将事件枚举类型映射为特定 CPU 的原始硬件事件描述符。例如对于 Intel x86 架构的 CPU，PERF_COUNT_HW_INSTRUCTIONS 事件映射为 0x00c0，在 arch/x86/events/intel/core.c 中定义：\n1 2 3 4 5 6 7 static u64 intel_perfmon_event_map[PERF_COUNT_HW_MAX] __read_mostly = { [PERF_COUNT_HW_CPU_CYCLES] = 0x003c, [PERF_COUNT_HW_INSTRUCTIONS] = 0x00c0, [PERF_COUNT_HW_CACHE_REFERENCES] = 0x4f2e, ... } 这些原始硬件事件描述符在相应的处理器软件开发人员手册中进行了说明。内核开发人员根据 CPU 厂商提供的软件开发人员手册，完成 PMC 事件和特定 CPU 代码的映射。\n例如 Intel x86 架构的 CPU可以参考 Intel® 64 and IA-32 Architectures Software Developer’s Manual 的第三卷第 20 章 “PERFORMANCE MONITORING”。Intel 还提供了一个网站 perfmon-events.intel.com，可以查询 CPU 的所有 PMC 事件。\n我们在使用 perf 时，可以直接指定特定 CPU 的原始事件代码：\n1 sudo perf stat -e r00c0 -e instructions -a sleep 1 对于 Intel CPU，instructions 事件的原始事件代码为 r00c0，两者可以等价使用。\n对于大部分通用事件，我们不需要记住这些原始事件代码，perf 都提供了可读的映射。但在某些情况下，例如最新 CPU 增加的事件还没有在 perf 中添加映射，或者某种 CPU 的特定事件不会通过可读的名称暴露出来，这时就只能通过指定原始硬件事件代码来监控事件。\n简单了解了 PMC 后，我们来看如何基于 PMC 事件进行 CPU Profiling。\n# CPU Profiling perf 是事件驱动的方式工作，通过 -e 参数指定想要收集的特定事件，例如：\n1 sudo perf stat -e LLC-loads,LLC-load-misses,LLC-stores,LLC-prefetches ls 我们在对整个系统的 CPU 进行30 秒的采样时，使用的命令如下：\n1 sudo perf record -F 99 -a -g -- sleep 30 这里并未明确指定事件（没有 -e 参数），perf 将默认使用以下预定义事件中第一个可用的：\ncycles:ppp cycles:pp cycles:p cycles cpu-clock 前四个事件都是 PMC 提供的 CPU cycles 事件，区别在于精确度不同，从最精确（ppp）到无精确设置（没有 p），最精确的事件优先被选择。cpu-clock 是基于软件的 CPU 频率采样，在没有硬件 cycles 事件可用时，会选择使用 cpu-clock 软件事件。\n那么什么是CPU cycles 事件，为什么对 cycles 事件进行采样可以分析 CPU 的性能？\n# cycles 事件 首先介绍一个关于 CPU 性能的重要概念，Clock Rate（时钟频率）。Clock（时钟）是驱动 CPU 的数字信号，CPU 以特定的时钟频率执行，例如 4 GHz的 CPU 每秒可执行40亿个 cycles（周期）。\nCPU cycles （周期）是 CPU 执行指令的时间单位，而时钟频率表示 CPU 每秒执行的 CPU 周期数。每个 CPU 指令执行可能需要一个或多个 CPU 周期。 通常情况下，更高的时钟频率意味着更快的CPU，因为它允许 CPU 在单位时间内执行更多的指令。\n每经过一个 CPU 周期都会触发一个 cycles 事件。可以认为，cycles 事件是均匀的分布在程序的执行期间。这样，以固定频率去采样的 cycles 事件，也是均匀的分布在程序的执行期间。我们在采样 cycles 事件时，记录 CPU 正在干什么，持续一段时间收集到多个采样后，我们就能基于这些信息分析程序的行为，多次出现的同样动作，可以认为是程序的热点，成为下一步分析重点关注的方面。\n因为 cycles 事件的均匀分布，通过以固定频率采样 cycles 事件获得的信息，我们就能进行 CPU 性能分析。那么如何指定采样频率呢？\n# 设置采样频率 在使用 perf record 记录 PMC 事件时，会使用一个默认的采样频率，不是每个事件都会被记录。例如记录 cycles 事件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ perf record -vve cycles -a sleep 1 Using CPUID GenuineIntel-6-45-1 DEBUGINFOD_URLS= nr_cblocks: 0 affinity: SYS mmap flush: 1 comp level: 0 ------------------------------------------------------------ perf_event_attr: size 128 { sample_period, sample_freq } 4000 sample_type IP|TID|TIME|ID|CPU|PERIOD read_format ID|LOST disabled 1 inherit 1 freq 1 sample_id_all 1 exclude_guest 1 ... [ perf record: Captured and wrote 0.422 MB perf.data (297 samples) ] 加了 -vv 选项可以输出更详细的信息。从输出中可以看出，即使我们没有明确设置采样频率，采样频率已经启用（freq 1），并且采样频率为 4000 （{ sample_period, sample_freq } 4000），即每 CPU 每秒采集约 4000个事件。cycles 事件每秒中有几十亿次，默认采样频率的设置很合理，否则记录事件的开销过高。\n可以使用 -F 选项明确设置事件采样频率，例如：\n1 perf record -F 99 -e cycles -a sleep 1 -F 99 设置采样频率为 99 Hertz，即每秒进行 99 次采样。Brendan Gregg 在大量的例子中都使用了 99 Hertz 这个采样频率，至于为什么这样设置，他在文章 perf Examples 中给出了解释，大意是：选择 99 Hertz 而不是100 Hertz，是为了避免意外地与一些周期性活动同步，这会产生偏差的结果。也就是说，如果程序中有周期性的定时任务，例如每秒钟执行的任务，以 100 Hertz 频率进行采样，那么每次周期性任务运行时都会被采样，这样产生的结果“放大”了周期性任务的影响，偏离了程序正常的行为模式。\nperf record 命令还可以使用 -c 选项来设置采样事件的周期，这个周期代表了采样事件之间的间隔。例如：\n1 sudo perf record -c 1000 -e cycles -a sleep 1 在这个示例中，-c 选项设置采样周期为 1000，即每隔 1000 次事件进行一次采样。\n现在我们知道了如何以固定的频率对 cycles 事件进行采样，那么如何获知在采样时，CPU 正在干什么呢？\n# 背景知识 要知道 cycles 事件发生时 CPU 正在干什么，我们需要了解一些硬件知识，以及内核与硬件是如何配合工作的。先看看 CPU 是如何执行指令的。\n# CPU 执行指令 CPU 内部有多种不同功能的寄存器，涉及到指令执行的，有三个重要的寄存器：\nPC 寄存器（PC，Program Counter），存放下一条指令的内存地址 指令寄存器（IR，Instruction Register），存放当前正在执行的指令 状态寄存器（SR，Status Register），用于存储 CPU 当前的状态，如条件标志位、中断禁止位、处理器模式标志等 CPU 还有其他用于存储数据和内存地址的寄存器，根据存放内容命名，如整数寄存器、浮点数寄存器、向量寄存器和地址寄存器等。有些寄存器既可以存放数据，又可以存放地址，被称为通用寄存器（GR，General register）。\n程序执行时，CPU 根据 PC 寄存器中的地址从内存中读取指令到 IR 寄存器中执行，并根据指令长度自增，加载下一条指令。\n只要我们在采样时获取CPU 的 PC 寄存器和 IR 寄存器的内容，就能推断出 CPU 当时正在干什么。\n在 x86-64 架构中，Program Counter 的功能是由 RIP (Instruction Pointer Register) 寄存器实现的。\n在编译程序时，可以让编译器生成一个映射，将源代码行与生成的机器指令关联起来，这个映射通常存储在 DWARF 格式（Debugging With Attributed Record Formats）的调试信息中。同时编译时会生成符号表（Symbol Table），其中包含了程序中各种符号（如函数名、变量名）及其地址的映射。perf 借助调试信息和符号表（symbol table），可以将采样时寄存器中的指令地址转换为对应的函数名、源代码行号等信息。\n知道了 CPU 当时的“动作”还不够，我们还需要知道 CPU 是怎么做这个“动作”的，也就是代码的执行路径。下面介绍函数调用栈的相关概念。\n# 还原函数调用栈 函数是软件中的一个关键抽象概念，它让开发者将具有特定功能的代码打包，然后这个功能可以在程序的多个位置被调用。\n假设函数 P 调用函数 Q，然后 Q 执行并返回结果给 P，这个过程涉及到以下机制：\n传递控制：在进入函数 Q 时，PC 寄存器设置为 Q 的起始地址；在从 Q 返回时，PC 寄存器设置为 P 中调用 Q 后的下一条指令处。 传递数据：P 能够向 Q 提供一个或多个参数，Q 也能够将一个值返回给 P。 分配和释放内存：Q 需要在开始时为局部变量分配空间，然后在返回前释放该存储空间。 x86-64 平台上的程序使用堆栈（Stack）来实现函数调用。堆栈（Stack）的特性是后进先出（LIFO），函数调用正是利用了这一特性。调用某个函数就是在堆栈上为这个函数分配所需的内存空间，这部分空间被称为栈帧（stack frame），从函数返回，就是将这个函数的**栈帧（stack frame）从堆栈中弹出，释放空间。多个栈帧（stack frame）**组成 Call stack，体现出了函数的调用关系。\n注意：编译后的程序存储在代码段，是静态的；而 Call stack 是动态的，反应了程序运行时的状态。\n下面以示例程序为例，x86-64 平台上如何利用**堆栈（Stack）**实现函数调用的。\nmain函数有三个局部变量a、b和 res 存储在自己的 stack frame 中。当 main 调用 Calc 函数时，会先将参数 i和j 压入 Stack，然后将 PC 寄存器中的值也压入 Stack。我们知道，PC 寄存器存放的是下一条指令的地址，这时 PC 寄存器中的值是函数调用指令（call）后紧跟着的那条指令的地址。把 PC 寄存器压入 Stack，相当于保留了函数调用结束后要执行的指令地址，这样Calc完成后程序知道从哪里继续执行。\nrbp 是栈基址寄存器（register base pointer ），又叫栈帧指针（Frame Pointer），存放了当前 stack frame 的起始地址。rsp 是栈顶寄存器（register stack pointer），称为栈指针（Stack Pointer），随着入栈出栈动作而移动，始终指向栈顶元素。x86-64 的堆栈是从高地址向低地址增长的。\n在为 calc 新建 stack frame 时，会先将 rbp 寄存器压入 Stack。当前 rbp 寄存器中存放的是 main stack frame 的起始地址，将 rbp 寄存器压入 Stack，也就是将 main stack frame 的起始地址压入了 Stack。\n随后把 rsp 的值复制到 rbp，因为 rsp 始终会指向栈顶，把 rsp 的值复制到 rbp 就是让 rbp 寄存器指向当前位置，即 calc stack frame 的起始位置。\n注意 Calc 的参数和返回地址包含在 main 的 stack frame 中，因为它们保存了与 main 相关的状态。Calc 函数局部变量sum和result 被分配在自己的 stack frame 上。\n在 Calc 调用 Sum 时，重复上面的动作：参数和返回地址入栈，保存并更新 rbp 寄存器，为 Sum 的局部变量分配地址。\n在函数 Sum 执行完成之后，会将之前保存的 rbp 出栈，恢复到 rbp 中，也就是让 rbp 指向 Calc stack frame 的起始地址，将 Sum的 stack frame 弹出了 Stack，释放了 Sum 占用的空间。然后将返回地址出栈，更新到 PC 寄存器中。返回地址是函数调用指令（call）后的下一条指令，即 Calc 调用完 Sum 后紧跟着的下一条指令，把这个指令的地址恢复到 PC 寄存器中，实际上是将控制权返回给了 Calc ，让 Calc 剩余部分接着执行。\nCalc 执行完也会做同样的出栈动作，释放 stack frame ，将控制权返回给 main。\n这样，函数调用利用了**堆栈（Stack）**传递参数，存储返回信息，保存寄存器中的值，以及存储函数的局部变量，来实现函数调用。\n每个函数的活动记录对应一个栈帧（stack frame），多个栈帧（stack frame）叠加在一起构成调用栈（ Call stack）。Frame Pointer（通常是 rbp 寄存器）指向当前激活的函数的栈帧（stack frame）的起始处，这个起始处保存了调用它的函数的栈帧（stack frame）的起始地址。通过这种链接，我们就能以 Frame Pointer 为起点，追溯整个调用链，即从当前函数开始，逐级访问到每个调用者的栈帧（stack frame），从而重构出程序执行的路径。\n需要注意的是，出于空间和时间效率的考虑，程序都会优先使用通用寄存器来传递参数，只有在寄存器不够用的时候才会将多出的参数压入栈中。\nperf 正是利用 Frame Pointer，还原采样时的代码执行路径。\n在开启编译器优化的情况下，程序会将 rbp 寄存器作为通用寄存器重新使用，这时就不能再使用 Frame Pointer 还原函数调用栈。perf 还可以使用其他方法进行 stack walking：\n\u0026ndash;call-graph dwarf ：使用调试信息 \u0026ndash;call-graph lbr： 使用 Intel 的 last branch record (LBR) \u0026ndash;call-graph fp：使用 Frame Pointer ，缺省方法 本文就不详细讨论其他两种还原调用栈的方法，感兴趣的可以参考《BPF Performance Tools》。\n在 Linux 上，进程的执行分为了用户态和内核态，要知道完整的代码执行路径，就需要分别还原用户栈和内核栈。什么是用户态和内核态呢？\n# 用户态和内核态 操作系统需要能够限制对关键系统资源的访问，提供对资源不同级别的访问权限，这样可以保护系统免受错误和恶意行为的侵害。这种访问权限由 CPU 在硬件级别上实现，例如 x86 架构定义了特权级别，也称为保护环（protection rings），从 Ring 0 到 Ring 3 ，每个级别定义了可使用的指令集和可访问的资源，Ring 0 具有最高的特权级别。\nRing 0: 最高特权级别，用于操作系统的内核，可以直接访问所有的硬件和系统资源。 Ring 1 和 Ring 2: 这些中间层次的环通常用于特定的系统任务，如设备驱动程序，但在现代操作系统中，这些任务通常也在 Ring 0 执行。 Ring 3: 最低特权级别，用于普通的应用程序，这些应用程序不能直接执行影响系统稳定性或安全性的操作。 Linux 主要使用了 Ring 0 和 Ring 3，将能够访问关键资源的内核放在 Ring0，称为内核态（Kernel Mode），普通的应用程序将放在 Ring3，称为用户态（User Mode）。\n# 系统调用 如果用户态代码需要访问核心资源，它必须通过系统调用（system call ）。系统调用是进入内核的入口点，内核通过系统调用向程序提供一系列服务，如文件读写、进程创建、输入输出操作等。应用程序通过系统调用请求内核代为执行这些服务。\n调用系统调用看起来很像调用 C 函数。但实际上，在系统调用的执行过程中，会进行多个步骤。以 x86 平台的实现为例，包括以下几个关键环节：\n应用程序通过调用 C 库中的包装函数来发起系统调用。 包装函数负责将所有系统调用参数传递给内核。这些参数通常通过栈传递给包装函数，然后被复制到特定的 CPU 寄存器中。 为了让内核识别不同的系统调用，包装函数会把系统调用的编号复制到一个特定的 CPU 寄存器（%eax）中。 包装函数执行 trap 机器指令（ x86 架构 sysenter 指令），使 CPU 从用户模式切换到内核模式。 内核响应中断，把当前的寄存器值保存到内核栈数据结构 struct pt_regs 中，根据编号在一个表格中找到相应的系统调用服务程序，并执行它，然后返回结果。 完成操作后，内核将寄存器值恢复到原始状态，并将控制权返回给用户空间的应用程序，同时返回系统调用的结果。 # 用户栈和内核栈 可以看出在执行系统调用时，进程具有两个栈：用户栈（User Stack）和内核栈（Kernel Stack）。\n用户栈（User Stack） 保留了进入系统调用前的状态，用户栈在系统调用期间不会改变 内核栈（Kernel Stack） 是在系统调用期间使用，用于存储在内核态下执行的状态信息，包括寄存器的值和系统调用的参数。此外处理中断和异常时，也会使用内核栈。 用户栈和内核栈在什么什么位置？我们需要先了解虚拟地址空间的概念。\n# 进程虚拟地址空间 在现代操作系统上，用户程序都不能直接操作物理内存。操作系统会给进程分配虚拟内存空间，所有进程看到的这个地址都是一样的，里面的内存都是从 0 开始编号。\n程序里指令操作的都是虚拟地址。内核会维护一个虚拟内存到物理内存的映射表，将不同进程的虚拟地址和不同的物理地址映射起来。当程序要访问虚拟地址时，会通过映射表进行转换，找到对应的物理内存地址。不同进程相同的虚拟地址，会映射到不同的物理地址，不会发生冲突。这样每个进程的地址空间都是独立的，相互隔离互不影响。\n我们来看一下进程的虚拟地址空间布局。\n一个进程的虚拟地址空间分为两个部分，一部分是用户态地址空间，一部分是内核态地址空间。\n用户空间是应用程序执行的场所，每个进程都有自己独立的用户空间，其布局包含代码、全局变量、堆、栈和内存映射区域等多种部分。\n内核空间是内核代码运行的内存区域，它并非专属于某个单独的进程，所有进程通过系统调用进入到内核之后，看到的虚拟地址空间都是一样的。\n用户空间与内核空间的这种分离，确保了用户应用程序不能直接干扰内核，保证了系统的安全稳定性。\nLinux 的可执行文件是 ELF（Executable and Linkable Format）格式，执行时从硬盘加载到内存，ELF 文件中的代码段和数据段被直接映射到进程虚拟地址空间用户态的数据段和代码段。\n用户空间的**堆（heap）**是动态内存分配的区域，可以使用系统调用 sbrk 、mmap 和 glibc 提供的 malloc 函数进行堆内存的申请。内核会维护一个变量 brk 指向堆的顶部，sbrk 通过改变 brk 来维护堆的大小。malloc 内部也使用了系统调用 sbrk 或 mmap，但 glibc 会维护一个内存池，并不是每次使用 malloc 申请内存时都直接进行系统调用。\n内存映射区域为共享库及文件映射提供空间。可以使用系统调用 mmap 将创建文件映射提升 IO 效率。\n用户空间的堆栈（Stack） 是用户态函数执行的活跃记录，%rsp指向当前堆栈顶部。\n内核空间也有代码段和数据段，映射内核的代码段和数据段。\n当进程执行系统调用时，会从用户空间切换到内核空间，进程的当前状态，包括栈指针（rsp 寄存器）、程序计数器（rip，也就是 PC 寄存器）等，会被保存在内核数据结构 struct pt_regs 内，以便在系统调用完成后能够准确地恢复，继续执行因系统调用而暂停的用户空间操作。\n执行内核代码会使用内核栈（Kernel-Stack）。\n现在有了进程虚拟地址空间的全景图，我们再回头看看还原函数调用栈的问题。\n# 还原完整调用栈 在 Linux 系统中，我们可以说在任何给定的时刻，CPU 处于下面三种状态之一：\n在用户空间，执行某个进程里的用户级代码 在内核空间，以进程的身份运行，为特定的进程服务，也就是执行系统调用 在内核空间，处于处理中断的状态，此时不与任何进程相关联，运行在内核线程中，专注于处理中断事件 对于第一、第三种情况， perf 在采样事件触发时，只要通过 Frame Pointer（ rsp 寄存器）就可以还原用户栈或内核栈，并且已经是完整的调用栈。\n对于第二种情况，进程正好陷入内核执行系统调用，那么通过 Frame Pointer（ rsp 寄存器）可以还原内核代码的执行路径。然后再通过内核数据结构 struct pt_regs 内保存的寄存器状态，还原进入内核前用户空间的代码执行路径。这样我们就能获得采样事件触发时，包含了用户态和内核态的完整代码执行路径。\n通过 PC 寄存器、 rsp 寄存器，以及内核数据结构pt_regs，我们能知道 CPU 瞬时的“动作”，以及它是怎么做这个动作的（代码执行路径），那么还剩最后一个问题，我们怎么知道采样发生时刻 CPU 寄存器的内容呢？\n# 中断处理 内核是被动工作模式，它“躺”在内核空间不会主动工作，要么通过系统调用让它为用户进程服务，要么由时钟中断和各种外部设备中断事件驱动执行。\n中断是 Linux 的核心功能之一，它允许 CPU 响应外部或内部事件，如源自硬件设备的键盘、鼠标、网卡，或来自软件的异常。当这些事件，也就是中断事件发生时，CPU 会暂停当前的工作，转入内核的中断处理程序（Interrupt Handler）。当处理完成后，会从中断处理程序返回到原来被中断的代码处继续执行。\n中断可以分为**同步（Synchronous）中断和异步（Asynchronous）**中断两类：\n同步中断：由CPU当前执行的指令序列引起的。 异步中断：由外部事件（定时中断和 I/O 设备）引起，与CPU当前执行的指令序列无关。 在 x86 平台上，同步中断被称为异常（Exception），而异步中断被称为中断（Interrupt）。注意“中断”这个词根据上下文，可以仅指异步中断，也可以指包含了异常的两类中断的总称。\n在 x86 架构中，每个中断或异常都通过一个 0 到 255 范围内的数字来识别，这个数字是一个 8 位的无符号数，被称为“向量（vector）”。其中，异常和不可屏蔽中断的向量值是固定不变的（0～31），而可屏蔽中断的向量可以通过可编程中断控制器（Programmable Interrupt Controller，PIC）进行调整。\n每个 I/O 设备通常有一个单独的输出线路，用来发送中断请求（Interrupt ReQuest, IRQ）。所有 IRQ 线路都连接到 PIC，然后 PIC 又连接到 CPU 的 INTR 引脚。当某个 I/O 设备发生了需要 CPU 注意的事件，例如用户敲击键盘，数据到达网卡，该设备在相应的 IRQ 线路上发送信号，PIC 将这个 IRQ 信号转换成一个中断向量，然后在 CPU 的 INTR 引脚上发起一个中断请求，等待 CPU 处理。\n不可屏蔽中断通过 NMI 引脚接入 CPU。\n我们可以在 /proc/interrupts 查看到系统硬件设备 IRQ 线路及对应 CPU 的统计信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ cat /proc/interrupts CPU0 CPU1 CPU2 CPU3 8: 0 0 0 0 IR-IO-APIC 8-edge rtc0 9: 0 4 0 0 IR-IO-APIC 9-fasteoi acpi 18: 0 2 0 0 IR-IO-APIC 18-fasteoi i801_smbus 23: 35 0 0 0 IR-IO-APIC 23-fasteoi ehci_hcd:usb1 40: 0 0 0 0 DMAR-MSI 0-edge dmar0 41: 0 0 0 0 DMAR-MSI 1-edge dmar1 42: 0 0 0 0 IR-PCI-MSI-0000:00:1c.0 0-edge PCIe PME ... NMI: 67 124 61 60 Non-maskable interrupts LOC: 7386692 9261862 8162396 7051922 Local timer interrupts ... PMI: 67 124 61 60 Performance monitoring interrupts ... 注意第一列输出的是 IRQ 线路编号，需要通过 PIC 转换为 CPU 使用的中断向量，对于 Intel CPU，IRQn 转换为的中断向量是 n+32，因为 0～31 是固定给了异常使用。\n×86 系列的 CPU 能处理 20 种不同类型的异常，内核必须为每一种异常都提供一个专门的异常处理程序。我们可以在 Intel\u0026rsquo;s Software Development Manual: System Programming Guide 中找到对这些异常的完整描述。\nVector Mnemonic Description Type Error Code Source 0 #DE Divide Error Fault No DIV and IDIV instructions. 1 #DB Debug Exception Fault/ Trap No Instruction, data, and I/O breakpoints; single-step; and others. 2 - NMI Interrupt Interrupt No Nonmaskable external interrupt. 3 #BP Breakpoint Trap No INT3 instruction. 4 #OF Overflow Trap No INTO instruction. 5 #BR BOUND Range Exceeded Fault No BOUND instruction. 6 #UD Invalid Opcode (Undefined Opcode) Fault No UD instruction or reserved opcode. 7 #NM Device Not Available (No Math Coprocessor) Fault No Floating-point or WAIT/FWAIT instruction. 8 #DF Double Fault Abort Yes (zero) Any instruction that can generate an exception, an NMI, or an INTR. 9 Coprocessor Segment Overrun (reserved) Fault No Floating-point instruction. 10 #TS Invalid TSS Fault Yes Task switch or TSS access. 11 #NP Segment Not Present Fault Yes Loading segment registers or accessing system segments. 12 #SS Stack-Segment Fault Fault Yes Stack operations and SS register loads. 13 #GP General Protection Fault Yes Any memory reference and other protection checks. 14 #PF Page Fault Fault Yes Any memory reference. 15 — (Intel reserved. Do not use.) No 16 #MF x87 FPU Floating-Point Error (Math Fault) Fault No x87 FPU floating-point or WAIT/FWAIT instruction. 17 #AC Alignment Check Fault Yes (Zero) Any data reference in memory. 18 #MC Machine Check Abort No Error codes (if any) and source are model dependent. 19 #XM SIMD Floating-Point Exception Fault No SSE/SSE2/SSE3 floating-point instructions 20 #VE Virtualization Exception Fault No EPT violations 21 #CP Control Protection Exception Fault Yes RET, IRET, RSTORSSP, and SETSSBSY instructions can generate this exception. When CET indirect branch tracking is enabled, this exception can be generated due to a missing ENDBRANCH instruction at target of an indirect call or jump. 22-31 - Intel reserved. Do not use. 下图描述了中断处理的大致流程：\n异常是由 CPU 执行的指令产生，中断是由外部设备的紧急事件产生。\nCPU 在收到中断请求后，根据中断号在中断向量表中找到相应的**中断处理程序（Interrupt Handler）**入口，从而做出相应的处理。中断向量表将每个中断或异常与对应的中断处理程序关联了起来。\n从上图可以看出，中断向量表的 0～31固定为异常处理，32～127 和 129～238 项用来处理外部 I/O 设备的请求。 PIC 会将 IRQ 编号转换为中断向量。\n当系统收到中断请求时，如果不在内核态，先会从用户态切换到内核态，并在内核栈中保存当前的状态信息（主要是寄存器信息）。\n接着使用中断向量号，在中断向量表中查找对应的处理代码的入口地址。然后系统跳转到这个地址，执行相应的中断处理程序。\n处理完成后，系统通过中断返回机制恢复被中断任务的现场（内核态或用户态），并继续执行原来的代码。\n中断处理的一个关键步骤是，保留中断发生时的现场信息，perf 的 CPU 采样功能正是利用了这一点。\n我们使用 perf record 进行 CPU 分析时，会通过 -F 指定采样频率。当达到预设的阈值（如一定数量的指令执行或特定时间间隔），硬件性能计数器会触发一个 PMI （Performance monitoring interrupts）中断。\nPMI 是个什么类型的中断呢？一般 PMI 是由本地 APIC（ Advanced PIC）产生，而 APIC 接入 CPU 的 INTR 引脚，你可能觉得它是一个可屏蔽中断。但根据Intel\u0026rsquo;s Software Development Manual: System Programming Guide ，非屏蔽中断 (NMI) 可通过两种方式生成：\n外部硬件激活 CPU 的 NMI 引脚 CPU 通过系统总线或 APIC 串行总线接收一条包含 NMI 传递模式的消息 也就是说，APIC 可以生成 NMI 模式的中断消息，以调用 NMI 中断处理程序。\n由于 NMI 无法被忽略，它们常被一些系统用作硬件监控工具。如果出现某些特定情况，比如在预定的时间内没有触发中断，NMI 处理程序就会产生警告并提供关于该问题的调试信息。这种机制有助于发现并预防系统死锁。\n硬件性能计数器触发的 PMI 中断被设置为了 NMI类型。我们以 x86 平台为例，当 PMI 中断发生时，处理入口位置在 arch/x86/entry/entry_64.S：\n1 2 3 4 5 6 7 8 9 10 11 12 13 SYM_CODE_START(asm_exc_nmi) ... pushq\t5*8(%rdx)\t/* pt_regs-\u0026gt;ss */ pushq\t4*8(%rdx)\t/* pt_regs-\u0026gt;rsp */ pushq\t3*8(%rdx)\t/* pt_regs-\u0026gt;flags */ pushq\t2*8(%rdx)\t/* pt_regs-\u0026gt;cs */ pushq\t1*8(%rdx)\t/* pt_regs-\u0026gt;rip */ UNWIND_HINT_IRET_REGS pushq $-1\t/* pt_regs-\u0026gt;orig_ax */ ... movq\t%rsp, %rdi movq\t$-1, %rsi call\texc_nmi arch/x86/entry/entry_64.S是用汇编语言编写，负责系统调用、中断异常处理、任务切换和信号处理，专门针对 x86_64 架构进行了优化。\nasm_exc_nmi 函数是处理 NMI 的入口，从截取代码片段的注释可以看出， asm_exc_nmi 会使用 pushq 指令将当前的寄存器状态保存到内核栈上，这些包括程序计数器（rip，也就是 PC 寄存器）、代码段（cs）、标志（flags）、堆栈指针（rsp）和堆栈段（ss）。这一步非常关键，保留中断发生时的现场信息。最后使用**call exc_nmi** 指令调用 exc_nmi 函数。exc_nmi 会根据类型，调用预先注册的 NMI 处理函数。\n如果是 PMI 类型的中断，最终调用的处理函数是 perf_event_nmi_handler ，定义在 arch/x86/events/core.c中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 static int perf_event_nmi_handler(unsigned int cmd, struct pt_regs *regs) { u64 start_clock; u64 finish_clock; int ret; /* * All PMUs/events that share this PMI handler should make sure to * increment active_events for their events. */ if (!atomic_read(\u0026amp;active_events)) return NMI_DONE; start_clock = sched_clock(); ret = static_call(x86_pmu_handle_irq)(regs); finish_clock = sched_clock(); perf_sample_event_took(finish_clock - start_clock); return ret; } 可以看到，第二个参数 pt_regs 在前面介绍系统调用时已经出现过，是内核用来保存寄存器状态的结构体。这样在perf_event_nmi_handler中，我们就可以使用 pt_regs 中保存的寄存器状态，还原出 PMI 中断发生时，CPU 当时的动作，以及包含了用户态和内核态的完整代码执行路径。\n至此，我们了解了 perf 在进行 CPU Profiling 时涉及的全部技术机制。\n# 总结 现在再回顾我们使用 perf 进行 CPU Profiling 时的命令：\n1 $ sudo perf record -F 99 -a -g -- sleep 30 perf 是事件驱动的方式工作，这个命令没有指定 -e 参数，会收集什么事件呢？perf 会默认收集 cycles 相关事件，从最精确的 cycles:ppp 到无精确设置的 cycles，优先选择可用且精度高的事件。如果没有硬件 cycles 事件可用，退而选择 cpu-clock 软件事件。\n为什么采样 cycles 事件就能分析程序的 CPU 性能？因为每个 CPU 周期都会触发一个 cycles 事件，cycles 事件均匀的分布在程序的执行期间，以固定频率采样的 cycles 事件同样均匀分布，如果我们在采样 cycles 事件时，记录 CPU 正在干什么，持续一段时间收集到多个采样后，就能基于这些信息分析程序的行为，多次出现的同样动作，就可以认为是程序的热点。\n如果指定采样频率？-F 99 设置采样频率为 99 Hertz，即每秒进行 99 次采样。也可以使用 **-c 1000 ** 设置采样周期，即每隔 1000 次事件进行一次采样。\n如果知道采样时 CPU 正在做什么？通过CPU 的 PC（Program Counter）寄存器（x86-64 平台上对应的是 rip 寄存器）、指令寄存器等状态信息，能推断出 CPU 的瞬时动作。\n知道了 CPU 采样时的“动作”还不够，还需要知道 CPU 是怎么做这个“动作”的，也就是代码的执行路径。系统利用了堆栈（Stack）的后进先出（LIFO）实现了函数调用，每个函数在堆栈上分配的空间称为栈帧（stack frame），多个栈帧（stack frame）组成 Call stack，体现出了函数的调用关系。通过栈帧指针Frame Pointer（rbp 寄存器），可以追溯整个调用链，逐级访问到每个调用者的栈帧（stack frame），重构出程序执行的路径。这就是 -g 参数的作用：使用 Frame Pointer 还原调用栈。\n操作系统为了安全会限制用户进程对关键资源的访问，将系统分为了用户态和内核态，用户态的代码必须通过**系统调用（system call ）**访问核心资源。所以在执行系统调用时，进程具有两个栈：用户栈（User Stack）和内核栈（Kernel Stack）。为了还原包含了用户栈和内核栈在内完整的调用栈，我们探索了进程虚拟地址空间的布局，以及系统调用的实现：原来在系统调用时，会将进程用户态的执行状态（rsp、rip等寄存器）保存在内核数据结构 struct pt_regs 内，这样就能通过 Frame Pointer 和 pt_regs 分别还原内核栈和用户栈。\n怎么获取采样发生时刻 CPU 寄存器的内容呢？在特定的时间间隔到达时，也就是该采样的时刻，APIC 会触发 PMI 中断，CPU 在将控制权转给中断处理程序之前，将当前的寄存器状态保存到pt_regs，然后作为参数传递给 perf_event_nmi_handler。这样 perf 就拿到了采样发生时刻，CPU 寄存器的内容。\n最后，我们可以看一下 perf record 收集了什么样的数据。使用 perf script 命令可以打印收集在 perf.data 中的每个样本：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 $ sudo perf script ... sshd 50588 430947.269426: 16854 cycles: ffffffff824b84f6 native_write_msr+0x6 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff82413dc5 intel_pmu_enable_all+0x15 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff82407abb x86_pmu_enable+0x1ab (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff8273d05a perf_ctx_enable+0x3a (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff8274646a __perf_event_task_sched_in+0x15a (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff82534bf9 finish_task_switch.isra.0+0x179 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff834a57bf __schedule+0x2bf (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff834a5b68 schedule+0x68 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff834ac3cb schedule_hrtimeout_range_clock+0x11b (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff834ac403 schedule_hrtimeout_range+0x13 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff8289de3a do_poll.constprop.0+0x22a (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff8289e136 do_sys_poll+0x166 (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff8289e7cc __x64_sys_ppoll+0xbc (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff834931ac do_syscall_64+0x5c (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) ffffffff836000eb entry_SYSCALL_64+0xab (/usr/lib/debug/boot/vmlinux-6.2.0-36-generic) 118e5f __ppoll+0x4f (inlined) 89a97 server_loop2.constprop.0+0x547 (/usr/sbin/sshd) 89a97 wait_until_can_do_something+0x547 (inlined) 89a97 server_loop2.constprop.0+0x547 (/usr/sbin/sshd) 2bcf7 do_authenticated2+0x1e7 (inlined) 2bcf7 do_authenticated+0x1e7 (/usr/sbin/sshd) 11b66 main+0x3616 (/usr/sbin/sshd) 29d8f __libc_start_call_main+0x7f (/usr/lib/x86_64-linux-gnu/libc.so.6) 29e3f __libc_start_main_impl+0x7f (inlined) 12844 _start+0x24 (/usr/sbin/sshd) ... 任意截取了其中一段输出，可以看到包含了用户态和内核态完整的调用栈。基于多个这样的代码执行路径，我们还能生成火焰图进一步进行分析。\n为了了解 perf record 的实现原理，我们在 Linux 内核进行了一段深入而刺激的旅程，感谢各位参与探险！\n我的博客即将同步至腾讯云开发者社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=1k39tbi20c30f\n","date":"2023-11-06T11:18:06+08:00","permalink":"https://mazhen.tech/p/%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2-perf-cpu-profiling-%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/","title":"深入探索 perf CPU Profiling 实现原理"},{"content":"perf 是 Linux 官方的性能分析工具，它具备 profiling、tracing 和脚本编写等多种功能，是内核 perf_events 子系统的前端工具。\nperf_events 也被称为 Performance Counters for Linux (PCL) ，是在 2009 年合并到 Linux内核主线源代码中，成为内核一个新的子系统。perf_events 最初支持 performance monitoring counters (PMC) ，后来逐步扩展支持基于多种事件源，包括：tracepoints、kprobes、uprobes 和 USDT。\n# 安装预编译二进制包 perf 包含在 linux-tools-common 中，首先安装该软件包：\n1 $ sudo apt install linux-tools-common 运行 perf 命令，可能会提示你安装另一个相关的软件包：\n1 2 3 4 5 6 7 8 9 10 $ perf WARNING: perf not found for kernel 6.2.0-35 You may need to install the following packages for this specific kernel: linux-tools-6.2.0-35-generic linux-cloud-tools-6.2.0-35-generic You may also want to install one of the following packages to keep up to date: linux-tools-generic linux-cloud-tools-generic 按照提示安装和内核版本相关的 package：\n1 $ sudo apt install linux-tools-6.2.0-35-generic # 安装 Debug Symbol perf 像其他调试工具一样，需要调试符号表信息（Debug symbols）。这些符号信息用于将内存地址转换为函数和变量名称。如果没有符号信息，你将看到代表被分析内存地址的十六进制数字。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 13.19% 0.00% sshd libc.so.6 [.] __libc_start_call_main | ---__libc_start_call_main | --12.91%--0x5641182e4b66 0x5641182fecf7 | |--9.50%--0x56411835c752 | | | --9.10%--0x5641183330dd | | | --8.80%--0x5641182ee730 | | | |--4.64%--0x564118303c43 通过 package 方式安装的软件，一般都会提供以 -dbgsym 或 -dbg 结尾的调试符号信息 package。在 Ubuntu 上，首先需要将-dbgsym 仓库添加到更新源中，执行下面的代码：\n1 2 3 4 echo \u0026#34;deb http://ddebs.ubuntu.com $(lsb_release -cs) main restricted universe multiverse deb http://ddebs.ubuntu.com $(lsb_release -cs)-updates main restricted universe multiverse deb http://ddebs.ubuntu.com $(lsb_release -cs)-proposed main restricted universe multiverse\u0026#34; | \\ sudo tee -a /etc/apt/sources.list.d/ddebs.list 然后从 Ubuntu 服务器导入调试符号 package 签名密钥：\n1 $ sudo apt install ubuntu-dbgsym-keyring 最后更新软件源，安装 glibc 和 openssh-server 的调试符号信息：\n1 2 3 $ sudo apt update $ sudo apt install libc6-dbg $ sudo apt install openssh-server-dbgsym 安装了调试符号信息后，再使用 perf report 就会将十六进制数字转换为对应的方法名：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 11.68% 0.00% sshd libc.so.6 [.] __libc_start_call_main | ---__libc_start_call_main main | --11.46%--do_authenticated do_authenticated2 (inlined) server_loop2.constprop.0 | |--8.30%--process_buffered_input_packets (inlined) | ssh_dispatch_run_fatal | ssh_dispatch_run (inlined) | | | --7.86%--server_input_channel_req (inlined) | | | --7.78%--session_input_channel_req (inlined) 我们还可以安装内核镜像和一些内置命令行工具的调试符号信息：\n1 2 $ sudo apt install coreutils-dbgsym $ sudo apt install linux-image-`uname -r`-dbgsym # Kernel Tracepoints Kernel tracepoint 是在内核源码中关键位置的埋点，允许开发人员监视内核中的各种事件和操作，例如系统调用、TCP事件、文件系统I/O、磁盘I/O等，以了解内核的行为，进行性能分析和故障诊断。\n使用 perf 可以对 tracepoints 进行统计、追踪和采样。例如可以使用下面的命令对上下文切换进行 1 秒钟的跟踪：\n1 $ sudo perf record -e sched:sched_switch -a -g sleep 1 在执行这个命令的时候可能遇到下面的错误：\n1 2 3 4 5 event syntax error: \u0026#39;sched:sched_switch\u0026#39; \\___ unsupported tracepoint libtraceevent is necessary for tracepoint support Run \u0026#39;perf list\u0026#39; for a list of valid events 错误信息说明不支持 sched:sched_switch 这个 tracepoint。我们运行 perf list 查看可用的 tracepoint：\n1 2 3 4 5 6 7 8 9 $ sudo perf list \u0026#39;sched:*\u0026#39; List of pre-defined events (to be used in -e or -M): ... sched:sched_stat_wait [Tracepoint event] sched:sched_stick_numa [Tracepoint event] sched:sched_swap_numa [Tracepoint event] sched:sched_switch [Tracepoint event] sched:sched_wait_task [Tracepoint event] ... 输出中明明包含了 sched:sched_switch，为什么 perf 不支持呢？\n使用 perf version --build-options 查看 perf 的 build 选项：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 $ perf version --build-options perf version 6.2.16 dwarf: [ on ] # HAVE_DWARF_SUPPORT dwarf_getlocations: [ on ] # HAVE_DWARF_GETLOCATIONS_SUPPORT glibc: [ on ] # HAVE_GLIBC_SUPPORT syscall_table: [ on ] # HAVE_SYSCALL_TABLE_SUPPORT libbfd: [ OFF ] # HAVE_LIBBFD_SUPPORT debuginfod: [ OFF ] # HAVE_DEBUGINFOD_SUPPORT libelf: [ on ] # HAVE_LIBELF_SUPPORT libnuma: [ on ] # HAVE_LIBNUMA_SUPPORT numa_num_possible_cpus: [ on ] # HAVE_LIBNUMA_SUPPORT libperl: [ OFF ] # HAVE_LIBPERL_SUPPORT libpython: [ OFF ] # HAVE_LIBPYTHON_SUPPORT libslang: [ on ] # HAVE_SLANG_SUPPORT libcrypto: [ on ] # HAVE_LIBCRYPTO_SUPPORT libunwind: [ on ] # HAVE_LIBUNWIND_SUPPORT libdw-dwarf-unwind: [ on ] # HAVE_DWARF_SUPPORT zlib: [ on ] # HAVE_ZLIB_SUPPORT lzma: [ on ] # HAVE_LZMA_SUPPORT get_cpuid: [ on ] # HAVE_AUXTRACE_SUPPORT bpf: [ on ] # HAVE_LIBBPF_SUPPORT aio: [ on ] # HAVE_AIO_SUPPORT zstd: [ OFF ] # HAVE_ZSTD_SUPPORT libpfm4: [ OFF ] # HAVE_LIBPFM libtraceevent: [ OFF ] # HAVE_LIBTRACEEVENT libtraceevent 提供了访问内核 tracepoint 事件的 API。注意到最后一行，说明 perf 在 build 时没有打开 libtraceevent的支持。因此我们安装的预编译二进制包不能进行 tracepoint 追踪。我们需要自己从源码构建 perf。\n# 从源码构建 perf # 源码下载 首先下载 perf 的源代码。perf 的源码位于 Linux 内核源码中的 tools/perf 目录下。perf 是一个复杂的用户空间应用程序，而它却位于Linux 内核源代码树中，可能是唯一一个被包含在 Linux 源代码中的复杂用户软件。\n为了下载和内核匹配的源码，先确定内核版本：\n1 2 $ uname -r 6.2.0-35-generic 然后去 https://www.kernel.org/pub/ 浏览并下载正确版本的源码。\n# 安装依赖 安装构建依赖：\n1 2 $ sudo apt-get install build-essential flex bison python3 python3-dev $ sudo apt-get install libelf-dev libnewt-dev libdw-dev libaudit-dev libiberty-dev libunwind-dev libcap-dev libzstd-dev libnuma-dev libssl-dev python3-dev python3-setuptools binutils-dev gcc-multilib liblzma-dev 我们需要支持 tracepoint，所以还要安装 libtraceevent package：\n1 $ sudo apt install libtraceevent-dev # 构建 解压下载的 Linux 源码，进入源码目录，运行下面的命令：\n1 $ PYTHON=python3 make -C tools/perf install 成功构建后 perf 被安装到了 $HOME/bin 目录。\n# 测试验证 卸载先前安装的预编译版本：\n1 $ sudo apt remove linux-tools-common 将 $HOME/bin 加入到环境变量 $PATH，确保我们构建的 perf 命令能被找到。注意一般我们都需要 sudo 执行 perf 命令，所以还要编辑 /etc/sudoers（必须使用 visudo）文件，在 Defaults\tsecure_path=\u0026quot;...\u0026quot; 中加入 perf 命令的路径。\n验证 perf 的构建选项：\n1 2 3 4 $ sudo perf version --build-options perf version 6.2.0 ... libtraceevent: [ on ] # HAVE_LIBTRACEEVENT 这次 libtraceevent 打开了，支持 tracepoint 的追踪。执行下面的命令追踪系统的上下文切换：\n1 $ sudo perf record -e sched:sched_switch -a --call-graph dwarf sleep 1 查看报告：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 $ sudo perf report # To display the perf.data header info, please use --header/--header-only options. # # # Total Lost Samples: 0 # # Samples: 499 of event \u0026#39;sched:sched_switch\u0026#39; # Event count (approx.): 499 # # Children Self Command Shared Object Symbol # ........ ........ ............... .......................... ..................................... # 48.70% 48.70% swapper [kernel.vmlinux] [k] __schedule | ---secondary_startup_64_no_verify | |--39.28%--start_secondary | cpu_startup_entry | do_idle | schedule_idle | __schedule | __schedule ... 另外一个例子，按类型统计整个系统的系统调用，持续 5 秒钟：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 $ sudo perf stat -e \u0026#39;syscalls:sys_enter_*\u0026#39; -a sleep 5 Performance counter stats for \u0026#39;system wide\u0026#39;: ... 25 syscalls:sys_enter_timerfd_settime 0 syscalls:sys_enter_timerfd_gettime 0 syscalls:sys_enter_signalfd4 0 syscalls:sys_enter_signalfd 0 syscalls:sys_enter_epoll_create1 0 syscalls:sys_enter_epoll_create 0 syscalls:sys_enter_epoll_ctl 37 syscalls:sys_enter_epoll_wait 80 syscalls:sys_enter_epoll_pwait 0 syscalls:sys_enter_epoll_pwait2 ... 5.004101037 seconds time elapsed 可以看到，我们自己 build 的 perf 可以对 tracepoint 进行追踪和统计。\n","date":"2023-10-26T09:30:20+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8E%E6%BA%90%E7%A0%81%E6%9E%84%E5%BB%BA-perf/","title":"从源码构建 perf"},{"content":" 题图使用 Microsoft Bing 图像创建器生成。\n对于冯·诺伊曼体系结构的计算机，CPU 要数据才能正常工作。如果没有可处理的数据，那么CPU的运算速度再快也没有用，它只能等待。\n在计算机和芯片发展的历史中，CPU 速度不断提高，但主内存的访问速度改进相对较慢，导致 CPU经常处于等待数据的状态，无法充分发挥其处理能力。为了解决这个问题，出现了 CPU 缓存。\n寄存器和 CPU 缓存共同构成 CPU 内部的高速缓冲存储体系。\n寄存器直接位于 CPU 内部，是距离 CPU 最近的存储单元。CPU 缓存分为多级，距离 CPU 最近的一级缓存(L1缓存)接在寄存器之后。\n在内存层次结构中，从和 CPU 的接近程度来看是：\n寄存器 \u0026gt; L1缓存 \u0026gt; L2缓存 \u0026gt; L3缓存 \u0026gt; 主内存\n寄存器访问速度最快，但容量最小。随着级别升高，访问速度下降但容量增大，以平衡访问速度和存储空间。\nBrown University 的 Fundamentals of Computer Systems课程有专门介绍计算机的存储层次结构：以大小和速度为标准，将不同类型的存储设备从最快但容量最小到最慢但容量最大进行排列。\n存储层次结构这样设计是基于不同存储设备的成本和性能特点。内存成本高但访问速度快，而硬盘成本低但访问速度慢。所以采用这种层次结构可以在平衡成本和性能的前提下更好地利用各种存储设备。\n在2010年，Google 的 Jeff Dean 在斯坦福大学发表了一次精彩的演讲 Designs, Lessons and Advice from Building Large Distributed Systems，在演讲中总结了计算机工程师应该了解的一些重要数字：\n后来有人做了一个非常好的交互式web UI，展示了这些数字随着时间的变化。\n这些数字对人的感觉不那么直观，它们之间的差异可以相差数个数量级，让我们很难真正理解这些差距有多大。于是 Brendan Gregg在他的书 Systems Performance中，以 3.3 GHz 的 CPU 寄存器访问开始，放大成日常生活的时间单位，直观感受各系统组件访问时间的数量级差异：\n如果一个 CPU cycle 是1秒，那么内存访问的延迟是6分钟，从旧金山到纽约（相当于深圳到乌鲁木齐）的网络延迟就是4年！\n","date":"2023-10-11T16:20:21+08:00","permalink":"https://mazhen.tech/p/latency-numbers-every-programmer-should-know/","title":"Latency Numbers Every Programmer Should Know"},{"content":" 题图使用 Microsoft Bing 图像创建器生成。\n# 计算机原理/体系结构 极客时间：深入浅出计算机组成原理\nComputer Systems: A Programmer\u0026rsquo;s Perspective 从程序员的角度学习计算机系统，了解计算机系统的各个方面，包括硬件、操作系统、编译器和网络。这本书涵盖了数据表示、C语言程序的机器级表示、处理器架构、程序优化、内存层次结构、链接、异常控制流（异常、中断、进程和Unix信号）、虚拟内存和内存管理、系统级I/O、基本的网络编程和并发编程等概念。这些概念由一系列有趣且实践性强的实验室作业支持。\nComputer Systems: A programmer\u0026rsquo;s Perspective 视频课\n编码 Code: The Hidden Language of Computer Hardware and Software\nComputer Science from the Bottom Up 采用“从下到上”的方法,从最基础的二进制、数据表示开始,逐步深入计算机内部工作原理,目的是帮助读者真正掌握计算机科学的基础知识。\n漫画计算机原理\n趣话计算机底层技术\n计算机底层的秘密\n穿越计算机的迷雾\n嵌入式C语言自我修养\n# 编程 征服C指针 彻底理解和掌握指针的各种用法和技巧 C专家编程 Sun公司编译器和OS核心开发团队成员，对C的历史、语言特性、声明、数组、指针、链接、运行时、内存等问题进行了细致的讲解和深入的分析 C from Scratch 一个学习 C 语言的从零开始的路线图，包括推荐的课程、项目和资源，以及进阶到 x86-64 汇编语言和操作系统内部的指导。 Online Compiler, Visual Debugger 独特的逐步可视化调试工具，强烈推荐！ 极客时间：深入 C 语言和程序运行原理 Linux/UNIX系统编程手册 The Linux Programming Interface: A Linux and UNIX System Programming Handbook UNIX环境高级编程 Advanced Programming in the UNIX Environment # Linux 极客时间：Linux 实战技能 100 讲 Efficient Linux at the Command Line 像黑客一样使用命令行 Linux是怎么工作的 Linux技术内幕 Linux Foundation 的认证考试 LFCA 和 LFCS Linux内核设计与实现 Linux Kernel Development 深入理解Linux网络 极客时间：Linux 内核技术实战课 极客时间：编程高手必学的内存知识 极客时间：容器实战高手课 Learning Modern Linux 交互式的 Linux 内核地图 Linux From Scratch step-by-step instructions for building your own customized Linux system entirely from source. # 网络 趣谈网络协议 极客时间：Web 协议详解与抓包实战 图解TCP/IP 图解HTTP 网络是怎样连接的 # 编译原理 程序是怎样跑起来的 程序员的自我修养：链接、装载与库 如何从对象文件中导入和执行代码 part1 part2 part3 # 数据结构和算法 极客时间：数据结构与算法之美 极客时间：算法面试通关 40 讲 极客时间：常用算法 25 讲 极客时间：算法训练营 Hello 算法 动画图解、一键运行的数据结构与算法教程 通过动画可视化数据结构和算法 # 综合 计算机自学指南 (GitHub仓库) YouTube视频课：Crash Course Computer Science Preview 计算机教育中缺失的一课 Developer Roadmaps 为开发者提供学习路线图和指南 Online Coding Classes – For Beginners 3000 小时的免费课程，涵盖了编程涉及到的方方面面 # 在线课程 educative 为开发者提供交互式在线课程，重点关注技术领域的知识与技能 edX 由麻省理工学院（MIT）和哈佛大学共同创立的在线教育平台 exercism 专注于通过有趣且具有挑战性的练习问题、支持建设性同行评审机制来促进积极参与和技能提升，从而培养对各种现代计算范式的熟练掌握。 # 技术面试 Leetcode 一个广受欢迎的在线编程题库 Cracking the coding interview book 一本深受程序员喜爱的面试指南书 Neetcode 另一个在线编程练习平台 编程面试大学 涵盖了算法、数据结构、面试准备和工作机会等主题，帮助你准备大公司的技术面试 interviewing.io 一个提供模拟技术面试的平台 Pramp 一个模拟面试平台 Meetapro 一个可以找到专业人士进行模拟面试的网站 # 交互式教程 Grep by example 如何使用命令行工具 grep 进行文本搜索的交互式指南 Learn Git Branching 一个交互式的在线教程，帮助用户学习并练习 Git 的基本使用方法 # 大语言模型 Learn Prompting 一个开源的、多元化社区构建的课程，旨在提供完整、公正的提示工程知识。 提示工程指南 介绍大语言模型（LLM）相关的论文研究、学习指南、模型、讲座、参考资料、大语言模型能力及其与其他工具的对接。 面向开发者的大模型手册 基于吴恩达大模型系列课程的翻译和复现项目，涵盖了从 Prompt Engineering 到 RAG 开发的全部流程，为国内开发者提供了学习和入门 LLM 相关项目的方式。 LLM 应用开发实践笔记 作者在学习基于大语言模型的应用开发过程中总结出来的经验和方法，包括理论学习和代码实践两部分。 动手学大模型应用开发 面向小白开发者的大模型应用开发教程，基于阿里云服务器，结合个人知识库助手项目，通过一个课程完成大模型开发的重点入门。 # iOS开发 iOS \u0026amp; Swift - The Complete iOS App Development Bootcamp The 100 Days of SwiftUI Stanford CS193p - Developing Apps for iOS iOS and SwiftUI for Beginners Meta iOS Developer Develop in Swift Tutorials 苹果官方教程 SwiftUI Tutorials 苹果官方教程 # 计算机科学史 信息简史 ","date":"2023-10-07T10:45:48+08:00","permalink":"https://mazhen.tech/p/incomplete-list-of-computer-science-learning-resources-for-college-students/","title":"Incomplete List of Computer Science Learning Resources for College Students"},{"content":"\n我在 Jakarta EE 中文技术大会上的分享 《Jakarta EE应用服务器的事务处理》\n","date":"2023-09-27T18:30:59+08:00","permalink":"https://mazhen.tech/p/jakarta-ee%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86/","title":"Jakarta EE应用服务器的事务处理"},{"content":"asadmin is a command-line tool for GlassFish , which provides a series of subcommands. Using asadmin, you can complete all management tasks of GlassFish.\nThe subcommand start-domain of asadmin can start GlassFish. The following will describe the main process of GlassFish startup, starting from the execution of the asadmin command.\n# asadmin Execution Process The entry point of the asadmin command is org.glassfish.admin.cli.AsadminMain, which is included in the ${AS_INSTALL_LIB}/client/appserver-cli.jar package.\nThe main process of AsadminMain execution is as follows:\nSome key points:\nThe CLICommand.getCommand() is called to obtain the subcommand to start the GlassFish. All subcommands of asadmin inherit from com.sun.enterprise.admin.cli.CLICommand and are loaded from the following directories or Jars:\n${com.sun.aas.installRoot}/lib/asadmin\n${com.sun.aas.installRoot}/modules/admin-cli.jar\nAll subcommands are executed by calling CLICommand.execute(String... argv).\nThe implementation class of the subcommand to start GlassFish is StartDomainCommand, which internally calls GFLauncher.launch() to start the GlassFish.\nFinally, GFLauncher uses ProcessBuilder to start a new process, which is the main process of GlassFish. The entry point of this new process is com.sun.enterprise.glassfish.bootstrap.ASMain.\nIn addition, if the verbose or watchdog is set, the parent process asadmin will not exit and will wait until GlassFish runs to the end:\n1 2 3 4 // If verbose, hang around until the domain stops if (getInfo().isVerboseOrWatchdog()) { wait(glassFishProcess); } Next, we analyze the startup process of the GlassFish main process.\n# Main Process Startup Process The entry point of the GlassFish main process is the main method of com.sun.enterprise.glassfish.bootstrap.ASMain , and the main process of the startup process is as follows:\nThe startup process is complicated, but the main steps are clear:\nCreate GlassFishRuntime using RuntimeBuilder Create a GlassFish instance using GlassFishRuntime Start the Glassfish instance by calling GlassFish.start() # Creating GlassFishRuntime The main steps to create GlassFishRuntime include:\nCreate RuntimeBuilder Create and initialize the OSGi Framework Load OSGi bundles Start the OSGi Framework Start the BundleActivator in the bundles During the startup process of HK2Main, it searches for and registers HK2 modules. During the creation of GlassFishRuntime, OSGiGlassFishRuntimeBuilder will create and initialize the OSGi Framework, and then use the installBundles() method of BundleProvisioner to install all bundles of GlassFish to OSGi.\nWhere does BundleProvisioner find the bundles to load? The glassfish.osgi.auto.install property in the ${com.sun.aas.installRoot}/config/osgi.properties file defines the loading path of OSGi bundles. The discoverJars() method of BundleProvisioner will scan these paths and discover the Jar packages that need to be loaded.\nAfter completing the loading of bundles, OSGiGlassFishRuntimeBuilder will call Framework.start() to start the OSGi Framework. During the startup process of the OSGi Framework, the BundleActivator in the bundles will be started. Two important BundleActivators are:\nGlassFishMainActivator HK2Main During the startup process of GlassFishMainActivator , EmbeddedOSGiGlassFishRuntime will be registered in OSGi.\nHK2Main will create ModulesRegistry. ModulesRegistry is a key component of HK2. All modules in HK2 are registered here. In the OSGi environment, the specific implementation class of ModulesRegistry is OSGiModulesRegistryImpl, which will find and register the HK2 modules contained in the META-INF/hk2-locator directory of all bundle Jars.\nModulesRegistry and HK2Main will be registered as OSGi\u0026rsquo;s service.\n# Creating GlassFish Instance Create a GlassFish instance through GlassFishRuntime.newGlassFish(). This process mainly does two things:\nCreate HK2\u0026rsquo;s ServiceLocator Get ModuleStartup from ServiceLocator In EmbeddedOSGiGlassFishRuntime, use ModulesRegistry.newServiceLocator() to create ServiceLocator, and then get ModuleStartup from ServiceLocator. In the GlassFish startup scenario, the specific implementation of ModuleStartup is AppServerStartup.\nServiceLocator is the registry of HK2 services, which provides a series of methods to get HK2 service.\nThe relationship between HK2 Module and Service can be regarded as the relationship between container and content. Module (container) contains a group of Service (content) and is responsible for registering these Services in ServiceLocator. When a Module is initialized, all its Services will be registered in ServiceLocator, and then these Services can be found and used by other Services.\nFinally, create a GlassFish instance with AppServerStartup and ServiceLocator as the parameters of the constructor.\n# Starting GlassFish Instance Use GlassFish.start() to start the Glassfish instance. The most critical step is to call AppServerStartup.start(), which starts the HK2 service in stages. The service of HK2 can specify the startup level, the lower the level, the earlier the startup.\nAfter AppServerStartup.start() runs, all services start, and Glassfish completes startup and runs.\n","date":"2023-07-21T16:48:55+08:00","permalink":"https://mazhen.tech/p/glassfish-startup-process/","title":"GlassFish Startup Process"},{"content":"asadmin 是 GlassFish 的命令行工具，它提供了一系列子命令，使用 asadmin 可以让你完成 Glassfish 的所有管理任务。\n使用 asadmin 的子命令 start-domain 可以启动 GlassFish。下面将描述 GlassFish启动过程的主要流程。先从 asadmin 命令的执行开始。\n# asadmin 执行流程 asadmin 命令的入口是 org.glassfish.admin.cli.AsadminMain， 包含在 ${AS_INSTALL_LIB}/client/appserver-cli.jar包中。\nAsadminMain 执行的主要流程如下：\n其中的一些关键点：\n调用CLICommand.getCommand()获得启动服务器的子命令。asadmin 的所有子命令的都继承自com.sun.enterprise.admin.cli.CLICommand ，从下列目录或 Jar 中加载：\n${com.sun.aas.installRoot}/lib/asadmin\n${com.sun.aas.installRoot}/modules/admin-cli.jar\n所有子命令的执行都是调用CLICommand.execute(String... argv) 。\n启动 GlassFish 的子命令实现类为StartDomainCommand，内部调用 GFLauncher.launch()启动服务器。\n最终 GFLauncher 使用 ProcessBuilder 启动一个新的进程，就是 GlassFish 的主进程。这个新进程的入口是com.sun.enterprise.glassfish.bootstrap.ASMain。\n另外，如果设置了 verbose 或 watchdog 参数，作为父进程的asadmin 不会退出，一直等到 GlassFish 运行结束：\n1 2 3 4 // If verbose, hang around until the domain stops if (getInfo().isVerboseOrWatchdog()) { wait(glassFishProcess); } 下面分析 GlassFish 主进程的启动流程。\n# 主进程启动流程 GlassFish 主进程的入口是 com.sun.enterprise.glassfish.bootstrap.ASMain 的 main方法，启动过程的主要流程如下：\n启动过程比较复杂，但主要步骤很清晰：\n使用 RuntimeBuilder 创建 GlassFishRuntime 使用 GlassFishRuntime 创建 GlassFish 实例 调用 GlassFish.start() 启动 Glassfish 实例 # 创建 GlassFishRuntime 创建 GlassFishRuntime 的主要步骤包括：\n创建 RuntimeBuilder 创建并初始化 OSGi Framework 加载 OSGi bundles 启动 OSGi Framework 启动 bundles 中的 BundleActivator 在 HK2Main 的启动过程中查找并注册 HK2 modules 在创建 GlassFishRuntime的过程中，OSGiGlassFishRuntimeBuilder 会创建并初始化 OSGi Framework ，然后使用 BundleProvisioner 的 installBundles() 方法向 OSGi 安装 GlassFish 的所有 bundles。\nBundleProvisioner从哪里找到要加载的 bundles？config/osgi.properties 文件中的 glassfish.osgi.auto.install 属性定义了 OSGi bundles 的加载路径。BundleProvisioner.discoverJars() 方法会扫描这些路径，发现需要加载的 Jar 包。\n在完成 bundles 的加载后，OSGiGlassFishRuntimeBuilder会调用 Framework.start() 启动 OSGi Framework。 Framework 的启动过程中，bundles 中的 BundleActivator 会被启动。其中两个重要的 BundleActivator 是：\nGlassFishMainActivator HK2Main GlassFishMainActivator 启动过程中会向 OSGi 中注册 EmbeddedOSGiGlassFishRuntime。\nHK2Main 会创建 ModulesRegistry。ModulesRegistry 是 HK2 的关键组件，HK2 中的 modules 都注册在这里。在 OSGi 环境下，ModulesRegistry 的具体实现类是 OSGiModulesRegistryImpl，它会从所有 bundle Jar 的 META-INF/hk2-locator 目录中查找并注册该 bundle 包含的 HK2 modules。\nModulesRegistry 和 HK2Main 都会注册为 OSGi 的 service。\n# 创建 GlassFish 实例 通过GlassFishRuntime.newGlassFish() 创建出 GlassFish 实例，这个过程主要做了两件事：\n创建出 HK2 的 ServiceLocator 从 ServiceLocator 获取 ModuleStartup 在 EmbeddedOSGiGlassFishRuntime 中使用 ModulesRegistry.newServiceLocator() 创建出 ServiceLocator，然后从 ServiceLocator 获取 ModuleStartup。在 GlassFish 启动场景获取的是 ModuleStartup 的一个具体实现 AppServerStartup。\nServiceLocator 是 HK2 service 的注册表，它提供了一系列获取 HK2 service 的方法。\nHK2 Module 和 Service 的关系可以看作是容器和内容的关系。Module（容器）包含了一组 Service（内容），并且负责将这些 Service 注册到 ServiceLocator 中。当一个 Module 被初始化时，它的所有 Service 都会被注册到 ServiceLocator 中，然后这些 Service 就可以被其他 Service 查找和使用。\n最后将 AppServerStartup 和 ServiceLocator 作为构造函数的参数，创建出 GlassFish 实例。\n# 启动 Glassfish 实例 使用 GlassFish.start() 启动 Glassfish 实例。其中最关键的步骤是调用 AppServerStartup.start()，分级启动 HK2 的 service。HK2 的 service 可以指定启动级别，级别越低，越先启动。\nAppServerStartup.start() 运行完成，所有 service 启动，Glassfish 完成启动并运行。\n","date":"2023-07-21T15:32:57+08:00","permalink":"https://mazhen.tech/p/glassfish-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/","title":"Glassfish 启动流程"},{"content":" # 获得源代码 首先从 Github 获取 OpenJDK的源代码\n1 $ git clone https://github.com/openjdk/jdk.git # 安装必要的软件 Xcode App Store 中获取 Xcode Command Line Tools 通过 xcode-select --install 命令安装 GNU Autoconf 使用 brew install autoconf 命令安装 freetype 使用 brew install freetype 命令安装 boot JDK 构建 JDK 需要预先存在的JDK，这被称为“boot JDK”。 经验法则是，用于构建 JDK 主版本N的 boot JDK应该是主版本 N-1 的 JDK 建议使用 SDKMAN! 来安装维护 JDK 的多个版本 # 配置构建 通过运行 bash configure 命令来完成配置构建。这个脚本将检查你的系统，确保所有必要的依赖项都已经满足。如果一切顺利，该脚本将汇总build的配置、将使用的工具，以及 build 将使用的硬件资源：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Configuration summary: * Name: macosx-x86_64-server-release * Debug level: release * HS debug level: product * JVM variants: server * JVM features: server: \u0026#39;cds compiler1 compiler2 dtrace epsilongc g1gc jfr jni-check jvmci jvmti management parallelgc serialgc services shenandoahgc vm-structs zgc\u0026#39; * OpenJDK target: OS: macosx, CPU architecture: x86, address length: 64 * Version string: 22-internal-adhoc.mazhen.jdk (22-internal) * Source date: 1689128166 (2023-07-12T02:16:06Z) Tools summary: * Boot JDK: openjdk version \u0026#34;20.0.1\u0026#34; 2023-04-18 OpenJDK Runtime Environment Temurin-20.0.1+9 (build 20.0.1+9) OpenJDK 64-Bit Server VM Temurin-20.0.1+9 (build 20.0.1+9, mixed mode, sharing) (at /Users/mazhen/.sdkman/candidates/java/20.0.1-tem) * Toolchain: clang (clang/LLVM from Xcode 14.3.1) * Sysroot: /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX13.3.sdk * C Compiler: Version 14.0.3 (at /usr/bin/clang) * C++ Compiler: Version 14.0.3 (at /usr/bin/clang++) Build performance summary: * Build jobs: 12 * Memory limit: 16384 MB # 构建 OpenJDK 一旦配置完成，你就可以开始构建 JDK 了。\n1 $ make images 这个命令将开始构建过程，在完成后生成一个 JDK 的 image。\n# 验证构建 新构建的 JDK 在 ./build/*/images/jdk目录下，运行命令查看JDK版本\n1 2 3 4 $ ./build/macosx-x86_64-server-release/images/jdk/bin/java -version openjdk version \u0026#34;22-internal\u0026#34; 2024-03-19 OpenJDK Runtime Environment (build 22-internal-adhoc.mazhen.jdk) OpenJDK 64-Bit Server VM (build 22-internal-adhoc.mazhen.jdk, mixed mode, sharing) # 在VS code中调试 OpenJDK 首先在 VS code 中安装 C++ extension for VS Code。在 VS cod 中配置C++ 开发环境可以参考这篇文档 Using Clang in Visual Studio Code。\n使用 VS code 打开 OpenJDK的源代码，在恰当的位置设置好断点，点击右上角三角运行图标，选择“Debug C/C++ file”：\n然后在弹出列表中选择“(lldb) Launch“：\n第一次运行会弹出错误信息，我们选择打开 launch.json，创建新的 debugger 配置。点击右下角的 “add configuration\u0026hellip;“，在弹出的列表中选择 \u0026ldquo;C/C++： (lldb) Launch\u0026rdquo;\nVS code会自动添加缺省的配置，我们需要修改的是 program 和 args，设置为上面build好的 OpenJDK，以及准备运行的Java程序。\n1 2 3 4 5 6 \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/build/macosx-x86_64-server-release/images/jdk/bin/java\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-cp\u0026#34;, \u0026#34;/Users/mazhen/Documents/works/javaprojects/samples/playground/target/classes\u0026#34;, \u0026#34;tech.mazhen.test.Main\u0026#34; ], 保存文件 launch.json，然后重新开始调试。可以在断点处停止，但是不能定位源代码，报错如下：\n1 Could not load source \u0026#39;make/src/java.base/unix/native/libnio/ch/Net.c\u0026#39;: \u0026#39;SourceRequest\u0026#39; not supported.. 为了正确的找到源代码，需要在launch.json中配置 sourceFileMap，将源代码的编译时路径映射到本地源代码位置。完整的配置如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 { \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;(lldb) Launch\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;cppdbg\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/build/macosx-x86_64-server-release/images/jdk/bin/java\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-cp\u0026#34;, \u0026#34;/Users/mazhen/Documents/works/javaprojects/samples/playground/target/classes\u0026#34;, \u0026#34;com.apusic.test.Main\u0026#34; ], \u0026#34;stopAtEntry\u0026#34;: false, \u0026#34;cwd\u0026#34;: \u0026#34;${fileDirname}\u0026#34;, \u0026#34;environment\u0026#34;: [], \u0026#34;externalConsole\u0026#34;: false, \u0026#34;MIMode\u0026#34;: \u0026#34;lldb\u0026#34;, \u0026#34;sourceFileMap\u0026#34;: { \u0026#34;make/\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34; }, } ] } 现在就可以在VS code 中正常调试OpenJDK的C++代码了。\n","date":"2023-07-11T17:26:22+08:00","permalink":"https://mazhen.tech/p/%E5%9C%A8macos%E4%B8%8A%E7%BC%96%E8%AF%91%E5%92%8C%E8%B0%83%E8%AF%95openjdk/","title":"在macOS上编译和调试OpenJDK"},{"content":"在计算机科学中，事务处理（transaction processing ）是将信息处理划分为独立的、不可分割的操作，称为事务（Transaction）。每个事务必须作为一个完整的执行单元，要么整个事务成功（提交），要么失败（中止，回滚），它永远不能只是部分完成。使用事务可以简化应用程序的错误处理，因为它不需要担心部分失败，系统（通常是数据库或某些现代文件系统）的完整性始终处于已知的、一致的状态。\n事务处理是一项关键技术，可以应用于多个问题领域——企业架构、电子商务解决方案、金融系统和许多其他领域。事务的一个很好的例子就是，资金从一家银行的账户转移到另一家银行的账户。资金转移涉及在一个账户上扣款，并在另一个账户上增加相同的金额。使用事务可以确保不会出现由于其中一项操作失败，而导致资金丢失或产生的不一致状态。\n# 事务处理简史 现代事务处理技术是在20世纪60年代开始的大型机计算背景下发展起来的，在许多方面，我们今天使用的技术是对这些模型的改进和调整。第一个事务处理系统（Transaction processing system）是著名的 SABRE 航空预订系统，由IBM和美国航空公司在20世纪50年代末和60年代初开发。以今天的标准来看，SABRE 相当粗糙： ACID 事务语义完全是由应用程序实现的。IBM 很快就意识到这种技术可以应用于其他行业，由此产生了CICS（Customer Information Control System）产品，它最初是完全用汇编语言编写，并没有使用现代意义上的数据库，而是依赖于扁平文件和其他低级数据结构，但 CICS 已经实现了基本的事务处理功能。\n1970年 Edgar F. Codd 发表了一篇名为《A Relational Model of Data for Large Shared Data Banks》的论文，首次提出了关系模型的概念，这一模型为后来的关系数据库管理系统（RDBMS）奠定了基础。随后关系数据库管理系统开始兴起，IBM 的 System R 项目是一个关键的里程碑，它是 SQL 的第一个实现，从此成为标准的关系数据查询语言。在 System R 项目中，事务处理技术被引入到关系数据库领域，为后来的数据库系统的发展奠定了基础。数据库管理系统成为了事务处理的核心组件，负责数据存储和管理、事务处理、并发控制和恢复等关键功能。\n随着计算机技术的发展和网络通信技术的普及，分布式计算逐渐成为企业应用的重要趋势。在分布式环境中实现事务处理面临着许多挑战，传统的单机事务处理系统无法满足需求，两阶段提交协议（2PC）等技术应运而生。为了管理这些分布式事务，提供更好的并发控制和容错能力，事务处理监视器（Transaction Processing Monitor 或 TP Monitor）被引入。\nTP Monitor 负责在分布式环境中管理和监控事务处理过程。它处理客户端请求、协调事务、确保数据一致性、管理资源访问以及处理故障恢复等。TP Monitor 是一个软件框架或应用程序执行环境，为应用程序提供了一个完整的运行时，允许应用以安全和事务性的方式访问后端系统（包括数据库）。TP Monitor 作为事务处理中间件，目标是让程序员更容易的编写和部署可靠、可扩展的事务应用程序，使程序员能够专注于业务逻辑，而不是底层的事务管理。\n在20世纪90年代初，X/Open 发布了X/Open DTP模型，为分布式事务处理提供了一个统一的框架和一组标准接口。遵循 X/Open DTP 模型的 TP Monitor 实现了该模型所定义的组件和接口，包括事务管理器（TM）、资源管理器（RM）和通信资源管理器（CRM）。一些广泛使用的遵循 X/Open DTP 模型的 TP Monitor 产品包括BEA 的 Tuxedo 和 Transarc 的 Encina 等。\nCORBA Object Transaction Service (OTS) 是一个定义在 CORBA（Common Object Request Broker Architecture） 规范中的分布式事务服务。OTS 将分布式事务处理模型（DTP）扩展到了对象领域，它提供了一种在分布式对象系统中进行事务处理的方法。OTS 定义了一组标准的接口和协议，允许 CORBA 对象参与分布式事务。\nJava EE 应用服务器是在 X/Open DTP 模型和 CORBA OTS 的基础上发展出来的事务处理监视器，TP Monitor 开始融入 Java EE应用服务器，提供更丰富的中间件服务和组件化的应用程序模型。TP Monitor 本质上是一个具有事务感知功能的应用服务器，事实上，Java EE 应用服务器中的许多功能都源于TP Monitor。同样地，许多现代的 TP Monitor 是带有事务服务核心的 Java EE 应用服务器。\n# 事务概念基础 本章我们简要地回顾一些事务处理的基本概念，它们塑造了中间件对事务的支持。\n# ACID 属性 事务提供的安全保障通常用缩写ACID来描述，它代表原子性（atomicity）、一致性（consistency）、隔离性（Isolation）和持久性（Durability）。事务是一个原子（Atomicity）工作单元，它将系统从一个一致（Consistency）的状态转换为另一个一致状态，执行时不受其他同时执行的事务的干扰（isolation），并且一旦提交，就不能因系统故障而撤消(Durability)。ACID 是1983年由 Theo Härder 和Andreas Reuter 在论文《 Principles of Transaction-Oriented Database Recovery》中首次提出。\n下面我们更详细地研究一下ACID属性，这将让我们更深入的理解事务。\n# Atomicity Atomicity 这个术语在不同的领域有着类似但又不相同的含义。例如在多线程编程领域，如果一个线程执行了一个原子操作，这意味着另一个线程不可能看到这个操作的半成品。系统只能处于操作前或操作后的状态，而不能介于两者之间。\n在 ACID 的背景下，Atomicity 不是关于并发性的，它并没有描述当多个进程试图同时访问相同的数据时会发生什么，因为关于并发访问的场景是在隔离性（Isolation）中描述的。\nACID 原子性描述的是，如果一个客户想要进行多次写入，但在处理部分写操作后出现故障的情况。这些故障可能是进程崩溃、网络连接中断、磁盘已满等。如果将这些写操作组合到一个事务中，由于故障无法完成事务提交，那么该事务将被中止，并且数据库必须撤消之前的任何写操作。\n在没有 Atomicity 保证的情况下，如果在进行多次修改的过程中发生错误，就很难知道哪些修改已经生效，哪些没有生效。Atomicity 简化了这个问题：如果事务被中止，应用程序可以确定它没有改变任何东西。\n所以 Atomicity 的本质是，在出错时中止事务，并丢弃该事务对数据的所有修改。\n# Consistency 事务应该确保系统从一个一致性状态转换到另一个一致性状态。在事务开始和结束时，系统的完整性约束必须得到满足。\n我们所说的一致性，有些是通过数据库的完整性约束来保证的，例如使用主键作为员工编号，那么由数据库来保证所有主键都是唯一的。\n但在很多情况下，一致性都是有具体业务含义的，应用程序定义了什么状态是有效或无效的，例如每个部门的支出必须小于或等于该部门的预算，这种一致性只能由应用程序来保证。\n原子性、隔离性和持久性是数据库的属性，而一致性是应用程序的属性。维护事务的一致性是应用程序和数据库的共同责任，应用程序是依靠数据库的原子性和隔离性来实现一致性。\n# Isolation 如果多个用户同时读写数据库相同的记录，就会遇到并发问题。Isolation 意味着同时执行的事务是相互隔离的，事务的执行不会受到其他并发事务的影响，每个事务都可以假装它是整个数据库中唯一运行的事务。\n# Durability 持久性是指一旦事务成功提交，它所写入的任何数据都不会丢失，即使出现硬件故障或数据库崩溃。改变系统持久状态的唯一方法是提交一个事务。\n对于单节点数据库，持久性通常意味着数据已写入硬盘或 SSD 等非易失性存储中。数据库一般都会使用WAL（write-ahead log）技术，在向持久化存储写入未提交的变更之前，先向日志中写入相应的事务日志记录，并确保事务日志记录在事务提交之前被持久化。当遇到故障重启系统时，数据库可以通过重新执行所有已提交事务的日志记录，撤消所有中止事务的日志记录，让数据库恢复到一致性状态。\n# 隔离级别 隔离性（Isolation）是事务 ACID 四个属性之一，它确保多个并发事务在操作数据库时，彼此之间不会互相干扰，从而保证数据的一致性。\n当一个事务读取被另一个事务同时修改的数据，或者两个事务试图同时修改相同的数据时，会出现并发性问题。数据库通过提供事务隔离向应用开发人员隐藏了并发性问题的复杂性。\n隔离级别（Isolation Level）定义了不同的隔离性强度，以在性能和数据一致性之间取得平衡。可串行化（serializable）的隔离意味着数据库保证事务具有与串行运行相同的效果，即事务一个接着一个的运行，没有任何并发。然而在实践中，可串行化（serializable）的隔离有一定的性能成本，因此使用较弱的隔离级别是很常见的。根据SQL标准，隔离级别分为四个等级，从低到高分别为：\n读未提交（Read Uncommitted）：这是最低的隔离级别。在这个级别下，一个事务可以看到其他事务尚未提交的数据。这意味着可能发生脏读（Dirty Read），即一个事务读取到了另一个尚未提交的事务所修改的数据。这个级别的优点是并发性能较高，但数据一致性较差。\n读已提交（Read Committed）：这个级别要求一个事务只能看到其他事务已经提交的数据。这意味着脏读不会发生，但仍然可能发生不可重复读（Non-repeatable Read），即在同一个事务中，多次读取同一数据可能得到不同的结果。这个级别在性能和数据一致性之间取得了一定的平衡。\n可重复读（Repeatable Read）：这个级别要求在同一个事务中，对同一数据的多次读取结果必须一致。这可以避免不可重复读的问题，但仍然可能发生幻读（Phantom Read），即在一个事务执行过程中，其他事务插入了满足查询条件的新数据。这个级别提供了较好的数据一致性保障，但并发性能受到一定影响。\n可串行化（Serializable）：这是最高的隔离级别。在这个级别下，事务被处理得就像是串行执行一样，完全避免了脏读、不可重复读和幻读问题。然而，这种级别的数据一致性保障是以牺牲并发性能为代价的，可能导致事务处理效率降低。\n较低的隔离级别增加了用户并发访问相同数据的能力，但也增加了用户可能遇到的并发问题（例如脏读或丢失更新）的数量。相反，较高的隔离级别减少了用户遇到的并发问题，但需要更多的系统资源，并增加了一个事务阻塞另一个事务的机会。\n在实践中，可串行化（serializable）隔离很少被使用，Oracle 数据库甚至没有实现它。在Oracle中，有一个叫做 \u0026ldquo;serializable \u0026ldquo;的隔离级别，但它实际上实现的是快照隔离（snapshot isolation）。\n快照隔离通过为每个事务提供一个数据快照来实现，在一个事务执行过程中，它只能看到和操作事务开始时的数据快照，而不会受到其他并发事务的影响。快照隔离的实现通常依赖于多版本并发控制（MVCC，Multi-Version Concurrency Control）技术。快照隔离能够解决脏读、不可重复读和幻读问题，但它会导致写偏斜（Write Skew），即两个或多个事务同时读取相同的数据，然后基于读取到的数据做出独立的修改，最终导致数据不一致的状态。因此，快照隔离比可串行化的保证要弱。 # 分布式事务 随着计算机网络的发展，分布式计算变得越来越普遍。这导致了分布式事务处理的需求，即在多个独立的数据库或资源管理器上执行的事务。分布式事务处理具有更高的复杂性，需要协调和管理跨越不同系统的事务。这样的事务处理通常需要遵循分布式事务处理的规范和算法，如两阶段提交协议。\n# 两阶段提交 为了使分布式事务的操作表现得像一个原子单元，参与的分布式资源必须根据事务的结果全部提交或全部放弃。两阶段提交（2PC，Two-Phase Commit）协议是一种用于分布式事务处理的原子性协议，它通过在所有事务参与者之间进行协调和同步，以确保分布式事务的原子性得以维护。\n两阶段提交协议引入了一个新的组件：协调器 coordinator，也被称为transaction manager。coordinator 通常和应用进程在同一个进程中，例如 Java EE应用服务器中 transaction manager，但它也可以是一个单独的程序或服务。\n两阶段提交协议包括两个阶段：提交请求阶段（Prepare Phase）和提交阶段（Commit Phase）。\n准备阶段（Prepare Phase）\n事务协调器（Transaction Coordinator）向所有事务参与者（Transaction Participants）发送准备（Prepare）消息，要求它们准备提交事务。 每个事务参与者在收到准备消息后，会执行本地事务操作（例如修改数据、写日志等），然后将其状态设置为“准备就绪”（Ready）。 如果事务参与者成功完成了本地操作并准备好提交事务，它会向事务协调者发送一个“同意”（Agree）消息。否则，它会发送一个“中止”（Abort）消息。 提交阶段（Commit Phase）\n当事务协调者收到所有事务参与者的响应后，会做出全局决策。如果所有参与者都发送了“同意”消息，协调者会决定提交事务。否则，协调者会决定中止事务。 事务协调者向所有事务参与者发送全局决策，即“提交”（Commit）或“中止”（Abort）消息。 事务参与者根据协调者的全局决策执行相应的操作。如果接收到“提交”消息，参与者会提交本地事务，并向协调者发送一个“已提交”（Committed）消息；如果接收到“中止”消息，参与者会回滚本地事务，并向协调者发送一个“已中止”（Aborted）消息。 两阶段提交协议的目标是确保分布式事务中的所有参与者要么都提交事务，要么都中止事务，从而满足原子性要求。\n两阶段提交协议也有一些局限性，例如性能开销、同步延迟和单点故障风险。\n# coordinator 故障 如果任何一个Prepare请求失败或超时，coordinator 将中止交易；如果任何一个 commit 或 abort 请求失败，coordinator 将无限期地重试它们。如果 coordinator 失败了会怎么样呢？\n如果 coordinator 在发送 Prepare 请求之前就失败了，参与者可以安全地中止事务。但是，一旦参与者收到Prepare 请求并投了 \u0026ldquo;yes\u0026rdquo;，参与者不能再单方面中止，必须等待 coordinator 的回复，以确定事务是被 commit 还是被 abort。如果 coordinator 在这时崩溃或发生网络故障，事务处于 in doubt 状态，参与者除了等待之外什么也做不了。\n# 单点故障风险 两阶段提交协议的问题是，一旦事务参与者完成投票，它必须等待 coordinator 给出指示，提交或放弃。如果这时coordinator 挂了，事务参与者除了等待什么也做不了，事务处于未决状态。coordinator 成为了整个系统的单点故障。\ncoordinator 在向参与者发送提交或中止请求之前，必须将事务的最终结果写入到磁盘上的事务日志中。当coordinator 从故障中恢复时，它通过事务日志来确定所有未决状态事务的处理。所以从本质上看，两阶段提交协议为了达到一致性，实际上是退化到由 coordinator 单节点来实现 atomic commit。\n因为两阶段提交协议会在等待 coordinator 恢复的过程中处于阻塞状态，所以它被称为阻塞原子提交（blocking atomic commit）协议。\n# coordinator 的故障恢复 如果 coordinator 发生故障，如何进行故障恢复呢？有三种解决方法：\n等待 coordinator 恢复，并接受在此期间系统将被阻塞的事实。 由人工选择一个新的 coordinator 节点，进行手动故障切换。 使用一个算法来自动选择一个新的 coordinator。 后两种解决方法的前提是，事务日志必须安全可靠的存储，不能因为 coordinator 的任何故障而被损坏。\n# 启发式决策 为了保证原子性，两阶段提交协议必须是阻塞的。这意味着，即使存在故障恢复机制，参与者也可能长时间内被阻塞，但一些应用可能无法容忍这种长时间的阻塞。更糟的情况是，如果事务日志丢失或损坏， 即使 coordinator 恢复了也不能决定事务的最终结果。 未决的事务不会自动解决，它们会驻留在数据库中，持有锁并阻塞其他事务。这时即使重启数据库也不能解决问题，因为数据库必须在重新启动时保留对未决事务的锁定，否则将可能违反两阶段提交的原子性保证。\n为了打破两阶段提交的阻塞性，事务参与者在没有 coordinator 的明确指示下，独立决定中止或提交一个未决事务，这就是启发式决策（heuristic decision）。\n启发式决策可能导致数据不一致，因为事务参与者在没有 coordinator 指示的情况下独立决定事务的命运。这可能导致某些参与者提交事务，而另一些参与者中止事务。事实上，启发式决策违反了两阶段提交协议的承诺，因此，做出启发式决策只是用于摆脱灾难性的情况，而不是常规使用。\nJTA 定义了几种与启发式决策有关异常。\njavax.transaction.HeuristicCommitException coordinator 要求事务参与者回滚，但事务参与者此前已经做出了提交的启发式决策。 javax.transaction.HeuristicRollbackException coordinator 要求事务参与者提交，但事务参与者此前已经做出了回滚的启发式决策。 javax.transaction.HeuristicMixedException 是最糟糕的启发式异常。抛出它表示事务的一部分已提交，而其他部分被回滚。当一些事务参与者进行启发式提交，而其他事务参与者进行启发式回滚时，coordinator会抛出此异常。 # 事务模式 事务模型（Transaction Models）是指在事务处理系统中使用的一组原则和方法，用于定义事务的结构、范围、行为和执行方式。不同的事务模型反映了不同的设计和实现方法，以满足特定的应用需求。以下是三种常见的事务模型。\n# 扁平事务模型 扁平事务模型（Flat Transaction Model）是最简单和最常见的事务模型，其中每个事务都是独立的，并且没有任何嵌套或链接关系。扁平事务模型规定在任何给定时间只有一个事务在其他事务中处于活动状态。\n我们是否可以在一个事务中同时开始另一个事务？有两种方法可以做到这一点：\n在第一个事务结束之前，我们可以禁止开始另一个事务。\n我们也可以暂停当前事务，并开始新事务，在新事务完成后，将恢复原始事务。\n扁平事务模型广泛应用于各种数据库系统和应用程序。应用服务器必须支持扁平事务模型。\n# 链式事务模型 在链式事务模型（Chained Transaction Model）中，多个事务可以相互链接，使得一个事务的结束与下一个事务的开始紧密相连。当一个事务提交或回滚时，立即启动另一个事务，而不需要显式地发出BEGIN TRANSACTION命令。链式事务模型有助于提高事务处理的效率，尤其是在需要频繁执行事务的应用场景中。\n# 嵌套事务模型 在嵌套事务模型（Nested Transaction Model）中，事务可以嵌套在其他事务之内，形成一个层次结构。这意味着一个事务可以包含一个或多个子事务，子事务又可以包含它们自己的子事务。一个嵌套的子事务可以单独提交或中止。因此，复杂的事务可以被分解成更容易管理的子事务。子交易可以提交或回滚，而不需要整个交易提交或回滚。\nJTA（Java Transaction API ）规范不要求支持嵌套事务模型。大多数 JTA 实现只支持扁平事务模型。\n# 分布式事务处理模型 分布式事务是一个涉及到由多个分布式应用程序执行的操作，以及可能涉及多个分布式数据库的事务。在分布式环境中保证事务遵守 ACID 原则是很困难的，需要协调和管理跨越不同系统的事务。对于复杂、异构的分布式系统来说，应用程序必须遵守同一个标准来协调事务工作，以进行分布式事务处理（DTP，distributed transaction processing ）。其中一个 DTP 标准是由 Open Group 开发的 X/Open DTP。Java EE 中的全局事务处理使用的就是 X/Open DTP 模型。在企业 Java 应用的世界中，X/Open DTP 是事务处理的基石。\n# X/Open DTP X/Open 是一家成立于1984年的非营利性质的技术联盟，其目标是制定开放系统标准，以便于实现操作系统、数据库、网络和分布式计算等领域的互操作性。1996 年，X/Open 与 Open Software Foundation合并，组成 The Open Group。\nX/Open 在1991年开发了一个分布式事务处理（DTP）模型，其中包括传统的 TP monitors 所提供的许多功能。大多数关系型数据库、消息队列都支持基于 X/Open DTP 的规范。该模型将一个交易处理系统分为几个部分：交易管理器、数据库或其他资源管理器以及交易通信管理器\nX/Open DTP 模型由事务管理器（TM）、资源管理器（RM）、通信资源管理器（CRM）和应用程序（AP）组成。X/Open DTP 标准规定了这些组件功能，以及组件之间的标准接口。\nX/Open 的资源管理器用于描述任何共享资源的管理进程，但它最常用于表示关系数据库。在 X/Open DTP模型下，应用程序和资源管理器之间的接口是对于不同的 RM 是不一样的，但是可以使用资源适配器作为接口，提供应用程序和各种资源管理器类进行通信的通用方法，例如 JDBC 可以被认为是资源适配器。\n事务管理器是 X/Open DTP 模型的核心，负责协调各分布式组件之间事务。资源管理器通过实现 XA 规范来参与分布式事务。XA 规范定义了事务管理器（TM）和资源管理器（RM）之间的双向接口。事务管理器实现了两阶段提交协议，确保所有的资源管理器都能同时提交完成事务，或在失败时回滚到原始状态。\n通信资源管理器 为连接分布式的事务管理器提供了一种标准方法，以便在不同事务域之间传播事务信息，实现更广泛的分布式事务。事务管理器和通信资源管理器之间的标准接口由 XA+接口 定义。通信资源管理器到应用程序的接口由三个不同的接口定义，即TxRPC、XATMI 和 CPI-C。\n# X/Open XA X/Open XA规范定义了事务管理器（Transaction Manager）与资源管理器（Resource Manager）之间的协作机制，以便在分布式环境中实现两阶段提交2PC协议。X/Open XA规范主要包括以下几个组成部分：\nXA接口： 这是一组标准的函数和数据结构，用于定义事务管理器和资源管理器之间的通信方式。XA接口包括一系列函数，如xa_open()、xa_close()、xa_start()、xa_end()、xa_prepare()、xa_commit()、xa_rollback()等，这些函数分别对应分布式事务处理过程中的不同阶段。\nXID（Transaction Identifier）：唯一的事务标识符，用于跟踪和管理分布式环境中的事务。XID 包括三个主要部分：全局事务ID（Global Transaction ID）、分支限定符（Branch Qualifier）和格式ID（Format ID）。全局事务ID用于唯一标识一个分布式事务，分支限定符用于标识事务中的不同资源管理器，而格式ID用于指定XID的表示格式。\n两阶段提交协议：X/Open XA规范采用两阶段提交协议来实现分布式事务处理。\n遵循X/Open XA规范的事务管理器和资源管理器可以跨平台、跨系统地协同工作，实现分布式事务处理的互操作性。\n# Java Transaction API (JTA) Java Transaction API (JTA) 是Java平台上的一个事务处理规范，它为 Java 应用程序提供了一组统一的事务处理接口。JTA 是 Java EE 规范的一部分，旨在简化分布式事务处理。JTA 遵循 X/Open DTP模型，将事务管理器和资源管理器的接口抽象为 Java 接口。\nJTA 规定了事务管理器和分布式事务系统中涉及的各方之间的 Java 接口：应用程序、资源管理器和应用服务器。\nJTA包由两部分组成：\n应用接口，由应用程序划定事务边界 事务管理器接口，由应用服务器控制事务边界的划分 上图显示了 JTA 的三个主要接口，包括 JTA TransactionManager、JTA UserTransaction 和 JTA XA XAResource。该图还显示了 JTA 与 Java事务服务（JTS）的关系。\nJTA 组件被定义在 javax.transaction和 javax.transaction.xa 两个包内。其中 javax.transaction.xa\n# JTA 事务管理接口 JTA 支持事务管理服务的标准接口，应用服务器主要通过 TransactionManager 和 Transaction 接口来访问这些服务。\n应用服务器使用 TransactionManager 接口来管理用户应用程序的事务。 TransactionManager 将事务与线程相关联。TransactionManager上的begin()、**commit()和rollback()方法被应用服务器调用，分别为当前线程开始、提交和回滚事务。TransactionManager还支持setRollbackOnly()**方法，指定对当前线程的事务只支持回滚。**setTransactionTimeout()**方法还以秒为单位定义事务超时，getStatus() 方法返回当前线程事务的静态常量 Status。\n调用 TransactionManager.getTransaction() 可以获得当前线程关联的事务对象 Transaction。通过调用 TransactionManager.suspend() 可以暂停当前事务并获得 Transaction 对象， TransactionManager.resume() 方法恢复当前事务。\nTransaction接口表示具体的事务实例。Transaction由TransactionManager创建，提供了一些与事务相关的方法，如commit()，rollback()和getStatus()等。可以使用 setRollbackOnly() 调用告诉Transaction对象仅允许回滚。enlistResource 方法用于将 XAResource 对象添加到事务上下文中，delistResource方法用于将 XAResource对象从事务上下文中移除。\nSynchronization接口用于在事务完成时接收回调通知。调用 Transaction.registerSynchronization() 可以将Synchronization注册到与当前线程关联的事务中。\nStatus 接口定义了一组静态常量，表示事务的状态。\n# JTA 应用接口 JTA 的应用接口是 UserTransaction ，被应用程序用来控制事务边界。\n**UserTransaction.begin()**方法可以被应用程序调用，开始一个与应用程序当前线程相关联的事务。\nUserTransaction.commit() 提交与当前线程关联的事务。UserTransaction.rollback() 回滚与当前线程关联的事务。通过调用UserTransaction.setRollbackOnly()，设置与当前线程相关的事务只能被回滚。\n通过调用UserTransaction.setTransactionTimeout()可以设置与事务相关的超时，超时的单位是秒。事务状态Status可以通过 UserTransaction.getStatus() 获得。\nEJB 可以依赖声明式和容器管理事务。但是如果希望 EJB 以编程方式管理自己的事务，就可以利用UserTransaction接口。Servlets 和 JSP 也可以利用 UserTransaction 接口来划分事务。UserTransaction 可以从JNDI查询中获得，或者直接从 EJB 容器环境中获得。\n# JTA 和 X/Open XA X/Open 制定的 XA 规范 定义了分布式资源管理器的接口，被 X/Open DTP 模型中的分布式事务管理器访问。JTA 使用 XAResource 和 Xid 接口封装 XA。TransactionManager 使用 XAResource 接口来管理资源间的分布式事务。\nXid 是分布式事务的标识符，可以从 Xid 获取标准的X/Open格式标识符、全局事务标识符字节和分支标识符。\nXAResource 接口是事务管理器和资源管理器之间标准 X/Open 接口的 Java 映射。资源管理器的资源适配器必须实现 XAResource 接口，使资源能够参与进分布式事务。一个资源管理器的例子是关系数据库，对应的资源适配器就是 JDBC 接口。\n**XAResource.start()**方法用于将分布式事务与资源关联。**XAResource.end()**将资源与事务分离。XAResource还提供了提交、准备提交、回滚、恢复和遗忘分布式事务的方法。事务超时也可以从XAResource中设置和获取。\n# Java Transaction Service (JTS) CORBA Object Transaction Service (OTS) 将分布式事务处理模型（DTP）扩展到了对象领域，它提供了一种在分布式对象系统中进行事务处理的方法。OTS 定义了一组标准的接口和协议，允许 CORBA 对象参与分布式事务。Java Transaction Service (JTS) 是 OTS 的 Java 映射， JTA 推荐使用 JTS 作为其底层事务系统的实现。\n从事务管理器的角度来看，JTA 接口是以 high-level 的形式出现，而 JTS 是事务管理器内部使用的 low-level 接口。应用服务器间的事务互操作性是通过底层使用 JTS 实现获得的。\n# CORBA 由 Object Management Group（OMG）定义的通用对象请求代理架构（Common Object Request Broker Architecture，CORBA）是一个由包括IBM、BEA和惠普在内的工业联盟制定的标准，它促进了可互操作应用程序的构建，这些应用程序基于分布式对象的概念。\nCORBA 使用一个标准的通信模型，在这个模型上，用不同的语言组合实现的客户和服务器，以及在不同的硬件和操作系统平台上运行的客户和服务器可以进行交互。CORBA 体系结构主要包含以下几个部分：\n对象请求代理（Object Request Broker，ORB），它使对象能够在分布式的异质环境中透明地发出和接收请求。这个组件是OMG参考模型的核心。 对象服务，一组支持使用和实现对象功能的服务集合。这些服务是构建分布式应用程序所必需的，例如Object Transaction Service (OTS)。 通用设施，应用程序可能需要的其他有用服务。 CORBA 比 Java EE 的出现早了十年，并且不受限于单一的实现语言。在 Java EE 出现之前，CORBA 是企业应用程序的标准开发平台。 EJB 采用的底层分布式对象通信协议是由 CORBA 定义的。EJB 使用 CORBA 通信协议将它们的服务暴露给客户，也可以使用 CORBA 通信协议与其他 EJB 和基于 CORBA 的服务器环境通信。一些 CORBA 服务，如 CORBA 命名服务、CORBA 事务和 CORBA 安全，被 Java EE 标准所接受，作为创建可互操作的 EJB 服务的手段。\n# ORB ORB是 CORBA 的核心组件，负责在客户端和服务端之间传递请求和响应。ORB的主要功能包括：\n为客户端提供透明访问：客户端可以像调用本地对象一样调用远程对象，而不用关心底层通信和数据交换的细节。 定位和激活服务对象：ORB负责在分布式系统中查找和激活服务对象，以便客户端能够与它们进行通信。 消息封装和解封装：ORB将客户端的请求封装为消息，并在服务端解封装，以便服务对象能够处理请求。响应也会经过类似的处理。 系统间通信：ORB处理不同系统间的通信，包括连接管理、错误处理和安全性。 跨平台和跨语言：通过 IDL，ORB 可以实现不同编程语言之间的对象互操作。 # GIOP 和 IIOP GIOP 是一种通用的协议，用于定义分布式系统中不同 ORB之间的通信。GIOP 指定了在 ORB 之间传递的消息格式和通信规则。IIOP 是一种基于 TCP/IP 协议的 GIOP 实现。\nGIOP 将 IDL 数据类型映射成二进制数据流，并通过网络发送。GIOP 使用通用数据表示（Common Data Representation ，CDR）语法来完成这一任务，以有效地在IDL数据类型和二进制数据流之间进行映射。\nIIOP 将 GIOP 消息数据映射到 TCP/IP 连接行为，以及对输入/输出流的读/写。当一个 CORBA 服务器对象要被分发时，ORB 通过Interoperable Object Reference (IOR) 使网络上唯一识别该对象的信息可用。IOR 包含 CORBA 服务器对象进程的 IP 地址和 TCP 端口。CORBA 客户端利用IOR 建立和CORBA 服务器的连接。\n# RMI/IIOP Java 远程方法调用（JAVA REMOTE METHOD INVOCATION，RMI）框架是Java的分布式对象通信框架。RMI允许客户端和服务器将对象作为方法参数和返回值通过值或引用来传递。如果在方法参数或返回类型中使用的类的类型对客户端或服务器都是未知的，它可以被动态加载。RMI还为分布式垃圾收集提供了一种方法，以清理不再被任何分布式客户端引用的任何分布式服务器对象。\nRMI 客户端与实现 Java 接口的对象对话，该接口与特定 RMI 服务器暴露的远程接口相对应。该接口实际上是由 RMI stub实现的，它接受来自 RMI 客户端的调用，并将其打包成可通过网络发送的序列化数据包。同样地，stub 将来自RMI服务器的序列化响应数据包解封为可由RMI客户端使用的Java对象。\nRemote Reference Layer 从RMI stub 获取序列化的数据，并处理建立在传输协议之上的 RMI 特定的通信协议。Remote Reference Layer 的职责包括解决RMI服务器的位置，启动连接，以及激活远程服务器。\nRMI目前支持两个网络传输协议。JRMP 是标准的 RMI 通信信息传递协议。CORBA的 IIOP 消息传输协议现在也可以通过 RMI/IIOP 标准扩展来实现。\nJRMP 是一个非标准的协议，不能实现与跨语言的 CORBA 对象的通信。与 JRMP 不同，RMI/IIOP 可以在不同平台和编程语言之间进行通信，因为它使用了 CORBA 的 IIOP 协议。RMI/IIOP 使用 IDL 来定义远程对象的接口，这样不同编程语言的客户端都可以调用远程对象。RMI/IIOP 使用 CORBA 的对象传输方式，而不是 Java 序列化，这样可以实现跨平台和跨编程语言的对象传输。\n# OTS OTS 定义了事务服务实现的接口。OTS 的接口基本上可以分为客户端可用的接口和服务器可用的接口。这些接口之间有一些重叠，因为在某些情况下需要同时提供给客户和服务器。\n简要地描述一下这些接口在OTS规范中的作用：\nCurrent 是应用开发者与事务实现的典型交互方式，允许事务的开始和结束。使用 Current 创建的事务会自动与调用的线程相关联。底层实现通常会使用 TransactionFactory 来创建top-level 事务。OTS 规范允许事务被嵌套。\nControl 接口提供对特定事务的访问，实际上包装了事务 Coordinator 和 Terminator 接口，分别用于 enlist 参与者和结束事务。把这个功能分成两个接口的原因之一是，为了更精细的控制可终止事务的实体。\nResource/SubtransactionAwareResource 接口代表事务参与者，可以兼容任何两阶段提交协议的实现，包括 X/Open XA。\n每个top-level 事务都有一个相关的RecoveryCoordinator，参与者可以使用它来进行故障恢复。\nTransaction Context 主要作用是存储和传递与当前事务相关的信息。通过使用 Transaction Context，OTS 中的事务参与者可以共享同一事务上下文，从而实现对事务的正确协调和管理。\n使用 OTS 接口进行事务划分和传播时，有两种使用模式：\nIndirect/Implicit 模式，事务使用 Current 接口创建、提交和回滚事务。事务传播根据目标对象 POA 中的策略自动进行。 Direct/Explicit 模式，事务使用 TransactionFactory 创建，并使用 Control 对象进行提交或回滚。事务传播是通过向每个 IDL 操作添加参数（例如，事务的控制对象）来完成。 大多数应用程序的首选 Indirect/Implicit 模式，Direct/Explicit 模式提供了更大的灵活性，但更难管理。\n# JTS Java Transaction Service（JTS）规范是 OTS 规范的Java语言映射。使用符合 JTS 的实现在理论上允许与其他 JTS 实现的互操作。\nJTS API 通过规范提供 IDL 生成，主要的接口在 org.omg.CosTransactions 和 org.omg.CosTSPortability 包中。 Java 应用服务器通过 JTA 接口访问事务管理功能，JTA 通过 JTS 与事务管理的实现进行交互。同样，JTS 可以通过 JTA XA 接口访问资源，也可以访问启用 OTS 的非 XA 资源。JTS 实现可以通过 CORBA OTS 接口进行互操作。JTS 必须支持扁平事务模型。JTS 可以支持嵌套事务模型，但不是必需的。\n从 Transaction Manager 的角度来看，JTS 的实现是不需要公开。上图 Transaction Manager 框中的虚线说明了JTA 和 JTS 之间的专用接口，允许 JTA 与底层 OTS 实现进行交互。\nJTS 使用 CORBA OTS 接口来实现互操作性和可移植性（即通过 CosTransactions 和 CosTSPortability），这些接口为利用 IIOP 在 JTS 之间生成和传播事务上下文的实现定义了标准机制。\n总之，JTA 是暴露给用户和应用服务器使用的接口，应用服务器内部可以使用 JTS 作为其底层事务系统的实现，应用服务器间的事务互操作性是通过底层使用 JTS 实现获得的。\n","date":"2023-04-20T18:35:13+08:00","permalink":"https://mazhen.tech/p/java-ee%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E4%BA%8B%E5%8A%A1%E7%AE%A1%E7%90%86/","title":"Java EE应用服务器的事务管理"},{"content":"数据库连接池是应用服务器的基本功能，但有时用户因为性能、监控等需求，想使用第三方的连接池。如果只是使用第三方连接池管理数据库连接，那么直接在应用中引入就可以了，但如果用户同时还需要应用服务器的分布式事务和安全服务，就没那么简单了。\n为了讲清楚，首先需要了解一下 JDBC 基本概念。\n# Connection 从 JDBC driver 的角度来看，Connection 表示客户端会话。应用程序可以通过以下两种方式获取连接：\nDriverManager 最初的JDBC 1.0 API中被引入，当应用程序首次尝试通过指定URL连接到数据源时，DriverManager将自动加载在 CLASSPATH 中找到的任何 JDBC driver。 DataSource 是在 JDBC 2.0 可选包API中引入的接口。它允许应用程序对底层数据源的细节是透明的。DataSource 对象的属性被设置为表示特定数据源。当调用其 getConnection方法时，DataSource 实例将返回到该数据源的连接。通过简单地更改DataSource对象的属性，可以将应用程序定向到不同的数据源；无需更改应用程序代码。同样，可以更改 DataSource 实现而不更改使用它的应用程序代码。 JDBC 还定义了 DataSource 接口的两个重要扩展：\nConnectionPoolDataSource - 支持物理连接的缓存和重用，从而提高应用程序的性能和可扩展性 XADataSource - 提供可以参与分布式事务的连接 从 DriverManager 和 DataSource 都可以获得 Connection。\nDataSource、ConnectionPoolDataSource 和 XADataSource 都继承自 CommonDataSource，但它们之间没有继承关系。\n从 ConnectionPoolDataSource 获得的是 PooledConnection，PooledConnection 并没有继承 Connection，但可以获得Connection。\n从 XADataSource 获得的是 XAConnection，XAConnection 继承了 PooledConnection，除了能获得 Connection，还可以获得 XAResource。\n# Application Server DataSource 应用服务器会为其客户端提供了一个 DataSource 接口的实现，并通过 JNDI 暴露给用户。这个 DataSource 包装了 jdbc driver 连接数据库的能力，并在此基础上提供连接池、事务和安全等服务。\n在配置应用服务器的 DataSource 时，一般需要指定 Connection 的获取方式：\njava.sql.Driver\njavax.sql.DataSource\njavax.sql.ConnectionPoolDataSource\njavax.sql.XADataSource\n这四种连接获取方式都是 JDBC driver 提供的能力，Driver 和 DataSource 是最基本方式。如果应用服务器的 DataSource 想要具备连接池化、分布式事务等服务，除了自身要实现这些功能以外，还需要底层 driver 提供相应的能力配合。\n# ConnectionPoolDataSource 以连接池为例，JDBC driver 提供了 ConnectionPoolDataSource 的实现，应用服务器使用它来构建和管理连接池。客户端在使用相同的 JNDI 和 DataSource API 的同时获得更好的性能和可扩展性。\n应用服务器维护维护一个从 ConnectionPoolDataSource 对象返回的 PooledConnection 对象池。应用服务器的实现还可以向 PooledConnection 对象注册ConnectionEventListener，以获得连接事件的通知，如连接关闭和错误事件。\n我们看到，应用程序客户端通过 JNDI 查找一个 DataSource 对象，并请求从 DataSource 获得一个连接。当连接池没有可用连接时，DataSource 的实现从 JDBC driver 的 ConnectionPoolDataSource 中请求一个新的 PooledConnection 。应用服务器的 DataSource 实现会向 PooledConnection 注册一个ConnectionEventListener，随后获得一个新的 Connection 对象。应用客户端在完成操作后调用 Connection.close()，会生成一个 ConnectionEvent 实例，该实例会返回给应用服务器的数据源实现。在收到连接关闭的通知后，应用服务器可以将连接对象放回连接池中。\n注意 ConnectionPoolDataSource 本身不是连接池，它是 driver 提供给应用服务器的接口契约，意思是你从 ConnectionPoolDataSource 获得的PooledConnection可以放心的缓存起来，同时连接关闭的时候，driver 会发送事件通知给应用服务器，真正的关闭连接还是放回连接池，由你自己决定。 一般 JDBC driver 提供的 ConnectionPoolDataSource 实现并没有内置连接池功能，需要配合应用服务器或其他第三方连接池一起使用。可以参考 MySQL Connector 的文档。\n# XADataSource 同样，如果想要分布式事务支持，应用服务器的 DataSource 需要依赖 driver 提供的 XADataSource 实现，同时通过 XAResource 和 Transaction Manager 交互。\nXADataSource 对象返回 XAConnection ，该对象扩展了 PooledConnection ，增加了对分布式事务的参与能力。应用服务器的 DataSource 实现在XAConnection 对象上调用 getXAResource() 以获得传递给事务管理器的 XAResource 对象。事务管理器使用 XAResource 来管理分布式事务。\n就像池化连接一样，这种分布式事务管理的标准API对应用程序客户端也是透明的。因此，应用服务器可以使用不同 JDBC driver 实现的XADataSource， 来组装可扩展的分布式事务支持的数据访问方案。\n# 直接整合外部连接池 如果想在应用服务器中直接整合第三方的连接池实现是比较困难的，下面分析一下原因。\nJTA 规范要求连接必须能够同时处理多个事务，这个功能被称为事务多路复用或事务交错。我们看一个例子：\n1 2 3 4 5 6 7 8 9 10 1. UserTransaction ut = getUserTransaction(); 2. DataSource ds = getDataSource(); 3. 4. ut.begin(); 5. 6. Connection c1 = ds.getConnection(); 7. // do some SQL 8. c1.close(); 9. 10. ut.commit(); 在第8行，连接将释放回连接池，另外一个线程就可以通过 getConnection() 获得刚释放的连接。但此时 c1 上的事务还没有提交，如果被其他线程获取，就有可能加入另一个事务，这就是为什么连接必须能够一次支持多个事务。\n大多数数据库都不支持事务多路复用，那么一种变通的做法是让事务独占连接，在 JTA 事务完成之前，连接不要释放连接回池中。\n因此，需要应用服务器的连接池实现能感知到事务，在第8行不会释放连接，而是连接被标记为关闭。在第10行事务提交后，标记为已关闭的所有连接才释放回连接池。\n现实中，应用服务器管理的连接池都是能够感知事务的存在，并通过 XAResource 和 Transaction Manager 进行交互：\n另外，应用服务器都实现了对 **JCA（Java EE Connector Architecture）**规范的支持。JCA 将应用服务器的事务、安全和连接管理等功能，与事务资源管理器集成，定义了一个标准的 SPI(Service Provider Interface) ，因此，一般应用服务器的连接池都在 JCA 中实现，JDBC DataSource 作为一种资源，被 JCA 统一管理。\n而外部连接池不能感知事务的存在，所以没办法做到事务对连接的独占，因此应用服务器不能简单的直接整合第三方连接池。\n# 解决方案 如果外部连接池实现了 XADataSource，那么我们可以把它当作普通的 JDBC driver，在配置应用服务器的 DataSource 时使用。需要注意几点：\n为外部连接池配置真正的 JDBC driver 时，要使用 driver的 XADataSource 作为连接的获取方式\n外部连接池作为特殊的 driver，已经内置了池化功能，连接池的相关参数最好和应用服务器的DataSource保持一致，因为连接池的实际大小受到外部连接池的约束\n外部连接池在使用前，一般需要进行初始化，同时，应用服务器在关闭 DataSource 时，也要关闭内置的外部连接池，避免连接泄漏。\n这个解决方案的问题是，应用服务器和外部连接池都对连接做了池化，实际上是建立了两个连接池，存在较大的浪费。一种变通的做法是，设置应用服务器连接池的空闲连接数为0，这样应用服务器的连接池不会持有连接，连接在使用完毕后会释放到外部连接池。连接由外部连接池管理，同时经过应用服务器 datasource的包装，能够享受应用服务器内置的事务和安全服务。\n当然更优的做法是，对外部连接池进行适当改造，让它能感知事务的存在，例如 Agroal 连接池能够被注入Transaction Manager，通过 Transaction Manager 感知到事务的存在，做到事务对连接的独占。\n","date":"2023-03-10T22:35:11+08:00","permalink":"https://mazhen.tech/p/%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%95%B4%E5%90%88%E7%AC%AC%E4%B8%89%E6%96%B9%E8%BF%9E%E6%8E%A5%E6%B1%A0/","title":"应用服务器整合第三方连接池"},{"content":"高度模块化的设计是 Nginx 的架构基础。Nginx主框架中只提供了少量的核心代码，大量强大的功能是在各模块中实现的。\n# 模块数据结构 # ngx_module_t 结构 Nginx 的模块化架构最基本的数据结构为 ngx_module_t，所有的模块都遵循着同样的接口设计规范。\nngx_module_t 是 ngx_module_s 的别名，定义在 src/core/ngx_core.h 中：\n1 typedef struct ngx_module_s ngx_module_t; 而 ngx_module_s 在 src/core/ngx_module.h 中定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 struct ngx_module_s { ngx_uint_t ctx_index; // 模块在同类型模块数组中的索引序号 ngx_uint_t index; // 模块在所有模块数组中的索引序号 char *name; // 模块的名称 ngx_uint_t spare0; // 保留变量 ngx_uint_t spare1; // 保留变量 ngx_uint_t version; // 模块的版本号 目前只有一种，默认为1 const char *signature; void *ctx; // 模块的上下文 不同的模块指向不同的上下文 ngx_command_t *commands; // 模块的命令集，指向一个 ngx_command_t 结构数组 ngx_uint_t type; // 模块类型 ngx_int_t (*init_master)(ngx_log_t *log); // master进程启动时回调 ngx_int_t (*init_module)(ngx_cycle_t *cycle); // 初始化模块时回调 ngx_int_t (*init_process)(ngx_cycle_t *cycle);// worker进程启动时回调 ngx_int_t (*init_thread)(ngx_cycle_t *cycle); // 线程启动时回调（nginx暂时无多线程模式） void (*exit_thread)(ngx_cycle_t *cycle); // 线程退出时回调 void (*exit_process)(ngx_cycle_t *cycle);// worker进程退出时回调 void (*exit_master)(ngx_cycle_t *cycle); // master进程退出时回调 uintptr_t spare_hook0; // 保留字段 uintptr_t spare_hook1; uintptr_t spare_hook2; uintptr_t spare_hook3; uintptr_t spare_hook4; uintptr_t spare_hook5; uintptr_t spare_hook6; uintptr_t spare_hook7; }; ngx_module_t定义了init_master，init_module，init_process，init_thread，exit_thread，exit_process，exit_master 这7个回调方法，分别在初始化 master、初始化模块、初始化 worker 进程、初始化线程、退出线程、退出 worker 进程、退出 master 时被调用。事实上，init_master、init_thread、exit_thread 这3个方法目前都没有使用。\n# ngx_command_t 结构 ngx_command_t 类型的 commands 数组指定了模块处理配置项的方法，在解析时配置文件会查找该表。ngx_command_t 是 ngx_command_s的别名，定义在 src/core/ngx_core.h 中：\n1 typedef struct ngx_command_s ngx_command_t; 而 ngx_command_s 在 src/core/ngx_conf_file.h 中定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 struct ngx_command_s { // 配置项的名称 ngx_str_t name; // 配置项的类型 ngx_uint_t type; // 配置项解析处理函数 char *(*set)(ngx_conf_t *cf, ngx_command_t *cmd, void *conf); // 用于指示配置项所处内存的相对偏移位置 ngx_uint_t conf; // 表示当前配置项在整个存储配置项的结构体中的偏移位置 ngx_uint_t offset; // 配置项读取后的处理方法 必须是ngx_conf_post_t结构体的指针 void *post; }; # 模块类型 Nginx 模块共有6种类型，由 ngx_module_t-\u0026gt;type 字段表示。类型常量分别定义在：\nsrc/core/ngx_conf_file.h 1 2 #define NGX_CORE_MODULE 0x45524F43 /* \u0026#34;CORE\u0026#34; */ #define NGX_CONF_MODULE 0x464E4F43 /* \u0026#34;CONF\u0026#34; */ src/event/ngx_event.h 1 #define NGX_EVENT_MODULE 0x544E5645 /* \u0026#34;EVNT\u0026#34; */ src/http/ngx_http_config.h 1 #define NGX_HTTP_MODULE 0x50545448 /* \u0026#34;HTTP\u0026#34; */ src/stream/ngx_stream.h 1 #define NGX_STREAM_MODULE 0x4d525453 /* \u0026#34;STRM\u0026#34; */ src/mail/ngx_mail.h 1 #define NGX_MAIL_MODULE 0x4C49414D /* \u0026#34;MAIL\u0026#34; */ 所有模块间分层次、分类别的。Nginx 官方共有6大类型的模块：核心模块、配置模块、事件模块、HTTP模块、mail模块和stream模块。\nngx_module_t 中有一个类型为 void* 的 ctx成员，其定义了该模块的公共接口，每类模块都有各自特有的属性，通过 void* 类型的ctx 变量进行抽象，同类型的模块遵循同一套通用性接口。\n模块都具备相同的 ngx_module_t 接口，但 ctx 指向不同的结构。由于配置类型 NGX_CONF_MODULE的模块只拥有1个模块 ngx_conf_module，所以没有具体化 ctx上下文成员。\n配置模块和核心模块这两种模块类型是由Nginx的框架代码所定义的。\n配置模块 ngx_conf_module 是所有模块的基础，它实现了最基本的配置项解析功能（解析 nginx.conf文件），其他模块在生效前都需要依赖配置模块处理配置指令并完成各自的准备工作。\n核心模块的模块类型是 NGX_CORE_MODULE。目前官方的核心类型模块中共有6个具体模块，分别是 ngx_core_module、ngx_errlog_module、ngx_events_module、ngx_openssl_module、ngx_http_module 和 ngx_mail_module。Nginx 框架代码只关注 6个核心模块，而大部分模块都是非核心模块。\n核心模块的 ctx 变量指向的是名为 ngx_core_module_t （src/core/ngx_module.h）的结构体。这个结构体很简单，除了一个 name 成员就只有 create_conf和 init_conf两个方法。\n1 2 3 4 5 6 7 typedef struct { ngx_str_t name; // 解析配置项前Nginx框架会调用 void *(*create_conf)(ngx_cycle_t *cycle); // 解析配置项完成后，Nginx框架会调用 char *(*init_conf)(ngx_cycle_t *cycle, void *conf); } ngx_core_module_t; ngx_core_module_t 是以配置项的解析作为基础的。 create_conf 回调方法来创建存储配置项的数据结构，init_conf回调方法使用解析出的配置项初始化核心模块功能。例如核心模块ngx_core_module （src/core/nginx.c）的 ctx 实例化为 ngx_core_module_ctx，定义了 ngx_core_module_create_conf 和 ngx_core_module_init_conf 回调方法。\n1 2 3 4 5 static ngx_core_module_t ngx_core_module_ctx = { ngx_string(\u0026#34;core\u0026#34;), ngx_core_module_create_conf, ngx_core_module_init_conf }; 核心模块可以定义全新的模块类型。例如核心模块 ngx_http_module（src/http/ngx_http.c）定义了 NGX_HTTP_MODULE 模块类型，所有HTTP类型的模块都由 ngx_http_module核心模块管理。同样的，ngx_events_module 定义了 NGX_EVENT_MODULE 模块类型，ngx_mail_module 定义了 NGX_MAIL_MODULE 模块类型。\n核心模块 ngx_http_module 作为所有 HTTP 模块的 “代言”，负责加载所有的 HTTP 模块。同时，在类型为NGX_HTTP_MODULE 的模块中，ngx_http_core_module（src/http/ngx_http_core_module.c）作为 HTTP 核心业务与管理功能的模块，决定了 HTTP 业务的核心逻辑，以及对于具体的请求该选用哪一个HTTP 模块处理这样的工作。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 static ngx_http_module_t ngx_http_core_module_ctx = { ngx_http_core_preconfiguration, /* preconfiguration */ ngx_http_core_postconfiguration, /* postconfiguration */ ngx_http_core_create_main_conf, /* create main configuration */ ngx_http_core_init_main_conf, /* init main configuration */ ngx_http_core_create_srv_conf, /* create server configuration */ ngx_http_core_merge_srv_conf, /* merge server configuration */ ngx_http_core_create_loc_conf, /* create location configuration */ ngx_http_core_merge_loc_conf /* merge location configuration */ }; ngx_module_t ngx_http_core_module = { NGX_MODULE_V1, \u0026amp;ngx_http_core_module_ctx, /* module context */ ngx_http_core_commands, /* module directives */ NGX_HTTP_MODULE, /* module type */ NULL, /* init master */ NULL, /* init module */ NULL, /* init process */ NULL, /* init thread */ NULL, /* exit thread */ NULL, /* exit process */ NULL, /* exit master */ NGX_MODULE_V1_PADDING }; 事件模块、mail模块和HTTP模块类似，它们都在核心模块中各有1个模块作为自己的“代言人”，并在同类模块中有1个作为核心业务与管理功能的模块。\n# 模块初始化 在Nginx的编译阶段执行 configure 后，会在 objs 目录下生成 nginx_modules.c 源文件。这个源文件中有两个很重要的全局变量 ngx_modules 和 ngx_module_names，前者保存了 Nginx 将要使用的全部模块，后者则记录了这些模块的名称。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ngx_module_t *ngx_modules[] = { \u0026amp;ngx_core_module, \u0026amp;ngx_errlog_module, \u0026amp;ngx_conf_module, \u0026amp;ngx_openssl_module, \u0026amp;ngx_regex_module, \u0026amp;ngx_events_module, \u0026amp;ngx_event_core_module, ... }; char *ngx_module_names[] = { \u0026#34;ngx_core_module\u0026#34; \u0026#34;ngx_errlog_module\u0026#34;, \u0026#34;ngx_conf_module\u0026#34;, \u0026#34;ngx_openssl_module\u0026#34;, \u0026#34;ngx_regex_module\u0026#34;, \u0026#34;ngx_events_module\u0026#34;, \u0026#34;ngx_event_core_module\u0026#34;, ... }; # main函数入口 main函数入口定义在 src/core/nginx.c 文件中。\n1 2 3 4 5 6 7 8 9 10 11 12 13 int ngx_cdecl main(int argc, char *const *argv) { ngx_buf_t *b; ngx_log_t *log; ngx_uint_t i; ngx_cycle_t *cycle, init_cycle; ngx_conf_dump_t *cd; ngx_core_conf_t *ccf; ngx_debug_init(); ... } # 预初始化 ngx_preinit_module 负责初始化 ngx_modules 数组所有模块的 index 和 name，计算ngx_max_module和 ngx_modules_n。\n1 2 3 4 5 6 7 8 9 int ngx_cdecl main(int argc, char *const *argv) { ... if (ngx_preinit_modules() != NGX_OK) { return 1; } ... } # ngx_cycle_t 结构体 ngx_cycle_t 是 Nginx 框架最核心的一个结构体，其存储在系统运行过程中的所有信息，包括配置文件信息、模块信息、客户端连接、读写事件处理函数等信息。Nginx 围绕着ngx_cycle_t (src/core/ngx_cycle.h) 来控制进程的运行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 typedef struct ngx_cycle_s ngx_cycle_t; struct ngx_cycle_s { // 保存着所有模块存储配置项的结构体的指针，它首先是一个数组，每个数组成员又是一个指针，这个指针指向另一个存储着指针的数组 void ****conf_ctx; ngx_pool_t *pool; ngx_log_t *log; ngx_log_t new_log; ngx_uint_t log_use_stderr; /* unsigned log_use_stderr:1; */ ngx_connection_t **files; ngx_connection_t *free_connections; ngx_uint_t free_connection_n; // 模块数组 ngx_module_t **modules; ngx_uint_t modules_n; ngx_uint_t modules_used; /* unsigned modules_used:1; */ ngx_queue_t reusable_connections_queue; ngx_uint_t reusable_connections_n; time_t connections_reuse_time; ngx_array_t listening; ngx_array_t paths; ngx_array_t config_dump; ngx_rbtree_t config_dump_rbtree; ngx_rbtree_node_t config_dump_sentinel; ngx_list_t open_files; ngx_list_t shared_memory; ngx_uint_t connection_n; ngx_uint_t files_n; ngx_connection_t *connections; ngx_event_t *read_events; ngx_event_t *write_events; ngx_cycle_t *old_cycle; ngx_str_t conf_file; ngx_str_t conf_param; ngx_str_t conf_prefix; ngx_str_t prefix; ngx_str_t error_log; ngx_str_t lock_file; ngx_str_t hostname; }; 模块初始化仅关注 cycle-\u0026gt;modules 和 cycle-\u0026gt;conf_ctx 。\n# ngx_init_cycle 模块的初始化是在 main 中调用函数 ngx_init_cycle 完成的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 int ngx_cdecl main(int argc, char *const *argv) { ... cycle = ngx_init_cycle(\u0026amp;init_cycle); if (cycle == NULL) { if (ngx_test_config) { ngx_log_stderr(0, \u0026#34;configuration file %s test failed\u0026#34;, init_cycle.conf_file.data); } return 1; } ... } 函数 ngx_init_cycle 定义在 src/ngx_cycle.c 文件中。\n# 创建核心模块配置解析上下文 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ngx_cycle_t * ngx_init_cycle(ngx_cycle_t *old_cycle) { ... //配置上下文 cycle-\u0026gt;conf_ctx = ngx_pcalloc(pool, ngx_max_module * sizeof(void *)); ... //处理core模块，cycle-\u0026gt;conf_ctx用于存放所有CORE模块的配置 for (i = 0; cycle-\u0026gt;modules[i]; i++) { if (cycle-\u0026gt;modules[i]-\u0026gt;type != NGX_CORE_MODULE) { //跳过非核心模块 continue; } module = cycle-\u0026gt;modules[i]-\u0026gt;ctx; //只有ngx_core_module有create_conf回调函数,这个会调用函数会创建ngx_core_conf_t结构， //用于存储整个配置文件main scope范围内的信息，比如worker_processes，worker_cpu_affinity等 if (module-\u0026gt;create_conf) { rv = module-\u0026gt;create_conf(cycle); if (rv == NULL) { ngx_destroy_pool(pool); return NULL; } cycle-\u0026gt;conf_ctx[cycle-\u0026gt;modules[i]-\u0026gt;index] = rv; } } ... } # 配置文件解析 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ngx_cycle_t * ngx_init_cycle(ngx_cycle_t *old_cycle) { ... //conf表示当前解析到的配置命令上下文，包括命令，命令参数等 conf.args = ngx_array_create(pool, 10, sizeof(ngx_str_t)); ... conf.temp_pool = ngx_create_pool(NGX_CYCLE_POOL_SIZE, log); ... conf.ctx = cycle-\u0026gt;conf_ctx; conf.cycle = cycle; conf.pool = pool; conf.log = log; conf.module_type = NGX_CORE_MODULE; //conf.module_type指示将要解析这个类型模块的指令 conf.cmd_type = NGX_MAIN_CONF; //conf.cmd_type指示将要解析的指令的类型 //真正开始解析配置文件中的每个命令 if (ngx_conf_parse(\u0026amp;conf, \u0026amp;cycle-\u0026gt;conf_file) != NGX_CONF_OK) { ... } ... } cycle-\u0026gt;conf_ctx 是一个长度为 ngx_max_module 的数组，每个元素是各个模块的配置结构体。conf_ctx 的创建过程和组织方式会在 ngx_conf_parse 中完成。\nngx_conf_parse 解析配置文件中的命令，conf 存放解析配置文件的上下文信息，如 module_type 表示将要解析模块的类型，cmd_type 表示将要解析的指令的类型，ctx指向解析出来信息的存放地址，args 存放解析到的指令和参数。具体每个模块配信息的存放如下图所示，NGX_MAIN_CONF表示的是全局作用域对应的配置信息，NGX_EVENT_CONF表示的是 EVENT模块对应的配置信息，NGX_HTTP_MAIN_CONF，NGX_HTTP_SRV_CONF，NGX_HTTP_LOC_CONF表示的是HTTP模块对应的main、server和local域的配置信息。\n# 初始化核心模块的配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ngx_cycle_t * ngx_init_cycle(ngx_cycle_t *old_cycle) { ... //初始化所有core module模块的config结构。调用ngx_core_module_t的init_conf, //在所有core module中，只有ngx_core_module有init_conf回调， //用于对ngx_core_conf_t中没有配置的字段设置默认值 for (i = 0; cycle-\u0026gt;modules[i]; i++) { if (cycle-\u0026gt;modules[i]-\u0026gt;type != NGX_CORE_MODULE) { continue; } module = cycle-\u0026gt;modules[i]-\u0026gt;ctx; if (module-\u0026gt;init_conf) { if (module-\u0026gt;init_conf(cycle, cycle-\u0026gt;conf_ctx[cycle-\u0026gt;modules[i]-\u0026gt;index]) == NGX_CONF_ERROR) { environ = senv; ngx_destroy_cycle_pools(\u0026amp;conf); return NULL; } } } ... } # 初始化各个模块 1 2 3 4 5 6 7 8 9 10 ngx_cycle_t * ngx_init_cycle(ngx_cycle_t *old_cycle) { ... if (ngx_init_modules(cycle) != NGX_OK) { /* fatal */ exit(1); } ... } ngx_init_modules 定义在 src/core/ngx_module.c中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 ngx_int_t ngx_init_modules(ngx_cycle_t *cycle) { ngx_uint_t i; for (i = 0; cycle-\u0026gt;modules[i]; i++) { if (cycle-\u0026gt;modules[i]-\u0026gt;init_module) { if (cycle-\u0026gt;modules[i]-\u0026gt;init_module(cycle) != NGX_OK) { return NGX_ERROR; } } } return NGX_OK; } ","date":"2022-11-23T15:06:21+08:00","permalink":"https://mazhen.tech/p/nginx%E7%9A%84%E6%A8%A1%E5%9D%97%E5%8C%96%E8%AE%BE%E8%AE%A1/","title":"Nginx的模块化设计"},{"content":" # 下载Nginx源码 在 nginx: download 选择当前稳定版本下载\n1 2 curl -OL https://nginx.org/download/nginx-1.22.1.tar.gz tar -zxvf nginx-1.22.1.tar.gz # 下载Nginx依赖 PCRE rewrite 模块依赖 从 sourceforge 下载 pcre-8.45.tar.gz，和Nginx源码解压到同级目录。\nzlib gzip 模块依赖 1 2 curl -OL https://zlib.net/zlib-1.2.13.tar.gz tar -zxvf zlib-1.2.13.tar.gz OpenSSL 1 2 curl -OL https://www.openssl.org/source/openssl-1.1.1s.tar.gz tar -zxvf openssl-1.1.1s.tar.gz # 修改默认配置 Nginx默认以 daemon 形式运行，会使用 double fork 技巧，调用 fork() 创建子进程并且把父进程直接丢弃，达到将 daemon 进程与会话的控制终端分离的目的。同时，Nginx 默认是多进程架构，有一个 master 父进程和多个 worker 子进程。为了调试方便，可以修改默认配置 conf/nginx.conf，关闭 daemon，并以单进程模式运行：\n1 2 daemon off; master_process off; # 编译选项配置 使用 configure 命令进行相关编译参数配置：\n--with-debug 启用 debugging log --with-cc-opt='-O0 -g' ，使用 -g 包含 debug 符号信息，-O0标志禁用编译器优化 --prefix 指定安装目录 --with-... 指定依赖的源码位置 1 2 3 4 5 6 ./configure --with-debug --with-cc-opt=\u0026#39;-O0 -g\u0026#39; \\ --prefix=./dist \\ --with-http_ssl_module \\ --with-pcre=../pcre-8.45 \\ --with-zlib=../zlib-1.2.13 \\ --with-openssl=../openssl-1.1.1s # 编译和安装 1 2 make make install # 配置VSCode 首先参考 VSCode 官方文档，完成 VS Code C++ 开发环境的配置。\n确认本机是否已经安装了 Clang 编译器： 1 2 3 4 # 确认是否安装了Clang $ clang --version # 安装开发者命令行工具，包括Clang、git等 $ xcode-select --install 安装 C++ extension for VS Code。 完成C++开发环境准备后，使用 VSCode 打开 nginx 源码，点击菜单 \u0026ldquo;Run -\u0026gt; Starting Debugging\u0026rdquo;，在提示中选择 LLDB，创建出 launch.json，编辑该文件进行 debug 配置。将 \u0026ldquo;program\u0026rdquo; 设置为上一步编译出带有debug信息的nginx。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;lldb\u0026#34;, \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Debug\u0026#34;, \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/dist/sbin/nginx\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34; } ] } 现在就可以在代码中设置断点，再次点击 \u0026ldquo;Run -\u0026gt; Starting Debugging\u0026rdquo;，开始调试 Nginx 吧。\n","date":"2022-11-21T11:18:25+08:00","permalink":"https://mazhen.tech/p/%E5%9C%A8macos%E4%B8%8A%E4%BD%BF%E7%94%A8vscode%E8%B0%83%E8%AF%95nginx/","title":"在macOS上使用VSCode调试NGINX"},{"content":" # Nginx 进程模型 Nginx其实有两种进程结构，一种是单进程结构，一种是多进程结构。单进程结构只适合我们做开发调试，在生产环境下，为了保持 Nginx 足够健壮，以及可以利用到 CPU 的多核特性，我们用到的是多进程架构的Nginx。\n多进程架构的Nginx，有一个父进程 master process，master 会有很多子进程，这些子进程分为两类，一类是worker 进程，一类是 cache 相关的进程。\n在一个四核的Linux服务器上查看Nginx进程：\n1 2 3 4 5 6 7 8 9 $ ps -ef --forest | grep nginx mazhen 20875 13073 0 15:25 pts/0 00:00:00 | \\_ grep --color=auto nginx mazhen 20862 1 0 15:25 ? 00:00:00 nginx: master process ./sbin/nginx mazhen 20863 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20864 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20865 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20866 20862 0 15:25 ? 00:00:00 \\_ nginx: worker process mazhen 20867 20862 0 15:25 ? 00:00:00 \\_ nginx: cache manager process mazhen 20868 20862 0 15:25 ? 00:00:00 \\_ nginx: cache loader process 可以看到，Nginx 的 master 进程创建了4个 worker 进程，以及用来管理磁盘内容缓存的缓存helper进程。\n为什么Nginx使用的是多进程结构，而不是多线程结构呢？因为多线程结构，线程之间是共享同一个进程地址空间，当某一个第三方模块出现了地址空间的断错误时，会导致整个Nginx进程挂掉，而多进程模型就不会出现这样的问题，Nginx的第三方模块通常不会在 master 进程中加入自己的功能代码。\nmaster 进程执行一些特权操作，比如读取配置以及绑定端口，它管理 worker 进程的，负责监控每个 worke进程是否在正常工作，是否需要重载配置文件，以及做热部署等。\nworker 进程处理真正的请求，从磁盘读取内容或往磁盘中写入内容，以及与上游服务器通信。\ncache manager 进程会周期性地运行，从磁盘缓存中删除条目，以保证缓存没有超过配置的大小。\ncache loader 进程在启动时运行，用于将磁盘上的缓存加载到内存中，随后退出。\nNginx 采用了事件驱动的模型，它希望 worker 进程的数量和 CPU 一致，并且每一个 worker 进程与某一颗CPU绑定，worker 进程以非阻塞的方式处理多个连接，减少了上下文切换，同时更好的利用到了 CPU 缓存，减少缓存失效。\n# 请求处理流程 Nginx 使用的是非阻塞的事件驱动处理引擎，需要用状态机来把这个请求正确的识别和处理。Nginx 内部有三个状态机，分别是处理4层 TCP 流量的传输层状态机，处理7层流量的HTTP状态机和处理邮件的email 状态机。\nworker 进程首先等待监听套接字上的事件，新接入的连接会触发事件，然后连接分配到一个状态机。\n状态机本质上是告诉 Nginx 如何处理请求的指令集。解析出的请求是要访问静态资源，那么就去磁盘加载静态资源，更多的时候 Nginx 是作为负载均衡或者反向代理使用，这个时候请求会通过4层或7层协议，传输到上游服务器。对于每一个处理完成的请求，Nginx会记录 access 日志和 error 日志。\n# Nginx 进程管理 Linux 上多进程之间进行通讯，可以使用共享内存和信号。Nginx 在做进程间的管理时，使用了信号。我们可以使用 kill 命令直接向 master 进程和 worker 进程发送信号，也可以使用 nginx 命令行。\nmaster 进程接收处理的信号：\nCHLD 在 Linux 系统中，当子进程终止的时候，会向父进程发送 CHLD 信号。master 进程启动的 worker 进程，所以 master 是 worker 的父进程。如果 worker 进程由于一些原因意外退出，那么 master 进程会立刻收到通知，可以重新启动一个新的 worker进程。\nTERM 和 INT 立刻终止 worker 和 master 进程。\nQUIT 优雅的停止 worker 和 master 进程。worker 不会向客户端发送 reset 立即结束连接。\nHUP 重新加载配置文件\nUSR1 重新打开日志文件，做日志文件的切割\nUSR2 通知 master 开始进行热部署\nWINCH 在热部署过程中，通知旧的 master ，让它优雅关闭 worker 进程\n我们也可以通过 nginx -s 命令向 master 进程发送信号。在 Nginx 启动过程中， Nginx 会把 master 的 PID 记录在文件中，这个文件的默认位置是 $nginx/logs/nginx.pid 。 当我们执行 nginx -s 命令时，nginx 命令会去读取 nginx.pid 文件中 master 进程的 PID，然后向 master 进程发送对应的信号。下面是 nginx -s 命令对应的信号：\nreload - HUP reopen - USR1 stop - TERM quit - QUIT 使用 nginx -s 和 直接使用 kill 命令向 master 进程发送信号，效果是一样的。\n注意，USR2 和 WINCH 没有对应的 nginx -s 命令，只能通过 kill 命令直接向 master 进程发送。\nworker 进程能接收的信号：\nTERM 和 INT QUIT USR1 WINCH worker 进程收到这些信号，会产生和发给 master 一样的效果。但我们通常不会直接向 worker 进程发送信号，而是通过 master 进程来管理 worker 进程，master 进程收到信号以后，会再把信号转发给 worker 进程。\n# Nginx 配置更新流程 当更改了 Nginx 配置文件后，我们都会执行 nginx -s reload 命令重新加载配置文件。Nginx 不会停止服务，在处理新的请求的同时，平滑的进行配置文件的更新。\n执行 nginx -s reload 命令，会向 master 进程发送 SIGHUP 信号。当 master 进程接收 SIGHUP信号后，会做如下处理：\n检查配置文件语法是否正确。 master 加载配置，启动一组新的 worker 进程。这些 worker 进程马上开始接收新连接和处理网络请求。子进程可以共享使用父进程已经打开的端口，所以新的 worker 可以和老的worker监听同样的端口。 master 向旧的 worker 发送 QUIT 信号，让旧的 worker 优雅退出。 旧的 worker 进程停止接收新连接，完成现有连接的处理后结束进程。 # Nginx 热部署流程 Nginx 支持热部署，在升级的过程中也实现了高可用性，不导致任何连接丢失，停机时间或服务中断。热部署的流程如下：\n备份旧的 nginx 二进制文件，将新的nginx二进制文件拷贝到 $nginx_home/sbin目录。 向 master 进程发送 USR2 信号。 master 进程用新的nginx文件启动新的master进程，新的master进程会启动新的worker进程。 向旧的 master 进程发送 WINCH 信号，让它优雅的关闭旧的 worker 进程。此时旧的 master 仍然在运行。 如果想回滚到旧版本，可以向旧的 master 发送 HUP 信号，向新的master 发送QUIT信号。 如果一切正常，可以向旧的 master 发送 QUIT 信号，关闭旧的 master。 ","date":"2022-11-18T11:16:48+08:00","permalink":"https://mazhen.tech/p/nginx%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/","title":"Nginx架构基础"},{"content":"反向代理（reverse proxy）是指用代理服务器来接受外部的访问请求，然后将请求转发给内网的上游服务器，并将从上游服务器上得到的结果返回外部客户端。作为反向代理是 Nginx 的一种常见用法。\n这里的负载均衡是指选择一种策略，尽量把请求平均地分布到每一台上游服务器上。下面介绍负载均衡的配置项。\n# upstream 作为反向代理，一般都需要向上游服务器的集群转发请求。upstream 块定义了一个上游服务器的集群，便于反向代理中的 proxy_pass使用。\n1 2 3 4 5 6 7 http { ... upstream backend { server 127.0.0.1:8080; } ... } upstream 定义了一组上游服务器，并命名为 backend。\n# proxy_pass proxy_pass 指令设置代理服务器的协议和地址。协议可以指定 \u0026ldquo;http \u0026ldquo;或 \u0026ldquo;https\u0026rdquo;。地址可以指定为域名或IP地址，也可以配置为 upstream 定义的上游服务器：\n1 2 3 4 5 6 7 8 9 10 http { server { listen 6888; server_name localhost; location / { proxy_pass http://backend; } } } # proxy_set_header 在传递给上游服务器的请求头中，可以使用proxy_set_header 重新定义或添加字段。一般我们使用 proxy_set_header 向上游服务器传递一些必要的信息。\n1 2 3 4 5 6 location / { proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://backend; } 上面的配置使用 proxy_set_header 添加了三个 HTTP header：\nHost Host 是表明请求的主机名。默认情况下，Nginx 向上游服务器发送请求时，请求头中的 Host 字段是上游真实服务器的IP和端口号。如果我们想让传递给上游服务器的 Host 字段，包含的是用户访问反向代理时使用的域名，就需要通过 proxy_set_header 设置 Host 字段，值可以为 $host 或 $http_host，区别是前者只包含IP，而后者包含IP和端口号。\nX-Real-IP 经过反向代理后，上游服务器无法直接拿到客户端的 ip，也就是说，在应用中使用request.getRemoteAddr() 获得的是 Nginx 的地址。通过 proxy_set_header X-Real-IP $remote_addr;，将客户端的 ip 添加到了 HTTP header中，让应用可以使用 request.getHeader(“X-Real-IP”) 获取客户端的真实ip。\nX-Forwarded-For 如果配置了多层反向代理，当一个请求经过多层代理到达上游服务器时，上游服务器通过 X-Real-IP 获得的就不是客户端的真实IP了。那么这个时候就要用到 X-Forwarded-For ，设置 X-Forwarded-For 时是增加，而不是覆盖，从客户的真实IP为起点，穿过多层级代理 ，最终到达上游服务器，都会被记录下来。\n# proxy_cache Nginx 作为反向代理支持的所有特性和内置变量都可以在 ngx_http_proxy_module 的文档页面找到：\n其中一个比较重要的特性是 proxy cache，对访问上游服务器的请求进行缓存，极大减轻了对上游服务的压力。\n配置示例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 http { ... proxy_cache_path /tmp/nginx/cache levels=1:2 keys_zone=myzone:10m inactive=1h max_size=10g use_temp_path=off; server { ... location / { ... proxy_cache myzone; proxy_cache_key $host$uri$is_args$args; proxy_cache_valid 200 304 302 12h; } } } 配置说明：\nproxy_cache_path 缓存路径，要把缓存放在哪里\nlevels=1:2：缓存的目录结构 keys_zone=myzone:10m：定义一块用于存放缓存key的共享内存区，命名为myzone，并分配 10MB 的内存；配至10MB的zone 大约可以存放 80000个key。 inactive=1d：不活跃的缓存文件 1 小时后将被清除 max_size=10g：缓存所占磁盘空间的上限 use_temp_path=off：不另设临时目录 proxy_cache myzone;：代表要使用上面定义的 myzone\nproxy_cache_key：用于生成缓存键，区分不同的资源。key 是决定缓存命中率的因素之一。\n$host：request header中的 Host字段 $uri：请求的uri $is_args 反映请求的 URI 是否带参数，若没有即为空值。 $args：请求中的参数 proxy_cache_valid：控制缓存有效期，可以针对不同的 HTTP 状态码可以设定不同的有效期。示例针对 200，304，302 状态码的缓存有效期为12小时。\n检验缓存配置的效果。\n首先查看缓存路径，没有存放任何内容：\n1 2 3 4 $ tree /tmp/nginx/cache/ /tmp/nginx/cache/ 0 directories, 0 files 然后访问Nginx反向代理服务器：\n1 2 3 ❯ curl -v http://172.21.32.84:6888/ ... 再次查看缓存路径：\n1 2 3 4 5 6 7 $ tree /tmp/nginx/cache/ /tmp/nginx/cache/ └── 6 └── ed └── 5e9596b7783c532f541535dd1a60eed6 2 directories, 1 file 经过请求后，缓存路径中已经有内容，并且目录结构是我们配置的 level=1:2。\n","date":"2022-11-16T10:56:47+08:00","permalink":"https://mazhen.tech/p/nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E9%85%8D%E7%BD%AE/","title":"Nginx反向代理配置"},{"content":" # 配置文件语法 Nginx的配置文件是一个文本文件，由指令和指令块构成。\n# 指令 指令以分号 ; 结尾，指令和参数间以空格分割。\n指令块作为容器，将相关的指令组合在一起，用大括号 {} 将它们包围起来。\n1 2 3 4 5 6 7 8 9 10 11 12 13 http { include mime.types; default_type application/octet-stream; server { listen 8080; server_name localhost; location / { root html; index index.html index.htm; } } } 上面配置中的http、server、location等都是指令块。指令块配置项之后是否如参数（例如 location /），取决于解析这个块配置项的模块。\n指令块配置项是可以嵌套的。内层块会继承父级块包含的指令的设置。有些指令可以出现在多层指令块内，你可以通过在内层指令块包含该指令，来覆盖从父级继承的设置。\n# Context 一些 top-level 指令被称为 context，将适用于不同流量类型的指令组合在一起。\nevents – 通用的连接处理 http – HTTP流量 mail – Mail 流量 stream – TCP 和 UDP 流量 放在这些 context 之外的指令是在 main context中。\n在每个流量处理 context 中，可以包括一个或多个 server 块，用来定义控制请求处理的虚拟服务器。\n对于HTTP流量，每个 server 指令块是对特定域名或IP地址访问的控制。通过一个活多个 location 定义如何处理特定的URI。\n对于 Mail 和 TCP/UDP 流量，server 指令块是对特定 TCP 端口流量的控制。\n# 静态资源服务 将个人网站的静态资源 clone 到 nginx 根目录：\n1 git clone https://github.com/mz1999/mazhen.git 在 conf/nginx.conf 文件中配置监听端口和 location：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 http { server { listen 8080; server_name localhost; #charset koi8-r; #access_log logs/host.access.log main; location / { alias mazhen/; #index index.html index.htm; } } location 的语法格式为：\n1 location [ = | ~ | ~* | ^~ ] uri { ... } location 会尝试根据用户请求中的 URI 来匹配上面的 uri 表达式，如果可以匹配，就选择这个 location 块中的配置来处理用户请求。\nlocation 指定文件路径有两种方式：root和alias。\nroot 与alias 会以不同的方式将请求映射到服务器的文件上，它们的主要区别在于如何解释 location 后面的 uri 。\nroot的处理结果是，root＋location uri。 alias的处理结果是，使用 alias 替换 location uri。 alias 作为一个目录别名的定义。 例如：\n1 2 3 location /i/ { root /data/w3; } 如果一个请求的 URI 是 /i/top.gif ，Nginx 将会返回服务器上的 /data/w3/i/top.gif 文件。\n1 2 3 location /i/ { alias /data/w3/images/; } 如果一个请求的 URI 是 /i/top.gif，Nginx 将会返回服务器上的 /data/w3/images/top.gif文件。alias 会把 location 后面配置的 uri 替换为 alias 定义的目录。\n最后要注意，使用 alias 时，目录名后面一定要加 /。\n# 开启gzip Nginx 的 ngx_http_gzip_module 模块是一个过滤器，它使用 \u0026ldquo;gzip \u0026ldquo;方法压缩响应。可以在 http context 下配置 gzip：\n1 2 3 4 5 6 7 8 9 10 11 http { ... gzip on; gzip_min_length 1000; gzip_comp_level 2; gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript; server { ... } } gzip_min_length：设置允许压缩的页面最小字节数 gzip_comp_level： 设置 gzip 压缩比，1 压缩比最小处理速度最快，9 压缩比最大但处理最慢 gzip_types：匹配MIME类型进行压缩。 更多的配置项，可以参考官方文档。\n# autoindex Nginx 的 ngx_http_autoindex_module 模块处理以斜线字符 / 结尾的请求，并产生一个目录列表。通常情况下，当 ngx_http_index_module 模块找不到index文件时，请求会被传递给 ngx_http_autoindex_module 模块。\nautoindex 的配置很简单：\n1 2 3 4 location / { alias mazhen/; autoindex on; } 注意，只有 index 模块找不到index文件时，请求才会被 autoindex 模块处理。我们可以把 mazhen 目录下的 index 文件删掉，或者为 index 指令配置一个不存在的文件。\n# limit_rate 由于带宽的限制，我们有时候需要限制某些资源向客户端传输响应的速率，例如可以对大文件限速，避免传输大文件占用过多带宽，从而影响其他更重要的小文件（css，js）的传输。我们可以使用 set 指令配合内置变量 $limit_rate 实现这个功能：\n1 2 3 4 location / { ... set $limit_rate 1k; } 上面的指令限制了Nginx向客户端发送响应的速率为 1k/秒。\n$limit_rate是Nginx的内置变量，Nginx的文档详细列出了每个模块的内置变量。以 ngx_http_core_module 为例，在 Nginx文档首页的 Modules reference 部分，点击进入 ngx_http_core_module ：\n在 ngx_http_core_module 文档目录的最下方，点击 Embedded Variables ，会跳转到 ngx_http_core_module 内置变量列表：\n这里有 http module 所有内置变量的说明，包括我们刚才使用 $limit_rate。\n# access log Nginx 的 access log 功能由 ngx_http_log_module 模块提供。ngx_http_log_module 提供了两个指令：\nlog_format 指定日志格式 access_log 设置日志写入的路径 举例说明：\n1 2 3 4 5 6 7 8 9 10 http { ... log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; server { ... access_log logs/mazhen.access.log main; } } log_format 使用内置变量定义日志格式，示例中的 log_format 可以使用 http module 定义的内置变量。log_format 还指定了这个日志格式的名称为 main，这样让我们定义多种格式的日志，为不同的 server 配置特定的日志格式。\naccess_log 设置了日志路径为 logs/mazhen.access.log，并指定了日志格式为 main。示例中的 access_log 定义在 server 下，那所有发往这个 server 的请求日志都使用 main 格式，被记录在 logs/mazhen.access.log文件中。\n","date":"2022-11-15T14:40:19+08:00","permalink":"https://mazhen.tech/p/nginx%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E6%9C%8D%E5%8A%A1%E7%9A%84%E9%85%8D%E7%BD%AE/","title":"Nginx静态资源服务的配置"},{"content":" # Nginx 常用命令 Nginx的指令格式为 nginx [options argument]。\n查看帮助 1 ./sbin/nginx -? 使用指定的配置文件 1 ./sbin/nginx -c filename 指定运行目录 1 ./sbin/nginx -p /home/mazhen/nginx/ 设置配置指令，覆盖配置文件中的指令 1 ./sbin/nginx -g directives 向 Nginx 发送信号 我们可以向 Nginx 进程发送信号，控制运行中的 Nginx。一种方法是使用 kill 命令，也可以使用 nginx -s ：\n1 2 3 4 5 6 7 8 9 10 11 # 重新加载配置 $ ./sbin/nginx -s reload # 立即停止服务 $ ./sbin/nginx -s stop # 优雅停止服务 $ ./sbin/nginx -s quit # 重新开始记录日志文件 $ ./sbin/nginx -s reopen 测试配置文件是否有语法错误 1 ./sbin/nginx -t/-T 打印nginx版本 1 ./sbin/nginx -v/-V # 热部署 在不停机的情况下升级正在运行的 Nginx 版本，就是热部署。\n首先查看正在运行的 Nginx：\n1 2 3 4 5 6 7 $ ps aux | grep nginx mazhen 4376 0.0 0.0 9896 2372 ? Ss 16:47 0:00 nginx: master process ./sbin/nginx mazhen 4402 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4403 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4404 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4405 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4407 0.0 0.0 12184 2316 pts/0 S+ 16:51 0:00 grep --color=auto nginx 备份现有 Nginx 的二进制文件：\n1 cp nginx nginx.old 将构建好的最新版 Nginx 的二进制文件拷贝到 $nginx/sbin 目录：\n1 cp ~/works/nginx-1.22.1/objs/nginx ~/nginx/sbin/ -f 给正在运行的Nginx的 master 进程发送信号，通知它我们要开始进行热部署：\n1 kill -USR2 4376 这时候 Nginx master 进程会使用新的二进制文件，启动新的 master 进程。新的 master 会生成新的 worker，同时，老的worker并没有退出，也在运行中，但不再监听 80/443 端口，请求会平滑的过度到新 worker 中。\n1 2 3 4 5 6 7 8 9 10 11 12 $ ps aux | grep nginx mazhen 4376 0.0 0.0 9896 2536 ? Ss 16:47 0:00 nginx: master process ./sbin/nginx mazhen 4402 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4403 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4404 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4405 0.0 0.0 10324 2104 ? S 16:50 0:00 nginx: worker process mazhen 4454 0.0 0.0 9768 6024 ? S 16:59 0:00 nginx: master process ./sbin/nginx mazhen 4455 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4456 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4457 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4458 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4461 0.0 0.0 12184 2436 pts/0 S+ 16:59 0:00 grep --color=auto nginx 向老的 Nginx master 发送信号，让它优雅关闭 worker 进程。\n1 kill -WINCH 4376 这时候再查看 Nginx 进程：\n1 2 3 4 5 6 7 8 $ ps aux | grep nginx mazhen 4376 0.0 0.0 9896 2536 ? Ss 16:47 0:00 nginx: master process ./sbin/nginx mazhen 4454 0.0 0.0 9768 6024 ? S 16:59 0:00 nginx: master process ./sbin/nginx mazhen 4455 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4456 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4457 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4458 0.0 0.0 10216 1980 ? S 16:59 0:00 nginx: worker process mazhen 4475 0.0 0.0 12184 2292 pts/0 S+ 17:07 0:00 grep --color=auto nginx 老的 worker 已经优雅退出，所有的请求已经切换到了新升级的 Nginx 中。\n老的 master 仍然在运行，如果需要，我们可以向它发送 reload 信号，回退到老版本的 Nginx。\n# 日志切割 首先使用 mv 命令，备份旧的日志：\n1 mv access.log bak.log Linux 文件系统中，改名并不会影响已经打开文件的写入操作，因为内核 inode 不变，这样操作不会出现丢日志的情况。\n然后给运行中的 Nginx 发送 reopen 信号：\n1 ./nginx -s reopen Nginx 会重新生成 access.log 日志文件。\n一般会写一个 bash 脚本，通过配置 crontab，每日进行日志切割。\n","date":"2022-11-11T17:26:02+08:00","permalink":"https://mazhen.tech/p/nginx%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/","title":"Nginx的基本使用"},{"content":"Nginx 是最流行的Web服务器，根据 W3Techs 最新的统计，世界上三分之一的网站在使用Nginx。\n# 准备工作 # Linux 版本 Nginx 需要 Linux 的内核为 2.6 及以上的版本，因为Linux 内核从 2.6 开始支持 epoll。可以使用 uname -a 查看 Linux 内核版本：\n1 2 $ uname -a Linux mazhen-laptop 5.15.0-52-generic #58-Ubuntu SMP Thu Oct 13 08:03:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux 从输出看到内核版本为 5.15.0，满足要求。现在已经很难找到内核 2.6 以下的服务器了吧。\n# 安装依赖 为了编译 Nginx 源码，需要安装一些依赖包。本文以 Ubuntu 为例。\nGCC编译器 GCC（GNU Compiler Collection）是必需的编译工具。使用下面的命令安装： 1 sudo apt install build-essential build-essential 是所谓的 meta-package，包含了 g++/GNU 编译器集合，GNU调试器，以及一些编译程序所需的工具和库。\nPCRE库 PCRE库支持正则表达式。如果我们在配置文件nginx.conf中使用了正则表达式，那么在编译Nginx时就必须把PCRE库编译进 Nginx，因为 Nginx的 HTTP 模块需要靠它来解析正则表达式。另外，pcre-devel 是使用PCRE做二次开发时所需要的开发库，包括头文件等，这也是编译Nginx所必须使用的。使用下面的命令安装： 1 sudo apt install libpcre3 libpcre3-dev zlib库 zlib库用于对 HTTP 包的内容做 gzip 格式的压缩，如果我们在 nginx.conf 中配置了gzip on，并指定对于某些类型（content-type）的 HTTP 响应使用 gzip 来进行压缩以减少网络传输量，则在编译时就必须把 zlib 编译进 Nginx。zlib-devel 是二次开发所需要的库。使用下面的命令安装：\n1 sudo apt install zlib1g-dev OpenSSL库 如果我们需要 Nginx 支持 SSL 加密传输，需要安装 OpenSSL 库。另外，如果我们想使用MD5、SHA1等散列函数，那么也需要安装它。使用下面的命令安装：\n1 sudo apt install openssl libssl-dev # 下载Nginx源码 从 http://nginx.org/en/download.html下载当前稳定版本的源码。\n当前稳定版为 1.22.1：\n1 wget https://nginx.org/download/nginx-1.22.1.tar.gz # Nginx配置文件的语法高亮 为了 Nginx 的配置文件在 vim 中能语法高亮，需要经过如下配置。\n解压 Nginx 源码：\n1 tar -zxvf nginx-1.22.1.tar.gz 将 Nginx 源码目录 contrib/vim/ 下的所有内容，复制到 $HOME/.vim 目录：\n1 2 mkdir ~/.vim cp -r contrib/vim/* ~/.vim/ 现在使用 vim 打开 nginx.conf，可以看到配置文件已经可以语法高亮了。\n# 编译前的配置 编译前需要使用 configure 命令进行相关参数的配置。\n使用 configure --help 查看编译配置支持的参数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ ./configure --help | more --help print this message --prefix=PATH set installation prefix --sbin-path=PATH set nginx binary pathname --modules-path=PATH set modules path --conf-path=PATH set nginx.conf pathname --error-log-path=PATH set error log pathname --pid-path=PATH set nginx.pid pathname --lock-path=PATH set nginx.lock pathname ...... --with-libatomic force libatomic_ops library usage --with-libatomic=DIR set path to libatomic_ops library sources --with-openssl=DIR set path to OpenSSL library sources --with-openssl-opt=OPTIONS set additional build options for OpenSSL --with-debug enable debug logging --with开头的模块缺省不包括在编译结果中，如果想使用需要在编译配置时显示的指定。--without开头的模块则相反，如果不想包含在编译结果中需要显示设定。\n例如我们可以这样进行编译前设置：\n1 ./configure --prefix=/home/mazhen/nginx --with-http_ssl_module 设置了Nginx的安装目录，以及需要http_ssl模块。\nconfigure命令执行完后，会生成中间文件，放在目录objs下。其中最重要的是ngx_modules.c文件，它决定了最终那些模块会编译进nginx。\n# 编译和安装 执行编译 在nginx目录下执行make编译：\n1 $ make 编译成功的nginx二进制文件在objs目录下。如果是做nginx的升级，可以直接将这个二进制文件copy到nginx的安装目录中。\n安装 在nginx目录下执行make install进行安装：\n1 $ make install 安装完成后，我们到 --prefix 指定的目录中查看安装结果：\n1 2 3 4 5 6 $ tree -L 1 /home/mazhen/nginx nginx/ ├── conf ├── html ├── logs └── sbin # 验证安装结果 编辑 nginx/conf/nginx.conf 文件，设置监听端口为8080：\n1 2 3 4 5 6 7 http { ... server { listen 8080; server_name localhost; ... 启动 nginx\n1 ./sbin/nginx 访问默认首页：\n1 2 3 4 5 6 7 8 9 10 $ curl -I http://localhost:8080 HTTP/1.1 200 OK Server: nginx/1.22.1 Date: Fri, 11 Nov 2022 08:04:46 GMT Content-Type: text/html Content-Length: 615 Last-Modified: Tue, 08 Nov 2022 09:54:09 GMT Connection: keep-alive ETag: \u0026#34;636a2741-267\u0026#34; Accept-Ranges: bytes ","date":"2022-11-11T16:06:29+08:00","permalink":"https://mazhen.tech/p/nginx%E7%9A%84%E7%BC%96%E8%AF%91%E5%92%8C%E5%AE%89%E8%A3%85/","title":"Nginx的编译和安装"},{"content":"Tower 是一个专注于对网络编程进行抽象的框架，最核心的抽象为 Service trait。Service::call 接受一个 request 进行处理，成功则返回 response，否则返回 error。\n1 fn call(Request) -\u0026gt; Result\u0026lt;Response, Error\u0026gt; # Service trait 的定义 我们希望 Service 是异步编程风格，也就是 call 为 async 方法：\n1 async fn call(Request) -\u0026gt; Result\u0026lt;Response, Error\u0026gt; 我们就可以在 Service::call 上 await：\n1 service.call(request).await 然而当前 Rust 不支持 async trait 方法。\n我们可以让 call 作为普通方法，返回一个实现了 Future 的类型：\n1 2 3 4 5 6 7 trait Service\u0026lt;Request\u0026gt; { type Response; type Error; // ERROR: `impl Trait` not allowed outside of function and inherent // method return types fn call(\u0026amp;mut self, req: Request) -\u0026gt; impl Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;; } 还是不行，目前 Rust 也不支持在 Trait 中使用 impl Trait 做返回值，上一篇文章\u0026lt;impl Trait 的使用\u0026gt;分析过原因。\nTower 在定义 Service 时，使用了关联类型 type Future ，其实是将问题留给了 Service 的实现者，由用户选择 type Future 的实际类型：\n1 2 3 4 5 6 7 8 9 pub trait Service\u0026lt;Request\u0026gt; { type Response; type Error; type Future: Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;; fn poll_ready(\u0026amp;mut self, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Result\u0026lt;(), Self::Error\u0026gt;\u0026gt;; fn call(\u0026amp;mut self, req: Request) -\u0026gt; Self::Future; } # 实现自己的 Future 类型 我们在实现 Service 时，仍然需要为type Future 设置具体的类型。\n既然没法让 call 直接返回 impl Future，一种方法是定义自己的 Future 类型，例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 pub struct HttpRequest { url: String, } pub struct HttpResponse { code: u32, } pub struct ResponseFuture { request: HttpRequest, } impl Future for ResponseFuture { type Output = Result\u0026lt;HttpResponse, Error\u0026gt;; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut std::task::Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; { println!(\u0026#34;process url:{}\u0026#34;, \u0026amp;self.as_ref().get_ref().request.url); Poll::Ready(Ok(HttpResponse { code: 200 })) } } 在实现 Service 时，关联类型 type Future 设置为我们手工实现的 Future：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 struct RequestHandler; impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { type Response = HttpResponse; type Error = Error; type Future = ResponseFuture; fn poll_ready(\u0026amp;mut self, cx: \u0026amp;mut std::task::Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Result\u0026lt;(), Self::Error\u0026gt;\u0026gt; { Poll::Ready(Ok(())) } fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { ResponseFuture { request: req } } } 然后就可以像正常的 Future 一样，使用 Service ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 #[tokio::main] async fn main() { let mut service = RequestHandler {}; match service .call(HttpRequest { url: \u0026#34;/user/mazhen\u0026#34;.to_owned(), }) .await { Ok(r) =\u0026gt; println!(\u0026#34;Response code: {}\u0026#34;, r.code), Err(e) =\u0026gt; println!(\u0026#34;process failed. {:?}\u0026#34;, e), } } # 使用 async blocks 手工实现 Future 会比较麻烦，我们一般都是用 async fn/async blocks语法糖生成 Future，那么这时 Service::call 返回什么类型呢？\n1 2 3 4 5 6 7 8 9 10 11 12 13 struct RequestHandler; impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { ... type Future = ??? fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { async move { println!(\u0026#34;process url {:?}\u0026#34;, \u0026amp;req.url); Ok(HttpResponse { code: 200 }) } } } 既然不能返回 impl Trait ，可以让 call 返回 trait object，用 trait object 统一返回值的类型。\n1 2 3 4 5 6 7 8 9 10 11 impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { ... type Future = Box\u0026lt;dyn Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;\u0026gt;; fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { Box::new(async move { println!(\u0026#34;process url {:?}\u0026#34;, \u0026amp;req.url); Ok(HttpResponse { code: 200 }) }) } } 这时候会报错，说 dyn Future 没有实现 Unpin：\n1 2 3 4 | 51 | type Future = Box\u0026lt;dyn Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;\u0026gt;; | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ the trait `Unpin` is not implemented for `(dyn std::future::Future\u0026lt;Output = Result\u0026lt;HttpResponse, std::io::Error\u0026gt;\u0026gt; + \u0026#39;static)` | 编译错误提示的没有实现 Unpin trait 是什么意思？那要先从 Pin 说起。\n# 一句话解释Pin Pin 本质上解决的问题是保证 Pin\u0026lt;P\u0026lt;T\u0026gt;\u0026gt; 中的 T 不会被 move，除非 T 满足 T: Unpin。\n# 什么是move 所有权转移的这个过程就是 move，例如：\n1 2 let s1 = \u0026#34;Hello, world\u0026#34;.to_owned(); let s2 = s1; s2 浅copy s1 的内容，同时 String 的所有权转移给了 s2。\n通过std::mem::swap()方法交换了两个可变借用 \u0026amp;mut 的内容，也发生了move。\n# 为什么要Pin 自引用结构体，move了以后会出问题。\n所以需要 Pin，不能move。\n# 怎么 Pin 住的 保证 T 不会被move，需要避免两种情况：\n不能暴露 T ，否则赋值、方法调用等都会move 不能暴露 \u0026amp;mut T，开发者可以调用 std::mem::swap() 或 std::mem::replace() 这类方法来 move 掉 T Pin\u0026lt;P\u0026lt;T\u0026gt;\u0026gt;没有暴露T，而且没法让你获得 \u0026amp;mut T，所以就 Pin 住了T。但注意有个前提条件：T 没有实现 Unpin\n# 谁没有实现 Unpin Unpin 是一个auto trait，编译器默认会给所有类型实现 Unpin。唯独有几个例外，他们实现的是 !Unpin。\nPhantomPinned 1 2 3 4 5 6 7 8 9 /// A marker type which does not implement `Unpin`. /// /// If a type contains a `PhantomPinned`, it will not implement `Unpin` by default. #[stable(feature = \u0026#34;pin\u0026#34;, since = \u0026#34;1.33.0\u0026#34;)] #[derive(Debug, Default, Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)] pub struct PhantomPinned; #[stable(feature = \u0026#34;pin\u0026#34;, since = \u0026#34;1.33.0\u0026#34;)] impl !Unpin for PhantomPinned {} 如果你使用了 PhantomPinned，你的类型自动实现 !Unpin 1 2 3 4 5 6 7 8 9 10 use std::marker::PhantomPinned; #[derive(Debug)] struct SelfReference { name: String, // 在初始化后指向 name name_ptr: *const String, // PhantomPinned 占位符 _marker: PhantomPinned, } 编译器为 async fn 生成的匿名结构体实现的是 !Unpin # async fn async fn 是语法糖，在编译时，编译器使用Generator为 async fn 生成匿名结构体，这个结构体实现了 Future。\n这个匿名结构体是自引用的。原因是，如果 async fn 内有多个 await，执行到 await 可能因为资源没准备好而让出 CPU 暂停执行，随后该 Future 可能被调度到其他线程接着执行。所以这个匿名结构体需要保存跨 await 的数据，形成了自引用结构。\n由于为 async fn 生成的结构体是自引用的，所以这个结构体实现了 !Unpin，表示它不能被 move。\n这也是为什么不能给Future::poll 直接传 \u0026amp;mut Self 的原因：生成的匿名结构体不能被move，而拿到 \u0026amp;mut Self就可以使用 swap 或 replace之类的方法进行move，这样不安全，所以必须使用 Pin\u0026lt;\u0026amp;mut Self\u0026gt;。\n# Future 都是 !Unpin 的吗 不一定。\nasync fn 语法糖生成的实现了 Future 的匿名结构，内部包含自引用，它会明确实现 !Unpin，不能 move。\n但如果你自己实现的 Future，内部没有自引用，它就不是 !Unpin，当然可以 move。\n也就是说，Future 和 !Unpin 是两个 trait，虽然它们经常联系在一起，但并不是实现了 Future 的类型都必须同时实现 !Unpin，没有包含自引用的 Future当然可以安全的 move 了。\n# 实现了 Unpin 的 Future 如果是可以 move 的 Future，也就是实现了 Unpin 的 Future，在调用 Future::poll 的时候，要求传入 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，会不会有什么问题呢？\n首先，如果 T：Unpin，那么 Pin\u0026lt;\u0026amp;mut T\u0026gt; 就完全等同于 \u0026amp;mut T。换句话说，Unpin 意味着这个类型可以被移动，即使是在 Pin 住的情况下，所以 Pin 对这样的类型没有影响。因为 Pin 是智能指针，它实现了 Deref/DerefMut，只要满足 T: Unpin，你就能拿到\u0026amp;mut T：\n1 2 3 4 5 6 #[stable(feature = \u0026#34;pin\u0026#34;, since = \u0026#34;1.33.0\u0026#34;)] impl\u0026lt;P: DerefMut\u0026lt;Target: Unpin\u0026gt;\u0026gt; DerefMut for Pin\u0026lt;P\u0026gt; { fn deref_mut(\u0026amp;mut self) -\u0026gt; \u0026amp;mut P::Target { Pin::get_mut(Pin::as_mut(self)) } } 其次，为了用户方便，FutureExt 提供了 poll_unpin，让你直接在 Unpin 的 Future 上 poll：\n1 2 3 4 5 6 7 8 9 pub trait FutureExt: Future { /// A convenience for calling `Future::poll` on `Unpin` future types. fn poll_unpin(\u0026amp;mut self, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; where Self: Unpin, { Pin::new(self).poll(cx) } } 所以，如果你的 Future 是 Unpin，那么即使Future::poll 要求传入的是 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，对你也没有任何影响。\n# Box\u0026lt;dyn Futrue\u0026lt;\u0026gt;\u0026gt; 的问题 回到上面的问题，我们想让 Service::call 返回 trait object，也就是 Box\u0026lt;dyn Futrue\u0026lt;\u0026gt;\u0026gt;，会编译不过，为什么呢？\n因为标准库为 Box 实现了 Future，但要求 Box 包装的 Future 必须同时实现了 Unpin ：\n1 2 3 4 impl\u0026lt;F, A\u0026gt; Future for Box\u0026lt;F, A\u0026gt; where F: Future + Unpin + ?Sized, A: Allocator + \u0026#39;static, 前面已经讲过，async 解语法糖生成的 Future 没有实现 Unpin，所以Box::new(async{...}) 不满足类型约束，它没有实现 Future，不能在它上面await。\n# Pin 实现了 Future 前面讲过，在 Future 上 poll 的时候，不能直接传入\u0026amp;mut Self，而需要传入 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，需要这样调用 Future::poll(Pin::new(\u0026amp;mut future), ctx)。如果 Pin 实现了 Future ，我们就可以直接这样 poll 了： Pin::new(\u0026amp;mut future).poll(ctx)。\n标准库也确实为 Pin 实现了 Future：\n1 2 3 4 impl\u0026lt;P\u0026gt; Future for Pin\u0026lt;P\u0026gt; where P: DerefMut, \u0026lt;P as Deref\u0026gt;::Target: Future, 我们来看对 P 的约束，P 可解引用为 Future，也就是说，P是 Future 的引用 \u0026amp;mut future，或者是智能指针 Box\u0026lt;dyn Future\u0026gt; 都可以满足约束。因为 Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt; 实现了 Future，我们可以用它作为 Service::call 返回值类型。\nBox 提供了 pin 方法，让用户构建 Pin\u0026lt;Box\u0026lt;T\u0026gt;\u0026gt;：\n1 pub fn pin(x: T) -\u0026gt; Pin\u0026lt;Box\u0026lt;T, Global\u0026gt;\u0026gt; 使用 Box::pin，RequestHanlder 可以这么修改：\n1 2 3 4 5 6 7 8 9 10 11 impl Service\u0026lt;HttpRequest\u0026gt; for RequestHandler { ... type Future = Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;Output = Result\u0026lt;Self::Response, Self::Error\u0026gt;\u0026gt;\u0026gt;\u0026gt;; fn call(\u0026amp;mut self, req: HttpRequest) -\u0026gt; Self::Future { Box::pin(async move { println!(\u0026#34;process url {:?}\u0026#34;, \u0026amp;req.url); Ok(HttpResponse { code: 200 }) }) } } 这回终于可以了。事实上，在异步场景下，我们经常会看到使用 Box::pin 去包装 async block。\n# Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt; Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt; 除了实现了Future，也实现了 Unpin。\n因为 Pin 实现了 Unpin，只要 P 是 Unpin 的：\n1 2 3 impl\u0026lt;P\u0026gt; Unpin for Pin\u0026lt;P\u0026gt; where P: Unpin 而 Box 正好是 Unpin 的：\n1 2 3 4 impl\u0026lt;T, A\u0026gt; Unpin for Box\u0026lt;T, A\u0026gt; where A: Allocator + \u0026#39;static, T: ?Sized, 因此 Pin\u0026lt;Box\u0026lt;T\u0026gt;\u0026gt;是 Unpin 的。可以这么理解，Pin 钉住了 T，但 Pin 本身是 Unpin的，可以安全的 move。\n很多异步方法需要你的 Future 同时实现了 Unpin ，例如tokio::select!()，而 async fn 返回的 Future 显然不满足 Unpin，这个时候仍然可以用 Box::pin把 Future pin 住，得到的Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;...\u0026gt;\u0026gt;\u0026gt; 同时实现了 Future 和 Unpin，满足你的要求。\n简单总结，在异步编程场景，我们经常会用Box::pin 包装 async block，获得同时实现了 Future 和 Unpin 的Pin\u0026lt;Box\u0026lt;dyn Future\u0026lt;\u0026gt;\u0026gt;\u0026gt;。\n","date":"2022-10-12T09:14:10+08:00","permalink":"https://mazhen.tech/p/pinboxdyn-future%E8%A7%A3%E6%9E%90/","title":"Pin\u003cBox\u003cdyn Future\u003c\u003e\u003e\u003e解析"},{"content":"Rust 通过 RFC conservative impl trait 增加了新的语法 impl Trait，它被用在函数返回值的位置上，表示返回的类型将实现这个 Trait。随后的 RFC expanding impl Trait 更进一步，允许 impl Trait 用在函数参数的位置，表示由调用者决定参数的具体类型，其实就等价于函数的泛型参数。\n# impl Trait 作为函数参数 根据 RFC on expanding impl Trait， impl Trait 可以用在函数参数中，作用是作为函数的匿名泛型参数。\nExpand impl Trait to allow use in arguments, where it behaves like an anonymous generic parameter.\n也就是说，impl Trait 作为函数参数，和泛型参数是等价的：\n1 2 3 // These two are equivalent fn map\u0026lt;U\u0026gt;(self, f: impl FnOnce(T) -\u0026gt; U) -\u0026gt; Option\u0026lt;U\u0026gt; fn map\u0026lt;U, F\u0026gt;(self, f: F) -\u0026gt; Option\u0026lt;U\u0026gt; where F: FnOnce(T) -\u0026gt; U 不过，impl Trait和泛型参数有一个不同的地方，impl Trait 作为参数，不能明确指定它的类型：\n1 2 3 4 5 fn foo\u0026lt;T: Trait\u0026gt;(t: T) fn bar(t: impl Trait) foo::\u0026lt;u32\u0026gt;(0) // this is allowed bar::\u0026lt;u32\u0026gt;(0) // this is not 除了这个差别，可以认为impl Trait 作为函数参数，和使用泛型参数是等价的。\n# impl Trait 作为函数返回值 impl Trait 作为函数的返回值，表示返回的类型将实现这个 Trait。\n1 2 3 4 5 6 7 8 fn foo(n: u32) -\u0026gt; impl Iterator\u0026lt;Item = u32\u0026gt; { (0..n).map(|x| x * 100) } fn main() { for x in foo(10) { println!(\u0026#34;{}\u0026#34;, x); } } 在这种情况下，需要注意函数的所有返回路径必须返回完全相同的具体类型。\n1 2 3 4 5 6 7 8 // 编译错误，即使这两个类型都实现了Bar fn f(a: bool) -\u0026gt; impl Bar { if a { Foo { ... } } else { Baz { ... } } } 可以把函数返回值位置的 impl Trait 替换为泛型吗？\n1 2 3 4 // 不能编译 fn bar\u0026lt;T: Iterator\u0026lt;Item = u32\u0026gt;\u0026gt;(n: u32) -\u0026gt; T { (0..n).map(|x| x * 100) } 编译器给的错误信息是，期待返回值的类型是泛型类型 T，却实际却返回了一个具体类型。编译器很智能的给出了使用 impl Iterator\u0026lt;Item = u32\u0026gt;作为返回类型的建议：\n1 2 3 4 5 6 7 8 9 10 11 12 --\u0026gt; src/main.rs:6:5 | 5 | fn bar\u0026lt;T: Iterator\u0026lt;Item = u32\u0026gt;\u0026gt;(n: u32) -\u0026gt; T { | - - | | | | | expected `T` because of return type | this type parameter help: consider using an impl return type: `impl Iterator\u0026lt;Item = u32\u0026gt;` 6 | (0..n).map(|x| x * 100) | ^^^^^^^^^^^^^^^^^^^^^^^ expected type parameter `T`, found struct `Map` | = note: expected type parameter `T` found struct `Map\u0026lt;std::ops::Range\u0026lt;u32\u0026gt;, [closure@src/main.rs:6:16: 6:27]\u0026gt;` # Universals vs. Existentials 在 RFC on expanding impl Trait 中使用了两个术语，Universal 和 Existential：\nUniversal quantification, i.e. \u0026ldquo;for any type T\u0026rdquo;, i.e. \u0026ldquo;caller chooses\u0026rdquo;. This is how generics work today. When you write fn foo\u0026lt;T\u0026gt;(t: T), you\u0026rsquo;re saying that the function will work for any choice of T, and leaving it to your caller to choose the T. Existential quantification, i.e. \u0026ldquo;for some type T\u0026rdquo;, i.e. \u0026ldquo;callee chooses\u0026rdquo;. This is how impl Trait works today (which is in return position only). When you write fn foo() -\u0026gt; impl Iterator, you\u0026rsquo;re saying that the function will produce some type T that implements Iterator, but the caller is not allowed to assume anything else about that type. 简单来说：\nimpl Trait 用在参数位置是 universal type，也就是泛型类型，它可以是任意类型，由函数的调用者指定具体的类型。\nimpl Trait 用在返回值位置是 existential type，它不能是任意类型，而是由函数的实现者指定，一个实现了 Trait 的具体类型。调用者不能对这个类型做任何假设。\n也就是说，impl Trait 用在返回位置不是泛型，编译时不需要单态化，抽象类型可以简单地替换为调用代码中的具体类型。\n# 在 Trait 中使用 impl Trait Rust 目前还不支持在 Trait 里使用 impl Trait 做返回值：\n1 2 3 4 5 trait Foo { // ERROR: `impl Trait` not allowed outside of function and inherent // method return types fn foo(\u0026amp;self) -\u0026gt; impl Iterator\u0026lt;Item=u8\u0026gt;; } 因为 impl Trait 用在返回值位置是 existential type，意味着这个函数将返回一个实现了这个 Trait 的单一类型，而函数定义在 Trait 中，意味着每个实现了 Trait 的类型，都可以让这个函数返回不同类型，对编译器来说这很难处理，因为它需要知道被返回类型的具体大小。\n一个简单的解决方法是让函数返回 trait object：\n1 2 3 trait Foo { fn foo(\u0026amp;self) -\u0026gt; Box\u0026lt;dyn Iterator\u0026lt;Item=u8\u0026gt;\u0026gt;; } 带有 trait object 的函数不是泛型函数，它只带有单一类型，这个类型就是 trait object 类型。Trait object 本身被实现为胖指针，其中，一个指针指向数据本身，另一个则指向虚函数表（vtable）。\n这样定义在 Trait 中的函数，返回的不再是泛型，而是一个单一的 trait object 类型，大小固定（两个指针大小），编译器可以处理。\n","date":"2022-10-08T17:58:36+08:00","permalink":"https://mazhen.tech/p/impl-trait-%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"impl Trait 的使用"},{"content":"Rust开发生态最重要的两个工具\nrustup Rust 官方的跨平台安装工具 Cargo Rust 构建系统和包管理器 它们的下载源都位于国外，为了改善在国内的使用体验，可以为它们配置国内源。\nrustup 国内源\n目前国内 Rust 工具链镜像源有清华大学源、中国科学技术大学源、上海交通大学源等，以清华大学源为例，设置环境变量：\n1 2 export RUSTUP_DIST_SERVER=https://mirrors.tuna.tsinghua.edu.cn/rustup export RUSTUP_UPDATE_ROOT=https://mirrors.tuna.tsinghua.edu.cn/rustup/rustup crates.io 国内源\nCargo 默认的源服务器为 crates.io，同样可以配置为国内的镜像源，以清华大学源为例，编辑 ~/.cargo/config 文件，添加以下内容：\n1 2 3 4 5 [source.crates-io] replace-with = \u0026#39;tuna\u0026#39; [source.tuna] registry = \u0026#34;https://mirrors.tuna.tsinghua.edu.cn/git/crates.io-index.git\u0026#34; 这样可加快 Cargo 读取软件包索引的速度。\n","date":"2022-09-30T22:36:25+08:00","permalink":"https://mazhen.tech/p/%E9%85%8D%E7%BD%AErustup%E5%92%8Ccargo%E5%9B%BD%E5%86%85%E6%BA%90/","title":"配置rustup和Cargo国内源"},{"content":"原文链接\nLinux 是内核，由Linus在90年代创造。不包括驱动程序，内核本身在编译时只有几兆字节。\nGNU是一个自由软件的大集合，可以和操作系统一起使用，包括你非常熟悉的 grep、sed、gcc等等。GNU 还包括 glibc，C 语言标准库的实现。\nGNU/Linux 是任何基于 GNU 集合的Linux发行版（内核+用户态应用）。Debian、Ubuntu、CentOS，甚至RHEL在技术上都是 GNU/Linux。因为有共同的C语言标准库和系统工具，通常你可以在 GNU/Linux 发行版之间跳来跳去，不会有什么麻烦。\nAlpine Linux 是另一个类型 Linux 发行版，它不是基于 GNU 集合。\n为了替代 GNU，Alpine 使用：\nBusyBox：一个小型软件套件 (~2MB)，在单个可执行文件中提供了多个 Unix 实用程序。 musl：一种现代且更强大的 C 标准库实现 因此，Alpine Linux 不是 Debian 或 CentOS 等 GNU/Linux 发行版的直接替代品，musl 的行为可能与 glibc 不同！\n","date":"2022-09-30T10:23:23+08:00","permalink":"https://mazhen.tech/p/linux-gnu/linux%E4%BB%A5%E5%8F%8Aalpine-linux/","title":"Linux, GNU/Linux以及Alpine Linux"},{"content":" # Future trait Rust 异步编程最核心的是 Future trait：\n1 2 3 4 5 6 7 8 pub trait Future { type Output; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt;; } pub enum Poll\u0026lt;T\u0026gt; { Ready(T), Pending, } Future 代表一个异步计算，它会产生一个值。通过调用 poll 方法来推进 Future 的运行，如果 Future 完成了，它将返回 Poll::Ready(result)，我们拿到运算结果。如果 Future 还不能完成，可能是因为需要等待其他资源，它返回 Poll::Pending。等条件具备，如资源已经准备好，这个 Future 将被唤醒，再次进入 poll，直到计算完成获得结果。\n# async/.await 如果产生一个 Future 呢，使用 async 是产生 Future 最方便的方法。使用 async 有两种方式：async fn 和 async blocks。每种方法都返回一个实现了Future trait 的匿名结构：\n1 2 3 4 5 6 7 8 9 10 // `foo()` returns a type that implements `Future\u0026lt;Output = u8\u0026gt;`. async fn foo() -\u0026gt; u8 { 5 } fn bar() -\u0026gt; impl Future\u0026lt;Output = u8\u0026gt; { // This `async` block results in a type that implements // `Future\u0026lt;Output = u8\u0026gt;`. async { 5 } } 这两种方式是等价的，都返回了 impl Future\u0026lt;Output = u8\u0026gt;。async 关键字相当于一个返回 impl Future\u0026lt;Output\u0026gt; 的语法糖。\n调用 async fn 并不会让函数执行，而是返回 impl Future\u0026lt;Output\u0026gt;，你只有在返回值上使用 .await，才能触发函数的实际执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 async fn say_world() { println!(\u0026#34;world\u0026#34;); } #[tokio::main] async fn main() { // Calling `say_world()` does not execute the body of `say_world()`. let op = say_world(); // This println! comes first println!(\u0026#34;hello\u0026#34;); // Calling `.await` on `op` starts executing `say_world`. op.await; } 上面的程序输出为：\n1 2 hello world 在 Future 上调用 await，相当于执行 Future::poll。如果 Future 被某些条件阻塞，它将放弃对当前线程的控制。当条件准备好后， Future会被唤醒恢复执行。\n简单总结，我们用async 生成 Future，用 await 来触发 Future 的执行。尽管其他语言也实现了async/.await，但 Rust 的 async 是 lazy 的，只有在主动 await 后才开始执行。\n我们当然也可以手工为数据结构实现 Future：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 struct Delay { when: Instant, } impl Future for Delay { type Output = \u0026amp;\u0026#39;static str; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;\u0026amp;\u0026#39;static str\u0026gt; { if Instant::now() \u0026gt;= self.when { println!(\u0026#34;Hello world\u0026#34;); Poll::Ready(\u0026#34;done\u0026#34;) } else { // Ignore this line for now. cx.waker().wake_by_ref(); Poll::Pending } } } 同样用 await 触发 Future 的实际执行：\n1 2 3 4 5 6 7 8 #[tokio::main] async fn main() { let when = Instant::now() + Duration::from_millis(10); let future = Delay { when }; let out = future.await; assert_eq!(out, \u0026#34;done\u0026#34;); } # Pin Future 被每个 await 分成多段，执行到 await 可能因为资源没准备好而让出 CPU 暂停执行，随后该 future 可能被调度到其他线程接着执行。所以 future 结构中需要保存跨await的数据，形成了自引用结构。\n自引用结构不能被移动，否则内部引用因为指向移动前的地址，引发不可控的问题。所以future需要被pin住，不能移动。\n如何让 future 不被move？ 方法调用时只传递引用，那么就没有移动 future。但是通过可变引用仍然可以使用 replace，swap 等方式移动数据。那么用 Pin 包装可变引用 Pin\u0026lt;\u0026amp;mut T\u0026gt;，让用户没法拿到 \u0026amp;mut T，就把这个漏洞堵上了。\n总之 Pin\u0026lt;\u0026amp;mut T\u0026gt; 不是数据的 owner，也没法获得 \u0026amp;mut T，所以就不能移动 T。\n注意，Pin 拿住的是一个可以解引用成 T 的指针类型 P，而不是直接拿原本的类型 T。Pin 本身是可 move 的，T 被 pin 住，不能 move。\n# async runtime的内部实现 要运行异步函数，必须将最外层的 Future 提交给 executor。 executor 负责调用Future::poll，推动异步计算的前进。\nexecutor 内部会有一个 Task 队列，executor 在 run 方法内，不停的从 receiver 获取 Task，然后执行。\nTask 包装了一个 future，同时内部持有一个 sender，用于将自身放回 executor 的 Task 队列。\nFuture 的 poll 方法，接收的是 Pin\u0026lt;\u0026amp;mut Self\u0026gt;，而不是 \u0026amp;mut Self。所以在向 executor 提交 Future 时，需要先 pin 住，然后才能用来初始化 Task：\n1 2 3 4 5 6 7 8 9 10 11 fn spawn\u0026lt;F\u0026gt;(future: F, sender: \u0026amp;channel::Sender\u0026lt;Arc\u0026lt;Task\u0026gt;\u0026gt;) where F: Future\u0026lt;Output = ()\u0026gt; + Send + \u0026#39;static, { let task = Arc::new(Task { future: Mutex::new(Box::pin(future)), executor: sender.clone(), }); let _ = sender.send(task); } 保存在 Task 字段中的 Future 是 Pin\u0026lt;Box\u0026lt;Future\u0026gt;\u0026gt;，保证了以后每次调用 poll 传入的是 Pin\u0026lt;\u0026amp;mut Self\u0026gt;。注意，Pin 是可以移动的，Task 也是可以移动的，只是 Future 不能移动。\n在执行 Future 时，如果遇到资源未准备好，需要让出 CPU，那么 Task 可以将自己放入 Reactor。Task 实现了 ArcWake trait，实际上放入 Reactor 的 Waker 就是 Task 的包装：\n1 2 3 4 5 6 7 8 9 10 11 12 fn poll(self: Arc\u0026lt;Self\u0026gt;) { // Get a waker referencing the task. let waker = task::waker(self.clone()); // Initialize the task context with the waker. let mut cx = Context::from_waker(\u0026amp;waker); // This will never block as only a single thread ever locks the future. let mut future = self.future.try_lock().unwrap(); // Poll the future let _ = future.as_mut().poll(\u0026amp;mut cx); } 当 Reactor 得到了满足条件的事件，它会调用 Waker.wake() 唤醒之前挂起的任务。Waker.wake 会调用 Task::wake_by_ref 方法，将 Task 放回 executor 的任务队列：\n1 2 3 4 5 impl ArcWake for Task { fn wake_by_ref(arc_self: \u0026amp;Arc\u0026lt;Self\u0026gt;) { let _ = arc_self.executor.send(arc_self.clone()); } } # Stream trait 对于 Iterator，可以不断调用其 next() 方法，获得新的值，直到 Iterator 返回 None。Iterator 是阻塞式返回数据的，每次调用 next()，必然独占 CPU 直到得到一个结果，而异步的 Stream 是非阻塞的，在等待的过程中会空出 CPU 做其他事情。\nStream::poll_next() 方法和 Future::poll() 类似, 除了它可以被重复调用，以便从 Stream 中接收多个值。然而，poll_next() 调用起来不方便，我们需要自己处理 Poll 状态。也就是说，await 语法糖只能应用在 Future 上，没法使用 stream.await 。所以，我们要想办法用 Future 包装 Stream，在 Future::poll() 中调用 Stream::poll_next()，这样就可以使用 await。 StreamExt 提供了 next() 方法，返回一个实现了 Future trait 的 Next 结构，这样，我们就可以直接通过 stream.next().await 来获取下一个值了。看一下 next() 方法以及 Next 结构的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 pub trait StreamExt: Stream { fn next(\u0026amp;mut self) -\u0026gt; Next\u0026lt;\u0026#39;_, Self\u0026gt; where Self: Unpin { assert_future::\u0026lt;Option\u0026lt;Self::Item\u0026gt;, _\u0026gt;(Next::new(self)) } } // next 返回的 Next 结构 pub struct Next\u0026lt;\u0026#39;a, St: ?Sized\u0026gt; { stream: \u0026amp;\u0026#39;a mut St, } // Next 实现了 Future，每次 poll() 实际上就是从 stream 中 poll_next() impl\u0026lt;St: ?Sized + Stream + Unpin\u0026gt; Future for Next\u0026lt;\u0026#39;_, St\u0026gt; { type Output = Option\u0026lt;St::Item\u0026gt;; fn poll(mut self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; { self.stream.poll_next_unpin(cx) } } 当手动实现一个 stream 时，它通常是通过合成 futures 和其他stream 来完成的。例如下面的例子，将 Lines 封装为 Stream，在 Stream::poll_next() 中利用了 Lines::poll_next_line()：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 #[pin_project] struct LineStream\u0026lt;R\u0026gt; { #[pin] lines: Lines\u0026lt;BufReader\u0026lt;R\u0026gt;\u0026gt;, } impl\u0026lt;R: AsyncRead\u0026gt; LineStream\u0026lt;R\u0026gt; { /// 从 BufReader 创建一个 LineStream pub fn new(reader: BufReader\u0026lt;R\u0026gt;) -\u0026gt; Self { Self { lines: reader.lines(), } } } /// 为 LineStream 实现 Stream trait impl\u0026lt;R: AsyncRead\u0026gt; Stream for LineStream\u0026lt;R\u0026gt; { type Item = std::io::Result\u0026lt;String\u0026gt;; fn poll_next(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Option\u0026lt;Self::Item\u0026gt;\u0026gt; { self.project() .lines .poll_next_line(cx) .map(Result::transpose) } } Stream 可以是 unpin 的，Future 也可以是 unpin 的，如果他们内部包含了其他 !Unpin 的 Stream 或 Future，只需要把他们用 pin 包装，外面的 Stream 和 Future 就可以是 unpin 的。\n一般我们使用的 Stream 都是 unpin 的，如果不是，就用 pin 把它变成 unpin 的。为啥我们用的都是 unpin 的？因为能 move 的 Stream 更加灵活，可以作为参数和返回值。\n# AsyncRead 和 AsyncWrite 所有同步的 Read / Write / Seek trait，前面加一个 Async，就构成了对应的异步 IO 接口。\nAsyncRead / AsyncWrite 的方法会返回一个实现了 Future 的 struct，这样我们才能使用 await ，将 future 提交到 async runtime，触发 future 的执行。例如 AsyncReadExt::read_to_end()方法，返回 ReadToEnd 结构，而 ReadToEnd 实现了 Future：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 pub trait AsyncReadExt: AsyncRead { ... fn read_to_end\u0026lt;\u0026#39;a\u0026gt;(\u0026amp;\u0026#39;a mut self, buf: \u0026amp;\u0026#39;a mut Vec\u0026lt;u8\u0026gt;) -\u0026gt; ReadToEnd\u0026lt;\u0026#39;a, Self\u0026gt; where Self: Unpin, { read_to_end(self, buf) } } pin_project! { #[derive(Debug)] #[must_use = \u0026#34;futures do nothing unless you `.await` or poll them\u0026#34;] pub struct ReadToEnd\u0026lt;\u0026#39;a, R: ?Sized\u0026gt; { reader: \u0026amp;\u0026#39;a mut R, buf: VecWithInitialized\u0026lt;\u0026amp;\u0026#39;a mut Vec\u0026lt;u8\u0026gt;\u0026gt;, // The number of bytes appended to buf. This can be less than buf.len() if // the buffer was not empty when the operation was started. read: usize, // Make this future `!Unpin` for compatibility with async trait methods. #[pin] _pin: PhantomPinned, } } impl\u0026lt;A\u0026gt; Future for ReadToEnd\u0026lt;\u0026#39;_, A\u0026gt; where A: AsyncRead + ?Sized + Unpin, { type Output = io::Result\u0026lt;usize\u0026gt;; fn poll(self: Pin\u0026lt;\u0026amp;mut Self\u0026gt;, cx: \u0026amp;mut Context\u0026lt;\u0026#39;_\u0026gt;) -\u0026gt; Poll\u0026lt;Self::Output\u0026gt; { let me = self.project(); read_to_end_internal(me.buf, Pin::new(*me.reader), me.read, cx) } } ","date":"2022-09-24T16:48:29+08:00","permalink":"https://mazhen.tech/p/rust-%E5%BC%82%E6%AD%A5%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/","title":"Rust 异步编程笔记"},{"content":" # 读写流程 Ledger 的元数据信息保存在zookeeper中。\n# 写入流程 当写入发生时，首先 entry 被写入一个 journal 文件。journal 是一个 write-ahead log（WAL），它帮助 BookKeeper 在发生故障时避免数据丢失。这与关系型数据库实现数据持久化的机制相同。\n同时 entry 会被加入到 write cache 中。write cache 中累积的 entry 会定期排序，异步刷盘到 entry log 文件中。同一个 ledger 的 entry 排序后会被放在一起，这样有利于提高读取的性能。\nwrite cache 中的 entry 也将被写入 RocksDB， RocksDB 记录了每个 entry 在 entry log 文件中的位置，是 (ledgerId, entryId) 到 (entry log file, offset) 的映射，即可以通过(ledgerId, entryId)，在 entry log 文件中定位到 entry。\n# 读取流程 读取时会首先查看 write cache ，因为 write cache 中有最新的 entry。如果 write cache 中没有，那么接着查看 read cache。如果 read cache 中还是没有，那么就通过 (ledgerId, entryId) 在 RocksDB 中查找到该 entry 所在的 entry log 文件及偏移量，然后从 entry log 文件中读取该 entry ，并更新到 read cache 中，以便后续请求能在 read cache 中命中。两层缓存让绝大多数的读取通常是从内存获取。\n# 读写隔离 BookKeeper 中的写入都是顺序写入 journal 文件，该文件可以存储在专用磁盘上，可以获得更大的吞吐量。write cache 中的 entry 会异步批量写入 entry log 文件和 RocksDB，通常配置在另外一块磁盘。因此，一个磁盘用于同步写入（ journal 文件），另一个磁盘用于异步优化写入，以及所有的读取。\n# 数据一致性 Bookie 操作基本都是在客户端完成和实现的，比如副本复制、读写 entry 等操作。这些有助于确保 BookKeeper 的一致性。\n客户端在创建 ledger 时，会出现 Ensemble、Write Quorum 和 Ack Quorum 这些数据指标。\nEnsemble —— 用哪几台 bookie 去存储 ledger 对应的 entry\nWrite Quorum ——对于一条 entry，需要存多少副本\nAck Quorum —— 在写 entry 时，要等几个 response\n我们会用（5,3,2）的实例进行讲述\n（5,3,2) 代表了对于一个 ledger ，会挑 5 台 bookie 去存储所有的 entry。所以当 entry 0 生成时，可以根据hash模型计算出应该放置到哪台 bookie。比如 E0 对应 B1，B2，B3，那 E1 就对应 B2，B3，B4，以此类推。\n虽然总体是需要 5 台 bookie，但是每条 entry 只会占用 3 台 bookie 去存放，并只需等待其中的 2 台 bookie 给出应答即可。\n# LastAddConfirm LAC（LastAddConfirm）是由 LAP（LastAddPush） 延伸而来是根据客户端进行维护并发布的最后一条 entry id，从 0 开始递增。所以 LAC 是由应答确认回来的最后一条 entry id 构成，如下图右侧显示。 LAC 以前的 entry ID （比它本身小的）都已确认过，它其实是一致性的边界，LAC 之前和之后的都是一致的。 同时 LAC 作为 bookie 的元数据，可以根据此来判断 entry 的确认与否。这样做的好处是，LAC 不会受限于一个集中的元数据管理，可以分散存储在存储节点。 # Ensemble change 当其中的某个 bookie 挂掉时，客户端会进行一个 ensemble change 的操作，用新的 bookie 替换挂掉的旧 bookie。比如 当bookie 4 挂掉时，可以使用 bookie 6 进行替换。 整个过程，只要有新的存储节点出现，就会保证不会中断读写操作是，即过程中随时补新。\nEnsemble change 对应到元数据存储，即对元数据的修改。之前的 E0-E6 是写在 B1～B5 上，E7 以后写在了 B1、B2、B3、B6、B5 上。这样就可以通过元数据的方式，看到数据到底存储在那个bookie上。 # Bookie Fencing BookKeeper 有一个极其重要的功能，叫做 fencing。fencing 功能让 BookKeeper 保证只有一个写入者（Pulsar broker）可以写入一个 ledger。\nBroker（B1）挂掉，Broker（B2）接管 B1 上topic X的流程：\n当前拥有 topic X 所有权的Pulsar Broker（B1）被认为已经挂掉或不可用（通过ZooKeeper）。\n另一个Broker（B2）将topic X 的当前 ledger 的状态从 OPEN 更新为 IN_RECOVERY。\nB2 向当前 ledger 的所有 bookie 发送 fencing LAC read 请求，并等待(Write Quorum - Ack Quorum)+1的响应。一旦收到这个数量的回应，ledger 就已经被 fencing。B1就算还活着，也不能再写入这个ledger，因为B1无法得到 Ack Quorum 的确认。\nB2采取最高的LAC响应，然后开始执行从 LAC+1 的恢复性读取。它确保从该点开始的所有 entry 都被复制到 Write Quorum 个bookie。当 B2 不能再读取和复制任何entry，ledger 完成恢复。\nB2将 ledger 的状态改为 CLOSED。\nB2打开一个新的 ledger，现在可以接受对Topic X的写入。\n整个失效恢复的过程，是没有回头复用 ledger 的。因为复用意味着所有元素都处于相同状态且都同意后才能继续去读写，这个是很难控制的。\n我们从主从复制方式进行切入，将其定义为物理文件。数据从主复制到从，由于复制过程的速度差异，为了保证所有的一致性，需要做一些「删除/清空类」的操作。但是这个过程中一旦包含覆盖的操作，就会在过程中更改文件状态，容易出现 bug。\nBookKeeper 在运行的过程中，不是一个物理文件，而是逻辑上的序。同时在失效恢复过程中，没有进行任何的复用，使得数据恢复变得简单又清晰。其次它在整个修复过程中，没有去额外动用 ledger X 的数据。\n# 自动恢复 当一个 bookie 失败时，这个 bookie 上所有 ledger 的 fragments 都将被复制到其他节点，以确保每个 ledger 的复制系数（Write quorum）得到保证。\nrecovery 不是 BookKeeper 复制协议的一部分，而是在 BookKeeper 外部运行，作为一种异步修复机制使用。\n有两种类型的recovery：手动或自动。\n自动恢复进程 AutoRecoveryMain 可以在独立的服务器上运行，也可以和bookie跑在一起。其中一个自动恢复进程被选为审核员，然后进行如下操作：\n从 zookeeper 读取所有的 ledger 列表，并找到位于失败 bookie上的 ledger。\n对于第一步找到的所有ledger，在ZooKeeper上创建一个复制任务。\nAutoRecoveryMain 进程会发现 ZooKeeper 上的复制任务，锁定任务，进行复制，满足Write quorum，最后删除任务。\n通过自动恢复，Pulsar集群能够在面对存储层故障时进行自我修复，只需确保部署了适量的bookie就可以了。\n如果审计节点失败，那么另一个节点就会被提升为审计员。\n","date":"2022-09-17T16:42:59+08:00","permalink":"https://mazhen.tech/p/bookkeeper%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/","title":"BookKeeper实现分析"},{"content":" Previous Next \u0026nbsp; \u0026nbsp; Page: / ","date":"2022-08-22T16:24:52+08:00","permalink":"https://mazhen.tech/p/%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF/","title":"深入浅出容器技术"},{"content":"GCC 会为不同 CPU 架构预定义宏，如 __x86_64__ 代表Intel 64位CPU， __aarch64__代表 ARM64。 网上已经有文档对 GCC 为 CPU 的预定义宏进行了总结。\n这些预定义的宏有什么用呢？我们在代码中可以判断出当前的 CPU 架构，那么可以针对 不同CPU的特性，进行优化实现。例如RocksDB 对于获取当前时间，在 x86 平台上，会用到 Time Stamp Counter (TSC) 寄存器，使用 RDTSC 指令提取 TSC 中值。对于 ARM 64 也有类似的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Get the value of tokutime for right now. We want this to be fast, so we // expose the implementation as RDTSC. static inline tokutime_t toku_time_now(void) { #if defined(__x86_64__) || defined(__i386__) uint32_t lo, hi; __asm__ __volatile__(\u0026#34;rdtsc\u0026#34; : \u0026#34;=a\u0026#34;(lo), \u0026#34;=d\u0026#34;(hi)); return (uint64_t)hi \u0026lt;\u0026lt; 32 | lo; #elif defined(__aarch64__) uint64_t result; __asm __volatile__(\u0026#34;mrs %[rt], cntvct_el0\u0026#34; : [ rt ] \u0026#34;=r\u0026#34;(result)); return result; #elif defined(__powerpc__) return __ppc_get_timebase(); #elif defined(__s390x__) uint64_t result; asm volatile(\u0026#34;stckf %0\u0026#34; : \u0026#34;=Q\u0026#34;(result) : : \u0026#34;cc\u0026#34;); return result; #else #error No timer implementation for this platform #endif } 而在将 RocksDB 移植到龙芯的过程中，需要修改上面的代码，判断出当前是龙芯 loongarch64 架构。\n网上没有搜到 GCC 对龙芯 CPU 的预定宏的文档说明，只能从源码中找答案：\n1 2 3 4 5 6 7 void loongarch_cpu_cpp_builtins (cpp_reader *pfile) { ... builtin_define (\u0026#34;__loongarch__\u0026#34;); ... } 可以看到，__loongarch__代表龙芯CPU。 在暂时不知道龙芯是否支持RDTSC的情况下，只能给出通用的实现，以后再查龙芯的CPU手册进行优化。\n1 2 3 4 5 6 7 8 9 10 11 12 13 #if defined(__x86_64__) || defined(__i386__) ... #elif defined(__aarch64__) ... #elif defined(__powerpc__) ... #elif defined(__loongarch__) struct timeval tv; gettimeofday(\u0026amp;tv,NULL); return tv.tv_sec*(uint64_t)1000000+tv.tv_usec; #else #error No implementation for this platform #endif ","date":"2022-06-17T16:19:54+08:00","permalink":"https://mazhen.tech/p/gcc-%E4%B8%BA%E9%BE%99%E8%8A%AF-cpu%E7%9A%84%E9%A2%84%E5%AE%9A%E4%B9%89%E5%AE%8F/","title":"GCC 为龙芯 CPU的预定义宏"},{"content":"我们经常会使用 kill 命令杀掉运行中的进程，对多次杀不死的进程进一步用 kill -9 干掉它。你可能知道这是在用 kill 命令向进程发送信号，优雅或粗暴的让进程退出。我们能向进程发送很多类型的信号，其中一些常见的信号 SIGINT 、SIGQUIT、 SIGTERM 和 SIGKILL 都是通知进程退出，但它们有什么区别呢？很多人经常把它们搞混，这篇文章会让你了解 Linux 的信号机制，以及一些常见信号的作用。\n# 什么是信号 信号（Signal）是 Linux 进程收到的一个通知。当进程收到一个信号时，该进程会中断其执行，并执行收到信号对应的处理程序。\n信号机制作为 Linux 进程间通信的一种方法。Linux 进程间通信常用的方法还有管道、消息、共享内存等。\n信号的产生有多种来源：\n硬件来源，例如 CPU 内存访问出错，当前进程会收到信号 SIGSEGV；按下 Ctrl+C 键，当前运行的进程会收到信号 SIGINT 而退出； 软件来源，例如用户通过命令 kill [pid]，直接向一个进程发送信号。进程使用系统调用 int kill(pid_t pid, int sig) 显示的向另一个进程发送信号。内核在某些情况下，也会给进程发送信号，例如当子进程退出时，内核给父进程发送 SIGCHLD 信号。 你可以使用 kill -l 命令查看系统实现了哪些信号：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ kill -l 1) SIGHUP\t2) SIGINT\t3) SIGQUIT\t4) SIGILL\t5) SIGTRAP 6) SIGABRT\t7) SIGBUS\t8) SIGFPE\t9) SIGKILL\t10) SIGUSR1 11) SIGSEGV\t12) SIGUSR2\t13) SIGPIPE\t14) SIGALRM\t15) SIGTERM 16) SIGSTKFLT\t17) SIGCHLD\t18) SIGCONT\t19) SIGSTOP\t20) SIGTSTP 21) SIGTTIN\t22) SIGTTOU\t23) SIGURG\t24) SIGXCPU\t25) SIGXFSZ 26) SIGVTALRM\t27) SIGPROF\t28) SIGWINCH\t29) SIGIO\t30) SIGPWR 31) SIGSYS\t34) SIGRTMIN\t35) SIGRTMIN+1\t36) SIGRTMIN+2\t37) SIGRTMIN+3 38) SIGRTMIN+4\t39) SIGRTMIN+5\t40) SIGRTMIN+6\t41) SIGRTMIN+7\t42) SIGRTMIN+8 43) SIGRTMIN+9\t44) SIGRTMIN+10\t45) SIGRTMIN+11\t46) SIGRTMIN+12\t47) SIGRTMIN+13 48) SIGRTMIN+14\t49) SIGRTMIN+15\t50) SIGRTMAX-14\t51) SIGRTMAX-13\t52) SIGRTMAX-12 53) SIGRTMAX-11\t54) SIGRTMAX-10\t55) SIGRTMAX-9\t56) SIGRTMAX-8\t57) SIGRTMAX-7 58) SIGRTMAX-6\t59) SIGRTMAX-5\t60) SIGRTMAX-4\t61) SIGRTMAX-3\t62) SIGRTMAX-2 63) SIGRTMAX-1\t64) SIGRTMAX 使用 man 7 signal 命令查看系统对每个信号作用的描述：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 Signal Standard Action Comment ──────────────────────────────────────────────────────────────────────── SIGABRT P1990 Core Abort signal from abort(3) SIGALRM P1990 Term Timer signal from alarm(2) SIGBUS P2001 Core Bus error (bad memory access) SIGCHLD P1990 Ign Child stopped or terminated SIGCLD - Ign A synonym for SIGCHLD SIGCONT P1990 Cont Continue if stopped SIGEMT - Term Emulator trap SIGFPE P1990 Core Floating-point exception SIGHUP P1990 Term Hangup detected on controlling terminal or death of controlling process SIGILL P1990 Core Illegal Instruction ... # 信号和中断 信号处理是一种典型的异步事件处理方式：进程需要提前向内核注册信号处理函数，当某个信号到来时，内核会就执行相应的信号处理函数。\n我们知道，硬件中断也是一种内核的异步事件处理方式。当外部设备出现一个必须由 CPU 处理的事件，如键盘敲击、数据到达网卡等，内核会收到中断通知，暂时打断当前程序的执行，跳转到该中断类型对应的中断处理程序。中断处理程序是由 BIOS 和操作系统在系统启动过程中预先注册在内核中的。\n中断和信号通知都是在内核产生。中断是完全在内核里完成处理，而信号的处理则是在用户态完成的。也就是说，内核只是将信号保存在进程相关的数据结构里面，在执行信号处理程序之前，需要从内核态切换到用户态，执行完信号处理程序之后，又回到内核态，再恢复进程正常的运行。\n可以看出，中断和信号的严重程度不一样。信号影响的是一个进程，信号处理出了问题，最多是这个进程被干掉。而中断影响的是整个系统，一旦中断处理程序出了问题，可能整个系统都会挂掉。\n# 信号处理 一旦有信号产生，进程对它的处理都有下面三个选择。\n执行缺省操作（Default）。Linux 为每个信号都定义了一个缺省的行为。例如，信号 SIGKILL 的缺省操作是 Term，也就是终止进程的意思。信号 SIGQUIT 的缺省操作是 Core，即终止进程后，通过 Core Dump 将当前进程的运行状态保存在文件里面。 捕捉信号（Catch）。这个是指让用户进程可以注册自己针对这个信号的处理函数。当信号发生时，就执行我们注册的信号处理函数。 忽略信号（Ignore）。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。 有两个信号例外，对于 SIGKILL 和 SIGSTOP 这个两个信号，进程是无法捕捉和忽略，它们用于在任何时候中断或结束某一进程。SIGKILL 和 SIGSTOP 为内核和超级用户提供了删除任意进程的特权。\n如果我们不想让信号执行缺省操作，可以对特定的信号注册信号处理函数：\n1 2 3 4 5 6 #include \u0026lt;signal.h\u0026gt; typedef void (*sighandler_t)(int); sighandler_t signal(int signum, sighandler_t handler); 例如下面的例子，程序捕获了信号 SIGINT ，并且只是输出不做其他处理，这样在键盘上按 Ctrl+C 并不能让程序退出：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; void sig_handler(int signo) { if (signo == SIGINT) { printf(\u0026#34;received SIGINT\\n\u0026#34;); } } int main(int argc, char *argv[]) { signal(SIGINT, sig_handler); printf(\u0026#34;Process is sleeping\\n\u0026#34;); while (1) { sleep(1000); } return 0; } 通过 signal 注册的信号处理函数，会保存在进程内核的数据结构 task_struct 中。由于信号都发给进程，并由进程在用户态处理，所以发送给进程的信号也保存在 task_struct 中。\ntask_struct-\u0026gt;sighand 和 task_struct-\u0026gt;signal 是线程组内共享，而 task_struct-\u0026gt;pending 是线程私有的。\nstask_struct-\u0026gt;sighand 里面有一个 action，这是一个数组，下标是信号，数组内容就是注册的信号处理函数。\ntask_struct-\u0026gt;pending 内包含了一个链表，保存了本线程所有的待处理信号。task_struct-\u0026gt;signal-\u0026gt;shared_pending 上也有一个待处理信号链表，这个链表保存的是线程组内共享的信号。\n# 常见信号 下面的列表列举了一些常见的信号。\nSingal Value Action comment key binding SIGHUP 1 Term Hangup detected on controlling terminal or death of controlling process SIGINT 2 Term Interrupt from keyboard Ctrl-c SIGQUIT 3 Core Quit from keyboard Ctrl-\\ SIGKILL 9 Term Kill signal SIGSEGV 11 Core Invalid memory reference SIGPIPE 13 Term Broken pipe: write to pipe with no readers SIGTERM 15 Term Termination signal SIGCHLD 17 Ign Child stopped or terminated SIGCONT 18 Cont Continue if stopped SIGSTOP 19 Stop Stop process SIGTSTP 20 Stop Stop typed at terminal Ctrl-z SIGTTIN 21 Stop Terminal input for background process SIGTTOU 22 Stop Terminal output for background process 第一列是信号名称，第二列是信号编号。使用 kill 向进程发送信号时，用信号名称和编号都可以，例如：\n1 2 kill -1 [pid] kill -SIGHUP [pid] Action 列是信号的缺省行为，主要有如下几个：\nTerm 终止进程 Core 终止进程并core dump Ign 忽略信号 Stop 停止进程 Cont 如果进程是已停止，则恢复进程执行 有一些信号在 TTY 终端做了键盘按键绑定，例如 CTRL+c 会向终端上运行的前台进程发送 SIGINT 信号。\n# SIGHUP 运行在终端中，由 bash 启动的进程，都是 bash 的子进程。终端退出结束时会向 bash 的每一个子进程发送 SIGHUP 信号。由于 SIGHUP 的缺省行为是 Term，因此，即使运行在后台的进程也会和终端一起结束。\n使用 nohup 命令可解决这个问题，它的作用是让进程忽略 SIGHUP 信号：\n1 $ nohup command \u0026gt;cmd.log 2\u0026gt;\u0026amp;1 \u0026amp; 这样，即使我们退出了终端，运行在后台的程序会忽视 SIGHUP 信号而继续运行。由于作为父进程的 bash 进程已经结束，因此 /sbin/init 就成为孤儿进程新的父进程。\n# SIGINT, SIGQUIT, SIGTERM 和 SIGKILL SIGTERM 和 SIGKILL 是通用的终止进程请求，SIGINT 和 SIGQUIT 是专门用于来自终端的终止进程请求。他们的关键不同点是：SIGINT 和 SIGQUIT 可以是用户在终端使用快捷键生成的，而 SIGTERM 和 SIGKILL 必须由另一个程序以某种方式生成（例如通过 kill 命令）。\n当用户按下 ctrl-c 时，终端将发送 SIGINT 到前台进程。 SIGINT 的缺省行为是终止进程（Term），但它可以被捕获或忽略。 信号 SIGINT 的目的是为进程提供一种有序、优雅的关闭机制。\n当用户按下 ctrl-\\ 时，终端将发送 SIGQUIT 到前台进程。 SIGQUIT 的缺省行为是终止进程并 core dump，它同样可以被捕获或忽略。 你可以认为 SIGINT 是用户发起的愉快的终止，而 SIGQUIT 是用户发起的不愉快终止，需要生成 Core Dump ，方便用户事后进行分析问题在哪里。\n在 ubuntu 上由 systemd-coredump 系统服务处理 core dump。我们可以使用 coredumpctl 命令行工具查询和处理 core dump 文件。\n1 2 3 $ coredumpctl list TIME PID UID GID SIG COREFILE EXE SIZE Tue 2022-04-12 22:09:52 CST 6754 1000 1000 SIGQUIT present /usr/bin/cat 17.1K core dump 文件缺省保存在 /var/lib/systemd/coredump 目录下。\nSIGTERM 默认行为是终止进程，但它也可以被捕获或忽略。SIGTERM 的目的是杀死进程，它允许进程有机会在终止前进行清理，优雅的退出。当我们使用 kill 命令时，SIGTERM 是默认信号。\nSIGKILL 唯一的行为是立即终止进程。 由于 SIGKILL 是特权信号，进程无法捕获和忽略，因此进程在收到该信号后无法进行清理，立刻退出。\n例如 docker 在停止容器的时候，先给容器里的1号进程发送 SIGTERM，如果不起作用，那么等待30秒后会会发送 SIGKILL，保证容器最终会被停止。\n# SIGSTOP 、 SIGTSTP 和 SIGCONT SIGSTOP 和 SIGTSTP 这两个信号都是为了暂停一个进程，但 SIGSTOP 是特权信息，不能被捕获或忽略。\nSIGSTOP 必须由另一个程序以某种方式生成（例如：kill -SIGSTOP pid），而SIGTSTP 也可以由用户在键盘上键入快捷键 Ctrl-z 生成。\n被暂停的进程通过信号 SIGCONT 恢复。当用户调用 fg 命令时，SIGCONT 由 shell 显式发送给被暂停的进程。\nLinux 使用他们进行作业控制，让你能够手动干预和停止正在运行的应用程序，并在未来某个时间恢复程序的执行。\n# SIGTTOU 和 SIGTTIN Linux 系统中可以有多个会话（session），每个会话可以包含多个进程组，每个进程组可以包含多个进程。\n会话是用户登录系统到退出的所有活动，从登录到结束前创建的所有进程都属于这次会话。会话有一个前台进程组，还可以有一个或多个后台进程组。只有前台进程可以从终端接收输入，也只有前台进程才被允许向终端输出。如果一个后台作业中的进程试图进行终端读写操作，终端会向整个作业发送 SIGTTOU 或 SIGTTIN 信号，默认的行为是暂停进程。\n# JVM 对信号的处理 如果你使用 strace 追踪 Java 应用，发现 Java 程序会抛出大量 SIGSEGV。\n1 2 3 4 5 6 7 8 9 10 11 $ strace -fe \u0026#39;trace=!all\u0026#39; java [app] ... [pid 21746] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061680} --- [pid 21872] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061480} --- [pid 21943] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061500} --- [pid 21844] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061780} --- [pid 21728] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061c00} --- [pid 21906] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061980} --- [pid 21738] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061100} --- [pid 21729] --- SIGSEGV {si_signo=SIGSEGV, si_code=SEGV_ACCERR, si_addr=0x7f9cbc061e00} --- ... SIGSEGV 信号的意思是 “分段错误”（segmentation fault），是当系统检测到进程试图访问不属于它的内存地址时，内核向进程发送的信号。SIGSEGV 对于一般应用来说是很严重的错误，但 Java 进程中的 SIGSEGV 几乎总是正常和安全的。\n在常规的 C/C++ 程序中，当你期望指针是指向某个结构，但实际指向的是 NULL，会导致应用程序崩溃。这种崩溃实际上是内核向进程发送了信号 SIGSEGV。如果应用程序没有为该信号注册信号处理程序，则信号会返回到内核，然后内核会终止应用。实际上 JVM 为 SIGSEGV 注册了一个信号处理程序，因为 JVM 想使用 SIGSEGV 和其他一些信号来实现自己的目的。\n这篇文档 描述了 JVM 对信号的特殊处理：\nSignal Description SIGSEGV, SIGBUS, SIGFPE, SIGPIPE, SIGILL These signals are used in the implementation for implicit null check, and so forth. SIGQUIT This signal is used to dump Java stack traces to the standard error stream. (Optional) SIGTERM, SIGINT, SIGHUP These signals are used to support the shutdown hook mechanism (java.lang.Runtime.addShutdownHook) when the VM is terminated abnormally. (Optional) SIGUSR1 This signal is used in the implementation of the java.lang.Thread.interrupt method. Not used starting with Oracle Solaris 10 reserved on Linux. (Configurable) SIGUSR2 This signal is used internally. Not used starting with Oracle Solaris 10 operating system. (Configurable) SIGABRT The HotSpot VM does not handle this signal. Instead it calls the abort function after fatal error handling. If an application uses this signal then it should terminate the process to preserve the expected semantics. 实际上，JVM 是使用 SIGSEGV、SIGBUS、SIGPIPE 等进行代码中的各种 NULL 检查。\n同样，我们在终端上键入 ctrl-\\，也不会让前台运行的 Java 进程终止并 core dump，而是会将 Java 进程的 stack traces 输出到终端的标准错误流。\n那么如何对 Java 进程进行 core dump 呢？需要在 Java 的启动命令里增加 JVM 选项 -Xrs ，它会让 JVM 不自己处理 SIGQUIT 信号，这样 SIGQUIT 会触发缺省行为 core dump。\n一般 Java 进程的运行时内存占用都比较大，在进行 core dump 时很容易超过缺省大小而被truncated，因此需要修改配置文件 /etc/systemd/coredump.conf，合理设置 ProcessSizeMax 和 ExternalSizeMax 的大小。\n","date":"2022-04-13T16:17:35+08:00","permalink":"https://mazhen.tech/p/linux-%E4%BF%A1%E5%8F%B7signal/","title":"Linux 信号(Signal)"},{"content":"长时间运行的Linux服务器，通常 free 的内存越来越少，让人觉得 Linux 特别能“吃”内存，甚至有人专门做了个网站 LinuxAteMyRam.com解释这个现象。实际上 Linux 内核会尽可能的对访问过的文件进行缓存，来弥补磁盘和内存之间巨大的延迟差距。缓存文件内容的内存就是 Page Cache。\nGoogle 的大神 Jeffrey Dean总结过一个Latency numbers every programmer should know，其中提到从磁盘读取 1MB 数据的耗时是内存的80倍，即使换成 SSD 也是内存延迟的 4 倍。\n我在本机做了实验，来体会一下 Page Cache 的作用。首先生成一个 1G 大小的文件：\n1 # dd if=/dev/zero of=/root/dd.out bs=4096 count=262144 清空 Page Cache：\n1 # sync \u0026amp;\u0026amp; echo 3 \u0026gt; /proc/sys/vm/drop_caches 统计第一次读取文件的耗时：\n1 2 3 4 5 # time cat /root/dd.out \u0026amp;\u0026gt; /dev/null real\t0m2.097s user\t0m0.010s sys\t0m0.638s 再此读取同一个文件，由于系统已经将读取过的文件内容放入了 Page Cache ，这次耗时大大缩短：\n1 2 3 4 5 # time cat /root/dd.out \u0026amp;\u0026gt; /dev/null real\t0m0.186s user\t0m0.004s sys\t0m0.182s Page Cache 不仅能加速对文件内容的访问，对共享库建立 Page Cache，可以在多个进程间共享，避免每个进程都单独加载，造成宝贵内存资源的浪费。\n# Page Cache 是什么 Page Cache 是由内核管理的内存，位于 VFS(Virtual File System) 层和具体文件系统层（例如ext4，ext3）之间。应用进程使用 read/write 等文件操作，通过系统调用进入到 VFS 层，根据 O_DIRECT 标志，可以使用 Page Cache 作为文件内容的缓存，也可以跳过 Page Cache 不使用内核提供的缓存功能。\n另外，应用程序可以使用 mmap ，将文件内容映射到进程的虚拟地址空间，可以像读写内存一样直接读写硬盘上的文件。进程的虚拟内存直接和 Page Cache 映射。\n为了了解内核是怎么管理 Page Cache 的，我们先看一下 VFS 的几个核心对象：\nfile 存放已打开的文件信息，是进程访问文件的接口； dentry 使用 dentry 将文件组织成目录树结构； inode 唯一标识文件系统中的文件。对于同一个文件，内核中只会有一个 inode 结构。 对于每一个进程，打开的文件都有一个文件描述符，内核中进程数据结构 task_struct 中有一个类型为 files_struct 的 files 字段，保存着该进程打开的所有文件。files_struct 结构的fd_array 字段是 file 数组， 数组的下标是文件描述符，内容指向一个 file 结构，表示该进程打开的文件。file 与打开文件的进程相关联，如果多个进程打开同一个文件，那么每个进程都有自己的 file ，但这些 file 指向同一个 inode。\n如上图所示，进程通过文件描述符与 VFS 中的 file 产生联系， 每个 file 对象又与一个 dentry 对应，根据 dentry 能找到 inode，而 inode 则代表文件本身。上图中进程 A 和进程 B 打开了同一个文件，进程 A 和进程 B 都维护着各自的 file ，但它们指向同一个 inode。\ninode 通过 address_space 管理着文件已加载到内存中的内容，也就是 Page Cache。address_space 的字段 i_pages 指向一棵 xarray 树，与这个文件相关的 Page Cache 页都挂在这颗树上。我们在访问文件内容的时候，根据指定文件和相应的页偏移量，就可以通过 xarray 树快速判断该页是否已经在 Page Cache 中。如果该页存在，说明文件内容已经被读取到了内存，也就是存在于 Page Cache 中；如果该页不存在，就说明内容不在 Page Cache 中，需要从磁盘中去读取。\n由于文件和 inode 一一对应，我们可以认为 inode 是 Page Cache 的宿主（host），内核通过 inode-\u0026gt;imapping-\u0026gt;i_pages 指向的树，管理维护着 Page Cache。\nPage Cache 是如何产生和释放，又是如何与进程相关联的呢？我们需要先了解进程虚拟地址空间。\n# 进程虚拟地址空间 Linux 是多任务系统，它支持多个进程的的并发执行。操作系统和 CPU 联手制造了一个假象：每个进程都独享连续的虚拟内存空间，并且各个进程的地址空间是完全隔离的，因此进程并不会意识到彼此的存在。从进程的角度来看，它会认为自己是系统中唯一的进程。\n进程看到的是虚拟内存的地址空间，它也不能直接访问物理地址。当进程访问某个虚拟地址的时候，该虚拟地址由内核负责转换成物理内存地址，即完成虚拟地址到物理地址的映射。这样不同进程在运行的时候，即使访问相同的虚拟地址，但内核会将它们映射到不同的物理地址，因此不会发生冲突。\n进程在 Linux 内核由 task_struct 所描述。估计 task_struct 是你学习内核时第一个熟悉的数据结构，因为它实在太重要了。 task_struct描述了进程相关的所有信息，包括进程状态，运行时统计信息，进程亲缘关系，进度调度信息，信号处理，进程内存管理，进程打开的文件等等。我们这里关注的进程虚拟内存空间，是由 task_struct 中的 mm 字段指向的 mm_struct 所描述，它是一个进程内存的运行时摘要信息。\n进程的虚拟地址是线性的，使用结构体 vm_area_struct 来描述。内核将每一段具有相同属性的内存区域当作一个 vm_area_struct 进行管理，每个 vm_area_struct 是一个连续的虚拟地址范围，这些区域不会互相重叠。 mm_struct 里面有一个单链表 mmap，用于将 vm_area_struct 串联起来，另外还有一颗红黑树 mm_rb ，vm_area_struct 根据起始地址挂在这颗树上。使用红黑树可以根据地址，快速查找一个内存区域。\nvm_area_struct 可以直接映射到物理内存，也可以关联文件。如果 vm_area_struct 是文件映射，由成员 vm_file 指向对应的文件指针。一个没有关联文件的 vm_area_struct 是匿名内存。\n开发者使用 malloc 等 glibc 库函数分配内存的时候，不是直接分配物理内存，而是在进程的虚拟内存空间中申请一段虚拟内存，生成相应的数据结构 vm_area_struct ，然后将它插进 mm_struct 的链表 mmap，同时挂在红黑树 mm_rb 上，就算完成了工作，根本没有涉及到物理内存的分配。只有当第一次对这块虚拟内存进行读写时，发现该内存区域没有映射到物理内存，这时会触发缺页中断，然后由内核填写页表，完成虚拟内存到物理内存的映射。\n当开发者使用 mmap 进行文件映射时，内核根据 vm_area_struct 中代表文件映射关系 vm_file，将文件内容从磁盘加载到物理内存，也就是 Page Cache 中，最后建立这段虚拟地址到物理地址的映射。\n另外，在虚拟内存中连续的页面，在物理内存中不必是连续的。只要维护好从虚拟内存页到物理内存页的映射关系，你就能正确地使用内存。由于每个进程都有独立的地址空间，为了完成虚拟地址到物理地址的映射，每个进程都要有独立的进程页表。在一个实际的进程里面，虚拟内存占用的地址空间，通常是两段连续的空间，而不是完全散落的随机的内存地址。基于这个特点，内核使用多级页表保存映射关系，可以大大减少页表本身的空间占用。最顶级的页表保存在 mm_struct 的 pgd 字段中。\n好了，我们对进程虚拟地址空间有了基本的了解，下面看看 Page Cache 的产生和释放，以及如何与进程空间发生联系的。\n# Page Cache 的产生和释放 Page Cache 的产生有两种不同的方式：\nBuffered I/O Memory-Mapped file 使用这两种方式访问磁盘上的文件时，内核会根据指定的文件和相应的页偏移量，判断文件内容是否已经在 Page Cache 中，如果内容不存在，需要从磁盘中去读取并创建 Page Cache 页。\n这两种方式的不同之处在于，使用 Buffered I/O，要先将数据从 Page Cache 拷贝到用户缓冲区，应用才能从用户缓冲区读取数据。而对于 Memory-Mapped file 而言，则是直接将 Page Cache 页映射到进程虚拟地址空间，用户可以直接读写 Page Cache 中的内容。由于少了一次 copy，使用 Memory-Mapped file 要比 Buffered I/O 的效率高一些。\n随着服务器运行时间的增加，系统中空闲内存会越来越少，其中很大一部分都被 Page Cache 占用。访问过的文件都被 Page Cache 缓存，内存最终会被耗尽，那什么时候回收 Page Cache 呢？ 内核认为，Page Cache 是可回收内存，当应用在申请内存时，如果没有足够的空闲内存，就会先回收 Page Cache，再尝试申请。回收的方式主要是两种：直接回收和后台回收。\n使用 Buffered I/O 时，Page Cache 并没有和进程的虚拟内存空间产生直接的关联，而是通过用户缓冲区作为中转。效率更好的Memory-Mapped file方式，看着比较简单，但背后的实现却有些复杂。下面我们看一下内核是如何实现 Memory-Mapped file 的。\n# 内存文件映射 前面我们介绍过， inode 是 Page Cache 的宿主（host），内核通过 inode-\u0026gt;imapping-\u0026gt;i_pages 指向的树，管理维护着 Page Cache。那么内核是如何完成内存文件映射，直接把缓存了文件内容的 Page Cache 映射到进程虚拟内存空间的呢？\n我们知道，进程结构体 task_struct 中的字段 mm 指向该进程的虚拟地址空间 mm_struct ，而一段虚拟内存由结构体 vm_area_struct 所描述，将 vm_area_struct 串在一起的链表 mmap 就代表了已经申请分配的虚拟内存。\n如果是进行内存文件映射，那么映射了文件的虚拟内存区域 vm_area_struct ，它的 vm_file 会指向被映射的文件结构体 file。file 表示进程打开的文件，它的成员 f_mapping 指向 address_space，这样就和管理文件着 Page Cache 的 address_space 关联了起来。\n当第一次访问文件映射的虚拟内存区域时，这段虚拟内存并没有映射到物理内存，这时会触发缺页中断。内核在处理缺页中断时，发现代表这段虚拟内存的 vm_area_struct 有关联的文件，即 vm_file 字段指向一个文件结构体 file。内核拿到该文件的 address_space，根据要访问内容的页偏移量，对 address_space-\u0026gt;i_pages 指向的 xarray 树进行查找。这颗树上挂的都是页偏移量对应的内存页，如果没找到，就说明文件内容还没加载进内存，就会分配内存页，将文件内容加载到内存中，然后把内存页挂在 xarray 树上。下次再访问同样的页偏移量时，文件内容已经在树上，可直接返回。 address_space-\u0026gt;i_pages 指向的树就是内核管理的 Page Cache。\n将文件内容加载到 Page Cache 后，内核就可以填写进程相关的页表项，将这块文件映射的虚拟地址区域，直接映射到 Page Cache 页，完成缺页中断的处理。\n当内存紧张需要回收 Page Cache 时，内核需要知道这些 Page Cache 页映射到了哪些进程，这样才能修改进程的页表，解除虚拟内存和物理内存的映射。我们知道，同一个文件可以映射到多个进程空间，所以需要保存反向映射关系，即根据 Page Cache 页找到进程。\nPage Cache 页的反向映射关系保存在 address_space 维护的另一颗树 i_mmap。address_space-\u0026gt;i_mmap 是一个优先查找树（Priority Search Tree），关联了这个文件 Page Cache 页的 vm_area_struct 就挂在这棵树上，而这些 vm_area_struct都将指向各自的进程空间描述符 mm_struct，从而建立了 Page Cache 页到进程的联系。\n当需要解除一个 Page Cache 页的映射时，利用 address_space-\u0026gt;i_mmap 指向的树，查找 Page Cache 页映射到哪些进程的哪些 vm_area_struct，从而确定需要修改的进程页表项内容。\n简单总结一下，一个文件对应的 address_space 主要管理着两颗树：i_pages 指向的 xarray 树，维护着的所有 Page Cache 页；i_mmap 指向的 PST 树，维护着文件映射所形成的 vm_area_struct 虚拟内存区域，用来在释放 Page Cache 页时，查找映射了该文件的进程。如果文件没有被映射到进程空间，那么 i_mmap 对应的 PST 树为空。\n# Page Cache 的观测 可以通过查看 /proc/meminfo 文件获知 Page Cache 相关的各种指标。\n/proc 是伪文件系统（Pseudo filesystems ）。Linux 通过伪文件系统，让系统和内核信息在用户空间可用。使用 free、vmstat等命令查看到的内存信息，数据实际上都来自 /proc/meminfo。\n我们看一个示例：\n$ cat /proc/meminfo MemTotal: 8052564 kB MemFree: 129804 kB MemAvailable: 4956164 kB Buffers: 175932 kB Cached: 4896824 kB SwapCached: 40 kB Active: 2748728 kB \u003c- Active(anon) + Active(file) Inactive: 4676540 kB \u003c- Inactive(anon) +Inactive(file) Active(anon): 3432 kB Inactive(anon): 2513172 kB Active(file): 2745296 kB Inactive(file): 2163368 kB Unevictable: 65496 kB Mlocked: 0 kB SwapTotal: 2097148 kB SwapFree: 2095868 kB Dirty: 12 kB Writeback: 0 kB AnonPages: 2411440 kB Mapped: 761076 kB Shmem: 170868 kB ... 关于 /proc/meminfo 每一项的详细解释，可以查看 [Linux 内核文档 - The /proc Filesystem](The /proc Filesystem — The Linux Kernel documentation)。我们重点看一下 Page Cache 相关的字段。\n当前系统 Page Cache 等于 Buffers + Cached 之和 ：\n1 Buffers + Cached = 5072756 kB 前面讨论过，如果 vm_area_struct 关联到文件，那么这段内存区域就是 File-backed 内存。没有关联文件的 vm_area_struct 内存区域是匿名内存。我们是否可以认为，和磁盘文件相关联的 File-backed 内存总和，应该等于 Page Cache 呢？\n1 Active(file) + Inactive(file) = 4908664 kB 好像有点对不上，还差了一些，差的这部分是共享内存（Shmem）。\nLinux 为了实现“共享内存”（shared memory）功能，即多个进程共同使用同一内存中的内容，需要使用虚拟文件系统。虚拟文件并不是真实存在于磁盘上的文件，它只是由内核模拟出来的。但虚拟文件也有自己的 inode 和 address_space结构。内核在创建共享匿名映射区域时，会创建出一个虚拟文件，并将这个文件与 vm_area_struct关联起来，这样多个进程的 vm_area_struct 会关联到同一个虚拟文件，最终映射到同样的物理内存页，从而实现了共享内存功能。这就是共享内存（Shmem）的实现原理。\n由于 Shmem 没有关联磁盘上的文件，因此它不属于 File-backed 内存，而是被记录在匿名内存（Active(anon) 或 Inactive(anon)）部分。但因为 Shmem 有自己的 inode ，inode-\u0026gt;address_sapce 维护的 Page Cache 页挂在 address_space-\u0026gt;i_pages 指向的 xarray 树上，因此 Shmem 部分的内存也应该算在 Page Cache 里。\n此外 File-backed 内存还有 Active 和 Inactive 的区别。刚被使用过的数据的内存空间被认为是 Active 的，长时间未被使用过的数据的内存空间则被认为是 Inactive 的。当物理内存不足，不得不释放正在使用的内存时，会首先释放 Inactive 的内存。\nPage Cache 和 匿名内存以及 File-backed 内存等之间的关系，如图下图所示。虽然难免存在误差，但大体来说下面的关系式是成立的：\n值得注意的是，AnonPages != Active(anon) + Inactive(anon)。Active(anon) 和 Inactive(anon) 是用来表示不可回收但是可以被交换到 swap 分区的内存，而 AnonPages 则是指没有对应文件的内存，两者的角度不一样。 Shmem 虽然属于Active(anon) 或者 Inactive(anon)，但是 Shmem 有对应的内存虚拟文件，所以它不属于 AnonPages。\n总之，Page Cache 肯定关联了文件，不管是真实存在的磁盘文件，还是虚拟内存文件。AnonPages 则没有关联任何文件。Shmem 关联了虚拟文件，它属于 Active(anon) 或者 Inactive(anon)，同时也算在 Page Cache 中。\n如果我们想知道某个文件有多少内容被缓存在 Page Cache ，可以使用 [fincore](fincore（1） - Linux 手册页 (man7.org)) 命令。例如：\n1 2 3 $ fincore /usr/lib/x86_64-linux-gnu/libc.so.6 RES PAGES SIZE FILE 2.1M 542 2.1M /usr/lib/x86_64-linux-gnu/libc.so.6 RES 是文件内容被加载进物理内存占用的内存空间大小。PAGES 是换算成文件内容占用了多少内存页。 在上面的例子中，文件 /usr/lib/x86_64-linux-gnu/libc.so.6 的全部内容，都被加载进了 Page Cache。\n结合 lsof 命令，我们可以查看某一进程打开的文件占用了多少 Page Cache：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ sudo lsof -p 1270 | grep REG | awk \u0026#39;{print $9}\u0026#39; | xargs sudo fincore RES PAGES SIZE FILE 64.8M 16580 89.9M /usr/bin/dockerd 32K 8 32K /var/lib/docker/buildkit/cache.db 16K 4 16K /var/lib/docker/buildkit/metadata_v2.db 16K 4 16K /var/lib/docker/buildkit/snapshots.db 16K 4 16K /var/lib/docker/buildkit/containerdmeta.db 284K 71 282.4K /usr/lib/x86_64-linux-gnu/libnss_systemd.so.2 244K 61 594.7K /usr/lib/x86_64-linux-gnu/libpcre2-8.so.0.10.2 156K 39 154.2K /usr/lib/x86_64-linux-gnu/libgpg-error.so.0.29.0 24K 6 20.6K /usr/lib/x86_64-linux-gnu/libpthread.so.0 908K 227 906.5K /usr/lib/x86_64-linux-gnu/libm.so.6 ... 另外，对于所有缓存类型，缓存命中率都是一个非常重要的指标。我们可以使用 bcc 内置的工具 cachestat 追踪整个系统的 Page Cache 命中率：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ sudo cachestat-bpfcc HITS MISSES DIRTIES HITRATIO BUFFERS_MB CACHED_MB 2059 0 32 100.00% 74 1492 522 0 0 100.00% 74 1492 32 0 7 100.00% 74 1492 135 0 69 100.00% 74 1492 97 1 3 98.98% 74 1492 512 0 82 100.00% 74 1492 303 0 86 100.00% 74 1492 2474 7 1028 99.72% 74 1494 815 0 964 100.00% 74 1497 2786 0 1 100.00% 74 1497 1051 0 0 100.00% 74 1497 ^C 502 0 0 100.00% 74 1497 Detaching... 使用 cachetop 可以按进程追踪 Page Cache 命中率：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ sudo cachetop-bpfcc 14:20:41 Buffers MB: 86 / Cached MB: 2834 / Sort: HITS / Order: descending PID UID CMD HITS MISSES DIRTIES READ_HIT% WRITE_HIT% 14237 mazhen java 12823 4594 3653 52.6% 13.2% 14370 mazhen ldd 869 0 0 100.0% 0.0% 14371 mazhen grep 596 0 0 100.0% 0.0% 14376 mazhen ldd 536 0 0 100.0% 0.0% 14369 mazhen env 468 0 0 100.0% 0.0% 14377 mazhen ldd 467 0 0 100.0% 0.0% 14551 mazhen grpc-default-ex 466 0 0 100.0% 0.0% 14375 mazhen ldd 435 0 0 100.0% 0.0% 14479 mazhen ldconfig 421 0 0 100.0% 0.0% 14475 mazhen BookieJournal-3 417 58 132 60.0% 6.1% ... # mmap系统调用 系统调用 mmap是最重要的内存管理接口。使用 mmap 可以创建文件映射，从而产生 Page Cache。使用 mmap 还可以用来申请堆内存。glibc 提供的 malloc，内部使用的就是 mmap 系统调用。 由于 mmap 系统调用分配内存的效率比较低， malloc 会先使用 mmap 向操作系统申请一块比较大的内存，然后再通过各种优化手段让内存分配的效率最大化。\nmmap 根据参数的不同， 可以从是不是文件映射，以及是不是私有内存这两个不同的维度来进行组合：\n私有匿名映射 在调用 mmap(MAP_ANON | MAP_PRIVATE) 时，只需要在进程虚拟内存空间分配一块内存，然后创建这块内存所对应的 vm_area_struct 结构，这次调用就结束了。当访问到这块虚拟内存时，由于这块虚拟内存都没有映射到物理内存上，就会发生缺页中断。 vm_area_struct关联文件属性为空，所以是匿名映射。内核会分配一个物理内存，然后在页表里建立起虚拟地址到物理地址的映射关系。\n私有文件映射 进程通过 mmap(MAP_FILE | MAP_PRIVATE) 这种方式来申请的内存，比如进程将共享库（Shared libraries）和可执行文件的代码段（Text Segment）映射到自己的地址空间就是通过这种方式。\n如果文件是只读的话，那这个文件在物理页的层面上其实是共享的。也就是进程 A 和进程 B 都有一页虚拟内存被映射到了相同的物理页上。但如果要写文件的时候，因为这一段内存区域的属性是私有的，所以内核就会做一次写时复制，为写文件的进程单独地创建一份副本。这样，一个进程在写文件时，并不会影响到其他进程的读。\n私有文件映射的只读页是多进程间共享的，可写页是每个进程都有一个独立的副本，创建副本的时机仍然是写时复制。\n共享文件映射 进程通过 mmap(MAP_FILE | MAP_SHARED) 这种方式来申请的内存。在私有文件映射的基础上，共享文件映射就很简单了：对于可写的页面，在写的时候不进行复制就可以了。这样的话，无论何时，也无论是读还是写，多个进程在访问同一个文件的同一个页时，访问的都是相同的物理页面。\n共享匿名映射 进程通过 mmap(MAP_ANON | MAP_SHARED) 这种方式来申请的内存。借助虚拟文件系统，多个进程的 vm_area_struct 会关联到同一个虚拟文件，最终映射到同样的物理内存页，实现进程间共享内存的功能。\nmmap 的四种映射类型，和上面介绍的 /proc/meminfo 内存指标之间的关系：\n私有映射都属于 AnonPages，共享映射都是 Page cache。前面讨论过，共享的匿名映射 Shmem，虽然没有关联真实的磁盘文件，但是关联了虚拟内存文件，所以也属于 Page Cache。\n私有文件映射，如果文件是只读的话，这块内存属于 Page Cache。如果有进程写文件，因为这一段内存区域的属性是私有的，所以内核就会做一次写时复制，为写文件的进程单独地创建一份副本，这个副本就属于 AnonPages 了。\n# 写在最后 Page Cache 机制涉及了进程空间，文件系统，内存管理等多个内核功能，Page Cache 就像一条线将这几部分串在了一起。因此深入理解 Page Cache 机制，对学习内核会有很大的帮助。\n","date":"2022-04-01T16:06:54+08:00","permalink":"https://mazhen.tech/p/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3-page-cache/","title":"深入理解 Page Cache"},{"content":" # 进程ID 进程相关的 ID 有多种，除了进程标识 PID 外，还包括：线程组标识 TGID，进程组标识 PGID，回话标识 SID。TGID/PGID/SID 分别是相关线程组长/进程组长/回话 leader 进程的 PID。\n下面分别介绍这几种ID。\n# PID 进程总是会被分配一个唯一标识它们的进程ID号，简称 PID。\n用 fork 或 clone 产生的每个进程都由内核自动地分配了一个唯一的 PID 。\nPID 保存在 task_struct-\u0026gt;pid中。\n# TGID 进程以 CLONE_THREAD 标志调用 clone 方法，创建与该进程共享资源的线程。线程有独立的task_struct，但它 task_struct内的 files_struct、fs_struct 、sighand_struct、signal_struct和mm_struct 等数据结构仅仅是对进程相应数据结构的引用。\n由进程创建的所有线程都有相同的线程组ID(TGID)。线程有自己的 PID，它的TGID 就是进程的主线程的 PID。如果进程没有使用线程，则其 PID 和 TGID 相同。\n在内核中进程和线程都用 task_struct表示，而有了 TGID，我们就可以知道 task_struct 代表的是一个进程还是一个线程。\nTGID 保存在 task_struct-\u0026gt;tgid 中。\n当 task_struct 代表一个线程时，task_struct-\u0026gt;group_leader 指向主线程的 task_struct。\n# PGID 如果 shell 具有作业管理能力，则它所创建的相关进程构成一个进程组，同一进程组的进程都有相同的 PGID。例如，用管道连接的进程包含在同一个进程组中。\n进程组简化了向组的所有成员发送信号的操作。进程组提供了一种机制，让信号可以发送给组内的所有进程，这使得作业控制变得简单。\n当 task_struct 代表一个进程，且该进程属于某一个进程组，则 task_struct-\u0026gt;group_leader 指向组长进程的 task_struct。\nPGID 保存在 task_struct-\u0026gt;signal-\u0026gt;pids[PIDTYPE_PGID].pid中。 pids[] 的数组下标是枚举类型，在 include/linux/pid.h 中定义了 PID 的类型：\n1 2 3 4 5 6 7 8 enum pid_type { PIDTYPE_PID, PIDTYPE_TGID, PIDTYPE_PGID, PIDTYPE_SID, PIDTYPE_MAX, }; task_struce-\u0026gt;signal 是 signal_struct 类型，维护了进程收到的信号，task_struce-\u0026gt;signal 被该进程的所有线程共享。从 PGID 保存在 task_struct-\u0026gt;signal-\u0026gt;pids[PIDTYPE_PGID]中可以看出进程组和信号处理相关。 # SID 用户一次登录所涉及所有活动称为一个会话（session），其间产生的所有进程都有相同的会话ID（SID），等于会话 leader 进程的 PID。\nSID 保存在 task_struct-\u0026gt;signal-\u0026gt;pids[PIDTYPE_SID].pid中。\n# PID/TGID/PGID/SID总结 用一幅图来总结 PID/TGID/PGID/SID ：\n# 进程间关系 内核中所有进程的 task_struct 会形成多种组织关系。根据进程的创建过程会有亲属关系，进程间的父子关系组织成一个进程树；根据用户登录活动会有会话和进程组关系。\n# 亲属关系 进程通过 fork() 创建出一个子进程，就形成来父子关系，如果创建出多个子进程，那么这些子进程间属于兄弟关系。可以用 pstree 命令查看当前系统的进程树。\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ pstree -p systemd(1)─┬─ModemManager(759)─┬─{ModemManager}(802) │ └─{ModemManager}(806) ├─NetworkManager(685)─┬─{NetworkManager}(743) │ └─{NetworkManager}(750) ├─acpid(675) ├─agetty(814) ├─avahi-daemon(679)───avahi-daemon(712) ├─bluetoothd(680) ├─canonical-livep(754)─┬─{canonical-livep}(1224) │ ├─{canonical-livep}(1225) │ ├─{canonical-livep}(1226) ... 进程描述符 task_struct 的 parent 指向父进程，children指向子进程链表的头部，sibling 把当前进程插入到兄弟链表中。\n通常情况下，real_parent 和 parent 是一样的。如果在 bash 上使用 GDB 来 debug 一个进程，这时候进程的 parent 是 GDB ，进程的 real_parent 是 bash。\n当一个进程创建了子进程后，它应该通过系统调用 wait() 或者 waitpid() 等待子进程结束，回收子进程的资源。而子进程在结束时，会向它的父进程发送 SIGCHLD 信号。因此父进程还可以注册 SIGCHLD 信号的处理函数，异步回收资源。\n如果父进程提前结束，那么子进程将把1号进程 init 作为父进程。总之，进程都有父进程，负责进程结束后的资源回收。在子进程退出且父进程完成回收前，子进程变成僵尸进程。僵尸进程持续的时间通常比较短，在父进程回收它的资源后就会消亡。如果父进程没有处理子进程的终止，那么子进程就会一直处于僵尸状态。\n# 会话、进程组关系 Linux 系统中可以有多个会话（session），每个会话可以包含多个进程组，每个进程组可以包含多个进程。\n会话是用户登录系统到退出的所有活动，从登录到结束前创建的所有进程都属于这次会话。登录后第一个被创建的进程（通常是 shell），被称为 会话 leader。\n进程组用于作业控制。一个终端上可以启动多个作业，也就是进程组，并能控制哪个作业在前台，前台作业可以访问终端，哪些作业运行在后台，不能读写终端。\n我们来看一个会话和进程组的例子。\n1 2 3 4 5 6 7 8 9 10 11 12 $ cat | head hello hello ^Z [1]+ 已停止 cat | head $ ps j | more PPID PID PGID SID TTY TPGID STAT UID TIME COMMAND 1522 1532 1532 1532 pts/0 1762 Ss 1000 0:00 -bash 1532 1760 1760 1532 pts/0 1762 T 1000 0:00 cat 1532 1761 1760 1532 pts/0 1762 T 1000 0:00 head 1532 1762 1762 1532 pts/0 1762 R+ 1000 0:00 ps j 1532 1763 1762 1532 pts/0 1762 S+ 1000 0:00 more 上面的命令通过 cat | head 创建了第一个进程组，包含 cat 和 head 两个进程。这时这个作业是前台任务，可以控制终端。当我们按下 Ctrl + z，会发送信号 SIGTSTP 给前台进程组的所有进程，该信号的缺省行为是暂停作业执行。暂停的作业会让出终端，并且进程不会再被调度，直到它们收到 SIGCONT 信号恢复执行。\n然后我们通过 ps j | more 创建了另一个进程组，包含 ps 和 more 两个进程。ps 的参数 j 表示用任务格式显示进程。输出中的 STAT 列是进程的状态码，前面的大写字母表示进程状态，我们可以从 ps 的 man page 查看其含义：\n1 2 3 4 5 6 7 8 9 D uninterruptible sleep (usually IO) I Idle kernel thread R running or runnable (on run queue) S interruptible sleep (waiting for an event to complete) T stopped by job control signal t stopped by debugger during the tracing W paging (not valid since the 2.6.xx kernel) X dead (should never be seen) Z defunct (\u0026#34;zombie\u0026#34;) process, terminated but not reaped by its parent 某些进程除了大写字母代表的进程状态，还跟着一个附加符号：\ns ：进程是会话 leader 进程 + ：进程位于前台进程组中 从输出可以看出，bash 是这个会话的 leader 进程，它的 PID、PGID 和 SID 相同，都是1532 。这个会话其他所有进程的 SID 也都是 1532。\ncat | head 进程组的 PGID 是 1760，ps j | more 进程组的 PGID 是 1762。用管道连接的进程包含在同一个进程组中，每个进程组内第一个进程成为 Group Leader，并以 Group Leader 的 PID 作为组内进程的 PGID。\n会话有一个前台进程组，还可以有一个或多个后台进程组，只有前台作业可以从终端读写数据。示例的进程组关系如图：\n注意到上图中显示，终端设备可以向进程组发送信号。我们可以在终端输入特殊字符向前台进程发送信号：\nCtrl + c 发送 SIGINT 信号，默认行为是终止进程； Ctrl + \\ 发送 SIGQUIT 信号，默认行为是终止进程，并进行 core dump； Ctrl + z 发送 SIGTSTP 信号，暂停进程。 只有前台进程可以从终端接收输入，也只有前台进程才被允许向终端输出。如果一个后台作业中的进程试图进行终端读写操作，终端会向整个作业发送 SIGTTOU 或 SIGTTIN 信号，默认的行为是暂停进程。\n当终端关闭时，会向整个会话发送 SIGHUP 信号，通常情况下，这个会话的所有进程都会被终止。如果想让运用在后台的进程不随着 session 的结束而退出，可以使用 nohup 命令忽略 SIGHUP 信号：\n1 $ nohup command \u0026gt;cmd.log 2\u0026gt;\u0026amp;1 \u0026amp; 即使 shell 结束，运行于后台的进程也能无视 SIGHUP 信号继续执行。另外一个方法是可以让进程运行在 screen 或 tmux 这种终端多路复用器（terminal multiplexer）中。\n","date":"2021-11-14T16:02:22+08:00","permalink":"https://mazhen.tech/p/%E8%BF%9B%E7%A8%8Bid%E5%8F%8A%E8%BF%9B%E7%A8%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/","title":"进程ID及进程间的关系"},{"content":"我们通常会使用 load average 了解服务器的健康状况，检查服务器的负载是否正常。但 load average 有几个缺点：\nload average 的计算包含了 TASK_RUNNING 和 TASK_UNINTERRUPTIBLE 两种状态的进程。TASK_RUNNING 是进程处于运行、或等待分配 CPU 的准备运行状态。TASK_UNINTERRUPTIBLE 是进程处于不可中断的等待，一般是等待磁盘的输入输出。因此 load average 值飙高，可能是因为 CPU 资源不够，让很多处于 TASK_RUNNING 状态的进程等待 CPU，也可能是由于磁盘 I/O 资源紧张，造成很多进程因为等待 IO 而处于 TASK_UNINTERRUPTIBLE 状态。你可以通过 load average 发现系统很忙，但没法区分是因为争夺 CPU 还是 IO 引起的。 load average 最短的时间窗口为1分钟，没法观察更短窗口的负载平均值，例如想了解最近10秒的load average。 load average 报告的是活跃进程数的原始数据，你还需要知道可用的 CPU 核数，这样 load average 的值才有意义。 所以，当用户遇到服务器 load average 飙高的时候，还需要继续查看 CPU、I/O 和内存等资源的统计数据，才能进一步分析问题。\n于是，Facebook的工程师 Johannes Weiner 发明了一个新的指标 PSI(Pressure Stall Information)，并向内核提交了这个patch。\n# PSI 概览 当 CPU、内存或 IO 设备争夺激烈的时候，系统会出现负载的延迟峰值、吞吐量下降，并可能触发内核的 OOM Killer。PSI(Pressure Stall Information) 字面意思就是由于资源（CPU、内存和 IO）压力造成的任务执行停顿。PSI 量化了由于硬件资源紧张造成的任务执行中断，统计了系统中任务等待硬件资源的时间。我们可以用 PSI 作为指标，来衡量硬件资源的压力情况。停顿的时间越长，说明资源面临的压力越大。\n如果持续监控 PSI 指标并绘制变化曲线图，可以发现吞吐量下降与资源短缺的关系，让用户在资源变得紧张前，采取更主动的措施，例如将任务迁移到其他服务器，杀死低优先级的任务等。\nPSI 已经包含在 4.20及以上版本的 Linux 内核中。\n# PSI 接口文件 CPU、内存和 IO 的压力信息导出到了 /proc/pressure/ 目录下对应的文件，你可以使用 cat 命令查询资源的压力统计信息：\n1 2 3 4 5 6 7 8 9 10 $ cat /proc/pressure/cpu some avg10=0.03 avg60=0.07 avg300=0.06 total=8723835 $ cat /proc/pressure/io some avg10=0.00 avg60=0.00 avg300=0.00 total=56385169 full avg10=0.00 avg60=0.00 avg300=0.00 total=54915860 $ cat /proc/pressure/memory some avg10=0.00 avg60=0.00 avg300=0.00 total=149158 full avg10=0.00 avg60=0.00 avg300=0.00 total=34054 内存和 IO 显示了两行指标：some 和 full，CPU 只有一行指标 some。关于 some 和 full 的定义下一节解释。\navg 给出了任务由于硬件资源不可用而被停顿的时间百分比。avg10、avg60和avg300分别是最近10秒、60秒和300秒的停顿时间百分比。\n例如上面 /proc/pressure/cpu 的输出，avg10=0.03 意思是任务因为CPU资源的不可用，在最近的10秒内，有0.03%的时间停顿等待 CPU。如果 avg 大于 40 ，也就是有 40% 时间在等待硬件资源，就说明这种资源的压力已经比较大了。\ntotal 是任务停顿的总时间，以微秒（microseconds）为单位。通过 total 可以检测出停顿持续太短而无法影响平均值的情况。\n# some 和 full 的定义 some 指标说明一个或多个任务由于等待资源而被停顿的时间百分比。在下图的例子中，在最近的60秒内，任务A的运行没有停顿，而由于内存紧张，任务B在运行过程中花了30秒等待内存，则 some 的值为50%。\nsome 表明了由于缺乏资源而造成至少一个任务的停顿。\nfull 指标表示所有的任务由于等待资源而被停顿的时间百分比。在下图的例子中，在最近的60秒内，任务 B 等待了 30 秒的内存，任务 A 等待了 10 秒内存，并且和任务 B 的等待时间重合。在这个重合的时间段10秒内，任务 A 和 任务 B 都在等待内存，结果是 some 指标为 50%，full 指标为 10/60 = 16.66%。\nfull 表明了总吞吐量的损失，在这种状态下，所有任务都在等待资源，CPU 周期将被浪费。\n请注意，some 和 full 的计算是用整个时间窗口内累计的等待时间，等待时间可以是连续的，也可能是离散的。\n理解了 some 和 full 的含义，就明白了 CPU 为什么没有 full 指标，因为不可能所有的任务都同时饿死在 CPU 上，CPU 总是在执行一个任务。\n# PSI 阈值监控 用户可以向 PSI 注册触发器，在资源压力超过自定义的阈值时获得通知。一个触发器定义了特定时间窗口内最大累积停顿时间，例如，在任何 500ms 的窗口内，累计 100ms 的停顿时间会产生一个通知事件。\n如何向 PSI 注册触发器呢？打开 /proc/pressure/ 目录下资源对应的 PSI 接口文件，写入想要的阈值和时间窗口，然后在打开的文件描述符上使用 select()、poll() 或 epoll() 方法等待通知事件。写入 PSI 接口文件的数据格式为：\n1 \u0026lt;some|full\u0026gt; \u0026lt;停顿阈值\u0026gt; \u0026lt;时间窗口\u0026gt; 阈值和时间窗口的单位都是微秒（us）。内核接受的窗口大小范围为500ms到10秒。\n举个例子，向 /proc/pressure/io 写入 \u0026ldquo;some 500000 1000000\u0026rdquo;，代表着在任何 1 秒的时间窗口内，如果一个或多个进程因为等待 IO 而造成的时间停顿超过了阈值 500ms，将触发通知事件。\n当用于定义触发器的 PSI 接口文件描述符被关闭时，触发器将被取消注册。\n我们通过一个例子演示触发器的使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 #include \u0026lt;errno.h\u0026gt; #include \u0026lt;fcntl.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;poll.h\u0026gt; #include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main() { const char trig[] = \u0026#34;some 500000 1000000\u0026#34;; struct pollfd fds; int n; fds.fd = open(\u0026#34;/proc/pressure/io\u0026#34;, O_RDWR | O_NONBLOCK); if (fds.fd \u0026lt; 0) { printf(\u0026#34;/proc/pressure/io open error: %s\\n\u0026#34;, strerror(errno)); return 1; } fds.events = POLLPRI; if (write(fds.fd, trig, strlen(trig) + 1) \u0026lt; 0) { printf(\u0026#34;/proc/pressure/io write error: %s\\n\u0026#34;, strerror(errno)); return 1; } printf(\u0026#34;waiting for events...\\n\u0026#34;); while (1) { n = poll(\u0026amp;fds, 1, -1); if (n \u0026lt; 0) { printf(\u0026#34;poll error: %s\\n\u0026#34;, strerror(errno)); return 1; } if (fds.revents \u0026amp; POLLERR) { printf(\u0026#34;got POLLERR, event source is gone\\n\u0026#34;); return 0; } if (fds.revents \u0026amp; POLLPRI) { printf(\u0026#34;event triggered!\\n\u0026#34;); } else { printf(\u0026#34;unknown event received: 0x%x\\n\u0026#34;, fds.revents); return 1; } } return 0; } 在服务器上编译并运行该程序，如果当前服务器比较空闲，我们会看到程序一直在等待 IO 压力超过阈值的通知：\n1 2 $ sudo ./monitor waiting for events... 我们为服务器制造点 IO 压力，生成一个5G大小的文件：\n1 $ dd if=/dev/zero of=/home/mazhen/testfile bs=4096 count=1310720 再回到示例程序的运行窗口，会发现已经收到事件触发的通知：\n1 2 3 4 5 6 7 8 $ sudo ./monitor waiting for events... event triggered! event triggered! event triggered! event triggered! event triggered! ... # PSI 应用案例 Facebook 是因为一些实际的需求开发了 PSI。其中一个案例是为了避免内核 OOM(Out-Of-Memory) killer 的触发。\n应用在申请内存的时候，如果没有足够的 free 内存，可以通过回收 Page Cache 释放内存，如果这时 free 内存还是不够，就会触发内核的 OOM Killer，挑选一个进程 kill 掉释放内存。这个过程是同步的，申请分配内存的进程一直被阻塞等待，而且内核选择 kill 掉哪个进程释放内存，用户不可控。因此，Facebook 开发了用户空间的 OOM Killer 工具 oomd。\noomd 使用 PSI 阈值作为触发器，在内存压力增加到一定程度时，执行指定的动作，避免最终 OOM 的发生。oomd 作为第一道防线，确保服务器工作负载的健康，并能自定义复杂的清除策略，这些都是内核做不到的。\ncgroup2 也支持 group 内任务的 PSI 指标追踪，这样就可以知道容器内 CPU、内存和 IO 的真实压力情况，进行更精细化的容器调度，在资源利用率最大化的同时保证任务的延迟和吞吐量。\n","date":"2021-08-01T11:51:05+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8psipressure-stall-information%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%B5%84%E6%BA%90/","title":"使用PSI（Pressure Stall Information）监控服务器资源"},{"content":"你可能听说过 TTY 和 PTY 这些缩写，也在 /dev 目录下看到过 /dev/tty[n] 设备，大概知道它们和 Linux 终端的概念有关。可是你清楚 TTY、PTY 具体指的是什么，它们有什么区别，以及它们和 shell 又是什么关系呢？为了理解这些，我们需要先回顾一下历史。\n# 回顾历史 在计算机诞生之前，人们发明了 Teleprinter(电传打字机)，通过长长的电线点对点连接，发送和接收打印的信息，用于远距离传输电报信息。\nTeleprinter 也可以写成 teletypewriter 或 teletype。\n后来人们将 Teleprinter 连接到早期的大型计算机上，作为输入和输出设备，将输入的数据发送到计算机，并打印出响应。\n在今天你很难想象程序的运行结果需要等到打印出来才能看到，Teleprinter 设备已经进了计算机博物馆。现在我们用 TTY 代表计算机终端（terminal），只是沿用了历史习惯，电传打字机（teletypewriter）曾经是计算机的终端，它的缩写便是 TTY(TeleTYpewriter)。\n为了把不同型号的电传打字机接入计算机，需要在操作系统内核安装驱动，为上层应用屏蔽所有的低层细节。\n电传打字机通过两根电缆连接：一根用于向计算机发送指令，一根用于接收计算机的输出。这两根电缆插入 UART （Universal Asynchronous Receiver and Transmitter，通用异步接收和发送器）的串行接口连接到计算机。\n操作系统包含一个 UART 驱动程序，管理字节的物理传输，包括奇偶校验和流量控制。然后输入的字符序列被传递给 TTY 驱动，该驱动包含一个 line discipline。\nline discipline 负责转换特殊字符（如退格、擦除字、清空行），并将收到的内容回传给电传打字机，以便用户可以看到输入的内容。line discipline 还负责对字符进行缓冲，当按下回车键时，缓冲的数据被传递给与 TTY 相关的前台用户进程。用户可以并行的执行几个进程，但每次只与一个进程交互，其他进程在后台工作。\n# 终端模拟器(terminal emulator) 今天电传打字机已经进了博物馆，但 Linux/Unix 仍然保留了当初 TTY驱动和 line discipline 的设计和功能。终端不再是一个需要通过 UART 连接到计算机上物理设备。终端成为内核的一个模块，它可以直接向 TTY 驱动发送字符，并从 TTY 驱动读取响应然后打印到屏幕上。也就是说，用内核模块模拟物理终端设备，因此被称为终端模拟器(terminal emulator)。\n上图是一个典型的Linux桌面系统。终端模拟器就像过去的物理终端一样，它监听来自键盘的事件将其发送到 TTY 驱动，并从 TTY 驱动读取响应，通过显卡驱动将结果渲染到显示器上。TTY驱动 和 line discipline的行为与原先一样，但不再有 UART 和 物理终端参与。\n如何看到一个终端模拟器呢？在 Ubuntu 20 桌面系统上，按 Ctrl+Alt+F3 就会得到一个由内核模拟的 TTY。Linux上这种模拟的文本终端也被称为虚拟终端（Virtual consoles）。每个虚拟终端都由一个特殊的设备文件 /dev/tty[n] 所表示，与这个虚拟终端的交互，是通过对这个设备文件的读写操作，以及使用ioctl系统调用操作这个设备文件进行的。通过执行 tty 命令可以查看代表当前虚拟终端的设备文件：\n1 2 $ tty /dev/tty3 可以看到，当前终端的设备文件是 /dev/tty3，也就是通过 Ctrl+Alt+F3 得到的虚拟终端。\n你可以通过 Ctrl+Alt+F3 到 Ctrl+Alt+F6 在几个虚拟终端之间切换。按 Ctrl+Alt+F2 回到桌面环境。X 系统也是运行在一个终端模拟器上，在 Ubuntu 20 上它对应的设备是 /dev/tty2，这也是为什么使用 Ctrl+Alt+F2 可以切换到 X 系统的原因。\n我们可以看看 X 系统打开的文件中是否包含了设备文件 /dev/tty2。先查找 X 系统的 PID：\n1 2 # ps aux | grep Xorg mazhen 1404 0.1 0.6 741884 49996 tty2 Sl+ 08:07 0:13 /usr/lib/xorg/Xorg vt2 -displayfd 3 -auth /run/user/1000/gdm/Xauthority -background none -noreset -keeptty -verbose 3 再看看这个进程(1404)打开了哪些文件：\n1 2 3 4 5 6 7 8 # ll /proc/1404/fd 总用量 0 dr-x------ 2 mazhen mazhen 0 7月 10 08:07 ./ dr-xr-xr-x 9 mazhen mazhen 0 7月 10 08:07 ../ lrwx------ 1 mazhen mazhen 64 7月 10 08:07 0 -\u0026gt; /dev/tty2 lrwx------ 1 mazhen mazhen 64 7月 10 08:07 1 -\u0026gt; \u0026#39;socket:[39965]\u0026#39; lrwx------ 1 mazhen mazhen 64 7月 10 10:09 10 -\u0026gt; \u0026#39;socket:[34615]\u0026#39; ... 可以看到，X 系统确实打开了 /dev/tty2。\n再做一个有趣的实验，在 tty3 下以 root 用户身份执行 echo 命令：\n1 # echo \u0026#34;hello from tty3\u0026#34; \u0026gt; /dev/tty4 再按 Ctrl+Alt+F4 切换到 tty4，能看到从 tty3 发送来的信息。\n# 伪终端（pseudo terminal, PTY） 终端模拟器(terminal emulator) 是运行在内核的模块，我们也可以让终端模拟程序运行在用户区。运行在用户区的终端模拟程序，就被称为伪终端（pseudo terminal, PTY）。\nPTY 运行在用户区，更加安全和灵活，同时仍然保留了 TTY 驱动和 line discipline 的功能。常用的伪终端有 xterm，gnome-terminal，以及远程终端 ssh。我们以 Ubuntu 桌面版提供的 gnome-terminal 为例，介绍伪终端如何与 TTY 驱动交互。\nPTY 是通过打开特殊的设备文件 /dev/ptmx 创建，由一对双向的字符设备构成，称为 PTY master 和 PTY slave。\ngnome-terminal 持有 PTY master 的文件描述符 /dev/ptmx。 gnome-terminal 负责监听键盘事件，通过PTY master接收或发送字符到 PTY slave，还会在屏幕上绘制来自PTY master的字符输出。\ngnome-terminal 会 fork 一个 shell 子进程，并让 shell 持有 PTY slave 的设备文件 /dev/pts/[n]，shell 通过 PTY slave 接收字符，并输出处理结果。\nPTY master 和 PTY slave 之间是 TTY 驱动，会在 master 和 slave 之间复制数据，并进行会话管理和提供 line discipline 功能。\n在 gnome-terminal 中执行 tty 命令，可以看到代表PTY slave的设备文件：\n1 2 $ tty /dev/pts/0 执行 ps -l 命令，也可以确认 shell 关联的伪终端是 pts/0：\n1 2 3 4 $ ps -l F S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD 0 S 1000 1842 1832 0 80 0 - 3423 do_wai pts/0 00:00:00 bash 0 R 1000 1897 1842 0 80 0 - 3626 - pts/0 00:00:00 ps 注意到 TTY 这一列指出了当前进程的终端是 pts/0。\n我们以实际的例子，看看在 terminal 执行一个命令的全过程。\n我们在桌面启动终端程序 gnome-terminal，它向操作系统请求一个PTY master，并把 GUI 绘制在显示器上 gnome-terminal 启动子进程 bash bash 的标准输入、标准输出和标准错误都设置为 PTY slave gnome-terminal 监听键盘事件，并将输入的字符发送到PTY master line discipline 收到字符，进行缓冲。只有当你按下回车键时，它才会把缓冲的字符复制到PTY slave。 line discipline 在接收到字符的同时，也会把字符写回给PTY master。gnome-terminal 只会在屏幕上显示来自 PTY master 的东西。因此，line discipline 需要回传字符，以便让你看到你刚刚输入的内容。 当你按下回车键时，TTY 驱动负责将缓冲的数据复制到PTY slave bash 从标准输入读取输入的字符（例如 ls -l ）。注意，bash 在启动时已经将标准输入被设置为了PTY slave bash 解释从输入读取的字符，发现需要运行 ls bash fork 出 ls 进程。bash fork 出的进程拥有和 bash 相同的标准输入、标准输出和标准错误，也就是PTY slave ls 运行，结果打印到标准输出，也就是PTY slave TTY 驱动将字符复制到PTY master gnome-terminal 循环从 PTY master 读取字节，绘制到用户界面上。 # Shell 我们经常不去区分 terminal 和 Shell，会说打开一个 terminal，或打开一个 Shell。从前面介绍的命令执行过程可以看出，Shell 不处理键盘事件，也不负责字符的显示，这是 terminal 要为它处理好的。\nShell是用户空间的应用程序，通常由 terminal fork出来，是 terminal 的子进程。Shell用来提示用户输入，解释用户输入的字符，然后处理来自底层操作系统的输出。\n通常我们使用较多的 shell 有 Bash、Zsh 和 sh。\n# 配置 TTY 设备 内核将使用 TTY 驱动来处理 terminal 和 Shell 之间的通信。line discipline 是 TTY 驱动的一个逻辑组件。line discipline 主要有以下功能：\n当用户输入时，字符会被回传到PTY master line discipline 会在内存中缓冲这些字符。当用户按回车键时，它才将这些字符发送到PTY slave line discipline 可以拦截处理一些特殊的功能键，例如： 当用户按 CTRL+c 时，它向连接到 PTY slave 的进程发送 kill -2（SIGINT） 信号 当用户按 CTRL+w 时，它删除用户输入的最后一个字 当用户按 CTRL+z 时，它向连接到 PTY slave 的进程发送 kill -STOP信号 当用户按退格键时，它从缓冲区中删除该字符，并向PTY master发送删除最后一个字符的指令 我们可以使用命令行工具 stty 查询和配置 TTY，包括 line discipline 规则。在 terminal 执行 stty -a 命令：\n1 2 3 4 5 6 7 8 9 10 11 $ stty -a speed 38400 baud; rows 40; columns 80; line = 0; intr = ^C; quit = ^\\; erase = ^?; kill = ^U; eof = ^D; eol = \u0026lt;undef\u0026gt;; eol2 = \u0026lt;undef\u0026gt;; swtch = \u0026lt;undef\u0026gt;; start = ^Q; stop = ^S; susp = ^Z; rprnt = ^R; werase = ^W; lnext = ^V; discard = ^O; min = 1; time = 0; -parenb -parodd -cmspar cs8 -hupcl -cstopb cread -clocal -crtscts -ignbrk -brkint -ignpar -parmrk -inpck -istrip -inlcr -igncr icrnl ixon -ixoff -iuclc -ixany -imaxbel iutf8 opost -olcuc -ocrnl onlcr -onocr -onlret -ofill -ofdel nl0 cr0 tab0 bs0 vt0 ff0 isig icanon iexten echo echoe echok -echonl -noflsh -xcase -tostop -echoprt echoctl echoke -flusho -extproc -a 标志告诉 stty 返回所有的设置，包括TTY 的特征和 line discipline 规则。\n让我们看一下第一行：\nspeed 表示波特率。当 terminal 和计算机通过物理线路连接时，speed 后的数字表示物理线路的波特率。波特率对 PTY 来说是没有意义。 rows, columns 表示 terminal 的行数和列数，以字符为单位。 line 表示line discipline 的类型。0 是 N_TTY。 stty 能够对 terminal 进行设置，让我们做个简单的测试验证一下。在第一个 terminal 中使用 vi 编辑一个文件。vi 在启动时会查询当前 terminal 的大小，以便 vi 能填满整个窗口。这时候我们在另一个 terminal 中输入：\n1 # stty -F /dev/pts/0 rows 20 这个命令将终端 pts/0 的行数设置为原来的一半，这将更新内核中 TTY 的数据结构，并向 vi 发送一个 SIGWINCH 信号，vi 接收到该信号后将根据 TTY 新的行列数重新绘制自己，这时 vi 就只使用了可用窗口区域的上半部分。\nstty -a 输出的第二行给出了 line discipline 能处理的所有特殊字符，包含了键的绑定。例如 intr = ^C 是指将 CTRL+c 映射到 kill -2 (SIGINT) 信号。你也可以更改这个绑定，例如执行 stty intr o 命令，将发送 SIGINT 信号的键从 CTRL+c 换成了字符 o。\n最后，stty -a 列出了一系列 line discipline 规则的开关。- 表示开关是关闭的，否则开关就是打开的。所有的开关在 man stty中都有解释。我举其中一个简单的例子，echo 是指示 line discipline 将字符回传的规则，我们可以执行命令关闭 echo 规则：\n1 $ stty -echo 这时候你再输入一些东西，屏幕上什么也不会出现。line discipline 不会将字符回传给 PTY master，因此 terminal 不会再显示我们输入的内容。然而其他一切都照常进行。例如你输入 ls，在输入时看不到字符 ls，然后你输入回车后，仍然会看到 ls 的输出。执行命令恢复 echo 规则：\n1 $ stty echo 可以通过 stty raw 命令来禁用所有的 line discipline 规则，这样的终端被称为 raw terminal。像 vi 这样的编辑器会将终端设置为 raw ，因为它需要自己处理字符。后面介绍的远程终端也是需要一个 raw terminal，同样会禁用所有的 line discipline 规则。\n# 远程终端 我们经常通过 ssh 连接到一个远程主机，这时候远程主机上的 ssh server 就是一个伪终端 PTY，它同样持有 PTY master，但 ssh server 不再监听键盘事件，以及在屏幕上绘制输出结果，而是通过 TCP 连接，向 ssh client 发送或接收字符。\n我们简单梳理一下远程终端是如何执行命令的。\n用户在客户端的 terminal 中输入 ssh 命令，经过 PTY master、TTY 驱动，到达 PTY slave。bash 的标准输入已经设置为了 PTY slave，它从标准输入读取字符序列并解释执行，发现需要启动 ssh 客户端，并请求和远程服务器建 TCP 连接。\n服务器端接收客户端的 TCP 连接请求，向内核申请创建 PTY，获得一对设备文件描述符。让 ssh server 持有 PTY master，ssh server fork 出的子进程 bash 持有 PTY slave。bash 的标准输入、标准输出和标准错误都设置为了PTY slave。\n当用户在客户端的 terminal 中输入命令 ls -l 和回车键，这些字符经过 PTY master 到达 TTY 驱动。我们需要禁用客户端 line discipline 的所有规则，也就是说客户端的 line discipline 不会对特殊字符回车键做处理，而是让命令 ls -l 和回车键一起到达 PTY slave。ssh client 从 PTY slave 读取字符序列，通过网络，发送给 ssh server。\nssh server 将从 TCP 连接上接收到的字节写入PTY master。TTY 驱动对字节进行缓冲，直到收到特殊字符回车键。\n由于服务器端的 line discipline 没有禁用 echo 规则，所以 TTY 驱动还会将收到的字符写回PTY master，ssh server 从 PTY master 读取字符，将这些字符通过 TCP 连接发回客户端。注意，这是发回的字符不是 ls -l 命令的执行结果，而是 ls -l 本身的回显，让客户端能看到自己的输入。\n在服务器端 TTY 驱动将字符序列传送给 PTY slave，bash 从 PTY slave读取字符，解释并执行命令 ls -l。bash fork 出 ls 子进程，该子进程的标准输入、标准输出和标准错误同样设置为了 PTY slave。ls -l 命令的执行结果写入标准输出 PTY slave，然后执行结果通过 TTY 驱动到达 PTY master，再由 ssh server 通过 TCP 连接发送给 ssh client。\n注意在客户端，我们在屏幕上看到的所有字符都来自于远程服务器。包括我们输入的内容，也是远程服务器上的 line discipline 应用 echo 规则的结果，将这些字符回显了回来。表面看似简单的在远程终端上执行了一条命令，实际上底下确是波涛汹涌。\n# 写在最后 简单回顾总结一下本文的主要内容：\n电传打字机（TTY）是物理设备，最初是为电报设计的，后来被连接到计算机上，发送输入和获取输出。 电传打字机（TTY）现在被运行在内核中的模块所模拟，被称为终端模拟器(terminal emulator)。 伪终端（pseudo terminal, PTY） 是运行在用户区的终端模拟程序。 Shell 由 terminal fork 出来，是 terminal 的子进程。Shell 不处理键盘事件，也不负责字符的显示，这些是由 terminal 处理。Shell 负责解释执行用户输入的字符。 可以使用 stty 命令对 TTY 设备进行配置。 远程终端 ssh 也是一种伪终端 PTY。 相信通过这篇文章，你已经能够理解终端、终端模拟器和伪终端的区别和联系。如果想进一步探究低层实现，可以阅读 TTY 驱动的源码 drivers/tty/tty_io.c和 line discipline 的源码 drivers/tty/n_tty.c。\n","date":"2021-07-12T11:46:39+08:00","permalink":"https://mazhen.tech/p/%E7%90%86%E8%A7%A3linux-%E7%BB%88%E7%AB%AF%E7%BB%88%E7%AB%AF%E6%A8%A1%E6%8B%9F%E5%99%A8%E5%92%8C%E4%BC%AA%E7%BB%88%E7%AB%AF/","title":"理解Linux 终端、终端模拟器和伪终端"},{"content":"操作系统内核对应用开发工程师来说就像一个黑盒，似乎很难窥探到其内部的运行机制。其实Linux内核很早就内置了一个强大的tracing工具：Ftrace，它几乎可以跟踪内核的所有函数，不仅可以用于调试和分析，还可以用于观察学习Linux内核的内部运行。虽然Ftrace在2008年就加入了内核，但很多应用开发工程师仍然不知道它的存在。本文就给你介绍一下Ftrace的基本使用。\n# Ftrace初体验 先用一个例子体验一下Ftrace的使用简单，且功能强大。使用 root 用户进入/sys/kernel/debug/tracing目录，执行 echo 和 cat 命令：\n# echo _do_fork \u003e set_graph_function # echo function_graph \u003e current_tracer # cat trace | head -20 # tracer: function_graph # # CPU DURATION FUNCTION CALLS # | | | | | | | 3) | _do_fork() { 3) | copy_process() { 3) 0.895 us | _raw_spin_lock_irq(); 3) | recalc_sigpending() { 3) 0.740 us | recalc_sigpending_tsk(); 3) 2.248 us | } 3) | dup_task_struct() { 3) 0.775 us | tsk_fork_get_node(); 3) | kmem_cache_alloc_node() { 3) | _cond_resched() { 3) 0.740 us | rcu_all_qs(); 3) 2.117 us | } 3) 0.701 us | should_failslab(); 3) 2.023 us | memcg_kmem_get_cache(); 3) 0.889 us | memcg_kmem_put_cache(); 3) + 12.206 us | } 我们使用Ftrace的function_graph功能显示了内核函数 _do_fork() 所有子函数调用。左边的第一列是执行函数的 CPU，第二列 DURATION 显示在相应函数中花费的时间。我们注意到最后一行的耗时之前有个 + 号，提示用户注意延迟高的函数。+ 代表耗时大于 10 μs。如果耗时大于 100 μs，则显示 ! 号。\n我们知道，fork 是建立父进程的一个完整副本，然后作为子进程执行。那么_do_fork()的第一件大事就是调用 copy_process() 复制父进程的数据结构，从上面输出的调用链信息也验证了这一点。\n使用完后执行下面的命令关闭function_graph：\n1 2 # echo nop \u0026gt; current_tracer # echo \u0026gt; set_graph_function 使用 Ftrace 的 function_graph 功能，可以查看内核函数的子函数调用链，帮助我们理解复杂的代码流程，而这只是 Ftrace 的功能之一。这么强大的功能，我们不必安装额外的用户空间工具，只要使用 echo 和 cat 命令访问特定的文件就能实现。Ftrace 对用户的使用接口正是tracefs文件系统。\n# tracefs 文件系统 用户通过tracefs文件系统使用Ftrace，这很符合一切皆文件的Linux哲学。tracefs文件系统一般挂载在/sys/kernel/tracing目录。由于Ftrace最初是debugfs文件系统的一部分，后来才被拆分为自己的tracefs。所以如果系统已经挂载了debugfs，那么仍然会保留原来的目录结构，将tracefs挂载到debugfs的子目录下。我们可以使用 mount 命令查看当前系统debugfs和tracefs挂载点：\n1 2 3 4 # mount -t debugfs,tracefs debugfs on /sys/kernel/debug type debugfs (rw,nosuid,nodev,noexec,relatime) tracefs on /sys/kernel/tracing type tracefs (rw,nosuid,nodev,noexec,relatime) tracefs on /sys/kernel/debug/tracing type tracefs (rw,nosuid,nodev,noexec,relatime) 我使用的系统是Ubuntu 20.04.2 LTS，可以看到，为了保持兼容，tracefs同时挂载到了/sys/kernel/tracing和/sys/kernel/debug/tracing。\ntracefs下的文件主要分两类：控制文件和输出文件。这些文件的名字都很直观，像前面例子通过 current_tracer 设置当前要使用的 tracer，然后从 trace中读取结果。还有像 available_tracers 包含了当前内核可用的 tracer，可以设置 trace_options 自定义输出。\n# ls -F /sys/kernel/tracing/ available_events max_graph_depth stack_max_size available_filter_functions options/ stack_trace available_tracers per_cpu/ stack_trace_filter buffer_percent printk_formats synthetic_events buffer_size_kb README timestamp_mode buffer_total_size_kb saved_cmdlines trace current_tracer saved_cmdlines_size trace_clock dynamic_events saved_tgids trace_marker dyn_ftrace_total_info set_event trace_marker_raw enabled_functions set_event_notrace_pid trace_options error_log set_event_pid trace_pipe events/ set_ftrace_filter trace_stat/ free_buffer set_ftrace_notrace tracing_cpumask function_profile_enabled set_ftrace_notrace_pid tracing_max_latency hwlat_detector/ set_ftrace_pid tracing_on instances/ set_graph_function tracing_thresh kprobe_events set_graph_notrace uprobe_events kprobe_profile snapshot uprobe_profile 本文后面的示例假定你已经处在了/sys/kernel/tracing或/sys/kernel/debug/tracing目录下。\n# 函数跟踪 Ftrace 实际上代表的就是function trace（函数跟踪），因此函数追踪是Ftrace最初的一个主要功能。\nFtrace 可以跟踪几乎所有内核函数调用的详细信息，这是怎么做到的呢？简单来说，在编译内核的时候使用了 gcc 的 -pg 选项，编译器会在每个内核函数的入口处调用一个特殊的汇编函数“mcount” 或 “__fentry__”，如果跟踪功能被打开，mcount/fentry 会调用当前设置的 tracer，tracer将不同的数据写入ring buffer。\n从上图可以看出，Ftrace 提供的 function hooks 机制在内核函数入口处埋点，根据配置调用特定的 tracer， tracer将数据写入ring buffer。Ftrace实现了一个无锁的ring buffer，所有的跟踪信息都存储在ring buffer中。用户通过 tracefs 文件系统接口访问函数跟踪的输出结果。\n你可能已经意识到，如果每个内核函数入口都加入跟踪代码，必然会非常影响内核的性能，幸好Ftrace支持动态跟踪功能。如果启用了CONFIG_DYNAMIC_FTRACE选项，编译内核时所有的mcount/fentry调用点都会被收集记录。在内核的初始化启动过程中，会根据编译期记录的列表，将mcount/fentry调用点替换为NOP指令。NOP就是 no-operation，不做任何事，直接转到下一条指令。因此在没有开启跟踪功能的情况下，Ftrace不会对内核性能产生任何影响。在开启追踪功能时，Ftrace才会将NOP指令替换为mcount/fentry。\n启用函数追踪功能，只需要将 current_tracer 文件的内容设置为 \u0026ldquo;function\u0026rdquo;：\n# echo function \u003e current_tracer # cat trace | head -20 # tracer: function # # entries-in-buffer/entries-written: 204981/2728851 #P:4 # # _-----=\u003e irqs-off # / _----=\u003e need-resched # | / _---=\u003e hardirq/softirq # || / _--=\u003e preempt-depth # ||| / delay # TASK-PID CPU# |||| TIMESTAMP FUNCTION # | | | |||| | | sshd-1388 [000] .... 44388.890787: _cond_resched \u003c-__flush_work curl-7231 [001] .... 44389.399226: PageHuge \u003c-find_get_entry curl-7231 [001] .... 44389.399227: fsnotify_parent \u003c-vfs_read curl-7231 [001] .... 44389.399227: _cond_resched \u003c-copy_page_to_iter curl-7231 [001] .... 44389.399227: rcu_all_qs \u003c-_cond_resched curl-7231 [001] .... 44389.399228: vmacache_find \u003c-find_vma curl-7231 [001] .... 44389.399228: atime_needs_update \u003c-touch_atime curl-7231 [001] .... 44389.399228: current_time \u003c-atime_needs_update # echo nop \u003e current_tracer 文件头已经很好的解释了每一列的含义。前两项是被追踪的任务名称和 PID，大括号内是执行跟踪的CPU。TIMESTAMP 是启动后的时间，后面是被追踪的函数，它的调用者在 \u0026lt;- 之后。\n我们可以设置 set_ftrace_filter 选择想要跟踪的函数：\n# echo '*sleep' \u003e set_ftrace_filter # echo function \u003e current_tracer # cat trace_pipe sleep-9445 [001] .... 45978.125872: common_nsleep \u003c-__x64_sys_clock_nanosleep sleep-9445 [001] .... 45978.125873: hrtimer_nanosleep \u003c-common_nsleep sleep-9445 [001] .... 45978.125873: do_nanosleep \u003c-hrtimer_nanosleep cron-568 [002] .... 45978.504262: __x64_sys_clock_nanosleep \u003c-do_syscall_64 cron-568 [002] .... 45978.504264: common_nsleep \u003c-__x64_sys_clock_nanosleep cron-568 [002] .... 45978.504264: hrtimer_nanosleep \u003c-common_nsleep cron-568 [002] .... 45978.504264: do_nanosleep \u003c-hrtimer_nanosleep sleep-9448 [001] .... 45978.885085: __x64_sys_clock_nanosleep \u003c-do_syscall_64 sleep-9448 [001] .... 45978.885087: common_nsleep \u003c-__x64_sys_clock_nanosleep sleep-9448 [001] .... 45978.885087: hrtimer_nanosleep \u003c-common_nsleep # echo nop \u003e current_tracer # echo \u003e set_ftrace_filter trace_pipe 包含了与 trace 相同的输出，从这个文件的读取会返回一个无尽的事件流，它也会消耗事件，所以在读取一次后，它们就不再在跟踪缓冲区中了。\n也许你只想跟踪一个特定的进程，可以通过设置 set_ftrace_pid 内容为PID指定想追踪的特定进程。让 tracer 只追踪PID列在这个文件中的线程：\n# echo [PID] \u003e set_ftrace_pid # echo function \u003e current_tracer 如果设置了 function-fork 选项，那么当一个 PID 被列在 set_ftrace_pid 这个文件中时，其子任务的 PID 将被自动添加到这个文件中，并且子任务也将被 tracer 追踪。\n# echo function-fork \u003e trace_options 取消function-fork 选项：\n# echo nofunction-fork \u003e trace_options # cat trace_options ... noevent-fork nopause-on-trace function-trace nofunction-fork nodisplay-graph nostacktrace ... 取消 set_ftrace_pid 的设置：\n# echo \u003e set_ftrace_pid # Ftrace function_graph 文章开始例子已经展示过，function_graph 可以打印出函数的调用图，揭示代码的流程。function_graph 不仅跟踪函数的输入，而且跟踪函数的返回，这使得 tracer 能够知道被调用的函数的深度。function_graph 可以让人更容易跟踪内核的执行流程。\n我们再看一个例子：\n# echo try_to_wake_up \u003e set_graph_function # echo function_graph \u003e current_tracer # cat trace | head -20 # tracer: function_graph # # CPU DURATION FUNCTION CALLS # | | | | | | | 0) | try_to_wake_up() { 0) 1.083 us | ttwu_queue_wakelist(); 0) 0.622 us | update_rq_clock(); 0) | ttwu_do_activate() { 0) | enqueue_task_fair() { 0) | enqueue_entity() { 0) 0.616 us | update_curr(); 0) 0.602 us | update_cfs_group(); 0) 0.662 us | account_entity_enqueue(); 0) 0.652 us | place_entity(); 0) 0.697 us | __enqueue_entity(); 0) + 12.890 us | } 0) 0.672 us | hrtick_update(); 0) + 17.781 us | } 0) | ttwu_do_wakeup() { 0) | check_preempt_curr() { # echo nop \u003e current_tracer # echo \u003e set_graph_function 前面提到过，函数耗时大于 10 μs，前面会有 + 号提醒用户注意，其他的符号还有：\n$ ：延迟大于1秒 @ ：延迟大于 100 ms * ：延迟大于 10 ms # ：延迟大于 1 ms ! ：延迟大于 100 μs + ：延迟大于 10 μs # 函数Profiler 函数Profiler提供了内核函数调用的统计数据，可以观察哪些内核函数正在被使用，并能发现哪些函数的执行耗时最长。\n# echo nop \u003e current_tracer # echo 1 \u003e function_profile_enabled # echo 0 \u003e function_profile_enabled # echo \u003e set_ftrace_filter 这里有一个要注意的地方，确保使用的是 0 \u0026gt;，而不是 0\u0026gt;。这两者的含义不一样，0\u0026gt;是对文件描述符 0 的重定向。同样要避免使用 1\u0026gt;，因为这是对文件描述符 1 的重定向。\n现在可以从 trace_stat 目录中读取 profile 的统计数据。在这个目录中，profile 数据按照 CPU 保存在名为 function[n] 文件中。我使用的4核CPU，看一下profile 结果：\n# ls trace_stat/ function0 function1 function2 function3 # head trace_stat/function* ==\u003e trace_stat/function0 \u003c== Function Hit Time Avg s^2 -------- --- ---- --- --- tcp_sendmsg 202 3791.163 us 18.768 us 659.733 us tcp_sendmsg_locked 202 3521.863 us 17.434 us 638.307 us tcp_recvmsg 125 2238.773 us 17.910 us 1062.699 us tcp_push 202 2168.569 us 10.735 us 467.879 us tcp_write_xmit 47 2107.768 us 44.846 us 414.934 us tcp_v4_do_rcv 49 871.318 us 17.782 us 126.562 us tcp_send_ack 50 849.091 us 16.981 us 164.986 us tcp_rcv_established 49 827.212 us 16.881 us 117.427 us ==\u003e trace_stat/function1 \u003c== Function Hit Time Avg s^2 -------- --- ---- --- --- tcp_recvmsg 312 3110.497 us 9.969 us 281.015 us tcp_sendmsg 86 1412.005 us 16.418 us 370.310 us tcp_sendmsg_locked 86 1313.847 us 15.277 us 362.495 us tcp_send_ack 47 863.222 us 18.366 us 121.567 us tcp_v4_do_rcv 60 825.359 us 13.755 us 102.550 us tcp_write_xmit 28 807.609 us 28.843 us 336.106 us tcp_push 86 805.776 us 9.369 us 299.815 us tcp_rcv_established 60 777.510 us 12.958 us 99.129 us ==\u003e trace_stat/function2 \u003c== Function Hit Time Avg s^2 -------- --- ---- --- --- tcp_v4_rcv 1618 27858.95 us 17.218 us 253.487 us tcp_v4_do_rcv 1216 22528.58 us 18.526 us 226.243 us tcp_rcv_established 1184 20535.08 us 17.343 us 210.765 us tcp_send_ack 487 7558.698 us 15.520 us 111.035 us tcp_write_xmit 328 6281.810 us 19.151 us 656.192 us tcp_tasklet_func 162 4258.312 us 26.285 us 797.278 us tcp_ack 575 4148.714 us 7.215 us 27.061 us tcp_tsq_handler 162 4123.507 us 25.453 us 791.961 us ==\u003e trace_stat/function3 \u003c== Function Hit Time Avg s^2 -------- --- ---- --- --- tcp_recvmsg 567 5773.997 us 10.183 us 397.950 us tcp_send_ack 127 1881.700 us 14.816 us 133.317 us tcp_v4_do_rcv 133 1783.527 us 13.409 us 86.122 us tcp_rcv_established 133 1690.142 us 12.707 us 83.527 us tcp_sendmsg 54 1652.290 us 30.597 us 698.120 us tcp_sendmsg_locked 54 1574.276 us 29.153 us 666.451 us tcp_write_xmit 40 1184.827 us 29.620 us 354.719 us tcp_push 54 1129.465 us 20.916 us 486.157 us 第一行是每一列的名称，分别是函数名称（Function），调用次数（Hit），函数的总时间（Time）、平均函数时间（Avg）和标准差（s^2）。输出结果显示，tcp_sendmsg() 在3个 CPU 上都是最频繁的，tcp_v4_rcv() 在 CPU2 上被调用了1618次，平均延迟为 17.218 us。\n最后要注意一点，在使用 Ftrace Profiler 时，尽量通过 set_ftrace_filter 限制 profile 的范围，避免对所有的内核函数都进行 profile。\n# 追踪点 Tracepoints Tracepoints是内核的静态埋点。内核维护者在他认为重要的位置放置静态 tracepoints 记录上下文信息，方便后续排查问题。例如系统调用的开始和结束，中断被触发，网络数据包发送等等。\n在Linux的早期，内核维护者就一直想在内核中加入静态 tracepoints，尝试过各种策略。Ftrace 创造了Event Tracing 基础设施，让开发者使用 TRACE_EVENT() 宏添加内核 tracepoints，不用创建自定义内核模块，使用 Event Tracing 基础设施来注册埋点函数。\n现在内核中的Tracepoints都使用了 TRACE_EVENT() 宏来定义，tracepoints 记录的上下文信息作为 Trace events 进入 Event Tracing 基础设施，这样我们就可以复用 Ftrace 的 tracefs ，通过文件接口来配置 tracepoint events，并使用 trace 或 trace_pipe 文件查看事件输出。\n所有的 tracepoint events 的控制文件都在 events 目录下，按照类别以子目录形式组织：\n# ls -F events/ alarmtimer/ ftrace/ iwlwifi/ oom/ smbus/ block/ gpio/ iwlwifi_data/ page_isolation/ sock/ bpf_test_run/ gvt/ iwlwifi_io/ pagemap/ spi/ bridge/ hda/ iwlwifi_msg/ page_pool/ swiotlb/ btrfs/ hda_controller/ iwlwifi_ucode/ percpu/ sync_trace/ cfg80211/ hda_intel/ jbd2/ power/ syscalls/ cgroup/ header_event kmem/ printk/ task/ clk/ header_page kvm/ pwm/ tcp/ ... 我们以 events/sched/sched_process_fork 事件为例，该事件是在 include/trace/events/sched.h 中由 TRACE_EVENT 宏所定义：\n1 2 3 4 5 6 7 8 9 10 /* * Tracepoint for do_fork: */ TRACE_EVENT(sched_process_fork, TP_PROTO(struct task_struct *parent, struct task_struct *child), TP_ARGS(parent, child), ... ); TRACE_EVENT 宏会根据事件名称 sched_process_fork 生成 tracepoint 方法 trace_sched_process_fork()。你会在 kernel/fork.c 的 _do_fork() 中看到调用这个 tracepoint 方法。_do_fork() 是进程 fork 的主流程，在这里放置 tracepoint 是一个合适的位置，trace_sched_process_fork(current, p) 记录当前进程和 fork 出的子进程信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 /* * Ok, this is the main fork-routine. * * It copies the process, and if successful kick-starts * it and waits for it to finish using the VM if required. * * args-\u0026gt;exit_signal is expected to be checked for sanity by the caller. */ long _do_fork(struct kernel_clone_args *args) { ... p = copy_process(NULL, trace, NUMA_NO_NODE, args); add_latent_entropy(); /* * Do this prior waking up the new thread - the thread pointer * might get invalid after that point, if the thread exits quickly. */ trace_sched_process_fork(current, p); pid = get_task_pid(p, PIDTYPE_PID); ... } 在 events/sched/sched_process_fork 目录下，有这个事件的控制文件：\n# ls events/sched/sched_process_fork enable filter format hist id inject trigger 我们演示如何通过 enable 文件开启和关闭这个 tracepoint 事件：\n# echo 1 \u003e events/sched/sched_process_fork/enable # cat trace_pipe bash-14414 [000] .... 109721.823843: sched_process_fork: comm=bash pid=14414 child_comm=bash child_pid=24001 bash-14468 [002] .... 109730.405810: sched_process_fork: comm=bash pid=14468 child_comm=bash child_pid=24002 bash-14468 [002] .... 109737.925336: sched_process_fork: comm=bash pid=14468 child_comm=bash child_pid=24003 test.sh-24003 [000] .... 109737.968891: sched_process_fork: comm=test.sh pid=24003 child_comm=test.sh child_pid=24004 curl-24004 [002] .... 109737.975038: sched_process_fork: comm=curl pid=24004 child_comm=curl child_pid=24005 ... # echo 0 \u003e events/sched/sched_process_fork/enable 前五列分别是进程名称，PID，CPU ID，irqs-off 等标志位，timestamp 和 tracepoint 事件名称。其余部分是 tracepoint 格式字符串，包含当前这个 tracepoint 记录的重要信息。格式字符串可以在 events/sched/sched_process_fork/format 文件中查看：\n# cat events/sched/sched_process_fork/format name: sched_process_fork ID: 315 format: field:unsigned short common_type;\toffset:0;\tsize:2;\tsigned:0; field:unsigned char common_flags;\toffset:2;\tsize:1;\tsigned:0; field:unsigned char common_preempt_count;\toffset:3;\tsize:1;\tsigned:0; field:int common_pid;\toffset:4;\tsize:4;\tsigned:1; field:char parent_comm[16];\toffset:8;\tsize:16;\tsigned:1; field:pid_t parent_pid;\toffset:24;\tsize:4;\tsigned:1; field:char child_comm[16];\toffset:28;\tsize:16;\tsigned:1; field:pid_t child_pid;\toffset:44;\tsize:4;\tsigned:1; print fmt: \"comm=%s pid=%d child_comm=%s child_pid=%d\", REC-\u003eparent_comm, REC-\u003eparent_pid, REC-\u003echild_comm, REC-\u003echild_pid 通过这个 format 文件，我们可以了解这个 tracepoint 事件每个字段的含义。\n我们再演示一个使用 trigger 控制文件的例子：\n# echo 'hist:key=parent_pid' \u003e events/sched/sched_process_fork/trigger # [do some working] # cat events/sched/sched_process_fork/hist # event histogram # # trigger info: hist:keys=parent_pid:vals=hitcount:sort=hitcount:size=2048 [active] # { parent_pid: 572 } hitcount: 1 { parent_pid: 24494 } hitcount: 1 { parent_pid: 24497 } hitcount: 1 { parent_pid: 14414 } hitcount: 1 { parent_pid: 24505 } hitcount: 1 { parent_pid: 14053 } hitcount: 1 { parent_pid: 24527 } hitcount: 1 { parent_pid: 24501 } hitcount: 1 { parent_pid: 24510 } hitcount: 2 { parent_pid: 24508 } hitcount: 3 { parent_pid: 24493 } hitcount: 24 Totals: Hits: 37 Entries: 11 Dropped: 0 # remove triger # echo '!hist:key=parent_pid' \u003e events/sched/sched_process_fork/trigger 这个例子使用了 hist triggers，通过 sched_process_fork 事件来统计 _do_fork 的次数，并按照进程ID生成直方图。输出显示了 PID 24493 在追踪期间 fork 了24个子进程，最后几行显示了统计数据。\n关于 Hist Triggers 的详细介绍可以参考文档 Event Histograms。\n我的系统内核版本是 5.8.0-59-generic，当前可用的 tracepoints events 有2547个：\n# cat available_events btrfs:btrfs_transaction_commit btrfs:btrfs_inode_new btrfs:btrfs_inode_request btrfs:btrfs_inode_evict btrfs:btrfs_get_extent btrfs:btrfs_handle_em_exist btrfs:btrfs_get_extent_show_fi_regular btrfs:btrfs_truncate_show_fi_regular btrfs:btrfs_get_extent_show_fi_inline ... # cat available_events | wc -l 2547 Event Tracing 基础设施应该是 Ftrace 的另一大贡献，它提供的 TRACE_EVENT 宏统一了内核 tracepoint 的实现方式，为 tracepoint events 提供了基础支持。perf 的 tracepoint events 也是基于 Ftrace 实现的。\n# 利用 Tracepoints 理解内核代码 由于 tracepoints 是内核维护者在流程重要位置设置的埋点，因此我们可以从 tracepoints 入手来学习内核代码。所有的 tracepoints 都定义在 include/trace/events/ 目录下的头文件中，例如进程调度相关的 tracepoints 定义在 include/trace/events/sched.h中，我们以 sched_switch 为例：\n1 2 3 4 5 6 7 8 9 10 /* * Tracepoint for task switches, performed by the scheduler: */ TRACE_EVENT(sched_switch, TP_PROTO(bool preempt, struct task_struct *prev, struct task_struct *next), TP_ARGS(preempt, prev, next), TRACE_EVENT 宏会根据事件名称 sched_switch 生成 tracepoint 方法 trace_sched_switch()，在源码中查找该方法，发现在 kernel/sched/core.c 的 __schedule()中调用了trace_sched_switch() ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /* * __schedule() is the main scheduler function. *... */ static void __sched notrace __schedule(bool preempt) { ... if (likely(prev != next)) { rq-\u0026gt;nr_switches++; ... trace_sched_switch(preempt, prev, next); ... else { ... } balance_callback(rq); } 这样我们就找到了 scheduler 的主流程，可以从这里开始阅读进程调度的源码。\n# 写在最后 Ftrace 就包含在内核源码中 kernel/trace，理解了 Ftrace 内核不再是黑箱，你会有豁然开朗的感觉，内核源码忽然有条理了起来。让我们从 Ftrace 开始内核探索之旅吧。\n","date":"2021-06-21T11:44:02+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8Eftrace%E5%BC%80%E5%A7%8B%E5%86%85%E6%A0%B8%E6%8E%A2%E7%B4%A2%E4%B9%8B%E6%97%85/","title":"从Ftrace开始内核探索之旅"},{"content":"GDB（GNU Debugger）是Linux上的调试程序，可用于C/C++、Go、Rust等多种语言。GDB可以让你在被调试程序执行时看到它的”内部“情况，观察程序在特定断点上的状态，并逐行运行代码。\nGDB还提供了“远程”模式，使用GDB协议通过网络或串行设备与被调试程序进行通信。程序需要链接GDB提供的stub，这个stub实现了GDB协议。或者可以使用GDBserver，这时程序不需要进行任何更改。\n类似的，Linux内核开发者可以使用GDB的远程模式，与调试应用程序几乎相同的方式来调试Linux内核。KGDB是Linux内核的源代码级调试器，你可以使用GDB作为KGDB的前端，在我们熟悉且功能强大的GDB调试界面中调试内核。\n使用KGDB需要两台机器，一台作为开发机，另一台是目标机器，要调试的内核在目标机器上运行。在开发机上使用gdb运行包含符号信息的vmlinux，然后通过指定网络地址和端口，连接到目标机器的KGDB。我们也可以使用QEMU/KVM虚拟机作为目标机器，让待调试的内核运行在虚拟机中，然后在宿主机上运行gdb，连接到虚拟机中的KGDB。\n本文将介绍如何在本机搭建Linux内核调试环境，步骤比较繁琐，还会涉及到编译内核。作为内核小白，我会尽量写的详细些，毕竟我折腾了很久才成功。\n# 本机环境 我使用的Ubuntu 20.04.2 LTS，gdb版本为9.2。\n# 安装QEMU/KVM和Virsh Virsh是Virtual Shell的缩写，是一个用于管理虚拟机的命令行工具。你可以使用Virsh创建、编辑、启动、停止、关闭和删除VM。Virsh目前支持KVM，LXC，Xen，QEMU，OpenVZ，VirtualBox和VMware ESX。这里我们使用Virsh管理QEMU/KVM虚拟机。\n在安装之前，首先要确认你的CPU是否支持虚拟化技术。使用grep查看cpuinfo是否有\u0026quot;vmx\u0026quot;(Intel-VT 技术)或\u0026quot;svm\u0026quot;(AMD-V 支持)输出：\n1 egrep \u0026#34;(svm|vmx)\u0026#34; /proc/cpuinfo 某些CPU型号在默认情况下，BIOS中可能禁用了VT支持。我们需要再检查BIOS设置是否启用了VT的支持。使用kvm-ok命令进行检查：\n1 2 $ sudo apt install cpu-checker $ kvm-ok 如果输出为：\n1 2 INFO: /dev/kvm exists KVM acceleration can be used 证明CPU的虚拟化支持已经在BIOS中启用。\n运行下面的命令安装QEMU/KVM和Virsh：\n1 $ sudo apt install qemu-kvm libvirt-daemon-system libvirt-clients bridge-utils virtinst virt-manager 检查libvirt守护程序是否已经启动：\n1 2 $ sudo systemctl is-active libvirtd active 如果没有输出active，运行下面的命令启动libvertd服务：\n1 2 $ sudo systemctl enable libvirtd $ sudo systemctl start libvirtd # 创建虚拟机镜像 创建一个虚拟机镜像，大小为40G，qcow2 格式为动态分配磁盘占用空间。\n1 qemu-img create -f qcow2 ubuntutest.img 40G # 创建虚拟机，安装操作系统 使用下面的命令启动虚拟机，-cdrom参数为虚拟机挂载了Ubuntu的安装光盘：\n1 qemu-system-x86_64 -enable-kvm -name ubuntutest -m 4096 -hda ubuntutest.img -cdrom ubuntu-20.04.2-live-server-amd64.iso -boot d -vnc :19 我们使用VNC客户端连接进虚拟机，完成Ubuntu的安装。注意上面的命令通过-vnc :19设置了虚拟机的VNC监听端口为5919。\n我使用的VNC客户端是VNC Viewer，支持Windows、macOS和Linux等主流平台。按照正常步骤，完成Ubuntu在虚拟机上的安装。\n安装完成后，可以用ctrl+c退出qemu-system-x86_64命令的执行来停止虚拟机。再次启动虚拟机，需要把 -cdrom 参数去掉。\n1 qemu-system-x86_64 -enable-kvm -name ubuntutest -m 4096 -hda ubuntutest.img -boot d -vnc :19 # 配置虚拟机网络 为了让虚拟机能访问外部网络，我们需要形成下面的结构：\n在宿主机上创建网桥br0，并设置一个IP地址：\n1 2 3 $ sudo brctl addbr br0 $ sudo ip link set br0 up $ sudo ifconfig br0 192.168.57.1/24 编辑宿主机的/etc/sysctl.conf文件，设置IP转发生效：\n1 net.ipv4.ip_forward=1 使用sysctl -p重新加载sysctl.conf配置使其生效。\n在宿主机上增加SNAT规则。\n1 sudo iptables -t nat -A POSTROUTING -o wlp2s0 -j MASQUERADE 虚拟机的IP地址外部并不认识，如果它要访问外网，需要在数据包离开前将源地址替换为宿主机的IP，这样外部主机才能用宿主机的IP作为目的地址发回响应。\n上面的命令的含义是：在nat表的POSTROUTING链增加规则，出口设备为wlp2s0时，就执行MASQUERADE动作。MASQUERADE是一种源地址转换动作，它会动态选择宿主机的一个IP做源地址转换。\n注意上面命令中的 -o 参数，指定了数据包的出口设备为wlp2s0。你需要使用ip link命令在你的机器上查看具体设备的名称：\n如果想进一步了解iptables，可以参见我的另一篇文章《Docker单机网络模型动手实验》。\n接着我们需要将虚拟机的网卡连接到网桥br0。后面我们使用libvirt来管理QEMU/KVM虚拟机，这样可以把虚拟机的配置参数记录在XML文件中，易于维护。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 \u0026lt;domain type=\u0026#39;kvm\u0026#39; xmlns:qemu=\u0026#39;http://libvirt.org/schemas/domain/qemu/1.0\u0026#39;\u0026gt; \u0026lt;name\u0026gt;ubuntutest\u0026lt;/name\u0026gt; \u0026lt;uuid\u0026gt;0f0806ab-531d-6134-5def-c5b4955292aa\u0026lt;/uuid\u0026gt; \u0026lt;memory unit=\u0026#39;GiB\u0026#39;\u0026gt;4\u0026lt;/memory\u0026gt; \u0026lt;currentMemory unit=\u0026#39;GiB\u0026#39;\u0026gt;4\u0026lt;/currentMemory\u0026gt; \u0026lt;vcpu placement=\u0026#39;static\u0026#39;\u0026gt;2\u0026lt;/vcpu\u0026gt; \u0026lt;os\u0026gt; \u0026lt;type arch=\u0026#39;x86_64\u0026#39; machine=\u0026#39;pc-i440fx-trusty\u0026#39;\u0026gt;hvm\u0026lt;/type\u0026gt; \u0026lt;boot dev=\u0026#39;hd\u0026#39;/\u0026gt; \u0026lt;/os\u0026gt; \u0026lt;features\u0026gt; \u0026lt;acpi/\u0026gt; \u0026lt;apic/\u0026gt; \u0026lt;pae/\u0026gt; \u0026lt;/features\u0026gt; \u0026lt;clock offset=\u0026#39;utc\u0026#39;/\u0026gt; \u0026lt;on_poweroff\u0026gt;destroy\u0026lt;/on_poweroff\u0026gt; \u0026lt;on_reboot\u0026gt;restart\u0026lt;/on_reboot\u0026gt; \u0026lt;on_crash\u0026gt;restart\u0026lt;/on_crash\u0026gt; \u0026lt;devices\u0026gt; \u0026lt;emulator\u0026gt;/usr/bin/kvm\u0026lt;/emulator\u0026gt; \u0026lt;disk type=\u0026#39;file\u0026#39; device=\u0026#39;disk\u0026#39;\u0026gt; \u0026lt;driver name=\u0026#39;qemu\u0026#39; type=\u0026#39;qcow2\u0026#39;/\u0026gt; \u0026lt;source file=\u0026#39;/home/mazhen/works/ubuntutest.img\u0026#39;/\u0026gt; \u0026lt;target dev=\u0026#39;vda\u0026#39; bus=\u0026#39;virtio\u0026#39;/\u0026gt; \u0026lt;/disk\u0026gt; \u0026lt;controller type=\u0026#39;pci\u0026#39; index=\u0026#39;0\u0026#39; model=\u0026#39;pci-root\u0026#39;/\u0026gt; \u0026lt;interface type=\u0026#39;bridge\u0026#39;\u0026gt; \u0026lt;mac address=\u0026#39;fa:16:3e:6e:89:ce\u0026#39;/\u0026gt; \u0026lt;source bridge=\u0026#39;br0\u0026#39;/\u0026gt; \u0026lt;target dev=\u0026#39;tap1\u0026#39;/\u0026gt; \u0026lt;model type=\u0026#39;virtio\u0026#39;/\u0026gt; \u0026lt;/interface\u0026gt; \u0026lt;serial type=\u0026#39;pty\u0026#39;\u0026gt; \u0026lt;target port=\u0026#39;0\u0026#39;/\u0026gt; \u0026lt;/serial\u0026gt; \u0026lt;console type=\u0026#39;pty\u0026#39;\u0026gt; \u0026lt;target type=\u0026#39;serial\u0026#39; port=\u0026#39;0\u0026#39;/\u0026gt; \u0026lt;/console\u0026gt; \u0026lt;graphics type=\u0026#39;vnc\u0026#39; port=\u0026#39;5919\u0026#39; autoport=\u0026#39;no\u0026#39; listen=\u0026#39;0.0.0.0\u0026#39;\u0026gt; \u0026lt;listen type=\u0026#39;address\u0026#39; address=\u0026#39;0.0.0.0\u0026#39;/\u0026gt; \u0026lt;/graphics\u0026gt; \u0026lt;video\u0026gt; \u0026lt;model type=\u0026#39;cirrus\u0026#39;/\u0026gt; \u0026lt;/video\u0026gt; \u0026lt;/devices\u0026gt; \u0026lt;qemu:commandline\u0026gt; \u0026lt;qemu:arg value=\u0026#39;-s\u0026#39;/\u0026gt; \u0026lt;/qemu:commandline\u0026gt; \u0026lt;/domain\u0026gt; 我们可以看到，source file指定的文件/home/mazhen/works/ubuntutest.img就是虚拟机镜像。devices中的interface定义了虚拟网卡，br0是我们前面创建的网桥，libvirt帮我们创建的虚拟网卡会连接到网桥br0上。\n将XML文件保存为domain.xml，然后在libvirt定义虚拟机：\n1 $ virsh define domain.xml 接着我们可以使用virsh list --all查看虚拟机列表：\n1 2 3 4 $ virsh list --all Id Name State ----------------------------- - ubuntutest shut off 使用命令virsh start ubuntutest启动虚拟机：\n1 2 3 4 5 6 7 $ virsh start ubuntutest Domain ubuntutest started $ virsh list Id Name State ---------------------------- 1 ubuntutest running 这时我们使用VNC Viewer连接进行虚拟机，为虚拟机配置IP地址。虚拟机安装的是ubuntu-20.04.2，编辑/etc/netplan/00-installer-config.yaml文件配置IP地址。\n1 2 3 4 5 6 7 8 9 10 network: ethernets: ens3: addresses: [192.168.57.100/24] gateway4: 192.168.57.1 dhcp4: no nameservers: addresses: [114.114.114.114] optional: true version: 2 我们可以看到，网关配置的就是br0的IP地址。然后，使用命令 netplan apply让配置生效。这样，虚拟机的网络就配置好了，可以在虚拟机里访问到外网。这时我们就可以在宿主机上使用ssh登录虚拟机，这样比使用VNC Viewer操作更方便一些。\n# 下载Linux内核源码 在虚拟机上下载Linux内核源码：\n1 $ sudo apt install linux-source-5.4.0 ubuntu-20.04.2对应的内核版本是5.4。可以使用uname -srm查看内核版本。\n源码被下载到来/usr/src/目录下，使用下面的命令解压缩：\n1 sudo tar vjxkf linux-source-5.4.0.tar.bz2 内核源码被解压缩到了/usr/src/linux-source-5.4.0目录下。\n# 编译Linux内核 首先我们需要安装编译内核用到的依赖包：\n1 $ sudo apt install libncurses5-dev libssl-dev bison flex libelf-dev gcc make openssl libc6-dev 编译前要定义内核编译选项。进入/usr/src/linux-source-5.4.0目录，运行下面的命令，会进入内核参数配置界面：\n1 $ sudo make menuconfig 为了构建能够调试的内核，我们需要配置以下几个参数。\nCONFIG_DEBUG_INFO 在内核和内核模块中包含调试信息，这个选项在幕后为gcc使用的编译器参数增加了-g选项。 这个选项的菜单路径为：\n1 2 3 Kernel hacking ---\u0026gt; Compile-time checks and compiler options ---\u0026gt; [*] Compile the kernel with debug info 实际上通过菜单进行设置比较麻烦。我们保存设置退出后，配置会保存在.config文件中。直接编辑这个文件会更方便一些。在.config中确认CONFIG_DEBUG_INFO的设置正确。\n1 CONFIG_DEBUG_INFO=y CONFIG_FRAME_POINTER 这个选项会将调用帧信息保存在寄存器或堆栈上的不同位置，使gdb在调试内核时可以更准确地构造堆栈回溯跟踪（stack back traces）。 在.config中设置：\n1 CONFIG_FRAME_POINTER=y 启用CONFIG_GDB_SCRIPTS，但要关闭CONFIG_DEBUG_INFO_REDUCED。 1 2 CONFIG_GDB_SCRIPTS=y CONFIG_DEBUG_INFO_REDUCED=n CONFIG_KGDB 启用内置的内核调试器，该调试器允许进行远程调试。 1 CONFIG_KGDB=y 关闭CONFIG_RANDOMIZE_BASE设置 1 CONFIG_RANDOMIZE_BASE=n KASLR会更改引导时放置内核代码的基地址。如果你在内核配置中启用了KASLR（CONFIG_RANDOMIZE_BASE=y），则无法从gdb设置断点。\n设置完必要的内核参数后，我们开始编译内核：\n1 2 3 sudo make -j8 sudo make modules_install sudo make install 编译的过程很漫长，可能需要数小时。当编译完毕之后，新内核的选项已经增加到了grub的配置中。我们可以查看配置文件/boot/grub/grub.cfg确认：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 submenu \u0026#39;Advanced options for Ubuntu\u0026#39; $menuentry_id_option \u0026#39;gnulinux-advanced-5506d28f-c9e7-46d4-a12e-42555d491eec\u0026#39; { menuentry \u0026#39;Ubuntu, with Linux 5.4.106\u0026#39; --class ubuntu --class gnu-linux --class gnu --class os $menuentry_id_option \u0026#39;gnulinux-5.4.106-advanced-5506d28f-c9e7-46d4-a12e-42555d491eec\u0026#39; { recordfail load_video gfxmode $linux_gfx_mode insmod gzio if [ x$grub_platform = xxen ]; then insmod xzio; insmod lzopio; fi insmod part_gpt insmod ext2 if [ x$feature_platform_search_hint = xy ]; then search --no-floppy --fs-uuid --set=root 5506d28f-c9e7-46d4-a12e-42555d491eec else search --no-floppy --fs-uuid --set=root 5506d28f-c9e7-46d4-a12e-42555d491eec fi echo \u0026#39;Loading Linux 5.4.106 ...\u0026#39; linux /boot/vmlinuz-5.4.106 root=UUID=5506d28f-c9e7-46d4-a12e-42555d491eec ro maybe-ubiquity echo \u0026#39;Loading initial ramdisk ...\u0026#39; initrd /boot/initrd.img-5.4.106 } vmlinuz-5.4.106就是我们新编译的内核。\n重启虚拟机。在GRUB界面选择 Ubuntu 高级选项，选择第一项进去，就进入了新的内核。\n# 启用gdb监听端口 QEMU有个命令行参数-s，它代表参数-gdb tcp::1234，意思是QEMU监听 1234端口，这样gdb 可以 attach 到这个端口上，调试QEMU里面的内核。\n实际上在前面的domain.xml中我们已经为QEMU加了-s参数。\n1 2 3 4 5 6 \u0026lt;domain type=\u0026#39;kvm\u0026#39; xmlns:qemu=\u0026#39;http://libvirt.org/schemas/domain/qemu/1.0\u0026#39;\u0026gt; ... \u0026lt;qemu:commandline\u0026gt; \u0026lt;qemu:arg value=\u0026#39;-s\u0026#39;/\u0026gt; \u0026lt;/qemu:commandline\u0026gt; \u0026lt;/domain\u0026gt; 所以这时运行在虚拟机里的内核已经可以被调试了。\n# 调试内核 在宿主机上运行gdb需要内核的二进制文件，这个文件就是在虚拟机GRUB里配置的/boot/vmlinuz-5.4.106。为了方便在调试过程中查看源代码，我们可以将虚拟机的/usr/src/linux-source-5.4.0整个目录都拷贝到宿主机上来。\n1 $ scp -r mazhen@virtual-node:/usr/src/linux-source-5.4.0 ./ 在/usr/src/linux-source-5.4.0目录下面的vmlinux文件也是内核的二进制文件。\n为了能让gdb在启动时能够加载Linux helper脚本，需要在~/.gdbinit文件中添加如下内容：\n1 add-auto-load-safe-path /path/to/linux-build /path/to/linux-build就是上面从虚拟机拷贝过来的Linux源码目录。\n必要的配置完成后，就可以启动gdb了。\n在宿主机的./linux-source-5.4.0目录下执行gdb vmlinux。\n然后在gdb的交互环境下使用target remote :1234命令attach到虚拟机的内核。\n1 2 3 4 5 6 $ gdb vmlinux ... Reading symbols from vmlinux... (gdb) target remote :1234 Remote debugging using :1234 0xffffffff81ade35e in native_safe_halt () at ./arch/x86/include/asm/irqflags.h:60 如果我们想调试进程fork的过程，可以用b _do_fork设置断点：\n1 2 (gdb) b _do_fork Breakpoint 1 at 0xffffffff81098450: file kernel/fork.c, line 2362. 我们可以看到，断点设置成功。如果你不确认fork的具体方法名，可以使用info functions命令搜索符号表：\n1 2 3 4 5 (gdb) info function do_fork All functions matching regular expression \u0026#34;do_fork\u0026#34;: File kernel/fork.c: 2361:\tlong _do_fork(struct kernel_clone_args *); 使用命令c让内核继续执行：\n1 2 (gdb) c Continuing. 这时在虚拟机里执行任意命令，例如ls，断点将被触发：\n1 2 3 4 5 6 (gdb) c Continuing. Thread 1 hit Breakpoint 1, _do_fork (args=0xffffc9000095fee0) at kernel/fork.c:2362 2362\t{ (gdb) 我们可以使用n执行下一条语句：\n1 2 3 4 5 6 7 (gdb) n 2376\tif (!(clone_flags \u0026amp; CLONE_UNTRACED)) { (gdb) n 2377\tif (clone_flags \u0026amp; CLONE_VFORK) (gdb) n 2379\telse if (args-\u0026gt;exit_signal != SIGCHLD) (gdb) l显示多行源码：\n1 2 3 4 5 6 7 8 9 10 11 (gdb) l 2374\t* for the type of forking is enabled. 2375\t*/ 2376\tif (!(clone_flags \u0026amp; CLONE_UNTRACED)) { 2377\tif (clone_flags \u0026amp; CLONE_VFORK) 2378\ttrace = PTRACE_EVENT_VFORK; 2379\telse if (args-\u0026gt;exit_signal != SIGCHLD) 2380\ttrace = PTRACE_EVENT_CLONE; 2381\telse 2382\ttrace = PTRACE_EVENT_FORK; 2383\tbt查看函数调用信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 (gdb) bt #0 _do_fork (args=0xffffc9000095fee0) at kernel/fork.c:2379 #1 0xffffffff810989f4 in __do_sys_clone (tls=\u0026lt;optimized out\u0026gt;, child_tidptr=\u0026lt;optimized out\u0026gt;, parent_tidptr=\u0026lt;optimized out\u0026gt;, newsp=\u0026lt;optimized out\u0026gt;, clone_flags=\u0026lt;optimized out\u0026gt;) at kernel/fork.c:2544 #2 __se_sys_clone (tls=\u0026lt;optimized out\u0026gt;, child_tidptr=\u0026lt;optimized out\u0026gt;, parent_tidptr=\u0026lt;optimized out\u0026gt;, newsp=\u0026lt;optimized out\u0026gt;, clone_flags=\u0026lt;optimized out\u0026gt;) at kernel/fork.c:2525 #3 __x64_sys_clone (regs=\u0026lt;optimized out\u0026gt;) at kernel/fork.c:2525 #4 0xffffffff81003fd7 in do_syscall_64 (nr=\u0026lt;optimized out\u0026gt;, regs=0xffffc9000095ff58) at arch/x86/entry/common.c:290 #5 0xffffffff81c0008c in entry_SYSCALL_64 () at arch/x86/entry/entry_64.S:175 #6 0x00005621191e2da0 in ?? () #7 0x000056211a7de450 in ?? () #8 0x00007ffc9f31a3e0 in ?? () #9 0x0000000000000000 in ?? () p用于打印内部变量值：\n1 2 (gdb) p clone_flags $1 = 18874368 你现在可以像调试普通应用程序一样，调试Linux内核了！\n# 写在最后 在本机搭建Linux内核调试环境的步骤有点繁杂，但使用GDB能调试内核，会成为我们学习内核的利器，进程管理、内存管理、文件系统，对源码有什么困惑就可以debug一下。 Enjoy it!\n","date":"2021-05-21T11:37:22+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8gdb%E8%B0%83%E8%AF%95linux%E5%86%85%E6%A0%B8/","title":"使用GDB调试Linux内核"},{"content":"Atomikos是一个轻量级的分布式事务管理器，实现了Java Transaction API (JTA)规范，可以很方便的和Spring Boot集成，支持微服务场景下跨节点的全局事务。\n本文为一个微服务的示例应用，通过引入Atomikos增加全局事务能力。\n示例代码可以在这里查看。\n用户访问Business服务，它通过RPC调用分别调用Order和Storage创建订单和减库存。三个服务需要加入到一个全局事务中，要么全部成功，任何一个服务失败，都会造成事务回滚，数据的状态始终保持一致性。\n蚂蚁金服开源的Seata就是为了解决这类问题，在微服务架构下提供分布式事务服务。传统的应用服务器通过JTA/JTS也能解决分布式场景下的事务问题，但需要和EJB绑定在一起才能使用。Atomikos是一个独立的分布式事务管理器，原先是为Spring和Tomcat提供事务服务，让用户不必只为了事务服务而引入应用服务器。\n现在Atomikos也能为微服务提供分布式事务服务，这时主要需要两个问题：\n事务上下文如何通过RPC在服务间传播 微服务如何参与进两阶段提交协议的过程 后面会结合示例应用介绍Atomikos是如何解决这两个问题。示例应用atomkos-sample的结构如下：\napi：定义了服务接口OrderService和StorageService order-service：OrderService的具体实现 storage-service：StorageService的具体实现 business-service：用户访问入口 # 事务上下文的传播 在项目主工程的pom文件中引入Atomikos依赖，注意要包括transactions-remoting，正是它才能让事务上下文在RPC调用时传递。\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.atomikos\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;transactions-remoting\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.0.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; transactions-remoting支持jaxrs，Spring Remoting和Spring rest等几种RPC方式，我们使用的是Spring Remoting。\n以order-service为例，通过TransactionalHttpInvokerServiceExporter将OrderService发布为远程服务：\n1 2 3 4 5 6 7 @Bean(name = \u0026#34;/services/order\u0026#34;) TransactionalHttpInvokerServiceExporter orderService(OrderServiceImpl orderService) { TransactionalHttpInvokerServiceExporter exporter = new TransactionalHttpInvokerServiceExporter(); exporter.setService(orderService); exporter.setServiceInterface(OrderService.class); return exporter; } OrderService的调用者business-service使用HttpInvokerProxyFactoryBean引入远程服务：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @Bean public HttpInvokerProxyFactoryBean orderService() { HttpInvokerProxyFactoryBean orderService = new HttpInvokerProxyFactoryBean(); orderService.setHttpInvokerRequestExecutor(httpInvokerRequestExecutor()); orderService.setServiceUrl(\u0026#34;http://localhost:8082/services/order\u0026#34;); orderService.setServiceInterface(OrderService.class); return orderService; } @Bean public TransactionalHttpInvokerRequestExecutor httpInvokerRequestExecutor() { TransactionalHttpInvokerRequestExecutor httpInvokerRequestExecutor = new TransactionalHttpInvokerRequestExecutor(); return httpInvokerRequestExecutor; } business-service负责发起全局事务，它使用Spring标准的@Transactional标记方法开启事务：\n1 2 3 4 5 @Transactional public void createOrder(String userId, String commodityCode, Integer count) { orderService.create(userId, commodityCode, count); storageService.deduct(commodityCode, count); } Atomikos提供了TransactionalHttpInvokerRequestExecutor和TransactionalHttpInvokerServiceExporter拦截请求和响应，利用HTTP header传递事务上下文。\nbusiness-service在调用远程服务OrderService时，请求发送前会经过TransactionalHttpInvokerRequestExecutor.prepareConnection处理，增加HTTP header，携带事务上下文：\n1 2 3 4 5 6 7 @Override protected void prepareConnection(HttpURLConnection con, int contentLength) throws IOException { String propagation = template.onOutgoingRequest(); con.setRequestProperty(HeaderNames.PROPAGATION_HEADER_NAME, propagation); super.prepareConnection(con, contentLength); } OrderService会使用TransactionalHttpInvokerServiceExporter.decorateInputStream进行请求拦截，能从HTTP header中解析出事务上下文：\n1 2 3 4 5 6 7 8 9 10 11 @Override protected InputStream decorateInputStream(HttpServletRequest request, InputStream is) throws IOException { try { String propagation = request.getHeader(HeaderNames.PROPAGATION_HEADER_NAME); template.onIncomingRequest(propagation); } catch (IllegalArgumentException e) { ... } return super.decorateInputStream(request, is); } OrderService处理完成返回响应时，会将该节点加入全局事务包装成Event，放入HTTP header返回给business-service：\n1 2 3 4 5 6 7 8 9 10 11 12 @Override protected OutputStream decorateOutputStream(HttpServletRequest request, HttpServletResponse response, OutputStream os) throws IOException { ... response.addHeader(HeaderNames.EXTENT_HEADER_NAME, extent); ... return super.decorateOutputStream(request, response, os); } business-service接收到响应，利用TransactionalHttpInvokerRequestExecutor.validateResponse解析出Event，注册进事务管理器，这样在全局事务提交时，可以让该分支参与到两阶段提交协议：\n1 2 3 4 5 6 7 @Override protected void validateResponse(HttpInvokerClientConfiguration config, HttpURLConnection con) throws IOException { super.validateResponse(config, con); String extent = con.getHeaderField(HeaderNames.EXTENT_HEADER_NAME); template.onIncomingResponse(extent); } # 两阶段提交过程 在处理RPC调用的响应时，Atomikos会将参与到全局事务的远程节点注册为Participants(Extent.addRemoteParticipants)，在事务提交时，所有的Participants都会参与到两阶段提交：\n1 2 3 4 5 6 7 8 9 10 11 12 13 synchronized ( fsm_ ) { if ( commit ) { if ( participants_.size () \u0026lt;= 1 ) { commit ( true ); } else { int prepareResult = prepare (); // make sure to only do commit if NOT read only if ( prepareResult != Participant.READ_ONLY ) commit ( false ); } } else { rollback (); } 可以看出，如果Participants大于1，会走prepare和commit两阶段提交的完整过程。那么OrderService和StorageService如何参与进两阶段提交呢？\nAtomikos提供了REST入口com.atomikos.remoting.twopc.AtomikosRestPort，你可以将AtomikosRestPort注册到JAX-RS，例如本示例选择的是Apache CFX，在application.properties进行配置：\n1 2 3 cxf.path=/api cxf.jaxrs.classes-scan=true cxf.jaxrs.classes-scan-packages=com.atomikos.remoting.twopc business-service在进行全局事务提交时，会访问所有Participants相应的REST接口进行两阶段提交：\nbusiness-service是怎么知道AtomikosRestPort的访问地址的呢？上面提到了，business-service在访问OrderService时，返回的响应header中包含了Event，地址就随着Event返回给了调用者。AtomikosRestPort的访问地址配置在jta.properties中：\n1 com.atomikos.icatch.rest_port_url=http://localhost:8082/api/atomikos 至此，我们解释清楚了Atomikos如何为微服务提供分布式事务服务的，主要解决了两个问题：事务上下文如何通过RPC在服务间传播，以及微服务如何参与进两阶段提交协议的过程。\n下一步我准备为Atomikos增加dubbo的支持，即事务上下文可以通过dubbo进行传播。\n","date":"2020-05-15T11:30:26+08:00","permalink":"https://mazhen.tech/p/atomikos%E5%9C%A8%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%9C%BA%E6%99%AF%E4%B8%8B%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"Atomikos在微服务场景下的使用"},{"content":" # 应用服务器的分布式事务支持 我们先看一下分布式事务的需求是如何产生的，以及应用服务器是如何支持分布式事务管理的。\n单体应用 首先看单体应用，所有的模块部署在一个应用服务器上，业务数据都保存在单个数据库中，这种场景本地事务就可以满足需求。\n数据库水平拆分 如果数据库按照业务模块进行水平拆分，完成一个业务请求会涉及到跨库的资源访问和更新，这时候就需要使用应用服务器的JTA进行两阶段提交，保证跨库操作的事务完整性。\n应用模块拆分 应用按照业务模块进一步拆分，每一个模块都作为EJB，部署在独立的应用服务器中。完成一个业务请求会跨越多个应用服务器节点和资源，如何在这种场景保证业务操作的事务呢？当访问入口EJB时JTA会自动开启全局事务，事务上下文随着EJB的远程调用在应用服务器之间传播，让被调用的EJB也加入到全局事务中。\n这就是应用因拆分而遇到分布式事务的问题，以及应用服务器是如何解决这个问题的。\n# 分布式事务中间件 微服务时代，没人再使用沉重的EJB，都是将Spring Bean直接暴露为远程服务。完成一个业务请求需要跨越多个微服务，同样需要面对分布式事务的问题。这时就需要引入分布式事务中间件。我们以蚂蚁金服开源的Seata为例，看看它是怎么解决微服务场景下的分布式事务问题。\n将上一小节跑在应用服务器上的业务，使用微服务 + Seata的重构后，部署架构如下：\n上图中黄色方框（RM，TM，TC）是Seata的核心组件，它们配合完成对微服务的分布式事务支持。可以看出，和应用服务器的EJB方案架构上类似，只是多了一个独立运行的TC组件。\n我们再看看Seata各组件的具体作用。\n# Seata的架构 Seata由三个组件构成：\nTransaction Coordinator (TC)： 事务协调器，维护全局事务的运行状态，负责协调并驱动全局事务的提交或回滚。 Transaction Manager (TM)： 控制全局事务的边界，负责开启一个全局事务，并最终发起全局提交或全局回滚的决议。 Resource Manager (RM)： 控制分支事务，负责分支注册、状态汇报，并接收事务协调器的指令，驱动分支（本地）事务的提交和回滚。 # Seata vs. 应用服务器 Seata和应用服务器的分布式事务支持主要有以下四个差异：\nSeata和应用服务器都可以实现业务无侵入分布式事务支持。但应用服务器的XA方案实现的是实时一致性，而Seata的AT 模式实现的是最终一致性。 Seata引入了独立运行的Transaction Coordinator，维护全局事务的运行状态。而应用服务器的访问入口节点承担了维护全局事务状态的职责。 Seata自己实现了Resource Manager，不需要依赖数据库的XA driver。这样就有可能将没有实现XA接口的资源加入的分布式事务中，例如NoSQL。同时，RM的实现要比JTA中的XAResource复杂很多。RM需要拦截并解析SQL，生成回滚语句，在事务rollback时自动进行数据还原。XAResource是对XA driver的包装，资源参与分布式事务的能力，都是由数据库提供的。 事务上下文的传播机制不同。应用服务器使用标准的RMI-IIOP协议进行事务上下文的跨节点传播。Seata是对各种RPC框架提供了插件，拦截请求和响应，事务上下文随着RPC调用进行跨节点传播。目前Seata已经支持了dubbo、gRPC、Motan和sofa-rpc等多种RPC框架。 Seata和应用服务器都支持在分布式场景下的全局事务，都可以做到对业务无侵入。Seata实现的是最终一致性，因此性能比应用服务器的XA方案好很多，具备海量并发处理能力，这也是互联网公司选择它的原因。由于Seata不依赖数据库的XA driver，只使用数据库的本地事务，就完成了对分布式事务的支持，相当于承担了部分数据库的职责，因此Seata的实现难度要比应用服务器的JTA大。\n# 应用服务器进入微服务时代 那么应用服务器的分布式事务支持在微服务时代还有用吗？或者说我们应该怎样改进，才能让应用服务器进入微服务时代？\n首先我们要看到JTA/XA的优势：支持数据的实时一致性，对业务开发更加友好。客户对原有的系统进行微服务改造时，如果把业务模型假定成数据最终一致性，客户就不得不做出很大的妥协和变更。特别是有些金融客户对一致性的要求会比较高。\n我们可以学习Seata的架构，抛弃掉沉重的EJB/RMI-IIOP，让Spring Bean通过dubbo等RPC框架直接对外暴露服务，同时事务上下文可以在RPC调用时进行传递：\n我们甚至可以将JTA独立出来，和Tomcat这样的Web容器整合，为微服务架构提供分布式事务支持。相信通过这样的改造，应用服务器的分布式事务能力在微服务时代又能焕发第二春。\n","date":"2020-04-21T11:25:09+08:00","permalink":"https://mazhen.tech/p/%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E6%94%AF%E6%8C%81%E5%92%8Cseata%E7%9A%84%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90/","title":"应用服务器的分布式事务支持和Seata的对比分析"},{"content":" # 性能分析工具的分类 性能分析的技术和工具可以分为以下几类：\nCounters 内核维护着各种统计信息，被称为Counters，用于对事件进行计数。例如，接收的网络数据包数量，发出的磁盘I/O请求，执行的系统调用次数。常见的这类工具有：\nvmstat: 虚拟和物理内存统计 mpstat: CPU使用率统计 iostat：磁盘的I/O使用情况 netstat：网络接口统计信息，TCP/IP协议栈统计信息，连接统计信息 Tracing Tracing是收集每个事件的数据进行分析。Tracing会捕获所有的事件，因此有比较大的CPU开销，并且可能需要大量存储来保存数据。\n常见的Tracing工具有：\ntcpdump: network packet tracing blktrace: block I/O tracing perf: Linux Performance Events, 跟踪静态和动态探针 strace: 系统调用tracing gdb: 源代码级调试器 Profiling Profiling 是通过收集目标行为的样本或快照，来了解目标的特征。Profiling可以从多个方面对程序进行动态分析，如CPU、Memory、Thread、I/O等，其中对CPU进行Profiling的应用最为广泛。\nCPU Profiling原理是基于一定频率对运行的程序进行采样，来分析消耗CPU时间的代码路径。可以基于固定的时间间隔进行采样，例如每10毫秒采样一次。也可以设置固定速率采样，例如每秒采集100个样本。\nCPU Profiling经常被用于分析代码的热点，比如“哪个方法占用CPU的执行时间最长”、“每个方法占用CPU的比例是多少”等等，然后我们就可以针对热点瓶颈进行分析和性能优化。\nLinux上常用的CPU Profiling工具有：\nperf的 record 子命令 BPF profile Monitoring 系统性能监控会记录一段时间内的性能统计信息，以便能够基于时间周期进行比较。这对于容量规划，了解高峰期的使用情况都很有帮助。历史值还为我们理解当前的性能指标提供了上下文。\n监控单个操作系统最常用工具是sar（system activity reporter，系统活动报告）命令。sar通过一个定期执行的agent来记录系统计数器的状态，并可以使用sar命令查看它们，例如：\n1 2 3 4 5 6 7 8 9 10 $ sar Linux 4.15.0-88-generic (mazhen) 03/19/2020 _x86_64_\t(4 CPU) 12:53:08 PM LINUX RESTART 12:55:01 PM CPU %user %nice %system %iowait %steal %idle 01:05:01 PM all 14.06 0.00 10.97 0.11 0.00 74.87 01:15:01 PM all 9.60 0.00 7.49 0.09 0.00 82.83 01:25:01 PM all 0.04 0.00 0.02 0.02 0.00 99.92 01:35:01 PM all 0.03 0.00 0.02 0.01 0.00 99.94 本文主要讨论如何使用perf和BPF进行CPU Profiling。\n# perf perf最初是使用Linux性能计数器子系统的工具，因此perf开始的名称是Performance Counters for Linux(PCL)。perf在Linux2.6.31合并进内核，位于tools/perf目录下。\n随后perf进行了各种增强，增加了tracing、profiling等能力，可用于性能瓶颈的查找和热点代码的定位。\nperf是一个面向事件（event-oriented）的性能剖析工具，因此它也被称为Linux perf events (LPE)，或perf_events。\nperf的整体架构如下：\nperf 由两部分组成：\nperf Tools：perf用户态命令，为用户提供了一系列工具集，用于收集、分析性能数据。 perf Event Subsystem：Perf Events是内核的子系统之一，和用户态工具共同完成数据的采集。 内核依赖的硬件，比如说CPU，一般会内置一些性能统计方面的寄存器（Hardware Performance Counter），通过软件读取这些特殊寄存器里的信息，我们也可以得到很多直接关于硬件的信息。perf最初就是用来监测CPU的性能监控单元（performance monitoring unit, PMU）的。\n# perf Events分类 perf支持多种性能事件：\n这些性能事件分类为：\nHardware Events: CPU性能监控计数器performance monitoring counters（PMC），也被称为performance monitoring unit（PMU） Software Events: 基于内核计数器的底层事件。例如，CPU迁移，minor faults，major faults等。 Kernel Tracepoint Events: 内核的静态Tracepoint，已经硬编码在内核需要收集信息的位置。 User Statically-Defined Tracing (USDT): 用户级程序的静态Tracepoint。 Dynamic Tracing: 用户自定义事件，可以动态的插入到内核或正在运行中的程序。Dynamic Tracing技术分为两类： kprobes：对于kernel的动态追踪技术，可以动态地在指定的内核函数的入口和出口等位置上放置探针，并定义自己的探针处理程序。 uprobes：对于用户态软件的动态追踪技术，可以安全地在用户态函数的入口等位置设置动态探针，并执行自己的探针处理程序。 可以使用perf的list子命令查看当前可用的事件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 $ sudo perf list List of pre-defined events (to be used in -e): branch-instructions OR branches [Hardware event] branch-misses [Hardware event] bus-cycles [Hardware event] cache-misses [Hardware event] cache-references [Hardware event] cpu-cycles OR cycles [Hardware event] ... alignment-faults [Software event] bpf-output [Software event] context-switches OR cs [Software event] cpu-clock [Software event] cpu-migrations OR migrations [Software event] ... alarmtimer:alarmtimer_cancel [Tracepoint event] alarmtimer:alarmtimer_fired [Tracepoint event] alarmtimer:alarmtimer_start [Tracepoint event] alarmtimer:alarmtimer_suspend [Tracepoint event] block:block_bio_backmerge [Tracepoint event] block:block_bio_bounce [Tracepoint event] ... # perf的使用 如果还没有安装perf，可以使用apt或yum进行安装：\n1 sudo apt install linux-tools-$(uname -r) linux-tools-generic perf的功能强大，支持硬件计数器统计，定时采样，静态和动态tracing等。本文只介绍几个常用的使用场景，如果想全面的了解perf的使用，可以参考perf.wiki。\nCPU Statistics 使用perf的stat命令可以收集性能计数器统计信息，精确统计一段时间内 CPU 相关硬件计数器数值的变化。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 -\u0026gt; % sudo perf stat dd if=/dev/zero of=/dev/null count=10000000 10000000+0 records in 10000000+0 records out 5120000000 bytes (5.1 GB, 4.8 GiB) copied, 12.2795 s, 417 MB/s Performance counter stats for \u0026#39;dd if=/dev/zero of=/dev/null count=10000000\u0026#39;: 12280.299325 task-clock (msec) # 1.000 CPUs utilized 16 context-switches # 0.001 K/sec 0 cpu-migrations # 0.000 K/sec 70 page-faults # 0.006 K/sec 41,610,802,323 cycles # 3.388 GHz 20,195,746,887 instructions # 0.49 insn per cycle 3,972,723,471 branches # 323.504 M/sec 90,061,565 branch-misses # 2.27% of all branches 12.280445133 seconds time elapsed CPU Profiling 可以使用perf record以任意频率收集快照。这通常用于CPU使用情况的分析。\nsudo perf record -F 99 -a -g sleep 10 对所有CPU（-a）进行call stacks（-g）采样，采样频率为99 Hertz（-F 99），即每秒99次，持续10秒（sleep 10）。\nsudo perf record -F 99 -a -g -p PID sleep 10 对指定进程（-p PID）进行采样。\nsudo perf record -F 99 -a -g -e context-switches -p PID sleep 10 perf可以和各种instrumentation points一起使用，以跟踪内核调度程序（scheduler）的活动。其中包括software events和tracepoint event（静态探针）。\n上面的例子对指定进程的上下文切换（-e context-switches）进行采样。\nreport perf record的运行结果保存在当前目录的perf.data文件中，采样结束后，我们使用perf report查看结果。\n交互式查看模式 1 $ sudo perf report 以+开头的行可以回车，展开详细信息。\n使用--stdio选项打印所有输出 1 $ sudo perf report --stdio context-switches的采样报告：\n后面我们会介绍火焰图，以可视化的方式展示stack traces，比perf report更加直观。\n# BPF BPF是Berkeley Packet Filter的缩写，最初是为BSD开发，第一个版本于1992年发布，用于改进网络数据包捕获的性能。BPF是在内核级别进行过滤，不必将每个数据包拷贝到用户空间，从而提高了数据包过滤的性能。tcpdump使用的就是BPF。\n2013年BPF被重写，被称为Extended BPF (eBPF)，于2014年包含进Linux内核中。改进后的BPF成为了通用执行引擎，可用于多种用途，包括创建高级性能分析工具。\nBPF允许在内核中运行mini programs，来响应系统和应用程序事件（例如磁盘I/O事件）。这种运作机制和JavaScript类似：JavaScript是运行在浏览器引擎中的mini programs，响应鼠标点击等事件。BPF使内核可编程化，使用户（包括非内核开发人员）能够自定义和控制他们的系统，以解决实际问题。\nBPF可以被认为是一个虚拟机，由指令集，存储对象和helper函数三部分组成。BPF指令集由位于Linux内核的BPF runtime执行，BPF runtime包括了解释器和JIT编译器。BPF是一种灵活高效的技术，可以用于networking，tracing和安全等领域。我们重点关注它作为系统监测工具方面的应用。\n和perf一样，BPF能够监测多种性能事件源，同时可以通过调用perf_events，使用perf已有的功能：\nBPF可以在内核运行计算和统计汇总，这样大大减少了复制到用户空间的数据量：\nBPF已经内置在Linux内核中，因此你无需再安装任何新的内核组件，就可以在生产环境中使用BPF。\n# BCC和bpftrace 直接使用BPF指令进行编程非常繁琐，因此很有必要提供高级语言前端方便用户使用，于是就出现了BCC和bpftrace。\nBCC（BPF Compiler Collection） 提供了一个C编程环境，使用LLVM工具链来把 C 代码编译为BPF虚拟机所接受的字节码。此外它还支持Python，Lua和C++作为用户接口。\nbpftrace 是一个比较新的前端，它为开发BPF工具提供了一种专用的高级语言。bpftrace适合单行代码和自定义短脚本，而BCC更适合复杂的脚本和守护程序。\nBCC和bpftrace没有在内核代码库，它们存放在GitHub上名为IO Visor的Linux Foundation项目中。\niovisor/bcc iovisor/bpftrace # BCC的安装 BCC可以参考官方的安装文档。以Ubuntu 18.04 LTS为例，建议从源码build安装：\n安装build依赖 1 2 3 4 sudo apt-get -y install bison build-essential cmake flex git libedit-dev \\ libllvm6.0 llvm-6.0-dev libclang-6.0-dev python zlib1g-dev libelf-dev sudo apt-get -y install luajit luajit-5.1-dev 编译和安装 1 2 3 4 5 git clone https://github.com/iovisor/bcc.git mkdir bcc/build; cd bcc/build cmake .. make sudo make install build python3 binding 1 2 3 4 5 cmake -DPYTHON_CMD=python3 .. pushd src/python/ make sudo make install popd make install完成后，BCC自带的工具都安装在了/usr/share/bcc/tools目录下。BCC已经包含70多个BPF工具，用于性能分析和故障排查。这些工具都可以直接使用，无需编写任何BCC代码。\n我们试用其中一个工具biolatency，跟踪磁盘I/O延迟：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 -\u0026gt; % sudo /usr/share/bcc/tools/biolatency Tracing block device I/O... Hit Ctrl-C to end. ^C usecs : count distribution 0 -\u0026gt; 1 : 0 | | 2 -\u0026gt; 3 : 0 | | 4 -\u0026gt; 7 : 0 | | 8 -\u0026gt; 15 : 0 | | 16 -\u0026gt; 31 : 2 |*** | 32 -\u0026gt; 63 : 0 | | 64 -\u0026gt; 127 : 3 |***** | 128 -\u0026gt; 255 : 7 |*********** | 256 -\u0026gt; 511 : 6 |********** | 512 -\u0026gt; 1023 : 11 |****************** | 1024 -\u0026gt; 2047 : 16 |************************** | 2048 -\u0026gt; 4095 : 24 |****************************************| 4096 -\u0026gt; 8191 : 1 |* | 8192 -\u0026gt; 16383 : 6 |********** | 16384 -\u0026gt; 32767 : 3 |***** | biolatency展示的直方图比iostat的平均值能更好的理解磁盘I/O性能。\nBCC已经自带了CPU profiling工具：\ntools/profile: Profile CPU usage by sampling stack traces at a timed interval. 此外，BCC还提供了Off-CPU的分析工具：\ntools/offcputime: Summarize off-CPU time by kernel stack trace 一般的CPU profiling都是分析on-CPU，即CPU时间都花费在了哪些代码路径。off-CPU是指进程不在CPU上运行时所花费的时间，进程因为某种原因处于休眠状态，比如说等待锁，或者被进程调度器（scheduler）剥夺了 CPU 的使用。这些情况都会导致这个进程无法运行在 CPU 上，但是仍然花费了时间。\noff-CPU分析是对on-CPU的补充，让我们知道线程所有的时间花费，更全面的了解程序的运行情况。\n后面会介绍profile，offcputime如何生成火焰图进行可视化分析。\n# bpftrace的安装 bpftrace 建议运行在Linux 4.9 kernel或更高版本。根据安装文档的说明，是因为kprobes、uprobes、tracepoints等主要特性是在4.x以上加入内核的：\n4.1 - kprobes 4.3 - uprobes 4.6 - stack traces, count and hist builtins (use PERCPU maps for accuracy and efficiency) 4.7 - tracepoints 4.9 - timers/profiling 可以运行scripts/check_kernel_features.sh脚本进行验证：\n1 2 $ ./scripts/check_kernel_features.sh All required features present! bpftrace对Linux的版本要求较高，以Ubuntu为例，19.04及以上才支持apt安装：\n1 sudo apt-get install -y libbpfcc-dev 18.04和18.10可以从源码build，但需要先build好BCC。\n安装依赖 1 2 3 sudo apt-get update sudo apt-get install -y bison cmake flex g++ git libelf-dev zlib1g-dev libfl-dev systemtap-sdt-dev binutils-dev sudo apt-get install -y llvm-7-dev llvm-7-runtime libclang-7-dev clang-7 编译和安装 1 2 3 4 5 git clone https://github.com/iovisor/bpftrace mkdir bpftrace/build; cd bpftrace/build; cmake -DCMAKE_BUILD_TYPE=Release .. make -j8 sudo make install make install完成后，bpftrace自带的工具安装在/usr/local/share/bpftrace/tools目录下，这些工具的说明文档可以在项目主页找到。\n我们同样试用查看Block I/O延迟直方图的工具：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 -\u0026gt; % sudo bpftrace /usr/local/share/bpftrace/tools/biolatency.bt Attaching 4 probes... Tracing block device I/O... Hit Ctrl-C to end. ^C @usecs: [128, 256) 6 |@@@@@@@@@@ | [256, 512) 4 |@@@@@@ | [512, 1K) 8 |@@@@@@@@@@@@@ | [1K, 2K) 20 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ | [2K, 4K) 30 |@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@| [4K, 8K) 1 |@ | [8K, 16K) 3 |@@@@@ | [16K, 32K) 0 | | [32K, 64K) 2 |@@@ | 关于bpftrace脚本编写不在本文的讨论范围，感兴趣的可以参考reference_guide。\n# 火焰图 火焰图是Brendan Gregg发明的将stack traces可视化展示的方法。火焰图把时间和空间两个维度上的信息融合在一张图上，将频繁执行的代码路径以可视化的形式，非常直观的展现了出来。\n火焰图可以用于可视化来自任何profiler工具的记录的stack traces信息，除了用来CPU profiling，还适用于off-CPU，page faults等多种场景的分析。本文只讨论 on-CPU 和 off-CPU 火焰图的生成。\n要理解火焰图，先从理解Stack Trace开始。\n# Stack Trace Stack Trace是程序执行过程中，在特定时间点的函数调用列表。例如，func_a()调用func_b()，func_b()调用func_c()，此时的Stack Trace可写为：\n1 2 3 func_c func_b func_a # Profiling Stack Traces 我们做CPU profiling时，会使用perf或bcc定时采样Stack Trace，这样会收集到非常多的Stack Trace。前面介绍了perf report会将Stack Trace样本汇总为调用树，并显示每个路径的百分比。火焰图是怎么展示的呢？\n考虑下面的示例，我们用perf定时采样收集了多个Stack Trace，然后将相同的Stack Trace归纳合并，统计出次数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func_e func_d func_b func_a 1 func_b func_a 2 func_c func_b func_a 7 可以看到，总共收集了10个样本，其中代码路径func_a-\u0026gt;func_b-\u0026gt;func_c有7次，该路径上的func_c在CPU上运行。 func_a-\u0026gt;func_b进行了两次采样，func_b在CPU上运行。func_a-\u0026gt;func_b-\u0026gt;func_d-\u0026gt;func_e一次采样，func_e在CPU上运行。\n# 火焰图 根据前面对Stack Trace的统计信息，可以绘制出如下的火焰图：\n火焰图具有以下特性：\n每个长方块代表了函数调用栈中的一个函数 Y 轴显示堆栈的深度（堆栈中的帧数）。调用栈越深，火焰就越高。顶层方块表示 CPU 上正在运行的函数，下面的函数即为它的祖先。 X 轴的宽度代表被采集的样本数量，越宽表示采集到的越多，即执行的时间长。需要注意的是，X轴从左到右不代表时间，而是所有的调用栈合并后，按字母顺序排列的。 拿到火焰图，寻找最宽的塔并首先了解它们。顶层的哪个函数占据的宽度最大，说明它可能存在性能问题。\n可以使用Brendan Gregg开发的开源项目FlameGraph生成交互式的SVG火焰图。该项目提供了脚本，可以将采集的样本归纳合并，统计出Stack Trace出现的频率，然后使用flamegraph.pl生成SVG火焰图。\n我们先把FlameGraph项目clone下来，后面会用到：\n1 git clone https://github.com/brendangregg/FlameGraph.git # Java CPU Profiling 虽然有很多Java专用的profiler工具，但这些工具一般只能看到Java方法的执行，缺少了GC，JVM的CPU时间消耗，并且有些工具的Method tracing性能损耗比较大。\nperf和BCC profile的优点是它很高效，在内核上下文中对堆栈进行计数，并能完整显示用户态和内核态的CPU使用，能看到native libraries（例如libc），JVM（libjvm），Java方法和内核中花费的时间。\n但是，perf和BCC profile这种系统级的profiler不能很好地与Java配合使用，它们识别不了Java方法和stack traces。这是因为：\nJVM的JIT（just-in-time）没有给系统级profiler公开符号表 JVM还使用帧指针寄存器（frame pointer register，x86-64上的RBP）作为通用寄存器，打破了传统的堆栈遍历 为了能生成包含Java栈与Native栈的火焰图，目前有两种解决方式：\n使用JVMTI agent perf-map-agent，生成Java符号表，供perf和bcc读取（/tmp/perf-PID.map）。同时要加上-XX:+PreserveFramePointer JVM 参数，让perf可以遍历基于帧指针（frame pointer）的堆栈。 使用async-profiler，该项目将perf的堆栈追踪和JDK提供的AsyncGetCallTrace结合了起来，同样能够获得mixed-mode火焰图。同时，此方法不需要启用帧指针，所以不用加上-XX:+PreserveFramePointer参数。 下面我们就分别演示这两种方式。\n# perf-map-agent perf期望能从/tmp/perf-\u0026lt;pid\u0026gt;.map中获得在未知内存区域执行的代码的符号表。perf-map-agent可以为JIT编译的方法生成/tmp/perf-\u0026lt;pid\u0026gt;.map文件，以满足perf的要求。\n首先下载并编译perf-map-agent：\n1 2 3 4 git clone https://github.com/jvm-profiling-tools/perf-map-agent.git cd perf-map-agent cmake . make # 配合perf使用 perf-map-agent提供了perf-java-flames脚本，可以一步生成火焰图。\nperf-java-flames接收perf record命令参数，它会调用perf进行采样，然后使用FlameGraph生成火焰图，一步完成，非常方便。\n注意，记得要给被profiling的Java进程加上-XX:+PreserveFramePointer JVM 参数。\n设置必要的环境变量：\n1 2 export FLAMEGRAPH_DIR=[FlameGraph 所在的目录] export PERF_RECORD_SECONDS=[采样时间] ./bin/perf-java-flames [PID] -F 99 -a -g -p [PID] 对指定进程（-p PID），在所有CPU（-a）上进行call stacks（-g）采样，采样频率为99 Hertz （-F 99），持续时间为PERF_RECORD_SECONDS秒。命令运行完成后，会在当前目录生成名为flamegraph-pid.svg的火焰图。\n./bin/perf-java-flames [PID] -F 99 -g -a -e context-switches -p [PID] 对指定进程的上下文切换（-e context-switches）进行采样，并生成火焰图。\n当然也可以只为perf生成Java符号表，然后直接使用perf采样 1 2 3 4 5 6 ./bin/create-java-perf-map.sh [PID]; sudo perf record -F 99 -p [PID] -a -g -- sleep 15 ./bin/create-java-perf-map.sh [PID]; sudo perf record -g -a -e context-switches -p [PID] sleep 15 # 查看报告 sudo perf report --stdio # 配合bcc profile使用 FlameGraph项目提供了jmaps脚本，它会调用perf-map-agent为当前运行的所有Java进程生成符号表。\n首先为jmaps脚本设置好JAVA_HOME和perf-map-agent的正确位置：\n1 2 JAVA_HOME=${JAVA_HOME:-/usr/lib/jvm/java-8-oracle} AGENT_HOME=${AGENT_HOME:-/usr/lib/jvm/perf-map-agent} # from https://github.com/jvm-profiling-tools/perf-map-agent 运行jmaps，可以看到它会为当前所有的Java进程生成符号表：\n1 2 3 4 $ sudo ./jmaps Fetching maps for all java processes... Mapping PID 30711 (user adp): wc(1): 3486 10896 214413 /tmp/perf-30711.map 我们在做任何profiling之前，都需要调用jmaps，保持符号表是最新的。\nCPU Profiling火焰图 1 2 3 4 5 # Profiling sudo ./jmaps ; sudo /usr/share/bcc/tools/profile -dF 99 -afp [PID] 10 \u0026gt; out.profile01.txt # 生成火焰图 ./flamegraph.pl --color=java --hash \u0026lt;out.profile01.txt \u0026gt; flamegraph.svg off-CPU火焰图 1 2 3 4 5 # Profiling sudo ./jmaps ; sudo /usr/share/bcc/tools/offcputime -fp [PID] 10 \u0026gt; out.offcpu01.txt # 生成火焰图 ./flamegraph.pl --color=java --bgcolor=blue --hash --countname=us --width=1024 --title=\u0026#34;Off-CPU Time Flame Graph\u0026#34; \u0026lt; out.offcpu01.txt \u0026gt; out.offcpu01.svg off-CPU，并过滤指定的进程状态 Linux的进程状态有：\n状态 描述 TASK_RUNNING 意味着进程处于可运行状态。这并不意味着已经实际分配了CPU。进程可能会一直等到调度器选中它。该状态确保进程可以立即运行，而无需等待外部事件。 TASK_INTERRUPTIBLE 可中断的等待状态，主要为恢复时间无法预测的长时间等待。例如等待来自用户的输入。 TASK_UNINTERRUPTIBLE 不可中断的等待状态。用于因内核指示而停用的睡眠进程。它们不能由外部信号唤醒，只能由内核亲自唤醒。例如磁盘输入输出等待。 TASK_STOPPED 响应暂停信号而运行中断的状态。直到恢复前都不会被调度 TASK_ZOMBIE 僵尸状态，子进程已经终止，但父进程尚未执行wait()，因此该进程的资源没有被系统释放。 在状态TASK_RUNNING（0）会发生非自愿上下文切换，而我们通常感兴趣的阻塞事件是TASK_INTERRUPTIBLE（1）或TASK_UNINTERRUPTIBLE（2），offcputime可以用--state过滤指定的进程状态：\n1 2 3 4 5 # Profiling sudo ./jmaps ; sudo /usr/share/bcc/tools/offcputime -K --state 2 -f 30 \u0026gt; out.offcpu01.txt # 生成火焰图 ./flamegraph.pl --color=io --countname=ms \u0026lt; out.offcpu01.txt \u0026gt; out.offcpu01.svg # async-profiler async-profiler将perf的堆栈追踪和JDK提供的AsyncGetCallTrace结合了起来，做到同时采样Java栈与Native栈，因此也就可以同时分析Java代码和Native代码中存在的性能热点。\nAsyncGetCallTrace是JDK内部提供的一个函数，它的原型如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 typedef struct { jint lineno; // BCI in the source file jmethodID method_id; // method executed in this frame } ASGCT_CallFrame; typedef struct { JNIEnv *env_id //Env where trace was recorded jint num_frames; // number of frames in this trace ASGCT_CallFrame *frames; } ASGCT_CallTrace; void AsyncGetCallTrace(ASGCT_CallTrace *trace, // pre-allocated trace to fill jint depth, // max number of frames to walk up the stack void* ucontext) // signal context 可以看出，该函数直接通过ucontext就能获取到完整的Java调用栈。\n# async-profiler的使用 下载并解压好async-profiler安装包。\n从Linux 4.6开始，从non-root进程使用perf捕获内核的call stacks，需要设置如下两个内核参数：\n1 2 # echo 1 \u0026gt; /proc/sys/kernel/perf_event_paranoid # echo 0 \u0026gt; /proc/sys/kernel/kptr_restrict async-profiler的使用非常简单，一步就能生成火焰图。另外，也不需要为被profiling的Java进程设置-XX:+PreserveFramePointer参数。\n1 ./profiler.sh -d 30 -f /tmp/flamegraph.svg [PID] # 总结 为Java生成CPU profiling火焰图，基本的流程都是：\n使用工具采集样本 使用FlameGraph项目提供的脚本，将采集的样本归纳合并，统计出Stack Trace出现的频率 最后使用flamegraph.pl利用上一步的输出，绘制SVG火焰图 为了能够生成Java stacks和native stacks完整的火焰图，解决perf和bcc profile不能识别Java符号和Java stack traces的问题，目前有以下两种方式：\nperf-map-agent 加上 perf或bcc profile async-profiler（内部会使用到perf） 如果只是对Java进程做on-CPU分析，async-profiler更加方便好用。如果需要更全面的了解Java进程的运行情况，例如分析系统锁的开销，阻塞的 I/O 操作，以及进程调度器（scheduler）的工作，那么还是需要使用功能更强大的perf和bcc。\n# 参考资料 perf Examples Linux Extended BPF (eBPF) Tracing Tools BPF Performance Tools (book) Off-CPU Analysis Flame Graphs ","date":"2020-03-23T11:01:58+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8%E7%81%AB%E7%84%B0%E5%9B%BE%E8%BF%9B%E8%A1%8Cjava%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/","title":"使用火焰图进行Java性能分析"},{"content":" Previous Next \u0026nbsp; \u0026nbsp; Page: / ","date":"2020-02-07T10:44:10+08:00","permalink":"https://mazhen.tech/p/consensus-and-distributed-transactions/","title":"Consensus and Distributed Transactions"},{"content":"在上一篇\u0026lt;Tomcat系统架构\u0026gt;中提到，Tomcat的网络通信层支持多种 I/O 模型。本文将介绍NioEndpoint，它是直接使用NIO实现了 I/O 多路复用。\n# NioEndpoint的处理流程 NioEndpoint的处理流程如下：\nAcceptor实现了Runnable接口，运行在一个独立的线程中。Acceptor的run方法在循环中调用ServerSocketChannel.accept()，将返回的SocketChannel包装成NioSocketWrapper，然后将NioSocketWrapper注册进Poller。\nPoller同样实现了Runnable接口，运行在一个独立的线程中。Poller的核心任务是检测I/O事件，它在无限循环中调用Selector.select()，会得到准备就绪的NioSocketWrapper列表，为每个NioSocketWrapper生成一个SocketProcessor任务，然后把任务扔进线程池Executor去处理。\nExecutor是可配置的线程池，负责运行SocketProcessor任务。SocketProcessor实现了Runnable接口，在run方法中会调用ConnectionHandler.process(NioSocketWrapper, SocketEvent)处理当前任务关联的NioSocketWrapper。\nConnectionHandler内部使用一个ConcurrentHashMap建立了NioSocketWrapper和Processor之间的映射。从上一篇\u0026lt;Tomcat系统架构\u0026gt;的介绍我们知道，Processor负责应用层协议的解析，那么我们需要为每个NioSocketWrapper创建并关联一个Processor。\n为什么要建立NioSocketWrapper和Processor之间的关联呢？因为Processor在从NioSocketWrapper中读取字节流进行协议解析时，数据可能并不完整，这时需要释放工作线程，当Poller再次触发I/O读取事件时，可以根据NioSocketWrapper找回关联的Processor，继续进行未完成的协议解析工作。\nProcessor解析的结果是生成Tomcat的Request对象，然后调用Adapter.service(request, response)方法。Adapter的职责是将Tomcat的Request对象转换为标准的ServletRequest后，传递给Servlet引擎，最终会调用到用户编写的Servlet.service(ServletRequest, ServletResponse)。\n# NioEndpoint的线程模型 我们注意到，在Tomcat 9的实现中，Acceptor和Poller都只有一个线程，并且不可配置。Poller检测到的I/O事件会被扔进Executor线程池中处理，最终Servlet.service也是在Executor中执行。这是一种常见的NIO线程模型，将I/O事件的检测和处理分开在不同的线程。\n但这种处理方式也有缺点。当Selector检测到数据就绪事件时，运行Selector线程的CPU已经在CPU cache中缓存了数据。这时切换到另外一个线程去读，这个读取线程很可能运行在另一个CPU核，此前缓存在CPU cache中的数据就没用了。同时这样频繁的线程切换也增加了系统内核的开销。\n同样是基于NIO，Jetty使用了不同的线程模型：线程自己产生的I/O事件，由当前线程处理，\u0026ldquo;Eat What You Kill\u0026rdquo;，同时，Jetty可能会新建一个新线程继续检测和处理I/O事件。\n这篇博客详细的介绍了Jetty的 \u0026ldquo;Eat What You Kill\u0026rdquo; 策略。Jetty也支持类似Tomcat的ProduceExecuteConsume策略，即I/O事件的产出和消费用不同的线程处理。\nExecuteProduceConsume策略，也就是 \u0026ldquo;Eat What You Kill\u0026rdquo;，I/O事件的生产者自己消费任务。\nJetty对比了这两种策略，使用ExecuteProduceConsume能达到更高的吞吐量。\n其实，Netty也使用了和 \u0026ldquo;Eat What You Kill\u0026rdquo; 类似的线程模型。\nChannel注册到EventLoop，一个EventLoop能够服务多个Channel。EventLoop仅在一个线程上运行，因此所有I/O事件均由同一线程处理。\n# blocking write的实现 当通过Response向客户端返回数据时，最终会调用NioSocketWrapper.write(boolean block, ByteBuffer from)或NioSocketWrapper.write(boolean block, byte[] buf, int off, int len)，将数据写入socket。\n我们注意到write方法的第一个参数block，它决定了write是使用blocking还是non-blocking方式。比较奇怪，虽然是NioEndpoint，但write动作也不全是non-blocking。\n一般NIO框架在处理write时都是non-blocking方式，先尝试SocketChannel.write(ByteBuffer)，如果buffer.remaining() \u0026gt; 0，将剩余数据以某种方式缓存，然后把SelectionKey.OP_WRITE添加到SelectionKey的interest set，等待被Selector触发时再次尝试写出，直到buffer中没有剩余数据。\n那是什么因素决定了NioSocketWrapper.write是blocking还是non-blocking呢？\n我们看一下Http11OutputBuffer.isBlocking的实现：\n1 2 3 4 5 6 7 /** * Is standard Servlet blocking IO being used for output? * @return \u0026lt;code\u0026gt;true\u0026lt;/code\u0026gt; if this is blocking IO */ protected final boolean isBlocking() { return response.getWriteListener() == null; } 如果response.getWriteListener()不为null，说明我们注册了WriteListener接收write事件的通知，这时我们肯定是在使用异步Servlet。\n也就是说，当我们使用异步Servlet时，才会使用NioSocketWrapper.write的non-blocking方式，普通的Servlet都是使用blocking方式的write。\nNioEndpoint在实现non-blocking的write时和一般的NIO框架类似，那它是如何实现blocking方式的write呢？\nTomcat的NIO connector有一个配置参数selectorPool.shared。selectorPool.shared的缺省值为true，这时会创建一个运行在独立线程中BlockPoller。调用者在发起blocking write时，会将SocketChannel注册到这个BlockPoller中，然后await在一个CountDownLatch上。当BlockPoller检测到准备就绪的SocketChannel，会通过关联的CountDownLatch唤醒被阻塞的调用者。这时调用者尝试往SocketChannel中写入，如果buffer中还有剩余数据，那么会再把SocketChannel注册回BlockPoller，并继续await，重复前面的过程，直到数据完全写出，最后调用者从blocking的write方法返回。\n当设置selectorPool.shared为false时，NioEndpoint会为每个发起blocking write的线程创建一个Selector，执行和上面类似的过程。当然NioEndpoint会使用NioSelectorPool来缓存Selector，并不是每次都创建一个新的Selector。NioSelectorPool中缓存的Selector的最大数量由selectorPool.maxSelectors参数控制。\n至此，相信你对NioEndpoint的内部实现已经有了整体的了解。\n","date":"2019-11-23T17:29:34+08:00","permalink":"https://mazhen.tech/p/tomcat%E7%9A%84nioendpoint%E5%AE%9E%E7%8E%B0%E5%88%86%E6%9E%90/","title":"Tomcat的NioEndpoint实现分析"},{"content":" # Tomcat系统架构图 从架构图可以看出，顶层组件Server代表一个Tomcat Server实例，一个Server中有一个或者多个Service，每个Service有多个Connector，以及一个Engine。\nConnector和Engine是Tomcat最核心的两个组件。\nConnector负责处理网络通信，以及应用层协议(HTTP，AJP)的解析，生成标准的ServletRequest和ServletResponse对象，然后传递给Engine处理。每个Connector监听不同的网络端口。\nEngine代表整个Servlet引擎，可以包含多个Host，表示它可以管理多个虚拟站点。Host代表的是一个虚拟主机，而一个虚拟主机下可以部署多个Web应用程序，Context表示一个Web应用程序。Wrapper表示一个Servlet，一个Web应用程序中可能会有多个Servlet。\n从Tomcat的配置文件server.xml也能看出Tomcat的系统架构设计。\n1 2 3 4 5 6 7 8 9 10 \u0026lt;Server\u0026gt; \u0026lt;Service\u0026gt; \u0026lt;Connector /\u0026gt; \u0026lt;Connector /\u0026gt; \u0026lt;Engine\u0026gt; \u0026lt;Host\u0026gt; \u0026lt;/Host\u0026gt; \u0026lt;/Engine\u0026gt; \u0026lt;/Service\u0026gt; \u0026lt;/Server\u0026gt; # Connector 我们再仔细看一下Connector的内部实现。\nEndpoint 负责网络通信 Processor 实现应用层协议(HTTP，AJP)解析 Adapter 将Tomcat的Request/Response转换为标准的ServletRequest/ServletResponse Tomcat的网络通信层支持多种 I/O 模型：\nNIO：使用Java NIO实现 NIO.2：异步I/O，使用JDK NIO.2实现 APR：使用了Apache Portable Runtime (APR)实现 Tomcat实现支持了多种应用层协议：\nHTTP/1.1 HTTP/2 AJP：二进制协议，Web Server和Tomcat之间的通信协议 Processor解析网络字节流生成Tomcat的Request对象后，会调用Adapter.service(request, response)方法。Adapter是Servlet引擎的入口，Adapter负责将Tomcat的Request对象转换为标准的ServletRequest，然后再调用Servlet引擎的service方法。\n# ProtocolHandler Tomcat允许一个Engine对接多个Connector，每个Connector可以使用不同的 I/O 模型，实现不同的应用层协议解析。Connector屏蔽了 I/O 模型和协议的区别，传递给Engine的是标准的ServletRequest/ServletResponse对象。\n由于 I/O 模型和应用层协议解析可以自由组合，Tomcat使用ProtocolHandler实现这种组合。各种组合都有相应的具体实现类。比如：Http11NioProtocol 和 AjpNio2Protocol。\n关于NioEndpoint和Nio2Endpoint组件的内部实现，会在后续文章进行分析。\n","date":"2019-11-21T17:22:56+08:00","permalink":"https://mazhen.tech/p/tomcat%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E7%AE%80%E4%BB%8B/","title":"Tomcat系统架构简介"},{"content":"程序员的工作需要长期坐在电脑前，可能还会经常熬夜，作息时间不规律，所以码农是身体素质比较差的一群人。因此程序员健身锻炼的首要目标，不是为了有好看的身材，而是让你精力充沛，面对高强度工作游刃有余，同时还有精力去享受生活，这才是最关键的。\n# 心肺系统的重要性 这一切的基础是你必须要有一个好的心血管系统。心脏和全身的血管组成了心血管系统。通过持续不间断地跳动，心脏把富含氧气的血液不断输送到人体各个部位，并且将各个部位产生的废物和有害物质带到相应的排泄器官排出体外，这才维持起人体的各项机能。\n心血管系统就好比汽车的发动机，没了发动机，其他零件再好也没有用，这个发动机是最重要的。\n如果通过运动，让心肌得到有效的锻炼，那受益的是整个心血管系统。至于体重的減轻、体型的变化，这些都是随时而来的副产品。\n# 自测心肺功能水平 那么如何锻炼才能有效的增强心血管系统呢？\n首先我们要知道自己当前心血管系统的水平如何。一个专业的指标叫做最大摄氧量(VO2 max)。这个指标可以看出你现在心血管和心肺功能的水平。简单来说，最大摄氧量就是你在运动中能获取的最大氧气量。这个指标越高，说明你的心血管系统、心肺功能越好。\n对一个正常成年人，男性这个指标达到40，女性达到36才算是及格。普通人54以上可以算是优秀，而职业长跑运动员，最大摄氧量指标能达到88以上。\n如何测量自己的最大摄氧量呢？一般专业的心率表都会有这个指标。例如Apple watch可以通过你的体能训练，估算出最大摄氧量。\niOS系统自带的“健康”App里能查看到这个指标。\n# 合适强度的运动改善心肺功能 了解了自己当前的心肺功能水平后，如何提高改善你的心肺功能呢？需要选择合适的运动强度，不能一上来就强度过大。如果平时缺乏锻炼，心肺功能不达标，高强度的训练是比较危险的。\n建议使用“卡氏公式”计算一下你合适的运动心率区间。这个心率区间是和你的年龄，以及早上起来的静态心率状况有关。\n适合心肺功能训练的卡氏公式\n1 2 3 心肺训练心率 = (220 - 年龄 − 静态心率)×(55% ~ 65%) + 静态心率 如果是刚开始健身，比较推荐你在跑步机上进行走路。因为跑步机的速度是恒定的，把跑步机调成上坡的时候，你会发现很容易达到你想要达到的心率，只要在这个心率范围之内，就是你最合适的运动区间。随着你的心肺能力不断提高，你会逐渐提高坡度和速度。\n# 如何有效减脂 健身锻炼除了能充沛精力，另外一个大家关心的问题是，如何通过运动有效的减脂呢？\n实际上，饮食才是最有效的控制体重的方法。对于想减脂的人来说，最好的办法就是控制好你糖和脂肪的摄入，然后多吃一些蛋白质类食物，这样你会有足够的饱腹感。\n此外，可以做一些低强度运动，身体会消耗更多的脂肪。\n为什么低强度的运动能消耗脂肪呢？因为脂肪的消耗是需要氧气参与，运动过程中必须有充足的氧气才能消耗脂肪，因此这个运动强度应该比心肺训练的强度更低。\n减脂的运动强度就是卡氏公式的35%到55%，在这个强度运动是消耗脂肪最多的。\n适合减脂训练的卡氏公式\n1 2 3 减脂训练心率 = (220 - 年龄 − 静态心率)×(35% ~ 55%) + 静态心率 最后，男性的健康体脂率应该是15%到20%，女性的健康体脂率是20%到25%。推荐买一个体脂秤测试跟踪自己的体脂率变化。\n","date":"2019-11-11T10:36:27+08:00","permalink":"https://mazhen.tech/p/%E7%BB%99%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E5%81%A5%E8%BA%AB%E9%94%BB%E7%82%BC%E6%8C%87%E5%8D%97/","title":"给程序员的健身锻炼指南"},{"content":" # Outline 回顾HTTP的发展简史，理解HTTP在设计上的关键转变，以及每次转变的动机\nHTTP简史 HTTP/1.1的主要特性和问题 HTTP/2 的核心概念、主要特性 HTTP/2 的升级与发现 HTTP/2 的问题及展望 # HTTP简史 HTTP(HyperText Transfer Protocol，超文本传输协议)是互联网上最普遍采用的一种应用协议 由欧洲核子研究委员会CERN的英国工程师Tim Berners-Lee在1991年发明 Tim Berners-Lee也是WWW的发明者 # HTTP简史 HTTP/0.9：只有一行的协议 请求只有一行，包括GET方法和要请求的文档的路径 响应是一个超文本文档，没有首部，也没有其他元数据，只有HTML 服务器与客户端之间的连接在每次请求之后都会关闭 HTTP/0.9的设计目标传递超文本文档 # HTTP简史 HTTP/0.9演示 1 2 3 4 5 6 7 8 9 10 11 $\u0026gt; telnet apache.org 80 Trying 95.216.24.32... Connected to apache.org. Escape character is \u0026#39;^]\u0026#39;. GET /foundation/ \u0026lt;!DOCTYPE html\u0026gt; ... Connection closed by foreign host. # HTTP简史 1996年HTTP工作组发布了RFC 1945，这就是HTTP/1.0 提供请求和响应的各种元数据 不局限于超文本的传输，响应可以是任何类型：HTML文件、图片、音频等 支持内容协商、内容编码、字符集、认证、缓存等 从超文本到超媒体传输 # HTTP简史 HTTP/1.0演示 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $\u0026gt; telnet apache.org 80 Trying 95.216.24.32... Connected to apache.org. GET /foundation/ HTTP/1.0 Accept: */* HTTP/1.1 200 OK Server: Apache/2.4.18 (Ubuntu) Content-Length: 46012 Connection: close Content-Type: text/html \u0026lt;!DOCTYPE html\u0026gt; ... Connection closed by foreign host. # HTTP简史 1997年1月定义HTTP/1.1标准的RFC 2068发布 1999年6月RFC 2616发布，取代了RFC 2068 性能优化 持久连接 除非明确告知，默认使用持久连接 分块编码传输 请求管道，支持并行请求处理（应用的非常有限） 增强的缓存机制 # HTTP简史 HTTP/1.1演示 1 2 3 4 5 6 7 8 9 10 11 12 13 14 \u0026gt;$ telnet www.baidu.com 80 Trying 14.215.177.38... Connected to www.a.shifen.com. GET /s?wd=http2 HTTP/1.1 Accept: text/html,application/xhtml+xml,application/xml Accept-Language: zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7 Connection: keep-alive Host: www.baidu.com HTTP/1.1 200 OK Connection: Keep-Alive Content-Type: text/html;charset=utf-8 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Date: Sun, 06 Oct 2019 12:49:28 GMT Server: BWS/1.1 Transfer-Encoding: chunked ffa \u0026lt;!DOCTYPE html\u0026gt; ... 1be7 ... 0 GET /img/bd_logo1.png HTTP/1.1 Accept: text/html,application/xhtml+xml,application/xml Accept-Language: zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7 Connection: close Host: www.baidu.com HTTP/1.1 200 OK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Content-Length: 7877 Content-Type: image/png Date: Sun, 06 Oct 2019 13:05:06 GMT Etag: \u0026#34;1ec5-502264e2ae4c0\u0026#34; Expires: Wed, 03 Oct 2029 13:05:06 GMT Last-Modified: Wed, 03 Sep 2014 10:00:27 GMT Server: Apache Set-Cookie: BAIDUID=0D01C3C9C00A6019C16F79CAEB1EFE91:FG=1 Connection: close . . . . . . Connection closed by foreign host. # HTTP简史 Google在2009年发布了实验性协议SPDY，主要目标是解决HTTP/1.1的性能限制 Google工程师在09年11月分享了实验结果 A 2x Faster Web So far we have only tested SPDY in lab conditions. The initial results are very encouraging: when we download the top 25 websites over simulated home network connections, we see a significant improvement in performance - pages loaded up to 55% faster.\n2012年，SPDY得到Chrome、Firefox和Opera的支持 HTTP-WG(HTTP Working Group)开始在SPDY的基础上制定官方标准 # HTTP简史 2015年正式发布HTTP/2 主要目标：改进传输性能，降低延迟，提高吞吐量 保持原有的高层协议语义不变 根据W3Techs的报告，截止2019年11月，全球已经有 42.1% 的网站开启了HTTP/2 # HTTP简史 Google在2012年设计开发了QUIC协议，让HTTP不再基于TCP 2018年底，HTTP/3标准发布 HTTP/3协议业务逻辑变化不大，可以简单理解为 HTTP/2 + QUIC # HTTP/1.1 持久连接 # HTTP/1.1 持久连接 # HTTP/1.1 持久连接 非持久HTTP连接的固定时间成本 至少两次网络往返： 握手、请求和响应 服务处理速度越快，固定延迟的影响就越大 持久连接避免TCP连接时的三次握手，消除TCP的慢启动 # HTTP/1.1 管道 多次请求必须满足先进先出(FIFO)的顺序 # HTTP/1.1 管道 尽早发送请求，不被每次响应阻塞 # HTTP/1.1 管道 HTTP/1.1的局限性 只能严格串行地返回响应，不允许一个连接上的多个响应交错到达 管道的问题 并行处理请求时，服务器必须缓冲管道中的响应，占用服务器资源 由于失败可能导致重复处理，非幂等的方法不能pipeline化 由于中间代理的兼容性，可能会破坏管道 管道的应用非常有限 # HTTP/1.1 的协议开销 每个HTTP请求都会携带500~800字节的header 如果使用了cookie，每个HTTP请求会增加几千字节的协议开销 HTTP header以纯文本形式发送，不会进行任何压缩 某些时候HTTP header开销会超过实际传输的数据一个数量级 例如访问RESTful API时返回JSON格式的响应 # HTTP/1.1性能优化建议 由于HTTP/1.1不支持多路复用 浏览器支持每个主机打开多个连接（例如Chrome是6个） 应用使用多域名，将资源分散到多个子域名 浏览器连接限制针对的是主机名，不是IP地址 缺点 消耗客户端和服务器资源 域名分区增加了额外的DNS查询 避免不了TCP的慢启动 # HTTP/1.1性能优化建议 使用多域名分区 # HTTP/1.1性能优化建议 减少请求次数 把多个JavaScript或CSS组合为一个文件 把多张图片组合为一个更大的复合的图片 inlining内联，将图片嵌入到CSS或者HTML文件中，减少网络请求次数 增加应用的复杂度，导致缓存、更新等问题，只是权宜之计\n# HTTP/2 的目标 性能优化 支持请求与响应的多路复用 支持请求优先级和流量控制 支持服务器端推送 压缩HTTP header降低协议开销 HTTP的语义不变 HTTP方法、header、状态码、URI # HTTP/2 二进制分帧层 引入新的二进制分帧数据层 将传输的信息分割为消息和帧，并采用二进制格式的编码 # HTTP/2 的核心概念 流(Stream) 已建立的连接上的双向字节流 该字节流可以携带一个或多个消息 消息(Message) 与请求/响应消息对应的一系列完整的数据帧 帧(Frame) 通信的最小单位 每个帧包含帧首部，标识出当前帧所属的流 # HTTP/2 的核心概念 # HTTP/2 的核心概念 所有HTTP/2通信都在一个TCP连接上完成 流是连接中的一个虚拟信道，可以承载双向的消息 一个连接可以承载任意数量的流，每个流都有一个唯一的整数标识符(1、2\u0026hellip;N) 消息是指逻辑上的HTTP消息，比如请求、响应等 消息由一或多个帧组成，这些帧可以交错发送，然后根据每个帧首部的流标识符重新组装 # HTTP/2请求与响应的多路复用 HTTP/1.x中，如果客户端想发送多个并行的请求，那么必须使用多个TCP连接 HTTP/2中，客户端可以使用多个流发送请求，同时HTTP消息被分解为互不依赖的帧，交错传输，最后在另一端重新组装 # HTTP/2 帧格式 详细说明请参考HTTP/2规范 # HTTP/2 帧类型 客户端通过HEADERS帧来发起新的流 服务器通过PUSH_PROMISE帧来发起推送流 帧类型 类型编码 用途 DATA 0x0 传输HTTP消息体 HEADERS 0x1 传输HTTP头部 PRIORITY 0x2 指定流的优先级 RST_STREAM 0x3 通知流的非正常 SETTINGS 0x4 修改连接或者流的配置 PUSH_PROMISE 0x5 服务端推送资源时的请求帧 PING 0x6 心跳检测，计算RTT往返时间 GOAWAY 0x7 优雅的终止连接，或者通知错误 WINDOW_UPDATE 0x8 针对流或者连接，实现流量控制 CONTINUATION 0x9 传递较大HTTP头部时的持续帧 # HTTP/2 请求优先级 HTTP/2允许每个流关联一个31bit的优先值 0 最高优先级 2^31 -1 最低优先级 浏览器会基于资源的类型、在页面中的位置等因素，决定请求的优先次序 服务器可以根据流的优先级，控制资源分配，优先将高优先级的帧发送给客户端 HTTP/2没有规定具体的优先级算法 # HTTP/2 流量控制 流量控制有方向性，即接收方可能根据自己的情况为每个流，乃至整个连接设置任意窗口大小 连接建立后，客户端与服务器交换SETTINGS帧，设置 双向的流量控制窗口大小 流量控制窗口大小通过WINDOW_UPDATE帧更新 HTTP/2流量控制和TCP流量控制的机制相同，但TCP流量控制不能对同一个连接内的多个流实施差异化策略 # HTTP/2 服务器端推送 服务器可以对一个客户端请求发送多个响应 服务器通过发送PUSH_PROMISE帧来发起推送流 客户端可以使用HTTP header向服务器发送信号，列出它希望推送的资源 服务器可以智能分析客户端的需求，自动推送关键资源 # HTTP header压缩 HTTP/2使用HPACK压缩格式压缩请求/响应头 通过静态霍夫曼码对发送的header字段进行编码，减小了它们的传输大小 客户端和服务器使用索引表来维护和更新header字段。对于相同的数据，不再重复发送 # HTTP header压缩 # HTTP/2 vs HTTP/1.1 https://http2.akamai.com/demo\n# HTTP/2的升级与发现 HTTP/1.x还将长期存在，客户端和服务器必须同时支持1.x和2.0 客户端和服务器在开始交换数据前，必须发现和协商使用哪个版本的协议进行通信 HTTP/2定义了两种协商机制 通过安全连接TLS和ALPN进行协商 基于TCP连接的协商机制 # HTTP/2的升级与发现 HTTP/2标准不要求必须基于TLS，但浏览器要求必须基于TLS Web上存在大量的代理和中间设备：缓存服务器、安全网关、加速器等等 如果任何中间设备不支持，连接都不会成功 建立TLS信道，端到端加密传输，绕过中间代理，实现可靠的部署 新协议一般都要依赖于建立TLS信道，例如WebSocket、SPDY # h2和h2c升级协商机制 基于TLS运行的HTTP/2被称为h2 直接在TCP之上运行的HTTP/2被称为h2c # h2c演示环境 客户端测试工具 curl (\u0026gt; 7.46.0) 服务器端 Tomcat 9.x 1 2 3 4 5 6 \u0026lt;Connector port=\u0026#34;8080\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34; connectionTimeout=\u0026#34;20000\u0026#34; redirectPort=\u0026#34;8443\u0026#34; \u0026gt; \u0026lt;UpgradeProtocol className=\u0026#34;org.apache.coyote.http2.Http2Protocol\u0026#34;/\u0026gt; \u0026lt;/Connector\u0026gt; # h2c协议升级 curl http://localhost:8080 --http2 -v 1 2 3 4 5 6 7 8 9 10 11 \u0026gt; GET / HTTP/1.1 \u0026gt; Host: localhost:8080 \u0026gt; User-Agent: curl/7.64.1 \u0026gt; Accept: */* \u0026gt; Connection: Upgrade, HTTP2-Settings \u0026gt; Upgrade: h2c \u0026gt; HTTP2-Settings: AAMAAABkAARAAAAAAAIAAAAA \u0026lt; HTTP/1.1 101 \u0026lt; Connection: Upgrade \u0026lt; Upgrade: h2c # HTTP/2连接建立 # HTTP/2连接建立 Magic帧 ASCII 编码，12字节 何时发送? 接收到服务器发送来的 101 Switching Protocols后 TLS 握手成功后 Preface 内容 # HTTP/2连接建立 交换settings帧(client -\u0026gt; server) # HTTP/2连接建立 交换settings帧(server -\u0026gt; client) # HTTP/2连接建立 settings ACK 帧 (client \u0026lt;-\u0026gt; server) # TLS协议的设计目标 保密性 完整性 身份验证 # TLS发展史 1994年，NetScape 设计了SSL协议(Secure Sockets Layer) 1.0，未正式发布 1995年，NetScape 发布 SSL 2.0 1996年，发布SSL 3.0 1999年，IETF标准化了SSL协议，更名为TLS(Transport Layer Security)，发布TLS 1.0 2006年4月，IETF 工作组发布了TLS 1.1 2008年8月，IETF 工作组发布了TLS 1.2 2018年8月，TLS 1.3正式发布 # TLS 1.2 握手过程 验证身份 达成安全套件共识 传递密钥 加密通讯 非对称加密只在建立TLS信道时使用，之后的通信使用握手时生成的共享密钥加密 # TLS 安全密码套件 密钥交换算法 双方在完全没有对方任何预先信息，通过不安全信道创建密钥 1976年，Diffie–Hellman key exchange，简称 DH 基于椭圆曲线(Elliptic Curve)升级DH协议，ECDHE 身份验证算法 非对称加密算法，Public Key Infrastructure(PKI) 对称加密算法、强度、工作模式 工作模式：将明文分成多个等长的Block模块，对每个模块分别加解密 hash签名算法 # TLS1.3的握手优化 An Overview of TLS 1.3 – Faster and More Secure # 测试TLS的支持情况 https://www.ssllabs.com/ssltest/index.html # Application-Layer Protocol Negotiation 基于TLS运行的HTTP/2使用ALPN扩展做协议协商\n客户端在ClientHello消息中增加ProtocolNameList字段，包含自己支持的应用协议 服务器检查ProtocolNameList字段，在ServerHello消息中以ProtocolName字段返回选中的协议 在TLS握手的同时协商应用协议，省掉了HTTP的Upgrade机制所需的额外往返时间\n# ALPN # h2演示环境 客户端：浏览器 服务器端：Tomcat 9.x Tomcat提供了三种不同的TLS实现 Java运行时提供的JSSE实现 使用了OpenSSL的JSSE实现 APR实现，默认情况下使用OpenSSL引擎 # Tomcat三种TLS实现 JSSE 非常慢 ALPN是因为HTTP/2才在2014年出现，JDK8不支持ALPN OpenSSL实现 只使用了OpenSSL，没有使用其他本地代码(native socket, poller等) 可以配合 NIO 和 NIO2 APR 大量的native code TLS同样使用了OpenSSL # TLS实现的性能对比 OpenSSL性能比纯Java实现好很多；使用TLS可以不再需要APR Linux上NIO.2是通过epoll来模拟实现的EPollPort.java # 使用JSSE 生成private key和自签名证书 keytool -genkey -alias tomcat -keyalg RSA 配置server.xml 1 2 3 4 5 6 7 8 9 10 11 \u0026lt;Connector protocol=\u0026#34;org.apache.coyote.http11.Http11NioProtocol\u0026#34; port=\u0026#34;8443\u0026#34; maxThreads=\u0026#34;200\u0026#34; sslImplementationName= \u0026#34;org.apache.tomcat.util.net.jsse.JSSEImplementation\u0026#34; scheme=\u0026#34;https\u0026#34; secure=\u0026#34;true\u0026#34; SSLEnabled=\u0026#34;true\u0026#34; keystoreFile=\u0026#34;${user.home}/.keystore\u0026#34; keystorePass=\u0026#34;changeit\u0026#34; clientAuth=\u0026#34;false\u0026#34; sslProtocol=\u0026#34;TLS\u0026#34;\u0026gt; \u0026lt;UpgradeProtocol className=\u0026#34;org.apache.coyote.http2.Http2Protocol\u0026#34; /\u0026gt; \u0026lt;/Connector\u0026gt; # 使用JSSE JDK8不支持ALPN 1 2 3 4 5 严重 [main] org.apache.coyote.http11.AbstractHttp11Protocol.configureUpgradeProtocol The upgrade handler [org.apache.coyote.http2.Http2Protocol] for [h2] only supports upgrade via ALPN but has been configured for the [\u0026#34;https-jsse-nio-8443\u0026#34;] connector that does not support ALPN. JDK11 1 2 3 4 信息 [main] org.apache.coyote.http11.AbstractHttp11Protocol.configureUpgradeProtocol The [\u0026#34;https-jsse-nio-8443\u0026#34;] connector has been configured to support negotiation to [h2] via ALPN # 使用OpenSSL 安装tomcat-native brew install tomcat-native 配置$CATALINA_HOME/bin/setenv.sh 1 CATALINA_OPTS=\u0026#34;$CATALINA_OPTS -Djava.library.path=/usr/local/opt/tomcat-native/lib\u0026#34; 配置server.xml 1 2 3 4 5 6 \u0026lt;Connector protocol=\u0026#34;org.apache.coyote.http11.Http11NioProtocol\u0026#34; sslImplementationName= \u0026#34;org.apache.tomcat.util.net.openssl.OpenSSLImplementation\u0026#34; ... \u0026gt; \u0026lt;/Connector\u0026gt; # 使用OpenSSL JDK8 \u0026amp; JDK11 1 2 3 4 5 6 7 8 9 10 信息 [main] org.apache.coyote.http11.AbstractHttp11Protocol.configureUpgradeProtocol The [\u0026#34;https-openssl-nio-8443\u0026#34;] connector has been configured to support negotiation to [h2] via ALPN ... 信息 [main] org.apache.coyote.AbstractProtocol.start 开始协议处理句柄 [\u0026#34;https-openssl-nio-8443\u0026#34;] # ALPN协议协商 ClientHello # ALPN协议协商 ServerHello # 使用Chrome开发者工具观察 # HTTP/2的问题 HTTP/2消除了HTTP协议的队首阻塞现象，但TCP层面上仍然存在队首阻塞 HTTP/2多请求复用一个TCP连接，丢包可能会block住所有的HTTP请求 # HTTP/2的问题 TCP及TCP+TLS建立连接需要多次round trips # QUIC Quick UDP Internet Connections 由Goolge开发，并已经在Google部署使用 # QUIC QUIC: next generation multiplexed transport over UDP\n# 参考资料 High Performance Browser Networking HTTP的前世今生 HTTP/3 的过去、现在和未来 HTTP/2协议 ","date":"2019-10-07T09:59:29+08:00","permalink":"https://mazhen.tech/p/http/2-%E7%AE%80%E4%BB%8B/","title":"HTTP/2 简介"},{"content":" # 安装helm客户端 在macOS上安装很简单：\n1 brew install kubernetes-helm 其他平台请参考Installing Helm\n# 配置RBAC 定义rbac-config.yaml文件，创建tiller账号，并和cluster-admin绑定：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: tiller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: tiller namespace: kube-system 执行命令：\n1 2 3 $ kubectl create -f rbac-config.yaml serviceaccount \u0026#34;tiller\u0026#34; created clusterrolebinding \u0026#34;tiller\u0026#34; created # 安装Tiller镜像 在强国环境内，需要参考kubernetes-for-china，将helm服务端部分Tiller的镜像下载到集群节点上。\n# 初始化helm 执行初始化命令，注意指定上一步创建的ServiceAccount：\n1 helm init --service-account tiller --history-max 200 命令执行成功，会在集群中安装helm的服务端部分Tiller。可以使用kubectl get pods -n kube-system命令查看：\n1 2 3 4 5 $kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE ... tiller-deploy-7fbf5fc745-lxzxl 1/1 Running 0 179m # Quickstart 增加Chart Repository（可选） 查看helm的Chart Repository：\n1 2 3 4 5 $ helm repo list NAME URL stable https://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts 如果你所处的网络环境无法访问缺省的Chart Repository，可以更换为其他repo，例如微软提供的 helm 仓库的镜像：\n1 2 3 4 5 $ helm repo add stable http://mirror.azure.cn/kubernetes/charts/ \u0026#34;stable\u0026#34; has been added to your repositories $ helm repo add incubator http://mirror.azure.cn/kubernetes/charts-incubator/ \u0026#34;incubator\u0026#34; has been added to your repositories 所有可用chart列表： 1 2 helm repo update helm search 搜索tomcat chart： 1 helm search tomcat 查看stable/tomcat的详细信息 1 helm inspect stable/tomcat stable/tomcat使用 sidecar 方式部署web应用，通过参数image.webarchive.repository指定war的镜像，不指定会部署缺省的sample应用。\n安装tomcat： 如果是在私有化集群部署，设置service.type为NodePort：\n1 helm install --name my-web --set service.type=NodePort stable/tomcat 测试安装效果 1 2 3 4 5 6 export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services my-web-tomcat) export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo http://$NODE_IP:$NODE_PORT # 访问sample应用 curl http://$NODE_IP:$NODE_PORT/sample/ 列表和删除 1 2 helm list helm del --purge my-web ","date":"2019-06-11T09:36:01+08:00","permalink":"https://mazhen.tech/p/helm%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","title":"Helm的安装和使用"},{"content":"Kubernetes 相关的docker镜像存放在Google的镜像仓库 k8s.gcr.io，在国内网络环境内无法访问。有人已经将这些镜像同步到了阿里云，你可以在阿里云容器镜像服务中搜索到它们。几乎所有的k8s镜像都已经同步到了阿里云。阿里云容器服务团队甚至还有一个开源项目sync-repo，专门做Docker Registry之间的同步。但是，如果你不放心别人同步的镜像，或者最新版本的镜像还没人同步过来，你可以按照本文将介绍的步骤，自己将gcr.io上的docker镜像搬到阿里云。\n# 安装配置shadowsocks客户端 先简单介绍下Shadowsocks协议，详细的工作原理可以参考这篇博客：\n当我们启动shadowsocks client时，实际上是启动了一个 ss-local 进程，左侧绿色的 Socks5 Client 可以是浏览器，也可以是Telegram等本地应用，它们和ss-local之间是使用 socks 协议进行通信。也就是说，浏览器像连接普通 socks 代理一样，连接到ss-local进程。ss-local 会将收到的请求，转发给ss-server，由ss-server完成实际的访问，并将结果通过ss-local返回给浏览器。ss-server部署在国内网络之外，和ss-local之间是加密传输，这样就实现了跨越长城。其实防火长城已经能够识别Shadowsocks协议，但发现我们是在努力学习先进技术，就先放我们过关。\n好了，我们现在首先要做的是在本机安装配置shadowsocks客户端。推荐使用shadowsocks-libev，纯C实现的shadowsocks协议，已经在很多操作系统的官方repository中 ，安装非常方便。\nmacOS 1 brew install shadowsocks-libev Ubuntu 1 sudo apt install shadowsocks-libev 接下来填写shadowsocks client配置文件，JSON格式，简单易懂：\n1 2 3 4 5 6 7 8 { \u0026#34;server\u0026#34;:\u0026#34;ss服务器IP\u0026#34;, \u0026#34;server_port\u0026#34;:443, // ss服务器port \u0026#34;local_port\u0026#34;:1080, // 本地监听端口 \u0026#34;password\u0026#34;:\u0026#34;xxxx\u0026#34;, // ss服务器密码 \u0026#34;timeout\u0026#34;:600, \u0026#34;method\u0026#34;:\u0026#34;aes-256-cfb\u0026#34; // ss服务器加密方法 } 至于shadowsocks服务器端，可以租用国内网络外的云主机自己搭建，也可以购买现成的机场服务，本文就不讨论了。\n然后启动shadowsocks client：\n1 nohup ss-local -c ss-client.conf \u0026amp; # 安装配置HTTP代理 shadowsocks client创建的是socks5代理，不过一些程序无法使用 socks5 ，它们需要通过 http_proxy 和 https_proxy 环境变量，使用 HTTP 代理。polipo 可以帮助我们将 socks5 代理转换为 HTTP 代理。\nmacOS下安装polipo 1 brew install polipo Ubuntu下安装polipo 1 2 3 4 5 sudo apt install polipo # 建议停掉polipo服务，需要的时候自己启动 sudo systemctl stop polipo.service sudo systemctl disable polipo.service 启动HTTP代理\n1 sudo polipo socksParentProxy=127.0.0.1:1080 proxyPort=1087 socksParentProxy配置为localhost和ss-local监听端口，proxyPort是启动的HTTP代理端口。\n我们可以在命令行终端测试HTTP代理的效果：\n1 2 3 $ export http_proxy=http://localhost:1087 $ export https_proxy=http://localhost:1087 $ curl https://www.google.com 应该可以正常访问到Google。\n# 设置Docker HTTP代理 如果是在macOS上使用Docker Desctop，可以在Preference中的Proxies设置上一步启动的HTTP代理：\n如果是Linux平台，请参考Docker的官方文档进行设置。\n# 在阿里云创建容器镜像的命名空间 为了将镜像同步到阿里云，首先需要在阿里云的容器镜像服务控制台创建镜像的命名空间。\n建议将仓库类型设置为“公开”，这样其他人也能搜索、下载到镜像。\n# 从gcr.io下载镜像 在本机从gcr.io下载镜像，我们以镜像pause:3.1为例：\n1 docker pull k8s.gcr.io/pause:3.1 # 给镜像标记新的tag 根据前面在阿里云创建的命名空间，给镜像标记新的tag：\n1 docker tag k8s.gcr.io/pause:3.1 registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 mz-k8s是在前面创建的命名空间。 查看tag结果：\n1 2 3 4 5 $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE k8s.gcr.io/pause 3.1 da86e6ba6ca1 17 months ago 742kB registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause 3.1 da86e6ba6ca1 17 months ago 742kB 通过IMAGE ID可以看出，两个镜像为同一个。\n# 将镜像上传到阿里云 登录阿里云镜像仓库：\n1 $ docker login --username=(阿里云账号) registry.cn-shenzhen.aliyuncs.com 根据提示输入password，登录成功后，显示Login Succeeded。\n上传镜像：\n1 docker push registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 # 从阿里云下载镜像 现在可以在其他机器上从阿里云下载pause:3.1镜像，这时候已经不需要科学上网了：\n1 $ docker pull registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 给镜像打上原来的tag，这样kubeadm等工具就可以使用本地仓库中的pause:3.1镜像了：\n1 $ docker tag registry.cn-shenzhen.aliyuncs.com/mz-k8s/pause:3.1 k8s.gcr.io/pause:3.1 至此，我们跨越长城，将一个docker镜像从gcr.io搬到了Aliyun。\n如果是需要批量、定时的从gcr.io同步镜像，建议考虑使用阿里开源的sync-repo。\n","date":"2019-06-06T09:53:24+08:00","permalink":"https://mazhen.tech/p/%E8%87%AA%E5%B7%B1%E5%8A%A8%E6%89%8B%E5%B0%86%E8%B0%B7%E6%AD%8Ck8s%E9%95%9C%E5%83%8F%E5%90%8C%E6%AD%A5%E5%88%B0%E9%98%BF%E9%87%8C%E4%BA%91/","title":"自己动手将谷歌k8s镜像同步到阿里云"},{"content":"用户在访问Kubernetes集群的API server时，访问请求需要经过身份验证、授权和准入控制这三个阶段的检查，才能真正到达API服务，如下图所示：\nKubernetes中的用户有两种类型：service accounts 和 normal users。service accounts 由 Kubernetes管理，它是Pod中的进程用于访问API服务的account，为Pod中的进程提供了一种身份标识。normal users是由外部系统管理，在Kubernetes中并没有对应的 user 对象，它为人类用户使用kubectl之类的工具访问API服务时提供身份标识。所有用户，不管是使用 kubectl、客户端lib、还是直接发起REST请求访问API server，都需要经过上述三个步骤的检查。\n本文将介绍Kubernetes集群的身份验证，即Kubernetes如何确认来访者的身份。\nKubernetes支持多种方式的身份验证：客户端证书，Password， Plain Tokens，JWT(JSON Web Token)，HTTP basic auth等。你可以同时启用多种认证，一般建议至少使用两种：\n为验证normal users身份的客户端证书方式 为验证Service accounts身份的 JWT Tokens方式 # 使用客户端证书进行身份验证 # 理解数字证书 非对称加密算法是证书的基础。数字签名、数字证书等一系列概念有点绕，但只要记住：公钥用来加密，私钥用来签名 就可以了。\n怎么理解呢？公钥可以随意分发，谁都可以持有，如果你用私钥加密，任何持有对应公钥的人都可以解密，这样做和没加密一样，没什么意义。因此，我们需要用公钥加密，只有持有私钥的那个人才能解密。私钥之所以称为私钥，一定会私密保存，不会向其他人泄漏。同时，用私钥加密虽然没有意义，但如果别人用公钥解开了私钥加密的信息，就能够证明信息是由私钥持有者发出的，验证了信息发送者的身份，这就是数字签名。\n每个人制作好自己的公钥和私钥，然后把公钥发布出去。两个人如果都有对方的公钥，就可以用对方的公钥给对方发送加密信息，同时附上用私钥加密的信息摘要作为数字签名，证明消息发送者的身份。\n通过加密防止了窃听风险，通过数字签名防止了冒充风险，数字签名内的消息摘要防止了篡改风险，一起看似很完美。\n等等，这里有个很重要的问题被忽略了：如何安全的将公钥发布出去？如果双方希望安全通信，最好当面交换公钥，以免被别人冒充，并且要保护好自己的电脑，避免公钥被别有用心的人替换。现实中不可能这样分发公钥，效率太低，几乎无法大规模实行。于是出现了CA(certificate authority)，为公钥做认证。CA用自己的私钥，对申请用户的公钥和一些身份信息加密，生成\u0026quot;数字证书\u0026quot;（Digital Certificate）。你现在可以用任何方式将内含公钥的数字证书发布出去，例如有客户发起请求，希望以HTTPS的方式访问你的WEB服务，你可以在第一次回复客户的响应中带上数字证书。客户拿到你的数字证书，用CA的公钥解开数字证书，安全的获得你的公钥。有了CA为你的数字证书背书，客户可以确定你的身份，不是有人在冒充你。\n那么CA的公钥如何安全的分发呢？首先，证书的签发是“链”式结构，给你签发证书的CA，它的证书可能还是由上一级CA机构签发的，这样一直往上追溯，最终会到某个“根证书”。如果“根证书”是被我们信任的，那么整条“链”上的证书都可信。\n其次，操作系统都内置了“受信任的根证书”。我们拿到某个证书，如果它的根证书在系统的“受信任的根证书”列表中，那么这个证书就是可信的。例如知乎的证书：\n可以看到，它的根证书是DigiCert Global Root CA，在操作系统的“受信任的根证书”列表中能找到它：\n根证书是通过预装的方式完成的分发，因此安装来源不明的操作系统有风险，可能潜伏了非法的根证书。一旦被植入了非法的根证书，一整套的安全体系瞬间土崩瓦解。同时，不能随意向系统中添加可信任的根证书，你很难验证根证书的真伪，它已经是root，没人能为它做背书了。12306网站早期的根证书就不在操作系统的“受信任根证书”列表中，需要用户手工安装，在网上引起轩然大波。最终12306在17年底的时候换成了Digicert的证书。\n简单总结一下基于非对称加密算法的公钥/私钥体系，公钥用来加密，私钥用来签名，引入CA保证公钥的安全分发。你可以找CA签发数字证书，那么你的客户就可以根据本地“受信任的根证书”验证你的数字证书，从而确认你的身份，然后用证书内包含的公钥给你发加密的信息。同样，你也可以要求对方的数字证书，以便确认对方的身份，并给他回加密的信息。\n理解了数字证书的基本原理，我们再看看Kubernetes中如何使用客户端证书进行身份验证。\n# 数字证书在Kubernetes中的应用 Kubernetes各组件之间的通信都是基于TLS，实现服务的加密访问，同时支持基于证书的双向认证。\n我们在搭建私有Kubernetes集群时，一般是自建root CA，因为参与认证的所有集群节点，包括远程访问集群的客户端桌面都完全由自己控制，我们可以安全的将根证书分发到所有节点。有了CA，我们再用CA的私钥/公钥为各个组件签发所需的证书。\nCA的创建，以及一系列客户端、服务端证书的签发，实际上是建立了Kubernetes集群的PKI(Public key infrastructure)。\nKubernetes中的组件比较多，所以需要的证书会非常多，这篇文档做了介绍。我按证书的用途归类总结一下：\nCA证书 Kubernetes 一般用途 etcd 集群根证书 aggregation 相关功能 服务端证书 API server etcd kubelet 访问API server时进行身份验证的客户端证书 kubelet　-\u0026gt; API server controller-manager -\u0026gt; API server kube-scheduler -\u0026gt; API server admin用户　-\u0026gt; API server API server 访问其他组件时进行身份验证的客户端证书 API server -\u0026gt; etcd API server -\u0026gt; kubelet API server -\u0026gt; aggregated API server etcd 相关功能 etcd 集群中节点互相通信使用的客户端证书 如果etcd是以Pod方式运行，针对etcd的 Liveness 需要的客户端证书 Service accounts 私钥/公钥对，用于生成Service accounts身份验证的 JWT Tokens 最后一个不是证书，不过也在Kubernetes PKI的管理范围。关于Service accounts 私钥/公钥对的作用，后面会讲到。\n理论上CA根证书可以只使用一个，不过为了安全和方便管理，官方强调在不同的上下文最好使用不同的CA：\nWarning: Do not reuse a CA that is used in a different context unless you understand the risks and the mechanisms to protect the CA’s usage.\n可以看出，API server是核心组件，其他组件、包括admin用户对它的访问都需要TLS双向认证，所以会有API server的服务端证书和各个组件的客户端证书。API server作为客户端需要访问etcd、kubelet和aggregated API server，所以也会有相应的服务端、客户端证书。\n当我们使用kubeadm安装Kubernetes时，kubeadm会为我们生成上述的一系列私钥和证书，放在/etc/kubernetes/目录下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 # tree --dirsfirst /etc/kubernetes/ /etc/kubernetes/ ├── manifests 组件的配置文件，以Pod方式运行在集群中 │ ├── etcd.yaml │ ├── kube-apiserver.yaml │ ├── kube-controller-manager.yaml │ └── kube-scheduler.yaml ├── pki │ ├── etcd │ │ ├── ca.crt etcd 集群CA证书 │ │ ├── ca.key etcd 集群CA私钥 │ │ ├── healthcheck-client.crt Liveness 健康检查使用的客户端证书 │ │ ├── healthcheck-client.key │ │ ├── peer.crt etcd节点间通信使用的客户端证书 │ │ ├── peer.key │ │ ├── server.crt etcd服务端证书 │ │ └── server.key │ ├── apiserver.crt API Server 服务端证书 │ ├── apiserver.key │ ├── apiserver-etcd-client.crt API server -\u0026gt; etcd │ ├── apiserver-etcd-client.key │ ├── apiserver-kubelet-client.crt API server -\u0026gt; kubelet │ ├── apiserver-kubelet-client.key │ ├── ca.crt CA证书 │ ├── ca.key CA私钥 │ ├── front-proxy-ca.crt aggregation 相关功能CA证书 │ ├── front-proxy-ca.key aggregation 相关功能CA私钥 │ ├── front-proxy-client.crt API server -\u0026gt; aggregated API server │ ├── front-proxy-client.key │ ├── sa.key Service accounts 私钥 │ └── sa.pub Service accounts 公钥 ├── admin.conf admin -\u0026gt; API server ├── controller-manager.conf controller-manager -\u0026gt; API server ├── kubelet.conf kubelet　-\u0026gt; API server └── scheduler.conf kube-scheduler -\u0026gt; API server 注意，最后四个*.conf是kubeconfig file，内容包含了集群、用户、namespace等信息，还有用来认证的CA证书、客户端证书和私钥。例如admin.conf就是kubectl访问集群用到的kubeconfig file，缺省情况下kubectl会使用$HOME/.kube/config，你也可以通过KUBECONFIG环境变量，或kubectl 的 --kubeconfig 参数进行设置。\nkubelet运行在每个工作节点，无法提前预知 node 的 IP 信息，所以 kubelet 一般不会明确指定服务端证书, 而是只指定 CA 根证书, 让 kubelet 根据本机信息自动生成服务端证书，保存到配置参数指定的\u0026ndash;cert-dir目录中。cert-dir的缺省值是/var/lib/kubelet/pki。\n1 2 3 4 5 6 7 # tree /var/lib/kubelet/pki /var/lib/kubelet/pki ├── kubelet-client-2019-04-28-10-48-13.pem ├── kubelet-client-current.pem -\u0026gt; /var/lib/kubelet/pki/kubelet-client-2019-04-28-10-48-13.pem ├── kubelet.crt kubelet服务端证书 └── kubelet.key kubelet服务端私钥 另外，API server有很多认证相关的启动参数，参数名称让人容易混淆，有人还专门提了issue，这个回答根据用途对这些参数进行了分类，说明的非常清晰。\n# API server 如何用客户端证书进行身份验证 前面提到，当用户使用kubectl访问API server时，需要以某种方式进行身份验证，最常用的方式就是使用客户端证书。Kubernetes是没有 user 这种 API 对象，kubectl的用户身份信息就包含在客户端证书中。API server验证了客户端证书，也就可以从证书中获得用户名和所属的group。\n我们以/etc/kubernetes/admin.conf 为例，看看客户端证书中提供了那些信息。\n先查看admin.conf文件的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $ kubectl --kubeconfig /etc/kubernetes/admin.conf config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://172.18.100.90:6443 name: test contexts: - context: cluster: test user: kubernetes-admin name: kubernetes-admin@test current-context: kubernetes-admin@test kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 这个文件提供了API server的地址，以及身份验证用到的证书：\ncertificate-authority-data ：CA证书 client-certificate-data ：客户端证书 client-key-data： 客户端私钥 客户端证书是以base64编码的方式保存在client-certificate-data字段中，我们将证书提取出来：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # cat /etc/kubernetes/admin.conf | grep client-certificate-data | cut -d \u0026#34; \u0026#34; -f 6 | base64 -d \u0026gt; admin.crt # cat admin.crt -----BEGIN CERTIFICATE----- MIIC8jCCAdqgAwIBAgIIaBuxevPYGaswDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE AxMKa3ViZXJuZXRlczAeFw0xOTA0MjgwMjQwMDBaFw0yMDA0MjcwMjQwMDNaMDQx FzAVBgNVBAoTDnN5c3RlbTptYXN0ZXJzMRkwFwYDVQQDExBrdWJlcm5ldGVzLWFk bWluMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvRTIQ4YEMh0mUWKP 17pLdUocgZMLVCK6tYmj0DJIihRk+wKvNzYSStfxsug9nnEqVVzmbW5/UxER776H 844y/1NGk/8LsDIkFGspf3cEmQ8OE8TlLNW7h9gWIGymLQ/K1qhYfNOPDYoJXPix eUWTJgn0+neJNbJ3JoJk2WRlDFwbE0uXgYYczuDcJablSdbb8Oc+E4qJ1U7u9YMN Bo/JBY68wYtdjXHl6Mg28aCioVZrs5eZWkNzNpXMVjQwFdZAdWbnS3OJGN1b6IrV gWk9PMoCE2TtFv5NdlHSYFtEAaBEwfl3/D3rGHKb4ZH/fgKWsepy8ffxxibM6pND pLnmAwIDAQABoycwJTAOBgNVHQ8BAf8EBAMCBaAwEwYDVR0lBAwwCgYIKwYBBQUH AwIwDQYJKoZIhvcNAQELBQADggEBALPYorv2mlXyu6jpX/6gE1kTvpPGK4vylfSY 9jl4PtQZgRaXvmVsUKpyIdtVhdMZp9EFaYNC4AYkqaVEOoAbU96SYhdO/6h7Rn8T 0Ae+f1Vwt+8GxErEN3xp4noHfXM0eSEuFLPXt43BBJInYRyx1J0urAjYtNCvc9wX uQFVmNKsqgmjvHQsRkvKcb8HEzcaD1TqqnTpq3usGjNggVZFTChB58R909yGPEXL n7VsilmN86gom3fgqwCn2C00iKcuzCOwYN2T+Mi8KI2DraDDoVeRMSaYQNUfKNIX Ngeod/C4piq+OAdyrPPFEINdLi404EYHyod0CgiD6uhoX5W06O4= -----END CERTIFICATE----- 使用openssl查看证书内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # openssl x509 -in ./admin.crt -text Certificate: Data: Version: 3 (0x2) Serial Number: 7501784745950845355 (0x681bb17af3d819ab) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=kubernetes Validity Not Before: Apr 28 02:40:00 2019 GMT Not After : Apr 27 02:40:03 2020 GMT Subject: O=system:masters, CN=kubernetes-admin Subject Public Key Info: Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) ...... 注意这一行：\n1 Subject: O=system:masters, CN=kubernetes-admin API server会将证书中CN(Common Name)作为用户名，O(Organization)作为用户所属的group。API server从这个证书得到的信息是：admin用户所属的group是system:masters。至此，身份验证阶段完成。\n下一步是授权检查，也就是检查用户有没有权限执行这个操作。这是另外一个话题，本文不做详细讨论，只是简单介绍一下。根据官方文档，Kubernetes提供了缺省的 ClusterRole - group 绑定关系，已经将system:masters group 和 角色 cluster-admin绑定到了一起：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # kubectl get clusterrolebindings cluster-admin -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2019-04-28T02:40:17Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: cluster-admin resourceVersion: \u0026#34;98\u0026#34; selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/cluster-admin uid: f58a87a7-695e-11e9-91ca-005056ac1c1c roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - apiGroup: rbac.authorization.k8s.io kind: Group name: system:masters 这个绑定关系的意思是，属于system:mastersgroup的用户，都拥有cluster-admin角色包含的权限。我们再看看角色cluster-admin的具体权限信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # kubectl get clusterrole cluster-admin -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; creationTimestamp: \u0026#34;2019-04-28T02:40:17Z\u0026#34; labels: kubernetes.io/bootstrapping: rbac-defaults name: cluster-admin resourceVersion: \u0026#34;44\u0026#34; selfLink: /apis/rbac.authorization.k8s.io/v1/clusterroles/cluster-admin uid: f5523c21-695e-11e9-91ca-005056ac1c1c rules: - apiGroups: - \u0026#39;*\u0026#39; resources: - \u0026#39;*\u0026#39; verbs: - \u0026#39;*\u0026#39; - nonResourceURLs: - \u0026#39;*\u0026#39; verbs: - \u0026#39;*\u0026#39; 从rules列表可以看出，cluster-admin这个角色对所有资源都有无限制的操作权限。因此，使用了这个kubeconfig file的kubectl的请求就有了操控和管理整个集群的权限。\n# 使用JWT Tokens进行身份验证 运行在Pod中的进程需要访问API server时，同样需要进行身份验证和授权检查。如何让Pod具有用户身份呢？通过给Pod指定service account来实现。service account是Kubernetes内置的一种 “服务账户”，它为Pod中的进程提供了一种身份标识。如果Pod没有显式的指定service account，系统会自动为其关联default service account。\n我们自己创建service account对象非常简单：\n1 2 3 4 5 6 7 8 //serviceaccount.yaml apiVersion: v1 kind: ServiceAccount metadata: name: nginx-example # kubectl apply -f serviceaccount.yaml serviceaccount/nginx-example created 查看刚刚创建的service account：\n1 2 3 4 5 6 7 8 9 10 11 # kubectl describe serviceaccounts nginx-example Name: nginx-example Namespace: default Labels: \u0026lt;none\u0026gt; Annotations: kubectl.kubernetes.io/last-applied-configuration: {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;ServiceAccount\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;nginx-example\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;default\u0026#34;}} Image pull secrets: \u0026lt;none\u0026gt; Mountable secrets: nginx-example-token-r2cv6 Tokens: nginx-example-token-r2cv6 Events: \u0026lt;none\u0026gt; 当service account对象创建成功，controller-manager会发现这个新对象，然后为它生成token。token实际上是secret对象，内部包含了用来身份验证的token。service account对象的Tokens列引用的就是controller-manager为它创建的token。\n我们来看看token的内容：\n1 2 3 4 5 6 7 8 9 10 11 # kubectl get secrets nginx-example-token-r2cv6 -o yaml apiVersion: v1 data: ca.crt: (APISERVER\u0026#39;S CA BASE64 ENCODED) namespace: ZGVmYXVsdA== token: (BEARER TOKEN BASE64 ENCODED) kind: Secret metadata: ...... type: kubernetes.io/service-account-token 可以看到，这个secret对象的type是service-account-token，包含了三部分数据：\nca.crt： API Server的CA证书，用于Pod中的进程访问API Server时对服务端证书进行校验 namespace： secret所在namespace，使用了base64编码 token：JWT Tokens JWT Tokens 是 controller-manager 用 service account私钥sa.key签发的，其中包含了用户的身份信息，API Server可以用sa.pub验证token，拿到用户身份信息，从而完成身份验证。\n如果是使用kubeadm安装的Kubernetes，我们可以在/etc/kubernetes/manifests/目录中的配置文件确认sa.key和sa.pub的作用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # cat /etc/kubernetes/manifests/kube-controller-manager.yaml ... spec: containers: - command: - kube-controller-manager ... - --service-account-private-key-file=/etc/kubernetes/pki/sa.key ... # cat /etc/kubernetes/manifests/kube-apiserver.yaml ... spec: containers: - command: - kube-apiserver ... - --service-account-key-file=/etc/kubernetes/pki/sa.pub ... controller-manager用私钥sa.key签名，API Server用公钥sa.pub验签。\n运行在Pod中的进程在向API server发起HTTP请求时，HTTP header中会携带token，API server从header中拿到token，进行身份验证：\n1 Authorization: Bearer [token] JWT Tokens的是由用.分割的三部分组成：\nHeader Payload Signature 因此，一个JWT Tokens看起来是这样的：\n1 xxxxxxx.yyyyyyyy.zzzzzzz Header和Payload都是base64编码的JSON，以上面nginx-example关联的token为例，看看Header和Payload的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ kubectl describe secrets nginx-example-token-r2cv6 | grep token: | cut -d \u0026#34; \u0026#34; -f 7 eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6Im5naW54LWV4YW1wbGUtdG9rZW4tcjJjdjYiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoibmdpbngtZXhhbXBsZSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjBmZWRlOWM1LTc2YjUtMTFlOS05MWNhLTAwNTA1NmFjMWMxYyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDpkZWZhdWx0Om5naW54LWV4YW1wbGUifQ.VOjVQBr5PKg1WyZYtIW0Fos5fxFN4cYE3Mz9p1eWbQP6rQRQGDEiGX-LBuM6ECI9cpSL-F4nYQAL9vmIlA4vbAgS4OFgC4nwu8SzLu2FVeE7RDpguvsdAsj-4T_LxEGX1RPljGTpvlt8HRjTnp9K8W4dy7PyJQEB5XvCf-IVNAs3zESgmuJ7wJwO7mXQe5WdeqhI5vXjcZiXP97oH0VRYT1vTKVP-GooC5YfaNhU7rHoJ0gmR10xNqZjwKGsHKkq5maC5BOrXFLlHRqVRwm9-hRn-ZLgAoCwujCIpLvPaFUR8HaatzX4GQ_HWev2soJnk1qcav0smxfjC-fu540vZA $ kubectl describe secrets nginx-example-token-r2cv6 | grep token: | cut -d \u0026#34; \u0026#34; -f 7 | cut -d \u0026#34;.\u0026#34; -f 1 | base64 -d | python -mjson.tool { \u0026#34;alg\u0026#34;: \u0026#34;RS256\u0026#34;, \u0026#34;kid\u0026#34;: \u0026#34;\u0026#34; } $ kubectl describe secrets nginx-example-token-r2cv6 | grep token: | cut -d \u0026#34; \u0026#34; -f 7 | cut -d \u0026#34;.\u0026#34; -f 2 | base64 -d | python -mjson.tool { \u0026#34;iss\u0026#34;: \u0026#34;kubernetes/serviceaccount\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/secret.name\u0026#34;: \u0026#34;nginx-example-token-r2cv6\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.name\u0026#34;: \u0026#34;nginx-example\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.uid\u0026#34;: \u0026#34;0fede9c5-76b5-11e9-91ca-005056ac1c1c\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;system:serviceaccount:default:nginx-example\u0026#34; } Header中的alg指明了签名用到的加密算法，Payload 中包含了用户的身份信息，可以知道这个service account属于的namespace为default，名称为nginx-example。\n第三部分Signature的构造方式如下，如果加密算法选择了PKCS SHA：\n1 2 3 4 PKCSSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), secret) controller-manager用sa.key签名，API Server用公钥sa.pub验签，进行身份验证。\n如果先深入了解JWT(JSON Web Token)，建议阅读这篇文档\u0026lt;JWT: The Complete Guide to JSON Web Tokens\u0026gt;\nPod中的进程如何获得这个token呢？Kubernetes在创建Pod时，会将service account token映射到Pod的/var/run/secrets/kubernetes.io/serviceaccount 目录下。我们通过一个例子演示一下。\n创建Pod：\n1 2 3 4 5 6 7 8 9 10 11 12 13 // simple.yaml apiVersion: v1 kind: Pod metadata: name: firstpod spec: serviceAccountName: nginx-example containers: - image: nginx name: stan $ kubectl apply -f simple.yaml pod/firstpod created 查看Pod内/var/run/secrets/kubernetes.io/serviceaccount目录的内容：\n1 2 3 4 $ kubectl exec firstpod -- ls /var/run/secrets/kubernetes.io/serviceaccount ca.crt namespace token 查看Pod内文件/var/run/secrets/kubernetes.io/serviceaccount/token的内容：\n1 2 3 4 5 6 7 8 9 $ kubectl exec firstpod -- cat /var/run/secrets/kubernetes.io/serviceaccount/token | cut -d \u0026#34;.\u0026#34; -f 2 | base64 -d | python -mjson.tool { \u0026#34;iss\u0026#34;: \u0026#34;kubernetes/serviceaccount\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/namespace\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/secret.name\u0026#34;: \u0026#34;nginx-example-token-r2cv6\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.name\u0026#34;: \u0026#34;nginx-example\u0026#34;, \u0026#34;kubernetes.io/serviceaccount/service-account.uid\u0026#34;: \u0026#34;0fede9c5-76b5-11e9-91ca-005056ac1c1c\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;system:serviceaccount:default:nginx-example\u0026#34; } 可以看到，映射进Pod中的token，正是我们在配置中通过serviceAccountName指定的nginx-example。Pod中的进程可以通过访问文件/var/run/secrets/kubernetes.io/serviceaccount/token拿到token。\n如何为service account授权？通过定义service account和role的绑定完成。本文简单演示一下，详细的说明参加官方文档。\n创建role：\n1 2 3 4 5 6 7 8 9 10 11 12 // example-role.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] $ kubectl apply -f example-role.yaml role.rbac.authorization.k8s.io/example-role created role可以理解为一组权限的集合，例如上面创建的example-role对default Namesapce内的Pods有get、watch和list操作权限。\n下一步就是将service account和role进行绑定：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //example-rolebinding.yaml kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: default subjects: - kind: ServiceAccount name: nginx-example namespace: default roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io $ kubectl apply -f example-rolebinding.yaml rolebinding.rbac.authorization.k8s.io/example-rolebinding created 通过绑定的创建，service account就拥有了role定义的权限。\n# 总结 用户对API server的访问需要通过身份验证、授权和准入控制这三个阶段的检查。\n一般集群外部用户访问API Server使用客户端证书进行身份验证。Kubernetes各组件之间的通信都使用了TLS加密传输，同时支持基于证书的双向认证。因此Kubernetes的安装过程涉及很多证书的创建，本文分类介绍了这些证书的作用。\n集群内Pod中的进程访问API server时，使用service account关联的token进行身份验证。\n每个Pod都会关联一个service account，没有明确指定时使用default。当我们创建service account对象，controller-manager会为这个service account生成Secret，内部包含了用来身份验证的JWT Tokens。Kubernetes会将token文件mount到Pod的/var/run/secrets/kubernetes.io/serviceaccount/token，Pod内的进程在向API server发起的HTTP时，就可以在请求头中携带这个token。\n","date":"2019-05-19T09:46:53+08:00","permalink":"https://mazhen.tech/p/kubernetes%E9%9B%86%E7%BE%A4%E7%9A%84%E8%BA%AB%E4%BB%BD%E9%AA%8C%E8%AF%81/","title":"Kubernetes集群的身份验证"},{"content":" # 界面概览 快捷键 描述 cmd + shift + e 文件资源管理器 cmd + shift + f 跨文件搜索 ctrl + shift + g 源代码管理 cmd + shift + d 启动和调试 cmd + shift + x 扩展管理 cmd + shift + p 查找并运行所有命令 cmd + j 打开、关闭panel # 命令行的使用 命令 描述 code $path 新窗口中打开这个文件或文件夹 code -r $path 窗口复用打开文件 code -r -g $file:lineno 打开文件，跳转到指定行 code -r -d $file1 $file2 比较两个文件 ls code - # 光标移动 快捷键 描述 option + 左/右方向键 针对单词的光标移动 cmd + 左/右方向键 移动到行首、行尾 cmd + shift + \\ 在花括号之间跳转 cmd + 上/下方向键 移动到文档的第一行、最后一行 # 文本选择 shift + 光标移动\n# 删除操作 可以先选择，再删除\n快捷键 描述 cmd + fn + del 删除到行尾 cmd + del 删除到行首 option + del 向前删除单词 option + fn + del 向后删除单词 # 代码行编辑 快捷键 描述 cmd + shift + k 删除行 cmd + x 剪切行 cmd + enter 在当前行下一行新开始一行 cmd + shift + enter 在当前行上一行新开始一行 option + 上/下方向键 将当前行上下移动 option + shift + 上/下方向键 将当前行上下复制 cmd + / 将一行代码注释 option + shift + a 注释整块代码 option + shift + f 代码格式化 cmd+k cmd+f 选中代码格式化 ctrl + t 光标前后字符调换位置 cmd+shift+p transform to up/low case 转换大小写 ctrl + j 合并代码行 cmd + u 撤销光标移动 # 创建多个光标 使用鼠标 option + 鼠标左键\n使用键盘 快捷键 描述 cmd + option + 上/下方向键 创建多个光标 cmd + d 选中相同单词，并创建多个光标 option + shift+ i 在选择的多行后创建光标 # 文件跳转 快捷键 描述 ctrl + tab 文件标签之间跳转 cmd + p 打开文件列表 # 行跳转 快捷键 描述 ctrl + g 跳转到指定行 # 符号跳转 快捷键 描述 cmd + shift + o 当前文件所有符号列表 @: 符号列表@后输入冒号，符号分类排列 cmd + t 在多个文件进行符号跳转 cmd + F12 跳转到函数的实现位置 shift + F12 函数引用列表 ctrl + - 跳回上一次光标所在位置 ctrl + shift + - 跳回下一次光标所在位置 # 代码自动补全 快捷键 描述 ctrl+ space 调出建议列表 cmd + shift + space 调出参数预览窗口 cmd + . 快速修复建议列表 F2 函数名重构 # 代码折叠 快捷键 描述 cmd+ option + [ 最内层折叠 cmd + option + ] 最内层展开 cmd+k cmd+0 全部折叠 cmd+k cmd+j 全部展开 # 搜索 快捷键 描述 cmd + f 搜索 cmd + g 搜索，光标在编辑器内跳转 cmd + option + f 查找替换 cmd + shift + f 多文件搜索 # 编辑器操作 快捷键 描述 cmd + \\ 拆分编辑器 option + cmd + 左/右方向键 编辑器间切换 cmd + num 在拆分的编辑器窗口跳转 Cmd +/- 缩放整个工作区 cmd + shift + p reset zoom 重置缩放 # 专注模式 快捷键 描述 cmd + b 打开或者关闭整个视图 cmd + j 打开或者关闭面板 cmd+shift+p Toggle Zen Mode 切换禅模式 cmd+shift+p Toggle Centered Layout 切换剧中布局 # 命令面板 快捷键 描述 cmd + shift + p 命令面板 命令面板的第一个符合对应着不同的功能：\n? 列出所有可用功能 \u0026gt; 用于显示所有的命令 @ 用于显示和跳转文件中的 “符号”（Symbols） @: 可以把符号们按类别归类 # 用于显示和跳转工作区中的 “符号”（Symbols）。 : 用于跳转到当前文件中的某一行。 edt 显示所有已经打开的文件 edt active 显示当前活动组中的文件 ext 插件的管理 ext install 搜索和安装插件。 task 任务 debug 调试功能 term创建和管理终端实例 view 打开各个 UI 组件 # 窗口管理 快捷键 描述 ctrl + w 窗口切换 ctrl + r 切换文件夹 ctrl+r cmd+enter 新建窗口打开文件夹 # 集成终端 快捷键 描述 ctrl + ` 切换集成终端 ctrl + shift + ` 新建集成终端 cmd+shift+p Run Active File In Active Terminal 在集成终端中运行当前脚本 cmd+shift+p Run Selected Text In Active Terminal 在集成终端中运行所选文本 # 任务管理 快捷键 描述 cmd+shift+p run task 自动检测当前项目中可运行的任务 cmd+shift+p Configure Task 配置任务 Cmd + Shift + b 运行默认的生成任务（build task） # 鼠标操作 文本选择 双击鼠标，选中单词 三击鼠标，选中一行 四击鼠标，选中整个文档 单击行号，选中行 文本编辑 选中后可以拖动文本到指定区域 拖动过程中按option，变成复制文本到指定区域 在悬停窗口上按下cmd，提示函数的实现 ","date":"2019-05-16T10:56:00+08:00","permalink":"https://mazhen.tech/p/vs-code-%E5%BF%AB%E6%8D%B7%E9%94%AE/","title":"vs code 快捷键"},{"content":"飞腾芯片 + 银河麒麟OS是目前国产自主可控市场上的主流基础平台。飞腾芯片是aarch64架构，是支持64位的ARM芯片。银河麒麟是基于Ubuntu的发行版。因此可以认为飞腾芯片 + 银河麒麟OS相当于 ARM64 + Ubuntu。\n本文介绍在飞腾平台上编译安装nginx的步骤。\n下载nginx源码 从http://nginx.org/en/download.html下载当前稳定版本的源码，例如\n1 wget http://nginx.org/download/nginx-1.14.2.tar.gz 解压nginx源码：\n1 tar -zxvf nginx-1.14.2.tar.gz nginx配置文件的语法高亮 将nginx源码目录下contrib/vim/的所有内容，copy到用户的$HOME/.vim目录，可以实现nginx配置文件在vim中的语法高亮。\n1 2 mkdir ~/.vim cp -r contrib/vim/* ~/.vim/ 再使用vim打开nginx.conf，可以看到配置文件已经可以语法高亮。\n编译前的配置 查看编译配置支持的参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 $ ./configure --help | more --help print this message --prefix=PATH set installation prefix --sbin-path=PATH set nginx binary pathname --modules-path=PATH set modules path --conf-path=PATH set nginx.conf pathname --error-log-path=PATH set error log pathname --pid-path=PATH set nginx.pid pathname --lock-path=PATH set nginx.lock pathname --user=USER set non-privileged user for worker processes --group=GROUP set non-privileged group for worker processes --build=NAME set build name --builddir=DIR set build directory --with-select_module enable select module --without-select_module disable select module --with-poll_module enable poll module --without-poll_module disable poll module ...... --with-libatomic force libatomic_ops library usage --with-libatomic=DIR set path to libatomic_ops library sources --with-openssl=DIR set path to OpenSSL library sources --with-openssl-opt=OPTIONS set additional build options for OpenSSL --with-debug enable debug logging --with开头的模块缺省不包括在编译结果中，如果想使用需要在编译配置时显示的指定。--without开头的模块则相反，如果不想包含在编译结果中需要显示设定。\n例如我们可以这样进行编译前设置：\n1 ./configure --prefix=/home/adp/nginx --with-http_ssl_module 设置了nginx的安装目录，需要http_ssl模块。\n如果报错缺少OpenSSL，需要先安装libssl。在/etc/apt/sources.list.d目录下增加支持ARM64的apt源，例如国内的清华，创建tsinghua.list，内容如下：\n1 2 3 4 5 6 7 8 deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial main multiverse restricted universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-security main multiverse restricted universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-updates main multiverse restricted universe deb http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-backports main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-security main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-updates main multiverse restricted universe deb-src http://mirrors.tuna.tsinghua.edu.cn/ubuntu-ports/ xenial-backports main multiverse restricted universe 执行命令安装OpenSSL：\n1 2 $ sudo apt-get update $ sudo apt-get install libssl-dev configure命令执行完后，会生成中间文件，放在目录objs下。其中最重要的是ngx_modules.c文件，它决定了最终那些模块会编译进nginx。\n执行编译 在nginx目录下执行make编译：\n1 $ make 编译成功的nginx二进制文件在objs目录下。如果是做nginx的升级，可以直接将这个二进制文件copy到nginx的安装目录中。\n安装 在nginx目录下执行make install进行安装：\n1 $ make install 安装完成后，我们到--prefix指定的目录中查看安装结果：\n1 2 3 4 5 6 7 $ tree -L 1 /home/adp/nginx nginx/ ├── conf ├── html ├── logs └── sbin 验证安装结果 编辑nginx/conf/nginx.conf文件，设置监听端口为8080：\n1 2 3 4 5 6 7 http { ... server { listen 8080; server_name localhost; ... 启动nginx\n1 ./sbin/nginx 访问默认首页：\n1 2 3 4 5 6 7 8 9 10 11 $ curl -I http://localhost:8080 HTTP/1.1 200 OK Server: nginx/1.14.2 Date: Tue, 02 Apr 2019 08:38:02 GMT Content-Type: text/html Content-Length: 612 Last-Modified: Tue, 02 Apr 2019 08:30:04 GMT Connection: keep-alive ETag: \u0026#34;5ca31d8c-264\u0026#34; Accept-Ranges: bytes 其他常用命令：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 查看帮助 $ ./sbin/nginx -? # 重新加载配置 $ ./sbin/nginx -s reload # 立即停止服务 $ ./sbin/nginx -s stop # 优雅停止服务 $ ./sbin/nginx -s quit # 测试配置文件是否有语法错误 $ ./sbin/nginx -t/-T # 打印nginx版本、编译信息 $ ./sbin/nginx -v/-V ","date":"2019-04-02T09:45:05+08:00","permalink":"https://mazhen.tech/p/%E5%9C%A8%E5%9B%BD%E4%BA%A7%E9%A3%9E%E8%85%BE%E5%B9%B3%E5%8F%B0%E4%B8%8A%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85nginx/","title":"在国产飞腾平台上编译安装nginx"},{"content":"kubectl会使用$HOME/.kube目录下的config文件作为缺省的配置文件。我们可以使用kubectl config view查看配置信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 $kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://172.18.100.90:6443 name: cluster-1 contexts: - context: cluster: cluster-1 user: cluster-1-admin name: cluster-1-admin@cluster-1 current-context: cluster-1-admin@cluster-1 kind: Config preferences: {} users: - name: cluster-1-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 可以看到，配置文件主要包含了clusters，users和contexts三部分信息。context是访问一个kubernetes集群所需要的参数集合。每个context有三个参数：\ncluster：要访问的集群信息 namespace：用户工作的namespace，缺省值为default user：连接集群的认证用户 缺省情况下，kubectl会使用current-context指定的context作为当前的工作集群环境。不难想象，切换context就可以切换到不同的kubernetes集群。\n在不了解context的概念之前，想访问不同的集群，每次都要把集群对应的config文件copy到$HOME/.kube目录下，同时要记得使用kubectl cluster-info确认当前访问的集群：\n1 2 3 4 5 6 $kubectl cluster-info Kubernetes master is running at https://172.18.100.90:6443 KubeDNS is running at https://172.18.100.90:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. 在看了这篇文档后，才知道kubectl可以切换context来管理多个集群。如果你有多个集群的config文件，可以在系统环境变量KUBECONFIG中指定每个config文件的路径，例如：\n1 export KUBECONFIG=/home/mazhen/kube-config/config-cluster-1:/home/mazhen/kube-config/config-cluster-1 再使用kubectl config view查看集群配置时，kubectl会自动合并多个config的信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 $ kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: DATA+OMITTED server: https://172.20.51.11:6443 name: cluster-2 - cluster: certificate-authority-data: DATA+OMITTED server: https://172.18.100.90:6443 name: cluster-1 contexts: - context: cluster: cluster-2 user: cluster-2-admin name: cluster-2-admin@cluster-2 - context: cluster: cluster-1 user: cluster-1-admin name: cluster-1-admin@cluster-1 current-context: cluster-1-admin@cluster-1 kind: Config preferences: {} users: - name: cluster-2-admin user: client-certificate-data: REDACTED client-key-data: REDACTED - name: cluster-1-admin user: client-certificate-data: REDACTED client-key-data: REDACTED 可以看到，配置中包含了两个集群，两个用户，以及两个context。我们可以使用kubectl config get-contexts查看配置中所有的context：\n1 2 3 4 5 $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE cluster-2-admin@cluster-2 cluster-2 cluster-2-admin * cluster-1-admin@cluster-1 cluster-1 cluster-1-admin 星号*标识了当前的工作集群。如果想访问另一个集群，使用kubectl config use-context进行切换：\n1 2 3 $ kubectl config use-context cluster-2-admin@cluster-2 Switched to context \u0026#34;cluster-2-admin@cluster-2\u0026#34;. 我们可以再次确认切换的结果：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * cluster-2-admin@cluster-2 cluster-2 cluster-2-admin cluster-1-admin@cluster-1 cluster-1 cluster-1-admin $ kubectl cluster-info Kubernetes master is running at https://172.20.51.11:6443 KubeDNS is running at https://172.20.51.11:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy kubernetes-dashboard is running at https://172.20.51.11:6443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. 看吧，kubectl切换context管理多集群是多么的方便。\n","date":"2019-03-29T09:43:28+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8kubectl%E7%AE%A1%E7%90%86%E5%A4%9A%E9%9B%86%E7%BE%A4/","title":"使用kubectl管理多集群"},{"content":"先使用visudo 查看当前的配置，这个命令编辑的是/etc/sudoers文件。可以直接在这个文件中为用户设置sudo权限：\n1 2 3 # User privilege specification root ALL=(ALL:ALL) ALL adp ALL=(ALL) ALL 也可以看看哪个group有root权限，然后将用户加入这个group。例如下面的配置，admin组有root权限：\n1 2 # Members of the admin group may gain root privileges %admin ALL=(ALL) ALL 可以将用户加入admin组，自然就有了sudo权限：\n1 usermod -a -G admin [user] 如果提示admin不存在，可以先创建这个组，再将用户加入这个group：\n1 2 groupadd admin usermod -a -G admin [user] 如果不想编辑/etc/sudoers，可以在/etc/sudoers.d/目录下，为需要sudo权限的用户创建独立的文件，在文件中分别为用户授权，格式和/etc/sudoers一样：\n1 adp ALL=(ALL) ALL 修改文件权限：\n1 chmod 440 adp 这样做的好处每个用户都有独立的配置文件，是方便管理。\n最后，建议将/sbin 和 /usr/sbin 加入到用户路径。\n1 PATH=$PATH:/usr/sbin:/sbin ","date":"2019-03-28T09:41:19+08:00","permalink":"https://mazhen.tech/p/%E5%A6%82%E4%BD%95%E8%AE%A9%E7%94%A8%E6%88%B7%E6%8B%A5%E6%9C%89sudo%E6%9D%83%E9%99%90/","title":"如何让用户拥有sudo权限"},{"content":"为了方便开发者体验Kubernetes，社区提供了可以在本地部署的Minikube。由于在国内网络环境内，无法顺利的安装使用Minikube，我们可以从阿里云的镜像地址来获取所需Docker镜像和配置。\n安装VirtualBox sudo apt-get install virtualbox\n安装 Minikube 1 curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v0.35.0/minikube-linux-amd64 \u0026amp;\u0026amp; chmod +x minikube \u0026amp;\u0026amp; sudo mv minikube /usr/local/bin/ 启动Minikube 1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ minikube start --registry-mirror=https://registry.docker-cn.com 😄 minikube v0.35.0 on linux (amd64) 🔥 Creating virtualbox VM (CPUs=2, Memory=2048MB, Disk=20000MB) ... 📶 \u0026#34;minikube\u0026#34; IP address is 192.168.99.100 🐳 Configuring Docker as the container runtime ... ✨ Preparing Kubernetes environment ... 🚜 Pulling images required by Kubernetes v1.13.4 ... 🚀 Launching Kubernetes v1.13.4 using kubeadm ... ⌛ Waiting for pods: apiserver proxy etcd scheduler controller addon-manager dns 🔑 Configuring cluster permissions ... 🤔 Verifying component health ..... 💗 kubectl is now configured to use \u0026#34;minikube\u0026#34; 🏄 Done! Thank you for using minikube! 检查状态 1 2 3 4 5 6 $ minikube status host: Running kubelet: Running apiserver: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100 kubernetes已经成功运行，可以使用kubectl访问集群：\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-89cc84847-2k67h 1/1 Running 0 18m coredns-89cc84847-95zsj 1/1 Running 0 18m etcd-minikube 1/1 Running 0 18m kube-addon-manager-minikube 1/1 Running 0 19m kube-apiserver-minikube 1/1 Running 0 18m kube-controller-manager-minikube 1/1 Running 0 18m kube-proxy-f66hz 1/1 Running 0 18m kube-scheduler-minikube 1/1 Running 0 18m kubernetes-dashboard-7d8d567b4d-h82vx 1/1 Running 0 18m storage-provisioner 1/1 Running 0 18m 停止Minikube 1 2 3 4 $ minikube stop ✋ Stopping \u0026#34;minikube\u0026#34; in virtualbox ... 🛑 \u0026#34;minikube\u0026#34; stopped. 删除本地集群 1 2 3 4 $ minikube delete 🔥 Deleting \u0026#34;minikube\u0026#34; from virtualbox ... 💔 The \u0026#34;minikube\u0026#34; cluster has been deleted. ","date":"2019-03-25T09:39:12+08:00","permalink":"https://mazhen.tech/p/%E5%9C%A8ubuntu%E4%B8%8A%E5%AE%89%E8%A3%85minikube/","title":"在Ubuntu上安装Minikube"},{"content":"刚接触Kubernetes时很容易被它繁多的概念（POD，Service，Deployment \u0026hellip;)以及比较复杂的部署架构搞晕，本文希望能通过一个简单的例子，讲解Kubernetes最基本的工作原理。\nKubernetes本质上是为用户提供了一个容器编排工具，可以管理和调度用户提交的作业。用户在 YAML 配置文件中描述应用所需的环境配置、参数等信息，以及应用期待平台提供的服务（负载均衡，水平扩展等），然后将 YAML 提交，Kubernetes会按照用户的要求，在集群上将应用运行起来。在遇到异常情况，或用户的主动调整时，Kubernetes 将始终保持应用实际的运行状态，符合用户的期待状态。\nKubernetes 是由 Master 和 Node 两种节点组成。Master由3个独立的组件组成：\n负责 API 服务的 kube-apiserver 负责容器编排的 kube-controller-manager 负责调度的 kube-scheduler Kubernetes 集群的所有状态信息都存储在 etcd，其他组件对 etcd 的访问，必须通过 kube-apiserver。\nKubelet 运行在所有节点上，它通过容器运行时（例如Docker），让应用真正的在节点上运行起来。\n下面通过一个简单的例子，描述 Kubernetes 的各个组件，是如何协作完成工作的。\n用户将 YAML 提交给 kube-apiserver，YAML 经过校验后转换为 API 对象，存储在 etcd 中。\nkube-controller-manager 是负责编排的组件，当它发现有新提交的应用，会根据配置的要求生成对应的 Pod 对象。Pod 是 Kubernetes 调度管理的最小单元，可以简单的认为，Pod 就是一个虚拟机，其中运行着关系紧密的进程，共同组成用户的应用。例如Web应用进程和日志收集agent，可以包含在一个Pod中。Pod 对象也存储在 etcd 中。本例子中用户定义 replicas 为2，也就是用户期待有两个 Pod 实例。\n其实kube-controller-manager 内部一直在做循环检查，只要发现有应用没有对应的 Pod，或者 Pod 的数量不满足用户的期望，它都会进行适当的调整，创建或删除Pod 对象。\nkube-scheduler 负责 Pod 的调度。kube-scheduler 发现有新的 Pod 出现，它会按照调度算法，为每个 Pod 寻找一个最合适的节点（Node）。kube-scheduler 对一个 Pod 的调度成功，实际上就是在 Pod 对象上记录了调度结果的节点名称。注意，Pod 调度成功，只是在 Pod 上标记了节点的名字，Pod 是否真正在节点上运行，就不是kube-scheduler的责任了。\nKubelet 运行在所有节点上，它会订阅所有 Pod 对象的变化，当发现一个 Pod 与 Node 绑定，也就是这个 Pod 上标记了Node的名字，而这个被绑定的 Node 就是它自己，Kubelet 就会在这个节点将 Pod 启动。\n至此，用户提交的应用在Kubernetes集群中就运行起来了。\n同时，上述的过程一直在循环往复。例如，用户更新了 YAML，将 replicas 改为3，并将更新后的 YAML 再次提交。kube-controller-manager会发现实际运行的 Pod 数量与用户的期望不符，它会生成一个新的 Pod 对象。紧接着 kube-scheduler 发现一个没有绑定节点的 Pod，它会按照调度算法为这个Pod寻找一个最佳节点完成绑定。最后，某个Kubelet 发现新绑定节点的 Pod 应该在本节点上运行，它会通过接口调用Docker完成 Pod 的启动。\n上面就是 Kubernetes 基本工作流程的简单描述，希望对你理解它的工作原理有所帮助。\n","date":"2019-02-24T17:30:11+08:00","permalink":"https://mazhen.tech/p/kubernetes%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E6%A6%82%E8%BF%B0/","title":"Kubernetes工作原理概述"},{"content":"Kubernetes在2017年赢得了容器编排之战，使得基于容器+Kubernetes来构建PaaS平台成为了云计算的主流方式。在人们把关注的目光都聚焦在Kubernetes上时，容器技术领域在2018年也发生了很多创新，包括amazon最近开源的轻量级虚拟机管理器 Firecracker，Google在今年5月份开源的基于用户态操作系统内核的 gVisor 容器，还有更早开源的虚拟化容器项目 KataContainers，可谓百花齐放。一般的开发者可能认为容器就等于Docker，没想到容器领域还在发生着这么多创新。我在了解这些项目时，发现如果没有一些背景知识，很难get到它们的创新点。我试着通过这篇文章进行一次背景知识的梳理。让我们先从最基本的问题开始：操作系统是怎么工作的？\n# 操作系统的工作方式 我们不去讨论操作系统的标准定义，而是想想操作系统的本质是什么，它是怎么为应用提供服务的呢？\n其实操作系统很“懒”，它不是一直运行着主动干活，而是躺在那儿等着被中断“唤醒”，然后根据中断它的事件类型做下一步处理：是不是有键盘敲击、网络数据包到达，还是时间片到了该考虑进程切换，或者是有应用向内核发出了服务请求。内核处理完唤醒它的事件，会将控制权返还给应用程序，然后等着再次被中断“唤醒”。内核就是这样由中断驱动，控制着CPU、内存等硬件资源，为应用程序提供服务。\n”唤醒“操作系统内核的事件主要分为三类：\n中断：来自硬件设备的处理请求； 异常：当前正在执行的进程，由于非法指令或者其他原因导致执行失败而产生的异常事情处理请求，典型的如缺页异常； 系统调用：应用程序主动向操作系统内核发出的服务请求。系统调用的本质其实也是中断，相对于硬件设备的中断，这种中断被称为软中断。 # CPU的特权等级 内核代码常驻在内存中，被每个进程映射到它的逻辑地址空间：\n进程逻辑地址空间的最大长度与实际可用的物理内存数量无关，由CPU的字长决定。进程的逻辑地址空间划分为两个部分，分别称为内核空间和用户空间。用户空间是彼此独立的，而逻辑地址空间顶部的内核空间是被所有进程共享。\n从每个进程的角度来看，地址空间中只有自身一个进程，它不会感知到其他进程的存在。由于内核空间被所有进程共享，为了防止进程修改彼此的数据而造成相互干扰，用户进程不能直接操作或读取内核空间中的数据。同时由于内核管理着所有硬件资源，也不能让用户进程直接执行内核空间中的代码。操作系统应该如何做到这种限制呢？\n实际上，操作系统的实现依赖 CPU 提供的功能。现代的CPU体系架构都提供几种特权级别，每个特权级别有各种限制。各级别可以看作是环，内环能够访问更多的功能，外环则较少，被称为protection rings：\nIntel 的 CPU 提供了4种特权级别， Linux 只使用了 Ring0 和 Ring3 两个级别。Ring 0 拥有最多的特权，它可以直接和CPU、内存等物理硬件交互。 Ring 0 被称为内核态，操作系统内核正是运行在Ring 0。Ring 3被称为用户态，应用程序运行在用户态。\n# 系统调用 在用户态禁止直接访问内核态，也就是说不同通过普通的函数调用方式调用内核代码，而必须使用系统调用陷入（trap）内核，完成从用户态到内核态的切换。内核首先检查进程是否允许执行想要的操作，然后代表进程执行所需的操作，完成后再返回到用户态。\n除了代表用户程序执行代码之外，内核还可以由硬件中断激活，然后在中断上下文中运行。另外除了普通进程，系统中还有内核线程在运行。内核线程不与任何特定的用户空间进程相关联。\nCPU 在任何时间点上的活动必然为下列三者之一 ：\n运行于用户空间，执行应用程序 运行于内核空间，处于进程上下文，即代表某个特定的进程执行 运行于内核空间，处于中断上下文，与任何进程无关，处理某个特定的中断 # 优化系统调用 从上面的讨论可以看出，由于系统调用会经过用户态到内核态的切换，开销要比普通函数调用大很多，因此在进行系统级编程时，减少用户态与内核态之间的切换是一个很重要的优化方法。\n例如同样是实现 Overlay网络，使用 VXLAN 完全在内核态完成封装和解封装，要比把数据包从内核态通过虚拟设备TUN传入用户态再进行处理要高效很多。\n对应的也可以从另外一个方向进行优化：使用 DPDK 跳过内核网络协议栈，数据从网卡直接到达用户态，在用户态处理数据包，也就是说网络协议栈完全运行在用户态，同样避免了用户态和内核态的切换。像腾讯开源的 F-Stack就是一个基于DPDK运行在用户空间的TCP/IP协议栈。\n了解了操作系统内核的基本工作方式，我们再看下一个话题：虚拟化。\n# 虚拟化技术 为了更高效灵活的使用硬件资源，同时能够实现服务间的安全隔离，我们需要虚拟化技术。运行虚拟化软件（Hypervisor或者叫VMM，virtual machine monitor）的物理设施我们称之为Host，安装在Hypervisor之上的虚拟机称为Guest。\n根据Hypervisor在系统中的位置，可以将它归类为type-1或type-2型。如果hypervisor直接运行在硬件之上，它通常被认为是Type-1型。如果hypervisor作为一个单独的层运行在操作系统之上，它将被认为是Type 2型。\nType-1型Hypervisor的概念图：\nType-2型Hypervisor的概念图：\n# KVM \u0026amp; QEMU 实际上Type-1和Type-2并没有严格的区分，像最常见的虚拟化软件 KVM（Kernel-based Virtual Machine）是一个Linux内核模块，加载KVM后Linux内核就转换成了Type-1 hypervisor。同时，Linux还是一个通用的操作系统，也可以认为KVM是运行在Linux之上的Type-2 hypervisor。\n为了在Host上创建出虚拟机，仅仅有KVM是不够的。对于 I/O 的仿真，KVM 还需要 QEMU的配合。QEMU 是一个运行在用户空间程序，它可以仿真处理器和一系列的物理设备：磁盘、网络、VGA、PCI、USB、串口/并口等等，基于QEMU可以构造出一个完整的虚拟PC。\n值得注意的是，QEMU 有两种运行模式：仿真模式和虚拟化模式。在仿真模式下，QEMU可以在一个Intel的Host上运行ARM或MIPS虚拟机。这是怎么做到的呢？实际上，QEMU 通过 TCG（Tiny Code Generator）技术进行了二进制代码转换，可以认为这是一种高级语言的VM，就像JVM。例如可以将运行在ARM上的二进制转换为一种中间字节码，然后让它运行在Host的Intel CPU上。很明显，这种二进制代码转换有着巨大的性能开销。\n相对应的，QEMU的另一种是虚拟化模式，它借助KVM完成处理器的虚拟化。由于和CPU的体系结构紧密关联，虚拟化模式能够带来更好的性能，限制是Guest必须使用和Host一样的CPU体系机构。这就是我们最常用到的虚拟化技术栈：KVM/QEMU\nKVM 和 QEMU 有两种交互方式：通过设备文件/dev/kvm 和通过内存映射页面。QEMU 和 KVM之间的大块数据传递会使用内存映射页面。/dev/kvm是KVM暴露的主要API，它支持一系列ioctl接口，QEMU 使用这些接口和KVM交互。/dev/kvm API分为三个层次：\nSystem Level: 用于KVM全局状态的维护，例如创建 VM； VM Level: 用于处理和特定VM相关工作的 API，vCPU 就是通过这个级别的API创建出来的； vCPU Level: 这是最细粒度的API，用于和特定vCPU的交互。QEMU会为每个vCPU分配一个专门的线程。 # CPU 虚拟化技术 VT-x KVM和QEMU配合完美，但和CPU的特权级别在一起就遇到了麻烦。我们知道，hypervisor需要管理宿主机的CPU、内存、I/O设备等资源，因此它需要运行在ring 0级别才能执行这些高特权操作。然而运行在VM中的操作系统希望得到访问所有资源的权限，它并不知道自己运行在虚拟机中。因为同一时间只有一个内核可以运行在ring 0，Guest OS不得不被“挤”到了ring 1，这一级别不能满足内核的需求。怎么解决？\n虽然我们可以使用软件的方式进行模拟，让hypervisor拦截应用发往ring 0的系统调用，再转发给Guest OS，但这么做会产生额外的性能损耗，而且方案复杂难以维护。Intel和AMD认识到了虚拟化的重要性，各自独立创建了X86架构的扩展指令集，分别称为 VT-x and AMD-V，从CPU层面支持虚拟化。\n以Intel CPU为例，VT-x不仅增加了虚拟化相关的指令集，还将CPU的指令划分会两种模式：root 和 non-root。hypervisor运行在 root 模式，而VM运行在non-root模式。指令在non-root模式的运行速度和root模式几乎一样，除了不能执行一些涉及CPU全局状态切换的指令。\nVMX（Virtual Machine Extensions）是增加到VT-x中的指令集，主要有四个指令：\nVMXON：在这个指令执行之前，CPU还没有root 和 non-root的概念。VMXON执行后，CPU进入虚拟化模式。 VMXOFF：VMXON的相反操作，执行VMXOFF退出虚拟化模式。 VMLAUNCH：创建一个VM实例，然后进入non-root模式。 VMRESUME：进入non-root模式，恢复前面退出的VM实例。当VM试图执行一个在non-root禁止的指令，CPU立即切换到root模式，类似前面介绍的系统调用trap方式，这就是VM的退出。 这样，VT-x/KVM/QEMU 构成了今天应用最广泛的虚拟化技术栈。\n有了上面的铺垫，终于要谈到容器技术了。\n# 容器的本质 虽然虚拟化技术在灵活高效的使用硬件资源方面前进了一大步，但人们还觉得远远不够。特别是在机器使用量巨大的互联网公司。因为虚拟机一旦创建，为它分配的资源就相对固定，缺乏弹性，很难再提高机器的利用率。而且创建、销毁虚拟机也是相对“重”的操作。这时候容器技术出现了。我们知道，容器依赖的底层技术，Linux Namesapce和Cgroups都是最早由Google开发，提交进Linux内核的。\n容器的本质就是一个进程，只不过对它进行了Linux Namesapce隔离，让它“看”不到外面的世界，用Cgroups限制了它能使用的资源，同时利用系统调用pivot_root或chroot切换了进程的根目录，把容器镜像挂载为根文件系统rootfs。rootfs中不仅有要运行的应用程序，还包含了应用的所有依赖库，以及操作系统的目录和文件。rootfs打包了应用运行的完整环境，这样就保证了在开发、测试、线上等多个场景的一致性。\n从上图可以看出，容器和虚拟机的最大区别就是，每个虚拟机都有独立的操作系统内核Guest OS，而容器只是一种特殊的进程，它们共享同一个操作系统内核。\n看清了容器的本质，很多问题就容易理解。例如我们执行 docker exec 命令能够进入运行中的容器，好像登录进独立的虚拟机一样。实际上这只不过是利用系统调用setns，让当前进程进入到容器进程的Namesapce中，它就能“看到”容器内部的情况了。\n由于容器就是进程，它的创建、销毁非常轻量，对资源的使用控制更加灵活，因此让Kubernetes这种容器编排和资源调度工具可以大显身手，通过合理的搭配，极大的提高了整个集群的资源利用率。\n# 虚拟化容器技术 前面提到，运行在一个宿主机上的所有容器共享同一个操作系统内核，这种隔离级别存在着很大的潜在安全风险。因此在公有云的多租户场景下，还是需要先用虚拟机进行租户强隔离，然后用户在虚拟机上再使用容器+Kubernetes部署应用。\n然而在Serverless的场景下，传统的先建虚拟机再创建容器的方式，在灵活性、执行效率方面难以满足需求。随着Serverless、FaaS（Function-as-a-Service）的兴起，各公有云厂商都将安全性容器作为了创新焦点。\n一个很自然能想到的方案，是结合虚拟机的强隔离安全性+容器的轻量灵活性，这就是虚拟化容器项目 KataContainers。\nOpenStack在2017年底发布的 KataContainers 项目，最初是由 Intel ClearContainer 和 Hyper runV 两个项目合并而产生的。在Kubernetes场景下，一个Pod对应于Kata Containers启动的一个轻量化虚拟机，Pod中的容器，就是运行在这个轻量级虚拟机里的进程。每个Pod都运行在独立的操作系统内核上，从而达到安全隔离的目的。\n可以看出，KataContainers 依赖 KVM/QEMU技术栈。\namazon最近开源的Firecracker也是为了实现在 functions-based services 场景下，多租户安全隔离的容器。\nFirecracker同样依赖KVM，然后它没有用到QEMU，因为Firecracker本身就是QEMU的替代实现。Firecracker是一个比QEMU更轻量级的VMM，它只仿真了4个设备：virtio-net，virtio-block，serial console和一个按钮的键盘，仅仅用来停止microVM。理论上，KataContainers可以用Firecracker换掉它现在使用的QEMU，从而将 Firecracker整合进Kubernetes生态圈。\n其实Google早就没有使用QEMU，而且对KVM进行了深度定制。我们可以从这篇介绍看出端倪：7 ways we harden our KVM hypervisor at Google Cloud: security in plaintext\nNon-QEMU implementation: Google does not use QEMU, the user-space virtual machine monitor and hardware emulation. Instead, we wrote our own user-space virtual machine monitor that has the following security advantages over QEMU\n\u0026hellip;\n# gVisor Google 开源的gVisor为了实现安全容器另辟蹊径，它用 Go 实现了一个运行在用户态的操作系统内核，作为容器运行的Guest Kernel，每个容器都依赖独立的操作系统内核，实现了容器间安全隔离的目的。\n虽然 gVisor 今年才开源，但它已经在Google App Engine 和 Google Cloud Functions运行了多年。\ngVisor作为运行应用的安全沙箱，扮演着Virtual kernel的角色。同时gVisor 包含了一个兼容Open Container Initiative (OCI) 的运行时runsc，因此可以用它替换掉 Docker 的 runc，整合进Kubernetes生态圈，为Kubernetes带来另一种安全容器的实现方案。\nKata Containers和gVisor的本质都是为容器提供一个独立的操作系统内核，避免容器共享宿主机的内核而产生安全风险。KataContainers使用了传统的虚拟化技术，gVisor则自己实现了一个运行在用户态、极小的内核。gVisor比KataContainers更加轻量级，根据这个分享的介绍，gVisor目前只实现了211个Linux系统调用，启动时间150ms，内存占用15MB。\ngVisor实现原理，简单来说是模拟内核的行为，使用某种方式拦截应用发起的系统调用，经过gVisor的安全控制，代替容器进程向宿主机发起可控的系统调用。目前gVisor实现了两种拦截方式：\n基于Ptrace 机制的拦截 使用 KVM 来进行系统调用拦截。 因为gVisor基于拦截系统调用的实现原理，它并不适合系统调用密集的应用。\n最后，对于像我这样没有读过Linux内核代码的后端程序员，gVisor是一个很好的窥探内核内部实现的窗口，又激起了我研究内核的兴趣。Twitter上看到有人和我有类似的看法：\n希望下次能分享gVisor深入研究系列。保持好奇心，Stay hungry. Stay foolish.\n","date":"2018-12-16T22:03:21+08:00","permalink":"https://mazhen.tech/p/%E5%AE%B9%E5%99%A8%E6%8A%80%E6%9C%AF%E5%88%9B%E6%96%B0%E6%BC%AB%E8%B0%88/","title":"容器技术创新漫谈"},{"content":"容器的跨主机通信主要有两种方式：封包模式和路由模式。上一篇文章演示了使用VXLAN协议的封包模式，这篇将介绍另一种方式，利用三层网络的路由转发实现容器的跨主机通信。\n# 路由模式概述 宿主机将它负责的容器IP网段，以某种方式告诉其他节点，然后每个节点根据收到的\u0026lt;宿主机-容器IP网段\u0026gt;映射关系，配置本机路由表。\n这样对于容器间跨节点的IP包，就可以根据本机路由表获得到达目的容器的网关地址，即目的容器所在的宿主机地址。接着在把IP包封装成二层网络数据帧时，将目的MAC地址设置为网关的MAC地址，IP包就可以通过二层网络送达目的容器所在的宿主机。\n至于用什么方式将\u0026lt;宿主机-容器IP网段\u0026gt;映射关系发布出去，不同的项目采用了不同的实现方案。\nFlannel是将这些信息集中存储在etcd中，每个节点从etcd自动获取数据，更新宿主机路由表。\nCalico则使用BGP（Border Gateway Protocol）协议交换共享路由信息，每个宿主机都是运行在它之上的容器的边界网关。\n如果宿主机之间跨了网段怎么办？宿主机之间的二层网络不通，虽然知道目的容器所在的宿主机，但没办法将目的MAC地址设置为那台宿主机的MAC地址。\nCalico有两种解决方案：\nIPIP 模式，在跨网段的宿主机之间建立“隧道” 让宿主机之间的路由器“学习”到容器路由规则，每个路由器都知道某个容器IP网段是哪个宿主机负责的，容器间的IP包就能正常路由了。 # 动手实验 路由模式的实验比较简单，关键在于宿主机上路由规则的配置。为了简化实验，这些路由规则都是我们手工配置，而且两个节点之间二层网络互通，没有跨网段。\n参照Docker跨主机Overlay网络动手实验，创建“容器”，veth pairs，bridge，设置IP，激活虚拟设备。\n然后在node-1上增加路由规则：\n1 sudo ip route add 172.18.20.0/24 via 192.168.31.192 在node-2上增加路由规则：\n1 sudo ip route add 172.18.10.0/24 via 192.168.31.183 当docker1访问docker2时，IP包会从veth到达br0，然后根据node-1上刚设置的路由规则，访问172.18.20.0/24网段的网关地址为node-2，这样，IP包就能路由到node-2了。\n同时，node-2的路由表中包含这样的一条规则：\n1 2 3 4 $ ip route ... 172.18.20.0/24 dev br0 proto kernel scope link src 172.18.20.1 到达node-2的IP包，会根据这条规则路由到网桥br0，最终到达docker-2。反过来从docker2访问docker1的过程也是类似。\n# 总结 两种容器跨主机的通信方案我们都实验了一下，现在做个简单总结对比：\n封包模式对基础设施要求低，三层网络通就可以了。但封包、解包带来的性能损耗较大。 路由模式性能好，但要求二层网络连通，或者在跨网段的情况下，要求路由器能配合“学习”路由规则。 至此，容器网络的三篇系列完成：\nDocker单机网络模型动手实验 Docker跨主机Overlay网络动手实验 Docker跨主机通信路由模式动手实验（本篇） ","date":"2018-11-11T22:00:22+08:00","permalink":"https://mazhen.tech/p/docker%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E4%BF%A1%E8%B7%AF%E7%94%B1%E6%A8%A1%E5%BC%8F%E5%8A%A8%E6%89%8B%E5%AE%9E%E9%AA%8C/","title":"Docker跨主机通信路由模式动手实验"},{"content":"上一篇文章我演示了docker bridge网络模型的实验，这次我将展示如何利用Overlay 网络实现跨主机容器的通信。\n两个容器docker1和docker2分别位于节点Node-1和Node-2，如何实现容器的跨主机通信呢？一般来说有两种实现方式：\n封包模式：利用Overlay网络协议在节点间建立“隧道”，容器之间的网络包被封装在外层的网络协议包中进行传输。 路由模式：容器间的网络包传输全部用三层网络的路由转发来实现。 本文主要介绍封包模式。Overlay网络主要有两种方式，一种是使用UDP在用户态封装，一种是利用VXLAN 在内核态封装。由于减少了用户态到内核态的切换，封包解包逻辑都在内核态进行，VXLAN 的性能更好，成为了容器网络的主流方案。\n关于路由模式，会在下一篇文章介绍。\n# VXLAN VXLAN（Virtual Extensible LAN）是一种网络虚拟化技术，它将链路层的以太网包封装到UDP包中进行传输。VXLAN最初是由VMware、Cisco开发，主要解决云环境下多租户的二层网络隔离。我们常听到公有云厂商宣称支持VPC（virtual private cloud），实际底层就是使用VXLAN实现的。\nVXLAN packet的结构：\n我们可以看到，最内部是原始的二层网络包，外面加上一个VXLAN header，其中最重要的是VNI（VXLAN network identifier)字段，它用来唯一标识一个VXLAN。也就是说，使用不同的VNI来区分不同的虚拟二层网络。VNI有24位，基本够公用云厂商使用了。要知道原先用来网络隔离的虚拟局域网VLAN只支持4096个虚拟网络。\n在 VXLAN header外面封装了正常的UDP包。VXLAN在UDP之上，实现了一个虚拟的二层网络，连接在这个虚拟二层网络上的主机，就像连接在普通的局域网上一样，可以互相通信。\n介绍完背景知识，我们可以开始动手实验了。\n# 实现方案一 参照Flannel的实现方案：\n配置内核参数，允许IP forwarding 分别在Node-1、Node-2上执行：\n1 sudo sysctl net.ipv4.conf.all.forwarding=1 创建“容器” 在Node-1上执行：\n1 sudo ip netns add docker1 在Node-2上执行：\n1 sudo ip netns add docker2 为什么创建个Namesapce就说是“容器”？请参考上一篇文章。\n创建Veth pairs 分别在Node-1、Node-2上执行：\n1 sudo ip link add veth0 type veth peer name veth1 将Veth的一端放入“容器” 在Node-1上执行：\n1 sudo ip link set veth0 netns docker1 在Node-2上执行：\n1 sudo ip link set veth0 netns docker2 创建bridge 分别在Node-1、Node-2上创建bridge br0：\n1 sudo brctl addbr br0 将Veth的另一端接入bridge 分别在Node-1、Node-2上执行：\n1 sudo brctl addif br0 veth1 为\u0026quot;容器“内的网卡分配IP地址，并激活上线 在Node-1上执行：\n1 2 sudo ip netns exec docker1 ip addr add 172.18.10.2/24 dev veth0 sudo ip netns exec docker1 ip link set veth0 up 在Node-2上执行：\n1 2 sudo ip netns exec docker2 ip addr add 172.18.20.2/24 dev veth0 sudo ip netns exec docker2 ip link set veth0 up Veth另一端的网卡激活上线 分别在Node-1、Node-2上执行：\n1 sudo ip link set veth1 up 为bridge分配IP地址，激活上线 在Node-1上执行：\n1 2 sudo ip addr add 172.18.10.1/24 dev br0 sudo ip link set br0 up 在Node-2上执行：\n1 2 sudo ip addr add 172.18.20.1/24 dev br0 sudo ip link set br0 up 将bridge设置为“容器”的缺省网关 在Node-1上执行：\n1 sudo ip netns exec docker1 route add default gw 172.18.10.1 veth0 在Node-2上执行：\n1 sudo ip netns exec docker2 route add default gw 172.18.20.1 veth0 创建VXLAN虚拟网卡 VXLAN需要在宿主机上创建一个虚拟网络设备对 VXLAN 的包进行封装和解封装，实现这个功能的设备称为 VTEP（VXLAN Tunnel Endpoint）。宿主机之间通过VTEP建立“隧道”，在其中传输虚拟二层网络包。\n在Node-1创建vxlan100：\n1 2 3 4 5 6 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.183 \\ dev enp0s3 \\ dstport 4789 \\ nolearning 为vxlan100分配IP地址，然后激活：\n1 2 sudo ip addr add 172.18.10.0/32 dev vxlan100 sudo ip link set vxlan100 up 为了让Node-1上访问172.18.20.0/24网段的数据包能进入“隧道”，我们需要增加如下的路由规则：\n1 sudo ip route add 172.18.20.0/24 dev vxlan100 在Node-2上执行相应的命令：\n1 2 3 4 5 6 7 8 9 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.192 \\ dev enp0s3 \\ dstport 4789 \\ nolearning sudo ip addr add 172.18.20.0/32 dev vxlan100 sudo ip link set vxlan100 up sudo ip route add 172.18.10.0/24 dev vxlan100 scope global 手动更新ARP和FDB 虚拟设备vxlan100会用ARP和FDB (forwarding database) 数据库中记录的信息，填充网络协议包，建立节点间转发虚拟网络数据包的“隧道”。\n我们知道，在二层网络上传输IP包，需要先根据目的IP地址查询到目的MAC地址，这就是ARP（Address Resolution Protocol）协议的作用。我们应该可以通过ARP查询到其他节点上容器IP地址对应的MAC地址，然后填充在VXLAN内层的网络包中。\nFDB是记录网桥设备转发数据包的规则。虚拟网络数据包根据上面定义的路由规则，从br0进入了本机的vxlan100“隧道”入口，应该可以在FDB中查询到“隧道”出口的MAC地址应该如何到达，这样，两个VTEP就能完成”隧道“的建立。\nvxlan为了建立节点间的“隧道”，需要一种机制，能让一个节点的加入、退出信息通知到其他节点，可以采用multicast的方式进行节点的自动发现，也有很多Unicast的方案，这篇文章\u0026lt;VXLAN \u0026amp; Linux\u0026gt;有很详细的介绍。总之就是要找到一种方式，能够更新每个节点的ARP和FDB数据库。\n如果是使用Flannel，它在节点启动的时候会采用某种机制自动更新其他节点的ARP和FDB数据库。现在我们的实验只能在两个节点上手动更新ARP和FDB。\n首先在两个节点上查询到设备vxlan100的MAC地址，例如在我当前的环境：\nNode-1上vxlan100的MAC地址是3a:8d:b8:69:10:3e Node-2上vxlan100的MAC地址是0e:e6:e6:5d:c2:da\n然后在Node-1上增加ARP和FDB的记录：\n1 2 sudo ip neighbor add 172.18.20.2 lladdr 0e:e6:e6:5d:c2:da dev vxlan100 sudo bridge fdb append 0e:e6:e6:5d:c2:da dev vxlan100 dst 192.168.31.192 我们可以确认下执行结果：\nARP中已经记录了Node-2上容器IP对应的MAC地址。再看看FDB的情况：\n根据最后一条新增规则，我们可以知道如何到达Node-2上“隧道”的出口vxlan100。“隧道”两端是使用UDP进行传输，即容器间通讯的二层网络包是靠UDP在宿主机之间通信。\n类似的，在Node-2上执行下面的命令：\n1 2 sudo ip neighbor add 172.18.10.2 lladdr 3a:8d:b8:69:10:3e dev vxlan100 sudo bridge fdb append 3a:8d:b8:69:10:3e dev vxlan100 dst 192.168.31.183 测试容器的跨节点通信 现在，容器docker1和docker1之间就可以相互访问了。\n我们从docker1访问docker2，在Node-1上执行：\n1 sudo ip netns exec docker1 ping -c 3 172.18.20.2 同样可以从docker2访问docker1，在Node-2上执行：\n1 sudo ip netns exec docker2 ping -c 3 172.18.10.2 在测试过程中如果需要troubleshooting，可以使用tcpdump在veth1、br0、vxlan100等虚拟设备上抓包，确认网络包是按照预定路线在转发：\n1 sudo tcpdump -i vxlan100 -n 测试环境恢复 在两个节点上删除我们创建的虚拟设备：\n1 2 3 4 sudo ip link set br0 down sudo brctl delbr br0 sudo ip link del veth1 sudo ip link del vxlan100 # 实现方案二 Docker原生的overlay driver底层也是使用VXLAN技术，但实现方案和Flannel略有不同：\n我们可以看到，vxlan100被“插”在了虚拟交换机br0上，虚拟网络数据包从br0到vxlan100不是通过本机路由，而是vxlan100根据FDB直接进行了转发。\n执行的命令略有差异，我不再赘述过程，直接提供了命令，大家自己实验吧：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 # 在Node-1上执行 sudo sysctl net.ipv4.conf.all.forwarding=1 sudo ip netns add docker1 sudo ip link add veth0 type veth peer name veth1 sudo ip link set veth0 netns docker1 sudo brctl addbr br0 sudo brctl addif br0 veth1 sudo ip netns exec docker1 ip addr add 172.18.10.2/24 dev veth0 sudo ip netns exec docker1 ip link set veth0 up sudo ip link set veth1 up sudo ip link set br0 up sudo ip netns exec docker1 route add default veth0 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.183 \\ dev enp0s5 \\ dstport 4789 \\ nolearning \\ proxy sudo ip link set vxlan100 up sudo brctl addif br0 vxlan100 sudo ip neigh add 172.18.20.2 lladdr [docker2的MAC地址] dev vxlan100 sudo bridge fdb append [docker2的MAC地址] dev vxlan100 dst 192.168.31.192 # 在Node-2上执行 sudo sysctl net.ipv4.conf.all.forwarding=1 sudo ip netns add docker2 sudo ip link add veth0 type veth peer name veth1 sudo ip link set veth0 netns docker2 sudo brctl addbr br0 sudo brctl addif br0 veth1 sudo ip netns exec docker2 ip addr add 172.18.20.2/24 dev veth0 sudo ip netns exec docker2 ip link set veth0 up sudo ip link set veth1 up sudo ip link set br0 up sudo ip netns exec docker2 route add default veth0 sudo ip link add vxlan100 type vxlan \\ id 100 \\ local 192.168.31.192 \\ dev enp0s5 \\ dstport 4789 \\ nolearning \\ proxy sudo ip link set vxlan100 up sudo brctl addif br0 vxlan100 sudo ip neigh add 172.18.10.2 lladdr [docker1的MAC地址] dev vxlan100 sudo bridge fdb append [docker1的MAC地址] dev vxlan100 dst 192.168.31.183 相信通过亲自动手实验，容器网络对你来说不再神秘。希望本文对你理解容器网络有所帮助。\n下一篇我将动手实验容器跨主机通信的路由模式。\n","date":"2018-11-07T21:52:36+08:00","permalink":"https://mazhen.tech/p/docker%E8%B7%A8%E4%B8%BB%E6%9C%BAoverlay%E7%BD%91%E7%BB%9C%E5%8A%A8%E6%89%8B%E5%AE%9E%E9%AA%8C/","title":"Docker跨主机Overlay网络动手实验"},{"content":" # 容器的本质 容器的本质就是一个进程，只不过对它进行了Linux Namesapce隔离，让它看不到外面的世界，用Cgroups限制了它能使用的资源，同时利用系统调用pivot_root或chroot切换了进程的根目录，把容器镜像挂载为根文件系统rootfs。rootfs中不仅有要运行的应用程序，还包含了应用的所有依赖库，以及操作系统的目录和文件。rootfs打包了应用运行的完整环境，这样就保证了在开发、测试、线上等多个场景的一致性。\n从上图可以看出，容器和虚拟机的最大区别就是，每个虚拟机都有独立的操作系统内核Guest OS，而容器只是一种特殊的进程，它们共享同一个操作系统内核。\n看清了容器的本质，很多问题就容易理解。例如我们执行 docker exec 命令能够进入运行中的容器，好像登录进独立的虚拟机一样。实际上这只不过是利用系统调用setns，让当前进程进入到容器进程的Namesapce中，它就能“看到”容器内部的情况了。\n关于容器涉及的基础技术，左耳朵耗子多年前写的系列文章仍然很有参考价值：\nDOCKER基础技术：LINUX NAMESPACE（上） DOCKER基础技术：LINUX NAMESPACE（下） DOCKER基础技术：LINUX CGROUP DOCKER基础技术：AUFS DOCKER基础技术：DEVICEMAPPER # 容器网络 如何让容器之间互相连接保持网络通畅，Docker有多种网络模型。对于单机上运行的多个容器，可以使用缺省的bridge网络驱动。而容器的跨主机通信，一种常用的方式是利用Overlay 网络，基于物理网络的虚拟化网络来实现。\n本文会在单机上实验展示bridge网络模型，揭示其背后的实现原理。下一篇文章会演示容器如何利用Overlay 网络进行跨主机通信。\n我们按照下图创建网络拓扑，让容器之间网络互通，从容器内部可以访问外部资源，同时，容器内可以暴露服务让外部访问。\n在开始动手实验之前，先简单介绍一下bridge网络模型会用到的Linux虚拟化网络技术。\n# Veth Pairs Veth是成对出现的两张虚拟网卡，从一端发送的数据包，总会在另一端接收到。利用Veth的特性，我们可以将一端的虚拟网卡\u0026quot;放入\u0026quot;容器内，另一端接入虚拟交换机。这样，接入同一个虚拟交换机的容器之间就实现了网络互通。\n# Linux Bridge 交换机是工作在数据链路层的网络设备，它转发的是二层网络包。最简单的转发策略是将到达交换机输入端口的报文，广播到所有的输出端口。当然更好的策略是在转发过程中进行学习，记录交换机端口和MAC地址的映射关系，这样在下次转发时就能够根据报文中的MAC地址，发送到对应的输出端口。\n我们可以认为Linux bridge就是虚拟交换机，连接在同一个bridge上的容器组成局域网，不同的bridge之间网络是隔离的。 docker network create [NETWORK NAME]实际上就是创建出虚拟交换机。\n# iptables 容器需要能够访问外部世界，同时也可以暴露服务让外界访问，这时就要用到iptables。另外，不同bridge之间的隔离也会用到iptables。\n我们说的iptables包含了用户态的配置工具(/sbin/iptables)和内核netfilter模块，通过使用iptables命令对内核的netfilter模块做规则配置。\nnetfilter允许在网络数据包处理流程的各个阶段插入hook函数，可以根据预先定义的规则对数据包进行修改、过滤或传送。\n从上图可以看出，网络包的处理流程有五个关键节点：\nPREROUTING：数据包进入路由表之前 INPUT：通过路由表后目的地为本机 FORWARDING：通过路由表后，目的地不为本机 OUTPUT：由本机产生，向外转发 POSTROUTIONG：发送到网卡接口之前 iptables 提供了四种内置的表 raw → mangle → nat → filter，优先级从高到低：\nraw 用于配置数据包，raw中的数据包不会被系统跟踪。不常用。 mangle 用于对特定数据包的修改。不常用。 nat: 用于网络地址转换（NAT）功能（端口映射，地址映射等）。 filter：一般的过滤功能，默认表。 每个表可以设置在多个指定的节点，例如filter表可以设置在INPUT、FORWARDING、OUTPUT等节点。同一个节点中的多个表串联成链。\niptables 是按照表的维度来管理规则，表中包含多个链，链中包含规则列表。例如我们使用sudo iptables -t filter -L 查看filter表：\n可以看到，filter表中包含三个链，每个链中定义了多条规则。由于filter是缺省表，上面的命令可以简化为：sudo iptables -L，即不通过-t指定表时，操作的就是filter表。\n在容器化网络场景，我们经常用到的是在nat表中设置SNAT和DNAT。源地址转换是发生在数据包离开机器被发送之前，因此SNAT只能设置在POSTROUTIONG阶段。DNAT是对目标地址的转换，需要在路由选择前完成，因此可以设置在PREROUTING和OUTPUT阶段。\n# 动手实验 有了前面的背景知识，我们就可以开始动手实验了。因为涉及到很多系统级设置，建议在一个“干净”的虚拟机内折腾，以免干扰到工作环境。我使用的实验环境是Ubuntu 18.04.1 LTS，不需要安装docker，我们使用系统命令模拟出容器网络环境。\n# 场景一：容器间的网络互通 创建“容器” 从前面的背景知识了解到，容器的本质是 Namespace + Cgroups + rootfs。因此本实验我们可以仅仅创建出Namespace网络隔离环境来模拟容器行为：\n1 2 sudo ip netns add docker0 sudo ip netns add docker1 查看创建出的网络Namesapce：\n1 2 3 $ ls -l /var/run/netns -r--r--r-- 1 root root 0 Nov 11 03:52 docker0 -r--r--r-- 1 root root 0 Nov 11 03:52 docker1 创建Veth pairs 1 2 sudo ip link add veth0 type veth peer name veth1 sudo ip link add veth2 type veth peer name veth3 查看创建出的Veth pairs：\n1 2 3 4 5 6 7 8 9 10 $ip addr show ... 3: veth1@veth0: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 3e:fe:2b:90:3e:b7 brd ff:ff:ff:ff:ff:ff 4: veth0@veth1: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 6a:a3:02:07:f4:92 brd ff:ff:ff:ff:ff:ff 5: veth3@veth2: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 76:14:e5:0e:26:98 brd ff:ff:ff:ff:ff:ff 6: veth2@veth3: \u0026lt;BROADCAST,MULTICAST,M-DOWN\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 6a:0a:84:0f:a7:f7 brd ff:ff:ff:ff:ff:ff 将Veth的一端放入“容器” 设置Veth一端的虚拟网卡的Namespace，相当于将这张网卡放入“容器”内：\n1 2 sudo ip link set veth0 netns docker0 sudo ip link set veth2 netns docker1 查看“容器” docker0 内的网卡：\n1 2 3 4 5 $ sudo ip netns exec docker0 ip addr show 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 4: veth0@if3: \u0026lt;BROADCAST,MULTICAST\u0026gt; mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 6a:a3:02:07:f4:92 brd ff:ff:ff:ff:ff:ff link-netnsid 0 ip netns exec docker0 ...的意思是在网络Namesapce docker0的限制下执行后面跟着的命令，相当于在“容器”内执行命令。\n可以看到，veth0已经放入了“容器”docker0内。同样使用命令sudo ip netns exec docker1 ip addr show查看“容器”docker1内的网卡。\n同时，在宿主机上查看网卡ip addr，发现veth0和veth2已经消失，确实是放入“容器”内了。\n创建bridge 安装bridge管理工具brctl\n1 sudo apt-get install bridge-utils 创建bridge br0：\n1 sudo brctl addbr br0 将Veth的另一端接入bridge 1 2 sudo brctl addif br0 veth1 sudo brctl addif br0 veth3 查看接入效果：\n1 sudo brctl show 两个网卡veth1和veth3已经“插”在bridge上。\n为\u0026quot;容器“内的网卡分配IP地址，并激活上线 docker0容器：\n1 2 sudo ip netns exec docker0 ip addr add 172.18.0.2/24 dev veth0 sudo ip netns exec docker0 ip link set veth0 up docker1容器：\n1 2 sudo ip netns exec docker1 ip addr add 172.18.0.3/24 dev veth2 sudo ip netns exec docker1 ip link set veth2 up Veth另一端的网卡激活上线 1 2 sudo ip link set veth1 up sudo ip link set veth3 up 为bridge分配IP地址，激活上线 1 2 sudo ip addr add 172.18.0.1/24 dev br0 sudo ip link set br0 up “容器”间的互通测试 我们可以先设置监听br0：\n1 sudo tcpdump -i br0 -n 从容器docker0 ping 容器docker1：\n1 sudo ip netns exec docker0 ping -c 3 172.18.0.3 br0上监控到的网络流量：\n1 2 3 4 5 6 7 8 9 10 05:53:10.859956 ARP, Request who-has 172.18.0.3 tell 172.18.0.2, length 28 05:53:10.859973 ARP, Reply 172.18.0.3 is-at 06:f4:01:c2:dd:6e, length 28 05:53:10.860030 IP 172.18.0.2 \u0026gt; 172.18.0.3: ICMP echo request, id 1310, seq 1, length 64 05:53:10.860050 IP 172.18.0.3 \u0026gt; 172.18.0.2: ICMP echo reply, id 1310, seq 1, length 64 05:53:11.878348 IP 172.18.0.2 \u0026gt; 172.18.0.3: ICMP echo request, id 1310, seq 2, length 64 05:53:11.878365 IP 172.18.0.3 \u0026gt; 172.18.0.2: ICMP echo reply, id 1310, seq 2, length 64 05:53:12.901334 IP 172.18.0.2 \u0026gt; 172.18.0.3: ICMP echo request, id 1310, seq 3, length 64 05:53:12.901350 IP 172.18.0.3 \u0026gt; 172.18.0.2: ICMP echo reply, id 1310, seq 3, length 64 05:53:16.006471 ARP, Request who-has 172.18.0.2 tell 172.18.0.3, length 28 05:53:16.006498 ARP, Reply 172.18.0.2 is-at c2:23:fe:ac:f5:4e, length 28 可以看到，先是172.18.0.2发起的ARP请求，询问172.18.0.3的MAC地址，然后是ICMP的请求和响应，最后是172.18.0.3的ARP请求。因为接在同一个bridge br0上，所以是二层互通的局域网。\n同样，从容器docker1 ping 容器docker0也是通的：\n1 sudo ip netns exec docker1 ping -c 3 172.18.0.2 # 场景二：从宿主机访问“容器”内网络 在“容器”docker0内启动服务，监听80端口：\n1 sudo ip netns exec docker0 nc -lp 80 在宿主机上执行telnet，可以连接到docker0的80端口：\n1 telnet 172.18.0.2 80 # 场景三：从“容器”内访问外网 配置内核参数，允许IP forwarding 1 sudo sysctl net.ipv4.conf.all.forwarding=1 配置iptables FORWARD规则 首先确认iptables FORWARD的缺省策略：\n1 sudo iptables -L 如果缺省策略是DROP，需要设置为ACCEPT：\n1 sudo iptables -P FORWARD ACCEPT 缺省策略的含义是，在数据包没有匹配到规则时执行的缺省动作。\n将bridge设置为“容器”的缺省网关 1 2 sudo ip netns exec docker0 route add default gw 172.18.0.1 veth0 sudo ip netns exec docker1 route add default gw 172.18.0.1 veth2 查看“容器”内的路由表：\n1 2 3 4 5 6 $sudo ip netns exec docker0 route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.18.0.1 0.0.0.0 UG 0 0 0 veth0 172.18.0.0 0.0.0.0 255.255.255.0 U 0 0 0 veth0 可以看出，“容器”内的缺省Gateway是bridge的IP地址，非172.18.0.0/24网段的数据包会路由给bridge。\n配置iptables的SNAT规则 容器的IP地址外部并不认识，如果它要访问外网，需要在数据包离开前将源地址替换为宿主机的IP，这样外部主机才能用宿主机的IP作为目的地址发回响应。\n另外一个需要注意的问题，内核netfilter会追踪记录连接，我们在增加了SNAT规则时，系统会自动增加一个隐式的反向规则，这样返回的包会自动将宿主机的IP替换为容器IP。\n1 sudo iptables -t nat -A POSTROUTING -s 172.18.0.0/24 ! -o br0 -j MASQUERADE 上面的命令的含义是：在nat表的POSTROUTING链增加规则，当数据包的源地址为172.18.0.0/24网段，出口设备不是br0时，就执行MASQUERADE动作。\nMASQUERADE也是一种源地址转换动作，它会动态选择宿主机的一个IP做源地址转换，而SNAT动作必须在命令中指定固定的IP地址。\n从“容器”内访问外部地址 1 2 sudo ip netns exec docker0 ping -c 3 123.125.115.110 sudo ip netns exec docker1 ping -c 3 123.125.115.110 我们确认在“容器”内是可以访问外部网络的。\n# 场景四：从外部访问“容器”内暴露的服务 配置iptables的DNAT规则 当外部通过宿主机的IP和端口访问容器内启动的服务时，在数据包进入PREROUTING阶段就要进行目的地址转换，将宿主机IP转换为容器IP。同样，系统会为我们自动增加一个隐式的反向规则，数据包在离开宿主机时自动做反向转换。\n1 sudo iptables -t nat -A PREROUTING ! -i br0 -p tcp -m tcp --dport 80 -j DNAT --to-destination 172.18.0.2:80 上面命令的含义是：在nat表的PREROUTING链增加规则，当输入设备不是br0，目的端口为80时，做目的地址转换，将宿主机IP替换为容器IP。\n从远程访问“容器”内暴露的服务 在“容器”docker0内启动服务：\n1 sudo ip netns exec docker0 nc -lp 80 在和宿主机同一个局域网的远程主机访问宿主机IP:80\n1 telnet 192.168.31.183 80 确认可以访问到容器内启动的服务。\n# 测试环境恢复 删除虚拟网络设备\n1 2 3 4 sudo ip link set br0 down sudo brctl delbr br0 sudo ip link del veth1 sudo ip link del veth3 iptablers和Namesapce的配置在机器重启后被清除。\n# 总结 本文我们在介绍了veth、Linux bridge、iptables等概念后，亲自动手模拟出了docker bridge网络模型，并测试了几种场景的网络互通。实际上docker network 就是使用了上述技术，帮我们创建和维护网络。通过动手实验，相信你对docker bridge网络理解的更加深入。\n下一篇我将动手实验容器如何利用Overlay 网络进行跨主机通信。\n","date":"2018-10-26T21:40:03+08:00","permalink":"https://mazhen.tech/p/docker%E5%8D%95%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E5%8A%A8%E6%89%8B%E5%AE%9E%E9%AA%8C/","title":"Docker单机网络模型动手实验"},{"content":"当TiDB 源码阅读系列更新到第六篇《Select 语句概览》时，我发现需要一些关系数据库的基础知识才能更好的理解，例如逻辑查询计划优化其实就是：使用代数定律对查询语句的代数表达式树做等价转换，使改进后的代数表达式树预期可以生成更有效的物理查询计划。有了这些基础知识，看代码才能做到知其然知其所以然。本文希望通过梳理关系数据库背后的知识，为读懂 TiDB 查询处理器部分的源码扫清障碍。\n# 极简数据库发展史 数据库的应用及其广泛，已经成为信息系统的核心技术和重要的基础设施。简单说数据库需要做两件事：存储数据，以及随后在你需要的时候能访问读取数据。\n最早的数据库是基于文件系统，虽然它满足了长期存储数据的需求，但没有提供对文件的查询语言，读取访问非常不便利。于是人们在文件系统上引入一层抽象：数据模型。数据模型是对现实世界数据特征的抽象，能比较真实地模拟现实世界，容易为人所理解，也便于在计算机上实现。\n最早出现的是层次模型（Hierarchical Model），数据被组织为一棵树，类似于今天文档数据库使用的JSON的结构。层次模型很适合处理one-to-many关系，但要表现many-to-many关系则非常困难，一般也不支持join。使用层次模型最著名的数据库是 IBM 的Information Management System (IMS)，它最初是为了解决阿波罗飞船登月计划的需求，协调分散在全球制造的200万个阿波罗飞船零部件的生产进度。\n随后出现了不同的方案解决层次模型的限制，其中最突出的两个模型是网络模型（Network Model）和关系数据模型，最终关系数据模型胜出。\n今天最著名和使用最广泛的数据模型是由 Edgar Codd 博士提出的关系数据模型，他在1970年发布的论文《A Relational Model of Data for Large Shared Data Banks》，奠定了关系数据库的理论基础。ACM在1983年把这篇论文列为从1958年以来的四分之一世纪中具有里程碑式意义的最重要的25篇研究论文之一。到了80年代中期，基于关系数据模型的关系数据库已经成为人们存储、查询结构化数据的首选工具。\n到了2010年，NoSQL兴起，试图颠覆关系数据模型的统治地位。随着互联网的爆发式发展，数据库领域又一次发生了摇摆，伴随着互联网的特殊需求，一批有着新鲜血液的 NoSQL 数据库涌现了出来，层次模型又重新站在了大家面前。NoSQL为了应对海量数据存储和高并发访问，决定放弃关系数据模型和事务等关系数据数据库的关键特性。自从 NoSQL 概念横空出世，关系数据库似乎成了低效、高成本、速度慢的数据处理模式的代名词。然而，NoSQL在解决问题的同时也给使用者带来了很多困扰， 最终一致让应用开发者要面对各种复杂的场景。\n数据库技术的发展是螺旋式上升，Google发布的Spanner和F1两篇论文，让人们看到了关系数据模型 和 NoSQL 融合的可能性。以 TiDB 为代表的 NewSQL 数据库，让人们重新享受关系模型、强一致性事务等对使用者友好的特性，同时也具备了 NoSQL 的水平扩展能力。\n# 关系数据模型 和 关系代数 数据模型是对现实世界事物的抽象，而关系数据模型将一切事物都抽象为关系，并通过集合运算的方式规定了关系之间的运算过程，模型相对的比较简单，数据证明严谨，因此很快被大家广泛接受。\n这一节我将介绍关系数据库的数学基础：关系数据模型和关系代数。\n# 关系数据模型 关系模型为人们提供了一种描述数据的方法：一个称为关系（relation）的二维表。现实世界的实体以及实体间的各种联系都可以用关系来表示。我们通过例子来了解关系模型的重要术语：\n雇员表\nemp_no name birth_date gender hire_date 1 汤唯 1990-06-08 女 2015-08-01 2 刘亦菲 1994-09-10 女 2017-05-06 3 刘德华 1986-04-18 男 2008-09-01 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; 关系（Relation） ：一个关系对应通常说的一张二维表 元组（Tuple） ： 表中的一行即为一个元组 属性（Attribute） ：表中的一列即为一个属性，给每一个属性起一个名称即属性名 键（Key）：表中的某个属性组，它可以唯一确定一个元组 域（Domain） : 是一组具有相同数据类型的值的集合。属性的取值范围来自某个域。例如性别的域是（男，女）。 关系模式（schema）：对关系的描述，先给出一个关系名，其后是用圆括号扩起来的所有属性，例如：employees（emp_no, name, birth_date, gender, hire_date） # 关系代数 一门代数是由一些操作符和操作数组成。例如算术代数的加、减、乘、除是操作符，变量x和常量8是操作数。任何一门代数都允许把操作符作用在操作数上构造出表达式（expression），例如算术表达式 (x+y)*3。\n关系代数 也是一门代数，它的操作数是关系，操作运算符有两类：集合运算符和专门的关系运算符。\n关系代数可以认为是一种抽象的查询语言，利用对关系的运算来表达查询，运算对象是关系，运算结果也是关系。因此，关系代数的表达式也被称为查询（query）。\n# 传统的集合运算 三个最常见的集合操作是：并（union）、交（intersection）、差（difference）。\nR ∪ S，表示关系R和S的并，得到的结果关系的元素来自R或者S，或R和S中都出现过。 R ∩ S，表示关系R和S的交，同时在R和S中存在的元素集合。 R - S，表示关系R和S的差，它是由在R中出现但不在S中出现的元素构成的集合。 另外一个集合操作是笛卡尔积（Cartesian Product）。\n关系R和S的笛卡尔积是一个有序对的集合，有序对的一个元素是关系R中的任何一个元组，第二个元素是关系S中的任何一个元组表示为 R × S。 关系R\nA B 1 2 3 4 关系S\nB C D 2 5 6 4 7 8 9 10 11 R × S的结果\nA R.B S.B C D 1 2 2 5 6 1 2 4 7 8 1 2 9 10 11 3 4 2 5 6 3 4 4 7 8 3 4 9 10 11 # 专门的关系运算 选择（selection)，当选择操作符应用到关系R上时，产生一个关系R的元组的子集合。结果关系元组必须满足某个涉及R中属性的条件C，表示为 σC( R )\n投影 （projection），用来从一个关系生成一个新的关系，这个关系只包含原来关系R中的部分列。表达式 πA1,A2,\u0026hellip;,An ( R ) 的值是这样一个关系，它只包含关系R属性A1,A2,...An所代表的列。\nθ连接，关系R和关系S满足条件C的θ连接可以用这样的符号来表示： R ⋈C S\nθ连接的结果这样构造：\n先得到R和S的笛卡尔积 在得到的关系中寻找满足条件C的元组 关系R\nA B C 1 2 3 6 7 8 9 7 8 关系S\nB C D 2 3 4 2 3 5 7 8 10 R ⋈ A\u0026lt;D AND R.B ≠ S.B S 的结果是：\nA R.B R.C S.B S.C D 1 2 3 7 8 10 有两类常用的连接运算：\n等值连接（equijoin）：比较运算符为 = 的连接运算称为等值连接。例如： R ⋈ R.A = S.B S 是从关系R与S的笛卡尔积中选取A、B属性值相等的那些元组。 自然连接（Natural join）：自然连接是一种特殊的等值连接，两个关系中进行比较的分量必须是相同的属性组，并在结果中把重复的属性列去掉。关系R和S的自然连接表示为 R ⋈ S 关系R\nA B 1 2 3 4 关系S\nB C D 2 5 6 4 7 8 9 10 11 R ⋈ S\nA B C D 1 2 5 6 3 4 7 8 两个关系R和S在做自然连接时，如果一个元组不能和另外关系中的任何一个元组配对的话，这个元组被称为悬浮元组（Dangling tuple）。上面的例子中，关系S的第三个元组就是悬浮元组。\n如果把悬浮元组也保存在结果关系中，而在其他属性上填空值(Null)，就叫做外连接（Outer Join）。\n左外连接(LEFT OUTER JOIN或LEFT JOIN)：只保留左边关系R中的悬浮元组 右外连接(RIGHT OUTER JOIN或RIGHT JOIN)：只保留右边关系S中的悬浮元组 # 关系代数的扩展操作符 消除重复操作符（duplicated-elimination operator）用 δ(R) 来返回一个没有重复元组的关系R 聚集操作符 （aggregation operator）用来汇总或者聚集关系某一列中出现的值，有 SUM，AVG，MIN，MAX，COUNT 等 分组操作（grouping）根据元组在一个或多个属性上的值把关系的元组拆成“组”。这样聚集操作就可以对分组的各个列进行计算。分组操作符 γ 是组合了分组和聚合操作的一个算子。例如表达式： γ gender, COUNT(emp_no)-\u0026gt;count(employees) 代表把性别（gender）作为分组属性，然后对每一个分组进行COUNT(emp_no)的操作。 排序算子（sorting operator）如果关系R的模式是 R(A,B,C)，那么 τC( R ) 就把R中的元组按照属性C的值排序。 # 关系代数小结 上面的知识有些枯燥，但非常容易理解，因为我们经常使用关系数据库，已经接受了这些概念。掌握了一些关系代数的知识，在阅读TiDB源码时，当看到selection、projection 这些术语就能一下想到它们对应的关系代数运算符。\n这里只介绍了关系代数最基本的概念，如果想完整学习，建议参考斯坦福大学大学的课程CS145: A First Course in Database Systems，对应的教材有中文版《数据库系统基础教程》。\n其实我们在查询时提交给数据库的就是关系代数表达式，它是关系运算符的组合，数据库会根据一些代数定律对最初的表达式做等价变换，得出一个最优的等价表达式（equivalent expression），即可以更快的被计算出结果的表达式。这个过程就是逻辑查询计划优化，后面我会简单的介绍相关概念。\n# SQL 的诞生 SQL(Structured Query Language) 结构化查询语言，是关系数据库的标准语言。\n在1970年Codd博士提出了关系模型之后，由于关系代数或者关系都太数学了，难以被普通用户接受。IBM在研制关系数据库管理系统原型System R的过程中，决定摈弃数学语言，以自然语言为方向，结果诞生了结构化英语查询语言（Structured English Query Language，SEQUEL），后来更名为SQL。System R 因此获得1988年度ACM“软件系统奖”。\nSQL是声明式查询语言，你只需要指定想要获得什么样的数据，而无须了解如何实现这个目标。SQL具体是如何执行的，取决于数据库系统的查询处理器，它来决定哪些索引和哪些连接方法可以使用，以及以什么样的顺序执行查询的各个部分。SQL隐藏了数据库引擎的实现细节，因此用户可以在不修改查询语句的情况下，享受到数据库性能优化带来的好处。\n下面我们来看看数据库的查询处理器。\n# 关系数据库的查询处理器 SQL是在很高层次上表达查询，那么数据库的查询处理器必须提供查询被如何执行的大量细节。下面我从概念上介绍查询处理器的处理流程，实际的数据库实现要复杂的多，特别是像 TiDB 这样的分布式数据库。如果想比较系统的了解数据库的实现技术，同样推荐斯坦福大学计算机科学专业的课程 CS245: Database System Implementation。上面提到的CS145是CS245的预修课。国内很少有讲数据库内部实现的书，这门课的教材值得阅读。当然最好的学习方法是理论联系实践，多去读 TiDB 的源代码:)\n一般查询处理可以简单的划分为以下几个步骤：\n对SQL进行语法分析，将查询语句转换成抽象语法树。 把抽象语法树转换成关系代数表达式树，这就是初始的逻辑查询计划。 使用关系代数中的多个代数定律改进初始的代数表达式树。利用一些代数定律，可以把代数表达式树转换成一个等价的表达式树，后者预期可以生成更有效的物理查询计划。这一步进行了查询重写，可以称为逻辑查询计划优化。 为逻辑查询计划的每一个操作符选择实现算法，并确定这些操作符的执行顺序，逻辑查询计划被转化为物理查询计划。物理查询计划指明了要执行的操作，操作的执行顺序，执行每步所用的算法，获取数据的方式，以及数据从一个操作传递给另一个操作的方式。 # 查询示例 本文准备以一个简单的例子来介绍查询处理的流程，下面是查询涉及的两张表：\nemployees\nemp_no name birth_date gender hire_date 1 汤唯 1990-06-08 女 2015-08-01 2 刘亦菲 1994-09-10 女 2017-05-06 3 刘德华 1986-04-18 男 2008-09-01 \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; \u0026hellip; salaries\nemp_no salary last_modified 1 8000 2018-04-01 2 6000 2018-04-01 3 15000 2018-04-01 \u0026hellip; \u0026hellip; \u0026hellip; 想要获取工资大于7000的员工姓名列表，SQL语句可以这么写：\n1 2 3 4 SELECT name FROM employees, salaries WHERE employees.emp_no = salaries.emp_no AND salary \u0026gt; 7000; # SQL 语法分析 SQL Parser的功能是把SQL语句按照SQL语法规则进行解析，将文本转换成抽象语法树（AST）。具体的实现可以参考这篇文章《TiDB SQL Parser 的实现》。 示例SQL解析完成后得到下面的语法树：\n# 生成逻辑查询计划 现在将上一步生成的语法树转换成关系代数表达式树，也就是逻辑查询计划。对于示例SQL的抽象语法树转换过程如下：\n\u0026lt;FromList\u0026gt; 中涉及的关系employees和salaries做笛卡尔积运算 选择（selection）运算 σC，其中C被替换成\u0026lt;Condition\u0026gt;表达式，即employees.emp_no = salaries.emp_no AND salary \u0026gt; 7000 投影（projection） πL，其中L是\u0026lt;SelList\u0026gt;中的属性列表，对于这个查询只有一个属性name 我们得到下面的关系代数表达式树：\n# 逻辑查询计划的改进 当我们把查询语句转换成关系代数表达式时，得到了一个初始的逻辑查询计划。现在我们可以使用关系代数中的多个代数定律改进逻辑查询计划。\n这里仅仅列出一小部分这样的代数定律，它们可以将一个表达式树转换成一个等价的表达式树。\n交换律和结合律 R × S = S × R; (R X S) × T = R × (S × T)\nR ⋈ S = S ⋈ R; (R ⋈ S) ⋈ T = R ⋈ (S ⋈ T)\nR ∪ S = S ∪ R; (R ∪ S) ∪ T = R ∪ (S ∪ T)\nR ∩ S = S ∩ R; (R ∩ S) ∩ T = R ∩ (S ∩ T)\n涉及选择的定律 σC1 AND C2 = σC1(σC2( R ))\nσC1 OR C2 = (σC1( R ) ∪ (σC2( R ) )\nσC1(σC2( R )) = σC2(σC1( R ))\n下推选择 在表达式中下推选择是查询优化器最强有力的工具。\nσC( R × S ) = σC( R ) × S\nσC( R ⋈ S) = σC( R ) ⋈ S\nσC( R ⋈D S) = σC( R ) ⋈D S\n涉及连接和笛卡尔积的定律 R ⋈C S = σC( R × S)\n涉及消除重复的定律 δ(R×S) = δ( R ) × δ(S)\nδ(R⋈CS) = δ( R ) ⋈C δ(S)\n另外还有涉及投影的定律、涉及分组和聚集的定律。这部分有些理论化，可以参考这篇《TiDB 源码阅读系列文章（七）基于规则的优化》看看 TiDB 具体是怎么做的。\n对于本例使用到的定律比较简单。先将选择的两部分分解为 σemployees.emp_no = salaries.emp_no 和 σsalary \u0026gt; 7000，后者可以在树中下推。第一个条件涉及笛卡尔积两边的属性，可以把上面提到的定律 R ⋈C S = σC( R × S)\n从右向左使用，把笛卡尔积转换成连接。使用了两个定律后，得到优化后的逻辑查询计划如下图：\n# 物理查询计划的生成 这一步我们需要把逻辑查询计划转换成物理查询计划。通常由逻辑计划可以得到多个物理计划，我们需要对每个物理计划进行评估，选择具有最小估计代价的物理查询计划。\n# 基于代价的物理计划选择 在从逻辑计划构建物理计划的时候，因为可能得到多个物理计划，我们需要通过估计物理计划执行的代价来确定最优选择。\n如何计算这个代价呢？我们可以用物理计划每一步的任务执行时发生的磁盘I/O数、网络吞吐量、占用的内存空间大小等近似估算。\n这些资源的访问和占用，又是由什么决定的呢？可能包括的决定因素有：\n参与运算任务执行的数据大小 数据的分布位置（连续的磁盘空间、离散的磁盘空间、网络节点等） 关系中属性的不同值的数目 属性值的最大值、最小值、以及值的分布情况 数据库会收集统计这些信息，用来估算具体任务的代价。\n逻辑查询计划在转换成物理计划的时候，每一步的转换都会面临多种情况的选择，最容易想到的是使用穷举法，估算每一种情况的代价，最后确定最优的物理计划。但使用穷举法的话，很可能估算本身的代价变得非常大，实践中可以采用动态规划（dynamic programming）等算法。\n# 枚举物理查询计划 以上一步输出的逻辑查询计划为例，看看在枚举物理查询计划时需要做出哪些选择。\n首先，逻辑查询计划的每个结点转换成什么样的物理运算符会遇到多种选择。我们从逻辑查询计划树自底往上来看：\n逻辑计划的叶子结点 逻辑查询计划树的叶子结点被一个扫描运算符替代。这些运算符一般包括：\nTableScan( R )：以任意顺序读人所有元组 SortScan(R, L)：按照顺序读入R的元组，并以列L中的属性进行排列 IndexScan(R, C)：C是一个带有比较运算符的条件，例如 A = 100，A是R的一个属性，如果A上建立的索引，可以通过索引来访问R的元组。如果比较运算符不是等值比较，则索引必须是一个支持范围查询的索引，例如B+ Tree IndexScan(R, A)：这里A是R的一个属性，关系R通过A上的索引被检索。 如果R的数据在磁盘上不是占用连续的存储空间，该运算符可能比TableScan更有效。 逻辑计划的σ选择（selection)结点 σ选择结点一般有两种选择：\n可以简单的用物理过滤运算符Filter( C )替代 σC( R ) 如果C是一个带有比较运算符的条件，例如 A = 100，并且属性A上有索引，可以把比较运算合并到扫描运算符：IndexScan(R, A = 100)。 对于本例，salary 一般都不会建立索引，因此可以把σ( salary \u0026gt; 7000 ) 替换为 Filter( salary \u0026gt; 7000 )\n逻辑计划的连接结点 常见的等值连接满足结合律和交换律，因此连接可以得到很多候选的物理执行计划。其中最关键的问题是确定连接顺序。\n当两个关系连接，只有两种选择，一般我们应该将估计值较小的关系放在前面。当连接有2个以上关系时，可能的连接树的数量会迅速增加，例如4个关系的连接将会有4!=24种连接方式。这一部分很复杂，就不在本文讨论了。\n除了连接顺序，还需要确定具体使用的连接算法。常见的算法有：\nnested loops 基于排序的join（sort merge join） hash join 基于索引的join 逻辑计划的投影结点 投影结点的任务比较明确，输出包含指定列的关系。\n除了将逻辑查询计划的结点转换成物理运算符，在选择物理计划时还要考虑数据如何在运算符之间流动（中间结果保存到磁盘或流水线方式），物理运算符的执行顺序等。这些细节本文就不再讨论。\n最后，假定我们在所有选择的组合中，确定了其中一个作为最优的物理查询计划，然后就可以把它交给查询执行器真正的执行了：\n# 写在最后 本文把关系数据库查询处理涉及的基础知识进行了梳理，希望对你理解 TiDB 的代码能有所帮助。\n数据库是一个非常迷人的领域，它有很强的理论基础，同时又涉及大量的工程实践，可以说是最复杂的系统软件之一。我相信能开发数据库是很多程序员的梦想。梦想还是要有的，让我们一起努力吧！\n","date":"2018-07-01T17:04:31+08:00","permalink":"https://mazhen.tech/p/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93%E6%9F%A5%E8%AF%A2%E5%A4%84%E7%90%86%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%89%AB%E7%9B%B2/","title":"关系数据库查询处理基础知识扫盲"},{"content":" # 为什么XA事务建议用SERIALIZABLE隔离级别 在MySQL最新的官方文档中，关于XA Transactions的介绍有这么一段描述：\nAs with nondistributed transactions, SERIALIZABLE may be preferred if your applications are sensitive to read phenomena. REPEATABLE READ may not be sufficient for distributed transactions.\n这段话表达的意思是，对于分布式XA事务， REPEATABLE READ 隔离级别是不够的。\nMySQL旧版本的文档，对于这个问题表达的更加直接：\nHowever, for a distributed transaction, you must use the SERIALIZABLE isolation level to achieve ACID properties. It is enough to use REPEATABLE READ for a nondistributed transaction, but not for a distributed transaction.\n怎么理解呢？举个简单的例子：假设MySQL使用的是REPEATABLE READ 隔离级别，XA事务 T1 修改的数据涉及两个节点 A 和 B，当事务 T1 在 A 上完成commit，而在 B 上还没commit之前，也就是说这时事务 T1 并没有真正结束，另一个XA事务 T2 已经可以访问到 T1 在 A 上提交后数据，这不是出现脏读了吗？\n那么使用SERIALIZABLE就能保证吗？还是看例子：事务 T1 修改节点 A 上的数据 a -\u0026gt; a'，修改 B 上的数据 b -\u0026gt; b'，在提交阶段，可能被其他事务 T2 读取到了 a'， 因为使用了SERIALIZABLE隔离级别， MySQL 会对所有读加锁，那么 T2 在 B 上读取 b 时会被一直阻塞，直到 T1 在 B 上完成commit，这时 T2 在 B 读取到的就是 b'。 也就是说，SERIALIZABLE隔离级别保证了读到 a' 的事务，不会读到 b ，而是读到 b'，确保了事务ACID的要求。\n更加详细的描述可以参考鹅厂 TDSQL XA 事务隔离级别的奥秘，他们的结论是：\n如果某个并发事务调度机制可以让具有依赖关系的事务构成一个有向无环图(DAG)，那么这个调度就是可串行化的调度。由于每个后端DB都在使用serializable隔离级别，所以每个后端DB上面并发执行的事务分支构成的依赖关系图一定是DAG。\n只要所有连接都是用serializable隔离级别，那么TDSQL XA执行的事务仍然可以达到可串行化隔离级别。\n# SERIALIZABLE性能差，有更好的实现方式吗 如果分布式事务想实现read-committed以上的隔离级别，又不想使用SERIALIZABLE，有什么更好的方式吗？\n当然有，想想看TiDB是怎么做的，底层TiKV是一个整体，有全局的MVCC，所以能够做到分布式事务的Snapshot隔离级别。\nPostgreSQL社区中，有Postgres-XC和Postgres-XL的方案，采用的并发机制是全局MVCC 和本地写锁。 Postgres-XC 维持了全局活跃事务列表，从而提供全局MVCC。\n虽然MySQL也实现了MVCC，但它没有将底层K/V带有时间戳的版本信息暴露出来。也就是说，多个MySQL实例组成的集群没有全局的MVCC，无法得到全局一致性快照，自然就很难做到分布式的Snapshot隔离级别。腾讯的这篇文章也分析了这么做比较困难：\n由于MySQL innodb使用MVCC做select（除了serializable和for update/lock in share mode子句），还需要将这个全局事务id给予innodb做事务id，同时，还需要TDSQL XA集群的多个set的innodb 共享各自的本地事务状态给所有其他innodb（这也是PGXL 所做的），任何一个innodb的本地事务的启动，prepare，commit，abort都需要通知给所有其他innodb实例。只有这样做，集群中的每个innodb实例才能够建立全局完全有一致的、当前集群中正在处理的所有事务的状态，以便做多版本并发控制。 这本身都会造成极大的性能开销，并且导致set之间的严重依赖，降低系统可靠性。这些都是我们要极力避免的。\n# 结论 根据上面的分析，如果使用MySQL 的 XA分布式事务，最安全的方式还是按照官方建议，使用SERIALIZABLE隔离级别。\n如果想基于MySQL做改造，实现全局MVCC，从而实现分布式事务的Snapshot隔离级别，目前还没有看到MySQL社区有这类项目，相信实现难度比较大。\n","date":"2018-06-05T14:44:59+08:00","permalink":"https://mazhen.tech/p/%E5%85%B3%E4%BA%8Emysql-xa%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/","title":"关于MySQL XA事务的隔离级别"},{"content":"现在新出现的MySQL中间件除了基本的数据sharding功能，都增加了很多高级特性，我觉得有三个特性比较重要：\n分布式事务的支持 数据的强一致复制，提高了数据的安全性和可用性 支持跨shard join 通过对这些特性的支持，MySQL中间件具备了一些newSQL数据库的能力，不再是个纯粹的中间件，让用户更容易使用。我调研了最近开源的青云RadonDB，希望了解下这方面最新的进展。\n先简单看下RadonDB的整体架构：存储/计算分离。存储节点是MySQL，3个一组用raft实现数据的强一致性，不再是异步/半同步复制，数据的安全性、可用性级别更高。上层是SQL节点，负责客户端连接、SQL语句的解析和执行、分布式事务协调、数据sharding逻辑等。右下脚计算节点的作用，后面会解释。\n在知乎上看到\u0026lt;如何评价青云开源的分布式数据库 radondb\u0026gt;，RandoDB被吐槽的很厉害。我们从这些吐槽可以了解产品宣传之外的一些信息，知道做这种中间件不是那么容易。大家对RandoDB的几个关键特性的实现方式都不太满意。让我们逐一看看。\n分布式事务的实现 对分布式事务的实现大家吐槽的最厉害：\n官方宣传用XA实现了Snapshot Isolation，然而众所周知XA是无法实现SI的。所谓的事务其实只支持单条SQL，BEGIN / SET AUTOCOMMIT=0 都不支持。\n单语句事务，就是不能 begin 开启事务。\n为了达到 SI 隔离级别，在执行用户 SQL 时，会加上一个 commitLock，防止其他事务提交。这决定了加锁必须时间很短，比如一条SQL，如果你从start transaction开始加锁，那其他事务全都无法提交了，系统事实上已经不可用。\n所谓分布式事物快照隔离级别是 radondb 层 query 和 commit 语句串行化实现的。这个应该是串行化隔离级别了。而且是和冲突没关系的串行化，就是说根本不管两个事物之间有没有冲突数据。性能自行脑补。\n没有 XA log 的 XA 事务原子性实现都是耍流氓。\n为什么XA是无法实现SI？ 我的理解是，单个MySQL实例虽然实现了MVCC，但它没有将底层K/V带有时间戳的版本信息暴露出来。也就是说，多个MySQL实例组成的集群没有全局的MVCC，每个实例内部的MVCC是独立的，无法得到全局一致性快照。XA事务跨越了多个节点，所以没办法实现Snapshot隔离级别。可以对比下TiDB的实现，底层TiKV是一个整体，有全局的MVCC，所以能在上层支持分布式事务的Snapshot隔离级别。\nRandoDB的实现能work，但相当于在Proxy层将所有事务串行化，即使两个事务之间没有数据冲突。而且只有单语句事务。\n对于XA log，开发者的解释是：\nproxy xa log只针对xa commit出错，目前通过分析log然后人工介入，这里没有再记log的必要\n我觉得这么做很不严谨。2PC协议有一个问题，一旦事务参与者投票，它必须等待coordinator给出指示，提交或放弃。如果这时coordinator挂了，事务参与者除了等待什么也做不了。事务处于未决状态，事务的最终结果记录在coordinator的事务日志中，只能等它recovery。因此，现在很多改进的做法是用Paxos/raft保证事务日志的高可用，coordinator挂了可以快速切换。即使不用raft，找一个地方可靠持久的保存事务日志是非常必要的。\n使用Raft保证强一致性 现在很多项目都会使用Paxos/Raft来改进MySQL的复制机制，实现数据的强一致性。如果主、备间任何时刻都完全一致，那么任何时刻都可以安全的进行主备切换。如果无法保证主、备间的强一致性，那么当有持续不断的更新时，主备切换就无法保证强一致性，需要在切换时暂停主库的写入，服务会有短暂的中断。\n腾讯的PhxSQL就是建立在Paxos的一致性和MySQL的binlog流水基础上，通过Paxos保证强一致和高可用的MySQL集群。关于PhxSQL的设计理念可以参见：\n谈谈PhxSQL的设计和实现哲学（上） 谈谈PhxSQL的设计和实现哲学（下） 采用类似做法的还有阿里云的MySQL金融版。另外，MySQL官方也从5.7开始推出了Group Replication机制，内部使用Paxos实现了多个副本的一致。\nRadonDB的实现机制和PhxSQL不太一样。它在一组MySQL集群内的复制还是通过Semi-Sync机制（Semi-Sync设置为无限大，不退化为异步复制），保证有一个slave和master完全一致。主备切换时会选择这个slave为主，然后结合MySQL的 Multi-threaded replication 和 GTID机制 进行快速回放，让主备重新一致。Raft用在哪里了？在 RadonDB 只使用 Raft 进行选主，当主挂掉之后，使用 Raft 选出新的主。Raft选主的逻辑是选出一个拥有最多的committed log的成员作为主，那么对于RadonDB来说，哪个MySQL的GTID最大就选哪个。\n我自己还没有使用Raft的经验，不确定RadonDB的实现机制是否合理。但利用Semi-Sync模拟同步复制的方案，我觉得有一个地方不妥。当和主库保持强同步的备库有问题时，这组MySQL整体就不可用，因为它需要至少一个备库和主库完全一致，这就因为单点降低了整个集群的可用性。如果是用Raft做数据复制，就不会有这种单点影响全局可用性的问题。\n另外，RandoDB被吐槽 Raft 的实现业余、不严谨：\n打开用来做HA的Xenon模块，一看又是作业级的肯写代码不肯写测试的raft练手实现。raft测试用例一共1500行go代码 刚才数了下，自己的raft库光election相关的单元测试用例就数千行代码了。做生产环境用的系统不是练手写作业，需要一个go的raft库，既然都不肯写完备的测试了，那就老老实实用etcd或者hashicorp的raft。自己私下撸一个raft库练手，和给自己全职项目选一个可靠的raft实现，两者真的不矛盾。最滑稽，只做选主干嘛自己撸一个raft实现？\njoin等复杂查询的实现 严格说RandoDB是不支持join的。它的做法是让计算节点通过binglog复制了全量数据，SQL节点会把join等复杂查处路由到计算节点执行。\n“计算节点”使用tokudb存储引擎存储全量数据，为了支持复杂查询。。。如果我一个分布式系统的数据总量有20T、100T，也用单个“计算节点”存储全量数据？而且这个数据同步过程是异步的，显然没法用在OLTP场景。\n通过一些实用的方式支持了Join，这种做法可以work，但RandoDB离它宣称的数据库还差很远，缺少全局的执行计划优化。\n总体来说，RandoDB的理想很宏大，用实用的方案解决了一些问题，但要成为真正成熟的数据库产品还差的比较远。RadonDB 的核心代码1万行左右。加上其它类库引入，Radon代码11万+， Xenon代码5万行+ 。\n最后，看到有人推荐腾讯的TDSQL，也顺便了解了一下。从资料看TDSQL很不错，可惜不是开源产品。除了水平扩张、安全增强、自动化运维以外，它具备了我们上面提到的数据库中间件的高级特性：\n支持分布式事务XA 全局事务的隔离级别最高可以达到serializable级别 分布式事务对业务透明，兼容单机事务语法 允许事务中多条语句分别发给多个分片 支持autocommit下单条语句写访问多个分片 默认采用强同步复制，即主从节点数据完全一致 复杂查询方面 允许以流式处理方式运行group by、order by 支持两个Shard使用shardkey（分表键）做等值连接，以及使用shardkey的子查询 支持了部分受限的复杂查询，对于数据库中间件来说已经算比较强大了。关于TDSQL的分布式事务，可以通过这两篇进行更多的了解：\n一文教你迅速解决分布式事务 XA 一致性问题 鹅厂 TDSQL XA 事务隔离级别的奥秘 如果我们做MySQL中间件，可以瞄准TDSQL，对于分布式事务、数据强一致性，以及复杂查询、跨shard join 等特性都要考虑支持。\n","date":"2018-06-03T14:43:12+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8Eradondb%E7%9C%8B%E6%96%B0%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%9A%84%E7%89%B9%E6%80%A7/","title":"从RadonDB看新型数据库中间件的特性"},{"content":"TiDB源码解读系列的《Insert语句概览》讲解了Insert执行的整体流程，并在最后用一幅图描述了整个流程：\n我按照自己的理解对这幅图扩展了一下，在原先数据结构转换流程的基础上，补充了代码的调用流程，个人感觉更加全面，希望对你阅读代码也有帮助。\n","date":"2018-05-15T14:38:05+08:00","permalink":"https://mazhen.tech/p/tidb-insert-%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B%E5%9B%BE/","title":"TiDB Insert 执行流程图"},{"content":" Previous Next \u0026nbsp; \u0026nbsp; Page: / ","date":"2018-05-11T17:01:04+08:00","permalink":"https://mazhen.tech/p/cursor%E5%8A%9F%E8%83%BD%E5%AE%9E%E7%8E%B0%E6%80%BB%E7%BB%93/","title":"Cursor功能实现总结"},{"content":"PingCAP发布了TiDB的源码阅读系列文章，让我们可以比较系统的去学习了解TiDB的内部实现。最近的一篇《SQL 的一生》，从整体上讲解了一条SQL语句的处理流程，从网络上接收数据，MySQL协议解析和转换，SQL语法解析，查询计划的制定和优化，查询计划执行，到最后返回结果。\n其中，SQL Parser的功能是把SQL语句按照SQL语法规则进行解析，将文本转换成抽象语法树（AST），这部分功能需要些背景知识才能比较容易理解，我尝试做下相关知识的介绍，希望能对读懂这部分代码有点帮助。\nTiDB是使用goyacc根据预定义的SQL语法规则文件parser.y生成SQL语法解析器。我们可以在TiDB的Makefile文件中看到这个过程，先build goyacc工具，然后使用goyacc根据parser.y生成解析器parser.go：\n1 2 3 4 5 6 goyacc: $(GOBUILD) -o bin/goyacc parser/goyacc/main.go parser: goyacc bin/goyacc -o /dev/null parser/parser.y bin/goyacc -o parser/parser.go parser/parser.y 2\u0026gt;\u0026amp;1 ... goyacc是yacc的Golang版，所以要想看懂语法规则定义文件parser.y，了解解析器是如何工作的，先要对Lex \u0026amp; Yacc有些了解。\n# Lex \u0026amp; Yacc 介绍 Lex \u0026amp; Yacc 是用来生成词法分析器和语法分析器的工具，它们的出现简化了编译器的编写。Lex \u0026amp; Yacc 分别是由贝尔实验室的Mike Lesk 和 Stephen C. Johnson在1975年发布。对于Java程序员来说，更熟悉的是ANTLR，ANTLR 4 提供了 Listener+Visitor 组合接口， 不需要在语法定义中嵌入actions，使应用代码和语法定义解耦。Spark的SQL解析就是使用了ANTLR。Lex \u0026amp; Yacc 相对显得有些古老，实现的不是那么优雅，不过我们也不需要非常深入的学习，只要能看懂语法定义文件，了解生成的解析器是如何工作的就够了。我们可以从一个简单的例子开始：\n上图描述了使用Lex \u0026amp; Yacc构建编译器的流程。Lex根据用户定义的patterns生成词法分析器。词法分析器读取源代码，根据patterns将源代码转换成tokens输出。Yacc根据用户定义的语法规则生成语法分析器。语法分析器以词法分析器输出的tokens作为输入，根据语法规则创建出语法树。最后对语法树遍历生成输出结果，结果可以是产生机器代码，或者是边遍历 AST 边解释执行。\n从上面的流程可以看出，用户需要分别为Lex提供patterns的定义，为 Yacc 提供语法规则文件，Lex \u0026amp; Yacc 根据用户提供的输入文件，生成符合他们需求的词法分析器和语法分析器。这两种配置都是文本文件，并且结构相同：\n1 2 3 4 5 ... definitions ... %% ... rules ... %% ... subroutines ... 文件内容由 %% 分割成三部分，我们重点关注中间规则定义部分。对于上面的例子，Lex 的输入文件如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ... %% /* 变量 */ [a-z] { yylval = *yytext - \u0026#39;a\u0026#39;; return VARIABLE; } /* 整数 */ [0-9]+ { yylval = atoi(yytext); return INTEGER; } /* 操作符 */ [-+()=/*\\n] { return *yytext; } /* 跳过空格 */ [ \\t] ; /* 其他格式报错 */ . yyerror(\u0026#34;invalid character\u0026#34;); %% ... 上面只列出了规则定义部分，可以看出该规则使用正则表达式定义了变量、整数和操作符等几种token。例如整数token的定义如下：\n1 2 3 4 [0-9]+ { yylval = atoi(yytext); return INTEGER; } 当输入字符串匹配这个正则表达式，大括号内的动作会被执行：将整数值存储在变量 yylval 中，并返回 token 类型 INTEGER 给 Yacc。\n再来看看 Yacc 语法规则定义文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 %token INTEGER VARIABLE %left \u0026#39;+\u0026#39; \u0026#39;-\u0026#39; %left \u0026#39;*\u0026#39; \u0026#39;/\u0026#39; ... %% program: program statement \u0026#39;\\n\u0026#39; | ; statement: expr { printf(\u0026#34;%d\\n\u0026#34;, $1); } | VARIABLE \u0026#39;=\u0026#39; expr { sym[$1] = $3; } ; expr: INTEGER | VARIABLE { $$ = sym[$1]; } | expr \u0026#39;+\u0026#39; expr { $$ = $1 + $3; } | expr \u0026#39;-\u0026#39; expr { $$ = $1 - $3; } | expr \u0026#39;*\u0026#39; expr { $$ = $1 * $3; } | expr \u0026#39;/\u0026#39; expr { $$ = $1 / $3; } | \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; { $$ = $2; } ; %% ... 第一部分定义了 token 类型和运算符的结合性。四种运算符都是左结合，同一行的运算符优先级相同，不同行的运算符，后定义的行具有更高的优先级。\n语法规则使用了BNF定义。BNF 可以用来表达上下文无关（context-free）语言，大部分的现代编程语言都可以使用 BNF 表示。上面的规则定义了三个产生式。产生式冒号左边的项（例如 statement）被称为非终结符， INTEGER 和 VARIABLE 被称为终结符,它们是由 Lex 返回的 token 。终结符只能出现在产生式的右侧。可以使用产生式定义的语法生成表达式：\n1 2 3 4 5 expr -\u0026gt; expr * expr -\u0026gt; expr * INTEGER -\u0026gt; expr + expr * INTEGER -\u0026gt; expr + INTEGER * INTEGER -\u0026gt; INTEGER + INTEGER * INTEGER 解析表达式是生成表达式的逆向操作，我们需要归约表达式到一个非终结符。Yacc 生成的语法分析器使用自底向上的归约（shift-reduce）方式进行语法解析，同时使用堆栈保存中间状态。还是看例子，表达式x + y * z的解析过程：\n1 2 3 4 5 6 7 8 9 10 11 12 13 1 . x + y * z 2 x . + y * z 3 expr . + y * z 4 expr + . y * z 5 expr + y . * z 6 expr + expr . * z 7 expr + expr * . z 8 expr + expr * z . 9 expr + expr * expr . 10 expr + expr . 11 expr . 12 statement . 13 program . 点（.）表示当前的读取位置，随着 . 从左向右移动，我们将读取的token压入堆栈，当发现堆栈中的内容匹配了某个产生式的右侧，则将匹配的项从堆栈中弹出，将该产生式左侧的非终结符压入堆栈。这个过程持续进行，直到读取完所有的tokens，并且只有启始非终结符（本例为 program）保留在堆栈中。\n产生式右侧的大括号中定义了该规则关联的动作，例如：\n1 expr: expr \u0026#39;*\u0026#39; expr { $$ = $1 * $3; } 我们将堆栈中匹配该产生式右侧的项替换为产生式左侧的非终结符，本例中我们弹出 expr '*' expr，然后把 expr 压回堆栈。 我们可以使用 $position 的形式访问堆栈中的项，$1引用的是第一项，$2引用的是第二项，以此类推。$$ 代表的是归约操作执行后的堆栈顶。本例的动作是将三项从堆栈中弹出，两个表达式相加，结果再压回堆栈顶。\n上面例子中语法规则关联的动作，在完成语法解析的同时，也完成了表达式求值。一般我们希望语法解析的结果是一棵抽象语法树（AST），可以这么定义语法规则关联的动作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ... %% ... expr: INTEGER { $$ = con($1); } | VARIABLE { $$ = id($1); } | expr \u0026#39;+\u0026#39; expr { $$ = opr(\u0026#39;+\u0026#39;, 2, $1, $3); } | expr \u0026#39;-\u0026#39; expr { $$ = opr(\u0026#39;-\u0026#39;, 2, $1, $3); } | expr \u0026#39;*\u0026#39; expr { $$ = opr(\u0026#39;*\u0026#39;, 2, $1, $3); } | expr \u0026#39;/\u0026#39; expr { $$ = opr(\u0026#39;/\u0026#39;, 2, $1, $3); } | \u0026#39;(\u0026#39; expr \u0026#39;)\u0026#39; { $$ = $2; } ; %% nodeType *con(int value) { ... } nodeType *id(int i) { ... } nodeType *opr(int oper, int nops, ...) { ... } 上面是一个语法规则定义的片段，我们可以看到，每个规则关联的动作不再是求值，而是调用相应的函数，该函数会返回抽象语法树的节点类型 nodeType，然后将这个节点压回堆栈，解析完成时，我们就得到了一颗由 nodeType 构成的抽象语法树。对这个语法树进行遍历访问，可以生成机器代码，也可以解释执行。\n至此，我们大致了解了Lex \u0026amp; Yacc的原理。其实还有非常多的细节，例如如何消除语法的歧义，但我们的目的是读懂TiDB的代码，掌握这些概念已经够用了。\n# goyacc 简介 goyacc 是golang版的 Yacc。和 Yacc的功能一样，goyacc 根据输入的语法规则文件，生成该语法规则的go语言版解析器。goyacc 生成的解析器 yyParse 要求词法分析器符合下面的接口：\n1 2 3 4 type yyLexer interface { Lex(lval *yySymType) int Error(e string) } 或者\n1 2 3 4 5 type yyLexerEx interface { yyLexer // Hook for recording a reduction. Reduced(rule, state int, lval *yySymType) (stop bool) // Client should copy *lval. } TiDB没有使用类似 Lex 的工具生成词法分析器，而是纯手工打造，词法分析器对应的代码是 parser/lexer.go， 它实现了 goyacc 要求的接口：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ... // Scanner implements the yyLexer interface. type Scanner struct { r reader buf bytes.Buffer errs []error stmtStartPos int // For scanning such kind of comment: /*! MySQL-specific code */ or /*+ optimizer hint */ specialComment specialCommentScanner sqlMode mysql.SQLMode } // Lex returns a token and store the token value in v. // Scanner satisfies yyLexer interface. // 0 and invalid are special token id this function would return: // return 0 tells parser that scanner meets EOF, // return invalid tells parser that scanner meets illegal character. func (s *Scanner) Lex(v *yySymType) int { tok, pos, lit := s.scan() v.offset = pos.Offset v.ident = lit ... } // Errors returns the errors during a scan. func (s *Scanner) Errors() []error { return s.errs } 另外lexer 使用了字典树技术进行 token 识别，具体的实现代码在parser/misc.go\n# TiDB SQL Parser的实现 终于到了正题。有了上面的背景知识，对TiDB 的 SQL Parser 模块会相对容易理解一些。先看SQL语法规则文件parser.y，goyacc 就是根据这个文件生成SQL语法解析器的。\nparser.y 有6500多行，第一次打开可能会被吓到，其实这个文件仍然符合我们上面介绍过的结构：\n1 2 3 4 5 ... definitions ... %% ... rules ... %% ... subroutines ... parser.y 第三部分 subroutines 是空白没有内容的， 所以我们只需要关注第一部分 definitions 和第二部分 rules。\n第一部分主要是定义token的类型、优先级、结合性等。注意 union 这个联合体结构体：\n1 2 3 4 5 6 7 %union { offset int // offset item interface{} ident string expr ast.ExprNode statement ast.StmtNode } 该联合体结构体定义了在语法解析过程中被压入堆栈的项的属性和类型。\n压入堆栈的项可能是终结符，也就是 token，它的类型可以是item 或 ident；\n这个项也可能是非终结符，即产生式的左侧，它的类型可以是 expr 、 statement 、 item 或 ident。\ngoyacc 根据这个 union 在解析器里生成对应的 struct 是：\n1 2 3 4 5 6 7 8 type yySymType struct { yys int offset int // offset item interface{} ident string expr ast.ExprNode statement ast.StmtNode } 在语法解析过程中，非终结符会被构造成抽象语法树（AST）的节点 ast.ExprNode 或 ast.StmtNode。抽象语法树相关的数据结构都定义在 ast 包中，它们大都实现了 ast.Node 接口：\n1 2 3 4 5 6 7 // Node is the basic element of the AST. // Interfaces embed Node should have \u0026#39;Node\u0026#39; name suffix. type Node interface { Accept(v Visitor) (node Node, ok bool) Text() string SetText(text string) } 这个接口有一个 Accept 方法，接受 Visitor 参数，后续对 AST 的处理，主要依赖这个 Accept 方法，以 Visitor 模式遍历所有的节点以及对 AST 做结构转换。\n1 2 3 4 5 // Visitor visits a Node. type Visitor interface { Enter(n Node) (node Node, skipChildren bool) Leave(n Node) (node Node, ok bool) } 例如 plan.preprocess 是对 AST 做预处理，包括合法性检查以及名字绑定。\nunion 后面是对 token 和 非终结符 按照类型分别定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 /* 这部分的token是 ident类型 */ %token \u0026lt;ident\u0026gt; ... add \u0026#34;ADD\u0026#34; all \u0026#34;ALL\u0026#34; alter \u0026#34;ALTER\u0026#34; analyze \u0026#34;ANALYZE\u0026#34; and \u0026#34;AND\u0026#34; as \u0026#34;AS\u0026#34; asc \u0026#34;ASC\u0026#34; between \u0026#34;BETWEEN\u0026#34; bigIntType \u0026#34;BIGINT\u0026#34; ... /* 这部分的token是 item 类型 */ %token \u0026lt;item\u0026gt; /*yy:token \u0026#34;1.%d\u0026#34; */ floatLit \u0026#34;floating-point literal\u0026#34; /*yy:token \u0026#34;1.%d\u0026#34; */ decLit \u0026#34;decimal literal\u0026#34; /*yy:token \u0026#34;%d\u0026#34; */ intLit \u0026#34;integer literal\u0026#34; /*yy:token \u0026#34;%x\u0026#34; */ hexLit \u0026#34;hexadecimal literal\u0026#34; /*yy:token \u0026#34;%b\u0026#34; */ bitLit \u0026#34;bit literal\u0026#34; andnot \u0026#34;\u0026amp;^\u0026#34; assignmentEq \u0026#34;:=\u0026#34; eq \u0026#34;=\u0026#34; ge \u0026#34;\u0026gt;=\u0026#34; ... /* 非终结符按照类型分别定义 */ %type \u0026lt;expr\u0026gt; Expression \u0026#34;expression\u0026#34; BoolPri \u0026#34;boolean primary expression\u0026#34; ExprOrDefault \u0026#34;expression or default\u0026#34; PredicateExpr \u0026#34;Predicate expression factor\u0026#34; SetExpr \u0026#34;Set variable statement value\u0026#39;s expression\u0026#34; ... %type \u0026lt;statement\u0026gt; AdminStmt \u0026#34;Check table statement or show ddl statement\u0026#34; AlterTableStmt \u0026#34;Alter table statement\u0026#34; AlterUserStmt \u0026#34;Alter user statement\u0026#34; AnalyzeTableStmt \u0026#34;Analyze table statement\u0026#34; BeginTransactionStmt \u0026#34;BEGIN TRANSACTION statement\u0026#34; BinlogStmt \u0026#34;Binlog base64 statement\u0026#34; ... %type \u0026lt;item\u0026gt; AlterTableOptionListOpt \u0026#34;alter table option list opt\u0026#34; AlterTableSpec \u0026#34;Alter table specification\u0026#34; AlterTableSpecList \u0026#34;Alter table specification list\u0026#34; AnyOrAll \u0026#34;Any or All for subquery\u0026#34; Assignment \u0026#34;assignment\u0026#34; ... %type \u0026lt;ident\u0026gt; KeyOrIndex \u0026#34;{KEY|INDEX}\u0026#34; ColumnKeywordOpt \u0026#34;Column keyword or empty\u0026#34; PrimaryOpt \u0026#34;Optional primary keyword\u0026#34; NowSym \u0026#34;CURRENT_TIMESTAMP/LOCALTIME/LOCALTIMESTAMP\u0026#34; NowSymFunc \u0026#34;CURRENT_TIMESTAMP/LOCALTIME/LOCALTIMESTAMP/NOW\u0026#34; ... 第一部分的最后是对优先级和结合性的定义：\n1 2 3 4 5 6 7 8 9 10 11 12 13 ... %precedence sqlCache sqlNoCache %precedence lowerThanIntervalKeyword %precedence interval %precedence lowerThanStringLitToken %precedence stringLit ... %right assignmentEq %left pipes or pipesAsOr %left xor %left andand and %left between ... parser.y文件的第二部分是SQL语法的产生式和每个规则对应的 aciton 。SQL语法非常复杂，parser.y 的大部分内容都是产生式的定义。\nSQL 语法可以参照MySQL参考手册的SQL Statement Syntax 部分，例如 SELECT 语法的定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 SELECT [ALL | DISTINCT | DISTINCTROW ] [HIGH_PRIORITY] [STRAIGHT_JOIN] [SQL_SMALL_RESULT] [SQL_BIG_RESULT] [SQL_BUFFER_RESULT] [SQL_CACHE | SQL_NO_CACHE] [SQL_CALC_FOUND_ROWS] select_expr [, select_expr ...] [FROM table_references [PARTITION partition_list] [WHERE where_condition] [GROUP BY {col_name | expr | position} [ASC | DESC], ... [WITH ROLLUP]] [HAVING where_condition] [ORDER BY {col_name | expr | position} [ASC | DESC], ...] [LIMIT {[offset,] row_count | row_count OFFSET offset}] [PROCEDURE procedure_name(argument_list)] [INTO OUTFILE \u0026#39;file_name\u0026#39; [CHARACTER SET charset_name] export_options | INTO DUMPFILE \u0026#39;file_name\u0026#39; | INTO var_name [, var_name]] [FOR UPDATE | LOCK IN SHARE MODE]] 我们可以在 parser.y 中找到 SELECT 语句的产生式：\n1 2 3 4 5 6 7 8 9 SelectStmt: \u0026#34;SELECT\u0026#34; SelectStmtOpts SelectStmtFieldList OrderByOptional SelectStmtLimit SelectLockOpt { ... } | \u0026#34;SELECT\u0026#34; SelectStmtOpts SelectStmtFieldList FromDual WhereClauseOptional SelectStmtLimit SelectLockOpt { ... } | \u0026#34;SELECT\u0026#34; SelectStmtOpts SelectStmtFieldList \u0026#34;FROM\u0026#34; TableRefsClause WhereClauseOptional SelectStmtGroup HavingClause OrderByOptional SelectStmtLimit SelectLockOpt { ... } 产生式 SelectStmt 和 SELECT 语法是对应的。\n我省略了大括号中的 action ，这部分代码会构建出 AST 的 ast.SelectStmt 节点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 type SelectStmt struct { dmlNode resultSetNode // SelectStmtOpts wraps around select hints and switches. *SelectStmtOpts // Distinct represents whether the select has distinct option. Distinct bool // From is the from clause of the query. From *TableRefsClause // Where is the where clause in select statement. Where ExprNode // Fields is the select expression list. Fields *FieldList // GroupBy is the group by expression list. GroupBy *GroupByClause // Having is the having condition. Having *HavingClause // OrderBy is the ordering expression list. OrderBy *OrderByClause // Limit is the limit clause. Limit *Limit // LockTp is the lock type LockTp SelectLockType // TableHints represents the level Optimizer Hint TableHints []*TableOptimizerHint } 可以看出，ast.SelectStmt 结构体内包含的内容和 SELECT 语法也是一一对应的。\n其他的产生式也都是根据对应的 SQL 语法来编写的。从 parser.y 的注释看到，这个文件最初是用工具从 BNF 转化生成的，从头手写这个规则文件，工作量会非常大。\n完成了语法规则文件 parser.y 的定义，就可以使用 goyacc 生成语法解析器：\n1 bin/goyacc -o parser/parser.go parser/parser.y 2\u0026gt;\u0026amp;1 TiDB对 lexer 和 parser.go 进行了封装，对外提供 parser.yy_parser 进行SQL语句的解析：\n1 2 3 4 // Parse parses a query string to raw ast.StmtNode. func (parser *Parser) Parse(sql, charset, collation string) ([]ast.StmtNode, error) { ... } 最后，我写了一个简单的例子，使用TiDB的 SQL Parser 进行SQL语法解析，构建出 AST，然后利用 visitor 遍历 AST ：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 package main import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/pingcap/tidb/parser\u0026#34; \u0026#34;github.com/pingcap/tidb/ast\u0026#34; ) type visitor struct{} func (v *visitor) Enter(in ast.Node) (out ast.Node, skipChildren bool) { fmt.Printf(\u0026#34;%T\\n\u0026#34;, in) return in, false } func (v *visitor) Leave(in ast.Node) (out ast.Node, ok bool) { return in, true } func main() { sql := \u0026#34;SELECT /*+ TIDB_SMJ(employees) */ emp_no, first_name, last_name \u0026#34; + \u0026#34;FROM employees USE INDEX (last_name) \u0026#34; + \u0026#34;where last_name=\u0026#39;Aamodt\u0026#39; and gender=\u0026#39;F\u0026#39; and birth_date \u0026gt; \u0026#39;1960-01-01\u0026#39;\u0026#34; sqlParser := parser.New() stmtNodes, err := sqlParser.Parse(sql, \u0026#34;\u0026#34;, \u0026#34;\u0026#34;) if err != nil { fmt.Printf(\u0026#34;parse error:\\n%v\\n%s\u0026#34;, err, sql) return } for _, stmtNode := range stmtNodes { v := visitor{} stmtNode.Accept(\u0026amp;v) } } 我实现的 visitor 什么也没干，只是输出了节点的类型。 这段代码的运行结果如下，依次输出遍历过程中遇到的节点类型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 *ast.SelectStmt *ast.TableOptimizerHint *ast.TableRefsClause *ast.Join *ast.TableSource *ast.TableName *ast.BinaryOperationExpr *ast.BinaryOperationExpr *ast.BinaryOperationExpr *ast.ColumnNameExpr *ast.ColumnName *ast.ValueExpr *ast.BinaryOperationExpr *ast.ColumnNameExpr *ast.ColumnName *ast.ValueExpr *ast.BinaryOperationExpr *ast.ColumnNameExpr *ast.ColumnName *ast.ValueExpr *ast.FieldList *ast.SelectField *ast.ColumnNameExpr *ast.ColumnName *ast.SelectField *ast.ColumnNameExpr *ast.ColumnName *ast.SelectField *ast.ColumnNameExpr *ast.ColumnName 了解了TiDB SQL Parser 的实现，我们就有可能实现TiDB当前不支持的语法，例如添加内置函数，也为我们学习查询计划以及优化打下了基础。希望这篇文章对你能有所帮助。\n","date":"2018-05-09T14:34:58+08:00","permalink":"https://mazhen.tech/p/tidb-sql-parser-%E7%9A%84%E5%AE%9E%E7%8E%B0/","title":"TiDB SQL Parser 的实现"},{"content":"配合这篇《基于代价的优化》 阅读。\nCBO的整体思路是：从逻辑查询计划树，自上而下枚举每个逻辑运算符可能的物理算子，从所有可能的执行路径中选择一条评估代价最小的作为物理查询计划。\n一个逻辑运算符受两个因素的影响，导致生成多个候选的物理执行计划：\n逻辑运算符可能有多种候选的物理算子供选择，如下表： 有些物理算子会根据参与运算的属性、属性的顺序等因素，生成多种物理执行计划，例如Join的物理算子会根据参与连接的表的顺序，生成多种可能的执行计划。 CBO核心流程的代码在plan/optimizer.go中的physicalOptimize：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func physicalOptimize(logic LogicalPlan) (PhysicalPlan, error) { logic.preparePossibleProperties() _, err := logic.deriveStats() if err != nil { return nil, errors.Trace(err) } t, err := logic.findBestTask(\u0026amp;requiredProp{taskTp: rootTaskType, expectedCnt: math.MaxFloat64}) if err != nil { return nil, errors.Trace(err) } p := t.plan() p.ResolveIndices() return p, nil } 三行关键的代码：\nlogic.preparePossibleProperties()：裁剪参与运算的属性，从而尽可能早的裁减掉成物理计划搜索路径上的分支 logic.deriveStats()：为每个逻辑计划节点生成统计信息，为评估物理计划的代价做准备 logic.findBestTask：生成执行代价最小的task findBestTask的核心逻辑：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 for _, pp := range p.self.exhaustPhysicalPlans(prop) { // find best child tasks firstly. childTasks = childTasks[:0] for i, child := range p.children { childTask, err := child.findBestTask(pp.getChildReqProps(i)) if err != nil { return nil, errors.Trace(err) } childTasks = append(childTasks, childTask) } // combine best child tasks with parent physical plan. curTask := pp.attach2Task(childTasks...) // get the most efficient one. if curTask.cost() \u0026lt; bestTask.cost() { bestTask = curTask } } 首先枚举可能的物理执行计划p.self.exhaustPhysicalPlans，然后遍历每种候选计划，找到代价最小的task。这是个递归的过程，当前节点的代价是由所有子节点的代价组成的，所以在遍历的过程中，又会调用child.findBestTask(pp.getChildReqProps(i))找到子节点的最佳task。\n如何评估物理执行计划的代价呢？根据参与运算的关系（表）的统计信息进行评估。代价评估相关逻辑涉及的代码：\n计算关系的统计信息：plan/stats.go 计算task的代价：plan/task.go中的attach2Task系列方法。 ","date":"2018-05-08T14:40:17+08:00","permalink":"https://mazhen.tech/p/%E5%9F%BA%E4%BA%8E%E4%BB%A3%E4%BB%B7%E4%BC%98%E5%8C%96cbo%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81%E5%AF%BC%E8%AF%BB/","title":"基于代价优化（CBO）实现代码导读"},{"content":"Go的错误处理机制很简洁，使用errors.New(text)创建 error，方法的调用者一般按照如下模式处理：\n1 2 3 if err != nil { return err } 这样做最大的问题是error中没有保存方法调用栈等上下文信息，只能靠创建时传递的string参数来区分error，很难定位错误发生的具体位置。例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import ( \u0026#34;fmt\u0026#34; \u0026#34;errors\u0026#34; ) func f1() error { return f2() } func f2() error { return f3() } func f3() error { return errors.New(\u0026#34;std error\u0026#34;) } func main() { if err := f1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) } } 执行的输出为：\n1 std error 在实际的程序中调用关系复杂，仅凭错误信息很难定位错误源头。TiDB 使用了juju/errors来记录调用栈：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 import ( \u0026#34;github.com/juju/errors\u0026#34; \u0026#34;fmt\u0026#34; ) func jf1() error { err := jf2() if err != nil { return errors.Trace(err) } return nil } func jf2() error { err := jf3() if err != nil { return errors.Trace(err) } return nil } func jf3() error { return errors.New(\u0026#34;juju error\u0026#34;) } func main() { if err := jf1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) } } 这段代码的输出为：\n1 2 3 github.com/mz1999/error/main.go:25: juju error github.com/mz1999/error/main.go:19: github.com/mz1999/error/main.go:11: 可以看到，如果想记录调用栈，每次都需要调用errors.Trace。这样做比较繁琐，而且每次trace时内部都会调用runtime.Caller，性能不佳。TiDB已经调研了新的第三方包pkg/errors准备替换掉juju/errors。\n使用pkg/errors会简单很多，和标准库的errors一致，但可以记录调用栈信息：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 import ( \u0026#34;fmt\u0026#34; \u0026#34;github.com/pkg/errors\u0026#34; ) func pf1() error { return pf2() } func pf2() error { return pf3() } func pf3() error { return errors.New(\u0026#34;pkg error\u0026#34;) } func main() { if err := pf1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) } } 这段代码的输出为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 pkg error main.pf3 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:17 main.pf2 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:13 main.pf1 /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:9 main.main /Users/mazhen/Documents/works/goworkspace/src/github.com/mz1999/error/main.go:21 runtime.main /usr/local/go/src/runtime/proc.go:198 runtime.goexit /usr/local/go/src/runtime/asm_amd64.s:2361 这样看，使用pkg/errors替代标准库的errors就可以满足我们的需求。\n另外，pkg/errors的作者还给了一些最佳实践的建议：\n在你自己的代码中，在错误的发生点使用errors.New 或 errors.Errorf ： 1 2 3 4 5 6 func parseArgs(args []string) error { if len(args) \u0026lt; 3 { return errors.Errorf(\u0026#34;not enough arguments, expected at least 3, got %d\u0026#34;, len(args)) } // ... } 如果你接收到一个error，一般简单的直接返回： 1 2 3 if err != nil { return err } 如果你是调用第三方的包或标准库时接收到error，使用 errors.Wrap or errors.Wrapf 包装这个error，它会记录在这个点的调用栈： 1 2 3 4 f, err := os.Open(path) if err != nil { return errors.Wrapf(err, \u0026#34;failed to open %q\u0026#34;, path) } Always return errors to their caller rather than logging them throughout your program.\n在程序的top level，或者是worker goroutine，使用 %+v 输出error的详细信息。\n1 2 3 4 5 6 7 func main() { err := app.Run() if err != nil { fmt.Printf(\u0026#34;FATAL: %+v\\n\u0026#34;, err) os.Exit(1) } } 如果需要抛出包含MySQL错误码的内部错误，可以使用errors.Wrap包装，附带上报错位置的调用栈信息： 1 2 3 func pf3() error { return errors.Wrap(mysql.NewErr(mysql.ErrCantCreateTable, \u0026#34;tablename\u0026#34;, 500), \u0026#34;\u0026#34;) } 这样，我们既拿到了完整调用栈，又可以使用errors.Cause获取MySQL的错误码等信息：\n1 2 3 4 5 6 7 8 9 10 11 12 if err := pf1(); err != nil { fmt.Printf(\u0026#34;%+v\u0026#34;, err) var sqlError *mysql.SQLError if m, ok := errors.Cause(err).(*mysql.SQLError); ok { sqlError = m } else { sqlError = mysql.NewErrf(mysql.ErrUnknown, \u0026#34;%s\u0026#34;, err.Error()) } fmt.Printf(\u0026#34;\\nMySQL error code: %d, state: %s\u0026#34;, sqlError.Code, sqlError.State) } ","date":"2018-04-06T14:23:40+08:00","permalink":"https://mazhen.tech/p/golang-error%E5%A4%84%E7%90%86%E5%AE%9E%E8%B7%B5/","title":"Golang error处理实践"},{"content":"Go中的引用类型不是指针，而是对指针的包装，在它的内部通过指针引用底层数据结构。每一种引用类型也包含一些其他的field，用来管理底层的数据结构。\n看一个例子比较直观：\n1 2 s := []string{\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;} fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) 简单解释一下这段代码。先初始化一个slice，然后使用unsafe.Pointer(\u0026amp;s)把slice的指针转换为通用指针Pointer。Pointer是可以代表任何数据类型的指针。最后把Pointer强制转换为*reflect.SliceHeader。SliceHeader代表的是slice运行时数据结构，定义如下：\n1 2 3 4 5 type SliceHeader struct { Data uintptr Len int Cap int } 可以看到，SliceHeader内部有一个用来指向底层数组的指针Data，另外还有两个属性Len和Cap用来保存slice的内部状态。\n上面的代码运行结果如下：\n\u0026amp;reflect.SliceHeader{Data:0xc420078180, Len:3, Cap:3}\nslice可以自动扩容，当底层数组容量不够时，会自动创建一个新的数组替换。让我们做个实验：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 s := []string{\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;} fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Println() s = append(s, \u0026#34;d\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Println() s = append(s, \u0026#34;e\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Println() 运行结果如下：\n对于初始化容量为3的slice，在向这个slice append 新元素时，底层会创建一个容量翻倍的新数组，并将原先的内容复制过来，再将新元素append到最后。我们可以看到这个slice内部保存底层数组的指针在第一次append后，指向了新的地址。当再向它append新元素时，由于底层数组还有空间，内部指针保持不变，只是更新Len属性为5。\n在Go中进行函数调用时，参数都是按值传递的。对于引用类型也是按值传递，会复制引用本身，但不会复制引用指向的底层数据结构。还是看代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func foo(s []string) { fmt.Println(\u0026#34;======= func foo =======\u0026#34;) s = append(s, \u0026#34;f\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Printf(\u0026#34;\u0026amp;s: %p \\n\u0026#34;, \u0026amp;s) fmt.Println(\u0026#34;========================\\n\u0026#34;) } func main() { ...... s = append(s, \u0026#34;e\u0026#34;) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Printf(\u0026#34;\u0026amp;s: %p \\n\u0026#34;, \u0026amp;s) fmt.Println() foo(s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, s) fmt.Printf(\u0026#34;%#v \\n\u0026#34;, (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;s))) fmt.Printf(\u0026#34;\u0026amp;s: %p \\n\u0026#34;, \u0026amp;s) } 运行结果为：\n在函数调用时，传递给函数的slice进行了复制，函数的参数是一个新的slice，但slice内部指针指向的底层数组还是同一个。\n完整的示例代码在https://play.golang.org/p/qwwSuskLfCa，可以在Playground中直接运行。\nGo语言的引用类型有slice, map, channel, interface和function。技术上，string也是引用类型：\n1 2 3 4 type StringHeader struct { Data uintptr Len int } 有时候为了性能优化，可以利用[]byte和string头部结构的“部分相同”，以非安全的指针类型转换来实现类型变更，避免底层数组的复制。例如 TiDB 中就使用了这个技巧：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 // String converts slice to string without copy. // Use at your own risk. func String(b []byte) (s string) { if len(b) == 0 { return \u0026#34;\u0026#34; } pbytes := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;b)) pstring := (*reflect.StringHeader)(unsafe.Pointer(\u0026amp;s)) pstring.Data = pbytes.Data pstring.Len = pbytes.Len return } // Slice converts string to slice without copy. // Use at your own risk. func Slice(s string) (b []byte) { pbytes := (*reflect.SliceHeader)(unsafe.Pointer(\u0026amp;b)) pstring := (*reflect.StringHeader)(unsafe.Pointer(\u0026amp;s)) pbytes.Data = pstring.Data pbytes.Len = pstring.Len pbytes.Cap = pstring.Len return } 上面两个函数实现了[]byte和string的互相转换，不需要底层数组的copy。\n","date":"2018-03-05T14:20:18+08:00","permalink":"https://mazhen.tech/p/go%E8%AF%AD%E8%A8%80%E7%9A%84%E5%BC%95%E7%94%A8%E7%B1%BB%E5%9E%8B/","title":"Go语言的引用类型"},{"content":"TiDB提供了docker compose的部署方式，可以很方便的在单机上搭建一个TiDB集群作为开发测试环境。如果修改了TiDB源码，可以使用这样方式，先在本机部署集群做一些验证。\n首先本机要安装docker和docker compose，建议参考官方文档Install Docker 和 Install Docker Compose\n下载tidb-docker-compose项目\n1 git clone https://github.com/pingcap/tidb-docker-compose.git 使用docker compose启动TiDB集群 1 cd tidb-docker-compose \u0026amp;\u0026amp; sudo docker-compose up -d 就这么简单，集群启动成功了。使用docker ps查看：\n可以看到，已经启动了三个tikv实例，一个tidb实例，三个pd实例，还有监控和tidb-vision。\n监控的访问地址是 http://localhost:3000，用户名/密码：admin/admin。\ntidb-vision 的访问地址是 http://localhost:8010\n使用MySQL客户端访问TiDB 如果本机有MySQL客户端，可以直接连接：\n1 mysql -h 127.0.0.1 -P 4000 -u root 如果本机没有MySQL客户端，可以使用docker启动一个MySQL容器，然后登录到容器内，再使用MySQL客户端连接TiDB集群。这种方式比较环保，只要有docker环境就行。先查看TiDB集群的docker网络：\n然后启动MySQL容器，注意要加入TiDB集群的docker网络：\n1 sudo docker run --network=tidbdockercompose_default --rm -it mysql /bin/bash 因为和TiDB集群在同一个docker网络，在MySQL容器内，可以使用tidb名称访问到TiDB：\n1 mysql -h tidb -P 4000 -u root 停止集群 1 sudo docker-compose down 如果自己build了TiDB版本想在本机run集群，文档写的很清楚，告诉你镜像应该放在什么位置。\nHave fun!\n","date":"2018-02-09T14:32:20+08:00","permalink":"https://mazhen.tech/p/%E5%88%A9%E7%94%A8docker-compose%E5%9C%A8%E5%8D%95%E6%9C%BA%E4%B8%8A%E7%8E%A9%E8%BD%ACtidb/","title":"利用docker compose在单机上玩转TiDB"},{"content":"翻了一下TiDB的文档，对TiDB有了个大概的了解。简单说，TiDB的实现架构是：底层是分布式KV引擎TiKV，上层是SQL引擎TiDB Servers。一般传统数据库也是这么分层实现的，只不过TiKV实现了一个分布式、强一致、支持事务的K/V，不像数据库是单机版K/V。在TiKV之上实现SQL引擎就简化了很多，因此TiDB Servers是无状态的。\n简化的抽象架构分层：\nTiDB官方文档里的架构图：\n可以看出，TiDB的基础工作和最突出的创新在TiKV，理论上有了这个KV，可以把单机版的SQl引擎实现方式搬过来，就有了一个可扩展的分布式数据库。\n那就看看TiKV的架构：用RocksDB作为单机存储引擎，然后上层用Raft实现了一个分布式、强一致性的K/V。有了这个很强大的分布式K/V，在上面实现了MVCC层，就是对每个Key加了version，然后基于MVCC层最终实现了分布式事务。\nRocksDB内部用的是LSM-Tree，写入性能肯定比MySQL的B+ tree好。读取性能看实现的优化情况了，不过RocksDB是Facebook做的，应该没啥问题。\nRaft的实现和测试用例是从Etcd完全拷贝过来的，可以认为Raft的实现也是稳定的。 作者的原话：\n我们做了一件比较疯狂的事情，就是我们把 Etcd 的 Raft 状态机的每一行代码，line by line 的翻译成了 Rust。而我们第一个转的就是所有 Etcd 本身的测试用例。我们写一模一样的 test ，保证这个东西我们 port 的过程是没有问题的。\n分布式事务参照的是Percolator。Percolator和Spanner差不多，只不过Spanner引入了专有硬件原子钟，而Percolator依靠单点的授时服务器。两者都是对两阶段提交协议的改进。我们搞过J2EE，对两阶段提交协议应该比较熟悉，2PC的问题是：一旦事务参与者投票，它必须等待coordinator给出指示：提交或放弃。如果这时coordinator挂了，事务参与者除了等待什么也做不了。事务处于未决状态，事务的最终结果记录在coordinator的事务日志中，只能等它recovery（HeuristicCommitException、HeuristicMixedException、HeuristicRollbackException等异常就是遇到了这种情况，只好资源自己做了决定）。这么看在本质上，2PC为了达到一致性，实际上是退化到由coordinator单节点来实现atomic commit. Spanner引入了trueTime api，底下存储是MVCC，每行数据都带一个时间戳做version，TrueTime API就是打时间戳的，用时间戳标识事务顺序，解决2PC依赖单点coordinator的问题。而依赖单点的授时服务器的问题，他们是这样解释的：\n因为 TSO 的逻辑极其简单，只需要保证对于每一个请求返回单调递增的 id 即可，通过一些简单的优化手段（比如 pipeline）性能可以达到每秒生成百万 id 以上，同时 TSO 本身的高可用方案也非常好做，所以整个 Percolator 模型的分布式程度很高。\nTiDB的事务隔离级别实现了Read committed和Repeatable read，没有实现最严格的Serializable。不过串行化的隔离级别在现实中很少使用，性能会很差。oracle 11g也没有实现它。oracle实现的是snapshot isolation，实际上比串行化的保证要弱。TiDB和oracle都用是MVCC保证了Repeatable read，简单说就是每个事务都读取一个一致性的snapshot，这个snapshot肯定就是完整状态。所以叫做snapshot isolation。按照TiDB的文档，TiDB 实现的 snapshot 隔离级别，该隔离级别不会出现幻读，但是会出现写偏斜。\n写偏斜是什么，举个简单的例子：两个事务都先分别查询在线值班的医生总数，发现还有两个在线的医生，然后各自更新不同的记录，分别让不同的医生下线。事务提交后，两个医生都下线了，没有一个医生在线值班，出现错误的业务场景。这种异常情况是两个事务分别更新不同的记录。引起写倾斜的的模式：先查询很多列看是否满足某种条件，然后依赖查询结果写入数据并提交。解决的方法有：真正的串行化隔离级别，或者显示的锁定事务依赖的行。\n从文档看，TiDB利用了成熟的开源项目，自己实现了分布式事务、分布式存储和SQL引擎，整体方案诱人，至于软件成熟程度，还需要经过实际的使用测试。\n","date":"2018-02-09T14:28:29+08:00","permalink":"https://mazhen.tech/p/tidb%E5%88%9D%E6%8E%A2/","title":"TiDB初探"},{"content":"Zion项目我们采用Feature Branch Workflow，即每个特性在branch中开发，master始终保持稳定。特性开发完成，需提交pull request，接受其他成员的code review，同时可以在PR中围绕该特性进行讨论，PR记录了开发过程的细节。\n由于是内部项目，我们没有使用fork机制，代码都维护在Github上的一个仓库：apusic/zion。在看具体的流程前，先有一个全局视图：\n# 基本工作流程 从远程clone respository 1 git clone https://github.com/apusic/zion.git 创建特性分支 首先让本地的master处于最新状态：\n1 2 3 git fetch git checkout master git rebase origin/master 创建分支\n1 git checkout -b myfeature 在分支上进行开发 1 2 3 git status # View the state of the repo git add # Stage a file git commit # Commit a file 进行一个功能特性开发时，可以多次提交到本地仓库Repository，不必每次commit都push到远程仓库Remote。\n开发过程中，保持分支和最新代码同步 1 2 3 # While on your myfeature branch. git fetch git rebase origin/master 关于rebase的详细说明，请参考 The \u0026ldquo;Git Branching - Rebasing\u0026rdquo; chapter from the Pro Git book.\n后面会单独介绍rebase冲突的处理。\n将分支发布到中心仓库 所有改动都提交后，执行：\n1 git push -f origin myfeature 创建pull request 访问项目主页，点击Compare \u0026amp; pull request创建pull request\ncode review \u0026amp; discussions 可以要求一个或两个项目成员进行review，也可以围绕该特性进行讨论。\nMerge pull requests 根据项目的配置，pull requests在merge进master之前要满足一些条件，例如至少两个成员review，通过集成测试等。\n所有的检查都通过后，这个pull request就可以merge了。详细的操作参见 Merging a pull request\n这里有三个merge选型：Merge pull request，Squash and merge，Rebase and merge，关于它们的区别请参考GitHub的帮助。 一般建议选择Squash and merge。\n至此，一个特性就开发完成了。\n删除分支 已经完成merged的 pull requests 关联的分支可以在GitHub删除，详细的操作步骤参见GitHub文档 Deleting and restoring branches in a pull request\n# 冲突处理 上面的流程中，在保持分支和最新代码同步时，最有可能产生冲突。\nrebase提示冲突，会列出冲突文件，执行下列步骤：\n手工解决冲突 git add \u0026lt;some-file\u0026gt; 将发生冲突的文件放回index区 git rebase --continue 继续进行rebase 提示rebase成功 在Merge pull requests过程中也可能产生冲突，可以在GitHub的界面上解决冲突，详细的操作轻参考Addressing merge conflicts。\n如果冲突较多，建议先在客户端执行rebase，按照上面的步骤解决完冲突，再进行Merge pull requests。\n","date":"2018-01-15T14:38:05+08:00","permalink":"https://mazhen.tech/p/git-feature-branch-workflow/","title":"Git Feature Branch Workflow"},{"content":"Wireshark是排查网络问题最常用的工具，它已经内置支持了上百种通用协议，同时它的扩展性也很好，对于自定义的应用层网络协议，你可以使用c或者lua编写协议解析插件，这样你就可以在Wireshark中观察到协议的内容而不是二进制流，为排查问题带来一定的便利性。\n最近在排查一个HSF超时的问题，顺便花了些时间为Wireshark写了一个HSF2协议解析插件，目前支持HSF2的request、response和heart beat协议，支持将多个packet还原为上层PDU。暂不支持HSF原先的TB Remoting协议。先看效果。\n首先在Packet List区域已经能识别HSF2协议：\nHSF的请求和响应\nHSF的心跳协议\n点击某个数据包，可以在Packet details区域查看详细的协议内容： HSF请求\n可以看到很多协议的重要信息，包括序列化方式，超时时间，服务名称、方法及参数\nHSF响应\nHeartBeat请求\n心跳协议比较简单，响应就不看了。\n插件是使用lua开发的，安装比较简单，以OS X平台为例：\n将协议解析脚本copy到/Applications/Wireshark.app/Contents/Resources/share/wireshark/ 目录\n编辑init.lua文件，设置disable_lua = false，确保lua支持打开\n在init.lua文件末尾增加\n1 dofile(\u0026#34;hsf2.lua\u0026#34;) 再次启动Wireshark，会对12200端口的数据流使用脚本解析，已经可以识别HSF协议了。\n备注\n附上hsf2.lua，边翻HSF代码边写的，写完眼已经花了，错误难免，欢迎试用。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 -- declare the protocol hsf2_proto = Proto(\u0026#34;hsf2\u0026#34;, \u0026#34;Taobao HSF2 Protocol\u0026#34;) -- declare the value strings local vs_id = { [12] = \u0026#34;HSF2 Heart Beat\u0026#34;, [13] = \u0026#34;HSF2 TB Remoting\u0026#34;, [14] = \u0026#34;HSF2 HSF Remoting\u0026#34; } local vs_version = { [1] = \u0026#34;HSF2\u0026#34; } local vs_op = { [0] = \u0026#34;request\u0026#34;, [1] = \u0026#34;response\u0026#34; } local vs_codectype = { [1] = \u0026#34;HESSIAN_CODEC\u0026#34;, [2] = \u0026#34;JAVA_CODEC\u0026#34;, [3] = \u0026#34;TOP_CODEC\u0026#34;, [4] = \u0026#34;HESSIAN2_CODEC\u0026#34;, [5] = \u0026#34;KRYO_CODEC\u0026#34;, [6] = \u0026#34;JSON_CODEC\u0026#34;, [7] = \u0026#34;CUSTOMIZED_CODEC\u0026#34;, } local vs_responsestatus = { [20] = \u0026#34;OK\u0026#34;, [30] = \u0026#34;client timeout\u0026#34;, [31] = \u0026#34;server timeout\u0026#34;, [40] = \u0026#34;bad request\u0026#34;, [50] = \u0026#34;bad response\u0026#34;, [60] = \u0026#34;service not found\u0026#34;, [70] = \u0026#34;service error\u0026#34;, [80] = \u0026#34;server error\u0026#34;, [90] = \u0026#34;client error\u0026#34;, [91] = \u0026#34;Unknow error\u0026#34;, [81] = \u0026#34;Thread pool is busy\u0026#34;, [82] = \u0026#34;Communication error\u0026#34;, [88] = \u0026#34;server will close soon\u0026#34;, [10] = \u0026#34;server send coders\u0026#34;, [83] = \u0026#34;Unkown code\u0026#34; } -- declare the fields local f_id = ProtoField.uint8(\u0026#34;hsf2.id\u0026#34;, \u0026#34;Identification\u0026#34;, base.Dec, vs_id) local f_version = ProtoField.uint8(\u0026#34;hsf2.version\u0026#34;, \u0026#34;version\u0026#34;, base.Dec, vs_version) local f_op = ProtoField.uint8(\u0026#34;hsf2.op\u0026#34;, \u0026#34;operation\u0026#34;, base.DEC, vs_op) local f_codectype = ProtoField.uint8(\u0026#34;hsf2.codectype\u0026#34;, \u0026#34;codectype\u0026#34;, base.DEC, vs_codectype) local f_reserved = ProtoField.uint8(\u0026#34;hsf2.reserved\u0026#34;, \u0026#34;reserved\u0026#34;, base.DEC) local f_req_id = ProtoField.uint64(\u0026#34;hsf2.req_id\u0026#34;, \u0026#34;RequestID\u0026#34;, base.DEC) local f_timeout = ProtoField.uint32(\u0026#34;hsf2.timeout\u0026#34;, \u0026#34;timeout\u0026#34;, base.DEC) local f_service_name_len = ProtoField.uint32(\u0026#34;hsf2.service_name_len\u0026#34;, \u0026#34;Service Name length\u0026#34;, base.DEC) local f_method_name_len = ProtoField.uint32(\u0026#34;hsf2.method_name_len\u0026#34;, \u0026#34;Method Name length\u0026#34;, base.DEC) local f_arg_count = ProtoField.uint32(\u0026#34;hsf2.arg.count\u0026#34;, \u0026#34;Argument Count\u0026#34;, base.DEC) local f_arg_type_len = ProtoField.uint32(\u0026#34;hsf2.arg.type.len\u0026#34;, \u0026#34;Argument Type length\u0026#34;, base.DEC) local f_arg_obj_len = ProtoField.uint32(\u0026#34;hsf2.arg.obj.len\u0026#34;, \u0026#34;Argument Object length\u0026#34;, base.DEC) local f_req_prop_len = ProtoField.uint32(\u0026#34;hsf2.req.prop.len\u0026#34;, \u0026#34;Request Prop Length\u0026#34;, base.DEC) local f_service_name = ProtoField.string(\u0026#34;hsf2.service.name\u0026#34;, \u0026#34;Service Name\u0026#34;) local f_method_name = ProtoField.string(\u0026#34;hsf2.method.name\u0026#34;, \u0026#34;Method Name\u0026#34;) local f_arg_type = ProtoField.string(\u0026#34;hsf2.arg.type\u0026#34;, \u0026#34;Argument Type\u0026#34;) local f_arg_obj = ProtoField.bytes(\u0026#34;hsf2.arg.obj\u0026#34;, \u0026#34;Argument Object\u0026#34;) local f_req_prop = ProtoField.bytes(\u0026#34;hsf2.req.prop\u0026#34;, \u0026#34;Request Prop\u0026#34;) local f_response_status = ProtoField.uint32(\u0026#34;hsf2.response.status\u0026#34;, \u0026#34;Response Status\u0026#34;, base.DEC, vs_responsestatus) local f_response_body_len = ProtoField.uint32(\u0026#34;hsf2.response.body.len\u0026#34;, \u0026#34;Response Body Length\u0026#34;, base.DEC) local f_response_body = ProtoField.bytes(\u0026#34;hsf2.response.body\u0026#34;, \u0026#34;Response Body\u0026#34;, base.DEC) hsf2_proto.fields = { f_id, f_version, f_op, f_codectype, f_reserved, f_req_id, f_timeout, f_service_name_len, f_method_name_len, f_arg_count, f_arg_type_len, f_arg_obj_len, f_req_prop_len, f_service_name, f_method_name, f_arg_type, f_arg_obj, f_req_prop, f_response_status, f_response_body_len, f_response_body } function get_pdu_length(buffer) local offset = 0 local id = buffer(offset, 1):uint() offset = offset + 1 -- heart beat if id == 12 then return 18 end -- TB REMOTING if id == 13 then -- TODO return 18 end -- HSF REMOTING if id == 14 then local version = buffer(offset, 1):uint() offset = offset + 1 local op = buffer(offset, 1):uint() offset = offset + 1 -- request if op == 0 then local service_name_len = buffer(19, 4):uint() local method_name_len = buffer(23,4):uint() local arg_count = buffer(27,4):uint() offset = 27 + 4 local arg_content_len = 0 for i = 1, arg_count do arg_content_len = arg_content_len + buffer(offset,4):uint() offset = offset + 4 end for i = 1, arg_count do arg_content_len = arg_content_len + buffer(offset,4):uint() offset = offset + 4 end local req_prop_len = buffer(offset,4):uint() local len = 30 + arg_count*4*2 + 5 + service_name_len + method_name_len + arg_content_len + req_prop_len return len end -- response if op == 1 then local body_len = buffer(16, 4):uint() return 20 + body_len end end end -- create the dissection function function hsf2_proto.dissector(buffer, pinfo, tree) -- check the protocol -- TODO support TB Remoting local check_proto = buffer(0, 1):uint() if check_proto \u0026lt; 12 or check_proto \u0026gt; 14 or check_proto == 13 then return end -- Set the protocol column pinfo.cols[\u0026#39;protocol\u0026#39;] = \u0026#34;HSF2\u0026#34; -- Reassembling packets into one PDU local pdu_len = get_pdu_length(buffer) if pdu_len \u0026gt; buffer:len() then pinfo.desegment_len = pdu_len - buffer:len() pinfo.desegment_offset = 0 return end -- create the HSF2 protocol tree item local t_hsf2 = tree:add(hsf2_proto, buffer()) local offset = 0 local id = buffer(offset, 1):uint() offset = offset + 1 t_hsf2:add(f_id, id) -- heart beat if id == 12 then local op = buffer(offset, 1):uint() offset = offset + 1 t_hsf2:add(f_op, op) -- Set the info column to the name of the function local info = vs_id[id]..\u0026#34;:\u0026#34;..vs_op[op] pinfo.cols[\u0026#39;info\u0026#39;] = info t_hsf2:add(f_version, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_req_id, buffer(offset, 8)) offset = offset + 8 t_hsf2:add(f_timeout, buffer(offset, 4)) end -- TB REMOTING if id == 13 then -- TODO end -- HSF REMOTING if id == 14 then t_hsf2:add(f_version, buffer(offset, 1)) offset = offset + 1 local op = buffer(offset, 1):uint() offset = offset + 1 t_hsf2:add(f_op, op) -- Set the info column to the name of the function local info = vs_id[id]..\u0026#34;:\u0026#34;..vs_op[op] pinfo.cols[\u0026#39;info\u0026#39;] = info -- request if op == 0 then t_hsf2:add(f_codectype, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_req_id, buffer(offset, 8)) offset = offset + 8 t_hsf2:add(f_timeout, buffer(offset, 4)) offset = offset + 4 local service_name_len = buffer(offset, 4):uint() t_hsf2:add(f_service_name_len, service_name_len) offset = offset + 4 local method_name_len = buffer(offset,4):uint() t_hsf2:add(f_method_name_len, method_name_len) offset = offset + 4 local arg_count = buffer(offset,4):uint() t_hsf2:add(f_arg_count, arg_count) offset = offset + 4 local arg_type_len_array = {} for i = 1, arg_count do arg_type_len_array[i] = buffer(offset, 4):uint(); offset = offset + 4 t_hsf2:add(f_arg_type_len, arg_type_len_array[i]) end local arg_obj_len_array = {} for i = 1, arg_count do arg_obj_len_array[i] = buffer(offset, 4):uint(); offset = offset + 4 t_hsf2:add(f_arg_obj_len, arg_obj_len_array[i]) end local prop_len = buffer(offset, 4):uint(); offset = offset + 4 t_hsf2:add(f_req_prop_len, prop_len) t_hsf2:add(f_service_name, buffer(offset, service_name_len)) offset = offset + service_name_len t_hsf2:add(f_method_name, buffer(offset, method_name_len)) offset = offset + method_name_len for i = 1, #arg_type_len_array do t_hsf2:add(f_arg_type, buffer(offset, arg_type_len_array[i])) offset = offset + arg_type_len_array[i] end for i = 1, #arg_obj_len_array do t_hsf2:add(f_arg_obj, buffer(offset, arg_obj_len_array[i])) offset = offset + arg_obj_len_array[i] end if prop_len \u0026gt; 0 then t_hsf2:add(f_req_prop, buffer(offset, prop_len)) end end -- response if op == 1 then t_hsf2:add(f_response_status, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_codectype, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_reserved, buffer(offset, 1)) offset = offset + 1 t_hsf2:add(f_req_id, buffer(offset, 8)) offset = offset + 8 local body_len = buffer(offset, 4):uint() t_hsf2:add(f_response_body_len, body_len) offset = offset + 4 t_hsf2:add(f_response_body, buffer(offset, body_len)) end end end -- load the tcp port table tcp_table = DissectorTable.get(\u0026#34;tcp.port\u0026#34;) -- register the protocol to port 12200 tcp_table:add(12200, hsf2_proto) ","date":"2014-12-21T14:15:17+08:00","permalink":"https://mazhen.tech/p/%E4%B8%BAwireshark%E7%BC%96%E5%86%99hsf2%E5%8D%8F%E8%AE%AE%E8%A7%A3%E6%9E%90%E6%8F%92%E4%BB%B6/","title":"为Wireshark编写HSF2协议解析插件"},{"content":"DNS（Domain Name System）简单说就是一个名称到IP地址的映射，使用容易记住的域名代替IP地址。基本原理就不讲了，网上的文章很多。\n了解了基本原理，你就可以使用dig(Domain Information Groper）命令进行探索。当对淘宝的几个域名进行dig时，你发现事情并不像想像的那么简单。\n为了使大家的输出一致，我在dig命令中显示的指定了DNS服务器（@222.172.200.68 云南电信DNS）。\ndig @222.172.200.68 login.alibaba-inc.com dig @222.172.200.68 guang.taobao.com dig @222.172.200.68 item.taobao.com 你会发现对于域名的查询，都不是直接返回IP地址这么简单，而是经过了神奇的CNAME。一般文档在介绍CNAME时只是说可以给一个域名指定别名（alias），其实这是DNS运维非常重要的手段，使得DNS配置具有一定的灵活性和可扩展性。结合上面三个域名的解析说一下。先给一张高大上的图，是按照我自己的理解画的，不一定完全正确:)\n图上分了三层，最上层是常规的DNS解析过程，用户通过local DNS做递归查询，最终定位到taobao.com权威DNS服务器。\n中间层可以称为GSLB（Global Server Load Balancing），作用是提供域名的智能解析，根据一定的策略返回结果。淘系目前有三套GSLB：\nF5 GTM：F5的硬件设备，基本已经被淘汰，全部替换为自研软件。GTM功能强大，但对用户而言是黑盒，性能一般价格昂贵。早期淘宝CDN智能调度就是基于F5 GTM做的。 ADNS：阿里自研权威DNS，替换GTM。ADNS很牛逼，可惜资料太少。 Pharos：阿里CDN的大脑，实现CDN流量精确，稳定，安全的调度。 taobao.com权威DNS服务器会根据不用的域名，CNAME到不同的GSLB做智能调度。CNAME的作用有点类似请求分发，taobao.com权威DNS服务器将域名解析请求转交给下一层域名服务器处理。\n最下层是应用层，提供真正的服务。\n现在再看看这三个域名的解析。\nlogin.alibaba-inc.com 被转交给了GTM做智能解析，GTM通过返回不同机房的VIP做流量调度，用户的请求最终经过LVS到达我们的应用。\nguang.taobao.com的解析过程和login.alibaba-inc.com类似，只不过智能调度换成了ADNS。\nitem.taobao.com有点小复杂。我们都知道CDN是做静态资源加速的，像这样的静态资源域名img04.taobaocdn.com会由Pharos解析调度，为用户返回就近的CDN节点。但什么时候动态内容也经过CDN代理了？这就是高大上的统一接入层。简单说下过程：item.taobao.com通过Pharos的智能调度，返回给用户就近的CDN节点。当用户的请求到达CDN节点时，这个节点会为动态内容的域名选择合适的后端服务，相当于每次都做回源处理。这个CDN节点可以理解为用户请求的代理。CDN在选择后端服务时，会执行单元化、小淘宝等逻辑，将请求发送到正确的机房。请求到达机房后，先进入统一接入层，注意这里的后端应用不需要申请VIP，IP地址列表保存在VIPServer中。统一接入层从VIPServer中拿到后端应用的IP地址列表，进行请求分发。VIPServer的作用类似HSF的ConfigServer，可以大大减少应用VIP的数量。以后做单元化部署的域名都会接入统一接入层，将单元化的逻辑上推到了CDN节点。\n貌似是讲清楚了，不过这个过程已经做了很大的简化，因为我也仅仅是了解个大概。作为业务开发重点关注的是最下面的Java应用，转岗到技术保障后，才发现有机会可以从全局了解网站架构，接触到网络、DNS、CDN、LVS\u0026amp;VIP等等基础设施。\n","date":"2014-09-10T11:04:27+08:00","permalink":"https://mazhen.tech/p/%E4%BB%8E%E5%BC%80%E5%8F%91%E8%A7%92%E5%BA%A6%E7%9C%8Bdns/","title":"从开发角度看DNS"},{"content":"在Linux上做网络应用的性能优化时，一般都会对TCP相关的内核参数进行调节，特别是和缓冲、队列有关的参数。网上搜到的文章会告诉你需要修改哪些参数，但我们经常是知其然而不知其所以然，每次照抄过来后，可能很快就忘记或混淆了它们的含义。本文尝试总结TCP队列缓冲相关的内核参数，从协议栈的角度梳理它们，希望可以更容易的理解和记忆。注意，本文内容均来源于参考文档，没有去读相关的内核源码做验证，不能保证内容严谨正确。作为Java程序员没读过内核源码是硬伤。\n下面我以server端为视角，从连接建立、数据包接收和数据包发送这3条路径对参数进行归类梳理。\n# 一、连接建立 简单看下连接的建立过程，客户端向server发送SYN包，server回复SYN＋ACK，同时将这个处于SYN_RECV状态的连接保存到半连接队列。客户端返回ACK包完成三次握手，server将ESTABLISHED状态的连接移入accept队列，等待应用调用accept()。\n可以看到建立连接涉及两个队列：\n半连接队列，保存SYN_RECV状态的连接。队列长度由net.ipv4.tcp_max_syn_backlog设置\naccept队列，保存ESTABLISHED状态的连接。队列长度为min(net.core.somaxconn, backlog)。其中backlog是我们创建ServerSocket(int port,int backlog)时指定的参数，最终会传递给listen方法：\n1 2 #include \u0026lt;sys/socket.h\u0026gt; int listen(int sockfd, int backlog); 如果我们设置的backlog大于net.core.somaxconn，accept队列的长度将被设置为net.core.somaxconn\n另外，为了应对SYN flooding（即客户端只发送SYN包发起握手而不回应ACK完成连接建立，填满server端的半连接队列，让它无法处理正常的握手请求），Linux实现了一种称为SYN cookie的机制，通过net.ipv4.tcp_syncookies控制，设置为1表示开启。简单说SYN cookie就是将连接信息编码在ISN(initial sequence number)中返回给客户端，这时server不需要将半连接保存在队列中，而是利用客户端随后发来的ACK带回的ISN还原连接信息，以完成连接的建立，避免了半连接队列被攻击SYN包填满。对于一去不复返的客户端握手，不理它就是了。\n# 二、数据包的接收 先看看接收数据包经过的路径：\n数据包的接收，从下往上经过了三层：网卡驱动、系统内核空间，最后到用户态空间的应用。Linux内核使用sk_buff(socket kernel buffers)数据结构描述一个数据包。当一个新的数据包到达，NIC（network interface controller）调用DMA engine，通过Ring Buffer将数据包放置到内核内存区。Ring Buffer的大小固定，它不包含实际的数据包，而是包含了指向sk_buff的描述符。当Ring Buffer满的时候，新来的数据包将给丢弃。一旦数据包被成功接收，NIC发起中断，由内核的中断处理程序将数据包传递给IP层。经过IP层的处理，数据包被放入队列等待TCP层处理。每个数据包经过TCP层一系列复杂的步骤，更新TCP状态机，最终到达recv Buffer，等待被应用接收处理。有一点需要注意，数据包到达recv Buffer，TCP就会回ACK确认，既TCP的ACK表示数据包已经被操作系统内核收到，但并不确保应用层一定收到数据（例如这个时候系统crash），因此一般建议应用协议层也要设计自己的ACK确认机制。\n上面就是一个相当简化的数据包接收流程，让我们逐层看看队列缓冲有关的参数。\n网卡Bonding模式 当主机有1个以上的网卡时，Linux会将多个网卡绑定为一个虚拟的bonded网络接口，对TCP/IP而言只存在一个bonded网卡。多网卡绑定一方面能够提高网络吞吐量，另一方面也可以增强网络高可用。Linux支持7种Bonding模式：\n- `Mode 0 (balance-rr)` Round-robin策略，这个模式具备负载均衡和容错能力 - `Mode 1 (active-backup)` 主备策略，在绑定中只有一个网卡被激活，其他处于备份状态 - `Mode 2 (balance-xor)` XOR策略，通过源MAC地址与目的MAC地址做异或操作选择slave网卡 - `Mode 3 (broadcast)` 广播，在所有的网卡上传送所有的报文 - `Mode 4 (802.3ad)` IEEE 802.3ad 动态链路聚合。创建共享相同的速率和双工模式的聚合组 - `Mode 5 (balance-tlb)` Adaptive transmit load balancing - `Mode 6 (balance-alb)` Adaptive load balancing 详细的说明参考内核文档Linux Ethernet Bonding Driver HOWTO。我们可以通过cat /proc/net/bonding/bond0查看本机的Bonding模式：\n一般很少需要开发去设置网卡Bonding模式，自己实验的话可以参考这篇文档\n网卡多队列及中断绑定 随着网络的带宽的不断提升，单核CPU已经不能满足网卡的需求，这时通过多队列网卡驱动的支持，可以将每个队列通过中断绑定到不同的CPU核上，充分利用多核提升数据包的处理能力。\n首先查看网卡是否支持多队列，使用lspci -vvv命令，找到Ethernet controller项：\n如果有MSI-X， Enable+ 并且Count \u0026gt; 1，则该网卡是多队列网卡。\n然后查看是否打开了网卡多队列。使用命令cat /proc/interrupts，如果看到eth0-TxRx-0表明多队列支持已经打开：\n最后确认每个队列是否绑定到不同的CPU。cat /proc/interrupts查询到每个队列的中断号，对应的文件/proc/irq/${IRQ_NUM}/smp_affinity为中断号IRQ_NUM绑定的CPU核的情况。以十六进制表示，每一位代表一个CPU核：\n``` （00000001）代表CPU0 （00000010）代表CPU1 （00000011）代表CPU0和CPU1 ``` 如果绑定的不均衡，可以手工设置，例如：\n``` echo \u0026quot;1\u0026quot; \u0026gt; /proc/irq/99/smp_affinity echo \u0026quot;2\u0026quot; \u0026gt; /proc/irq/100/smp_affinity echo \u0026quot;4\u0026quot; \u0026gt; /proc/irq/101/smp_affinity echo \u0026quot;8\u0026quot; \u0026gt; /proc/irq/102/smp_affinity echo \u0026quot;10\u0026quot; \u0026gt; /proc/irq/103/smp_affinity echo \u0026quot;20\u0026quot; \u0026gt; /proc/irq/104/smp_affinity echo \u0026quot;40\u0026quot; \u0026gt; /proc/irq/105/smp_affinity echo \u0026quot;80\u0026quot; \u0026gt; /proc/irq/106/smp_affinity ``` Ring Buffer\nRing Buffer位于NIC和IP层之间，是一个典型的FIFO（先进先出）环形队列。Ring Buffer没有包含数据本身，而是包含了指向sk_buff（socket kernel buffers）的描述符。\n可以使用ethtool -g eth0查看当前Ring Buffer的设置：\n上面的例子接收队列为4096，传输队列为256。可以通过ifconfig观察接收和传输队列的运行状况：\nRX errors：收包总的错误数\nRX dropped: 表示数据包已经进入了Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃。\nRX overruns: overruns意味着数据包没到Ring Buffer就被网卡物理层给丢弃了，而CPU无法及时的处理中断是造成Ring Buffer满的原因之一，例如中断分配的不均匀。\n当dropped数量持续增加，建议增大Ring Buffer，使用ethtool -G进行设置。\nInput Packet Queue(数据包接收队列) 当接收数据包的速率大于内核TCP处理包的速率，数据包将会缓冲在TCP层之前的队列中。接收队列的长度由参数net.core.netdev_max_backlog设置。\nrecv Buffer recv buffer是调节TCP性能的关键参数。BDP(Bandwidth-delay product，带宽延迟积) 是网络的带宽和与RTT(round trip time)的乘积，BDP的含义是任意时刻处于在途未确认的最大数据量。RTT使用ping命令可以很容易的得到。为了达到最大的吞吐量，recv Buffer的设置应该大于BDP，即recv Buffer \u0026gt;= bandwidth * RTT。假设带宽是100Mbps，RTT是100ms，那么BDP的计算如下：\n1 BDP = 100Mbps * 100ms = (100 / 8) * (100 / 1000) = 1.25MB Linux在2.6.17以后增加了recv Buffer自动调节机制，recv buffer的实际大小会自动在最小值和最大值之间浮动，以期找到性能和资源的平衡点，因此大多数情况下不建议将recv buffer手工设置成固定值。\n当net.ipv4.tcp_moderate_rcvbuf设置为1时，自动调节机制生效，每个TCP连接的recv Buffer由下面的3元数组指定：\n1 net.ipv4.tcp_rmem = \u0026lt;MIN\u0026gt; \u0026lt;DEFAULT\u0026gt; \u0026lt;MAX\u0026gt; 最初recv buffer被设置为，同时这个缺省值会覆盖net.core.rmem_default的设置。随后recv buffer根据实际情况在最大值和最小值之间动态调节。在缓冲的动态调优机制开启的情况下，我们将net.ipv4.tcp_rmem的最大值设置为BDP。\n当net.ipv4.tcp_moderate_rcvbuf被设置为0，或者设置了socket选项SO_RCVBUF，缓冲的动态调节机制被关闭。recv buffer的缺省值由net.core.rmem_default设置，但如果设置了net.ipv4.tcp_rmem，缺省值则被\u0026lt;DEFAULT\u0026gt;覆盖。可以通过系统调用setsockopt()设置recv buffer的最大值为net.core.rmem_max。在缓冲动态调节机制关闭的情况下，建议把缓冲的缺省值设置为BDP。\n注意这里还有一个细节，缓冲除了保存接收的数据本身，还需要一部分空间保存socket数据结构等额外信息。因此上面讨论的recv buffer最佳值仅仅等于BDP是不够的，还需要考虑保存socket等额外信息的开销。Linux根据参数net.ipv4.tcp_adv_win_scale计算额外开销的大小：\nBuffer / 2tcp_adv_win_scale\n如果net.ipv4.tcp_adv_win_scale的值为1，则二分之一的缓冲空间用来做额外开销，如果为2的话，则四分之一缓冲空间用来做额外开销。因此recv buffer的最佳值应该设置为：\nBDP / (1 – 1 / 2tcp_adv_win_scale)\n# 三、数据包的发送 发送数据包经过的路径：\n和接收数据的路径相反，数据包的发送从上往下也经过了三层：用户态空间的应用、系统内核空间、最后到网卡驱动。应用先将数据写入TCP send buffer，TCP层将send buffer中的数据构建成数据包转交给IP层。IP层会将待发送的数据包放入队列QDisc(queueing discipline)。数据包成功放入QDisc后，指向数据包的描述符sk_buff被放入Ring Buffer输出队列，随后网卡驱动调用DMA engine将数据发送到网络链路上。\n同样我们逐层来梳理队列缓冲有关的参数。\nsend Buffer 同recv Buffer类似，和send Buffer有关的参数如下：\n1 2 3 net.ipv4.tcp_wmem = \u0026lt;MIN\u0026gt; \u0026lt;DEFAULT\u0026gt; \u0026lt;MAX\u0026gt; net.core.wmem_default net.core.wmem_max 发送端缓冲的自动调节机制很早就已经实现，并且是无条件开启，没有参数去设置。如果指定了tcp_wmem，则net.core.wmem_default被tcp_wmem的覆盖。send Buffer在tcp_wmem的最小值和最大值之间自动调节。如果调用setsockopt()设置了socket选项SO_SNDBUF，将关闭发送端缓冲的自动调节机制，tcp_wmem将被忽略，SO_SNDBUF的最大值由net.core.wmem_max限制。\nQDisc QDisc（queueing discipline ）位于IP层和网卡的ring buffer之间。我们已经知道，ring buffer是一个简单的FIFO队列，这种设计使网卡的驱动层保持简单和快速。而QDisc实现了流量管理的高级功能，包括流量分类，优先级和流量整形（rate-shaping）。可以使用tc命令配置QDisc。\nQDisc的队列长度由txqueuelen设置，和接收数据包的队列长度由内核参数net.core.netdev_max_backlog控制所不同，txqueuelen是和网卡关联，可以用ifconfig命令查看当前的大小：\n使用ifconfig调整txqueuelen的大小：\n1 ifconfig eth0 txqueuelen 2000 Ring Buffer 和数据包的接收一样，发送数据包也要经过Ring Buffer，使用ethtool -g eth0查看：\n其中TX项是Ring Buffer的传输队列，也就是发送队列的长度。设置也是使用命令ethtool -G。\nTCP Segmentation和Checksum Offloading 操作系统可以把一些TCP/IP的功能转交给网卡去完成，特别是Segmentation(分片)和checksum的计算，这样可以节省CPU资源，并且由硬件代替OS执行这些操作会带来性能的提升。\n一般以太网的MTU（Maximum Transmission Unit）为1500 bytes，假设应用要发送数据包的大小为7300bytes，MTU1500字节 － IP头部20字节 － TCP头部20字节＝有效负载为1460字节，因此7300字节需要拆分成5个segment：\nSegmentation(分片)操作可以由操作系统移交给网卡完成，虽然最终线路上仍然是传输5个包，但这样节省了CPU资源并带来性能的提升：\n可以使用ethtool -k eth0查看网卡当前的offloading情况：\n上面这个例子checksum和tcp segmentation的offloading都是打开的。如果想设置网卡的offloading开关，可以使用ethtool -K(注意K是大写)命令，例如下面的命令关闭了tcp segmentation offload：\n1 sudo ethtool -K eth0 tso off 网卡多队列和网卡Bonding模式 在数据包的接收过程中已经介绍过了。\n至此，终于梳理完毕。整理TCP队列相关参数的起因是最近在排查一个网络超时问题，原因还没有找到，产生的“副作用”就是这篇文档。再想深入解决这个问题可能需要做TCP协议代码的profile，需要继续学习，希望不久的将来就可以再写文档和大家分享了。\n参考文档\nQueueing in the Linux Network Stack TCP Implementation in Linux: A Brief Tutorial Impact of Bandwidth Delay Product on TCP Throughput Java程序员也应该知道的系统知识系列之网卡 ","date":"2014-08-16T11:11:24+08:00","permalink":"https://mazhen.tech/p/linux-tcp%E9%98%9F%E5%88%97%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E7%9A%84%E6%80%BB%E7%BB%93/","title":"Linux TCP队列相关参数的总结"},{"content":"Java中通过Socket.setSoLinger设置SO_LINGER选项，有三种组合形式：\nSocket.setSoLinger(false, linger) 设置为false，这时linger值被忽略。摘自unix network programming：\nThe default action of close with a TCP socket is to mark the socket as closed and return to the process immediately. The socket descriptor is on longer usable by the process: it can\u0026rsquo;t be used as an argument to read or write. TCP will try to send any data that is already queued to be sent to the other end, and after this occurs, the normal TCP connection termination sequence takes place.\n如果设置为false，socket主动调用close时会立即返回，操作系统会将残留在缓冲区中的数据发送到对端，并按照正常流程关闭(交换FIN-ACK），最后连接进入TIME_WAIT状态。\n我们可以写个演示程序，客户端发送较大的数据包后，立刻调用close，而server端将Receive Buffer设置的很小。close会立即返回，客户端的Java进程结束，但是当我们用tcpdump/Wireshark抓包会发现，操作系统正在帮你发送数据，内核缓冲区中的数据发送完毕后，发送FIN包关闭连接。\nSocket.setSoLinger(true, 0) TCP discards any data still remaining in the socket send buffer and sends an RST to the peer, not the normal four-packet connection termination sequence.\n主动调用close的一方也是立刻返回，但是这时TCP会丢弃发送缓冲中的数据，而且不是按照正常流程关闭连接（不发送FIN包），直接发送RST，对端会收到java.net.SocketException: Connection reset异常。同样使用tcpdump抓包可以很容易观察到。\n另外有些人会用这种方式解决主动关闭放方有大量TIME_WAIT状态连接的问题，因为发送完RST后，连接立即销毁，不会停留在TIME_WAIT状态。一般不建议这么做，除非你有合适的理由：\nIf the a client of your server application misbehaves (times out, returns invalid data, etc.) an abortive close makes sense to avoid being stuck in CLOSE_WAIT or ending up in the TIME_WAIT state. If you must restart your server application which currently has thousands of client connections you might consider setting this socket option to avoid thousands of server sockets in TIME_WAIT (when calling close() from the server end) as this might prevent the server from getting available ports for new client connections after being restarted.\nOn page 202 in the aforementioned book it specifically says: \u0026ldquo;There are certain circumstances which warrant using this feature to send an abortive close. One example is an RS-232 terminal server, which might hang forever in CLOSE_WAIT trying to deliver data to a stuck terminal port, but would properly reset the stuck port if it got an RST to discard the pending data.\u0026rdquo;\nSocket.setSoLinger(true, linger \u0026gt; 0)\nif there is any data still remaining in the socket send buffer, the process will sleep when calling close() until either all the data is sent and acknowledged by the peer or the configured linger timer expires. if the linger time expires before the remaining data is sent and acknowledged, close returns EWOULDBLOCK and any remaining data in the send buffer is discarded.\n如果SO_LINGER选项生效，并且超时设置大于零，调用close的线程被阻塞，TCP会发送缓冲区中的残留数据，这时有两种可能的情况：\n数据发送完毕，收到对方的ACK，然后进行连接的正常关闭（交换FIN-ACK） 超时，未发送完成的数据被丢弃，连接发送RST进行非正常关闭 类似的我们也可以构造demo观察这种场景。客户端发送较大的数据包，server端将Receive Buffer设置的很小。设置linger为1，调用close时等待1秒。注意SO_LINGER的单位为秒，好多人被坑过。假设close后1秒内缓冲区中的数据发送不完，使用tcpdump/Wireshark可以观察到客户端发送RST包，服务端收到java.net.SocketException: Connection reset异常。\n最后，在使用NIO时，最好不设置SO_LINGER，以后会再写一篇文章分析。\n","date":"2014-08-10T11:08:57+08:00","permalink":"https://mazhen.tech/p/tcp-so_linger-%E9%80%89%E9%A1%B9%E5%AF%B9socket.close%E7%9A%84%E5%BD%B1%E5%93%8D/","title":"TCP `SO_LINGER` 选项对Socket.close的影响"},{"content":"早上毕玄转给我一个问题，vsearch在上海机房部署的应用，在应用关闭后，端口释放的时间要比杭州机房的时间长。\nTCP的基本知识，主动关闭连接的一方会处于TIME_WAIT状态，并停留两倍的MSL（Maximum segment lifetime）时长。\n那就检查一下MSL的设置。网上有很多文章说，可以通过设置net.ipv4.tcp_fin_timeout来控制MSL。其实这有点误导人。查看Linux kernel的文档 ，发现tcp_fin_timeout是指停留在FIN_WAIT_2状态的时间：\ntcp_fin_timeout - INTEGER The length of time an orphaned (no longer referenced by any application) connection will remain in the FIN_WAIT_2 state before it is aborted at the local end. While a perfectly valid \u0026ldquo;receive only\u0026rdquo; state for an un-orphaned connection, an orphaned connection in FIN_WAIT_2 state could otherwise wait forever for the remote to close its end of the connection. Default: 60 seconds\n幸好这个问题原先在内部请教过：\nsysctl调节不了，只能调节复用和回收。 以前改小是改下面文件，重新编译内核的。 grep -i timewait_len /usr/src/kernels/2.6.32-220.el6.x86_64/include/net/tcp.h define TCP_TIMEWAIT_LEN (60HZ) / how long to wait to destroy TIME-WAIT define TCP_FIN_TIMEOUT TCP_TIMEWAIT_LEN\n而阿里内核支持修改TIME_WAIT时间：\nnet.ipv4.tcp_tw_timeout 然后找了两台机器做对比，用sysctl命令查看。杭州机房的机器：\nsudo sysctl -a | grep net.ipv4.tcp_tw_timeout net.ipv4.tcp_tw_timeout = 3 上海机房的机器：\n$sudo sysctl -a | grep net.ipv4.tcp_tw_timeout net.ipv4.tcp_tw_timeout = 60 原因很明显，上海机器的设置为60S。\n","date":"2014-07-01T11:00:10+08:00","permalink":"https://mazhen.tech/p/%E5%BA%94%E7%94%A8%E5%85%B3%E9%97%AD%E5%90%8E%E5%8D%A0%E7%94%A8%E7%AB%AF%E5%8F%A3%E6%97%B6%E9%97%B4%E8%BF%87%E9%95%BF%E7%9A%84%E9%97%AE%E9%A2%98/","title":"应用关闭后占用端口时间过长的问题"},{"content":"遇到性能问题怎么分析定位？这个问题太难回答了，各种底层环境、依赖系统、业务场景，怎么可能有统一的答案。于是产生了各种分析性能问题的“流派”。两个典型的 ANTI-METHODOLOGIES：\nblame-someone-else 使用此方法的人遵循下列步骤：\n找到一个不是他负责的系统或环境 假定问题和这个组件有关 将问题转交个负责这个组件的团队 如果证明是错误的，重复步骤1 路灯法 没有系统的方法论，只是使用自己擅长的工具去观察，而不管问题到底出现在哪儿。就像丢了钥匙的人去路灯下寻找，仅仅是因为路灯下比较亮。这种行为被称为路灯效应。\n相信很多同学已经脑补出上述的两个场景，他们的行为模式让人抓狂。于是有聪明人总结出了《The USE Method》。USE是Utilization，Saturation 和 Errors的缩写，简单说USE是一套分析系统性能问题的方法论，具体表现为一个checklist，分析过程就是对照checklist一项项检查，希望能快速定位瓶颈资源或错误。\n初看这个方法感觉有点太简单了吧，这也能称为方法论？不过这确实体现出了老外的做事风格，任何事情都会去做定量分析，力求逻辑完整。而我们往往讳莫高深的一笑，只可意会不可言传。\n简单介绍下USE，详细内容推荐看这篇《The USE Method》。USE的一句话总结：\nFor every resource, check utilization, saturation, and errors.\n术语解释\nresource：CPU，内存，磁盘，网络等一切物理设备资源 utilization：资源利用率。例如CPU的资源利用率90% saturation：当资源繁忙时仍能接收新的任务，这些额外的任务一般都放入了等待队列。saturation就表现为队列的长度，例如CPU的平均运行队列为4（Linux上使用vmstat命令获得）。 errors：系统的错误报告数，例如TCP监听队列overflowed次数。 列出系统中的所有资源，然后逐项检查利用率、等待队列和错误数，就这么简单！下表是一个范例：\nresource type metric CPU utilization CPU utilization (either per-CPU or a system-wide average) CPU saturation run-queue length Memory capacity utilization available free memory (system-wide) Memory capacity saturation anonymous paging or thread swapping Network interface utilization RX/TX throughput / max bandwidth Storage Storage device I/O utilization device busy percent Storage device I/O saturation wait queue length Storage device I/O errors device errors (\u0026ldquo;soft\u0026rdquo;, \u0026ldquo;hard\u0026rdquo;, \u0026hellip;) 对于资源测量数据的解读，作者给了一些建议，例如：资源利用率100%肯定表示该资源是系统瓶颈，70%以上的利用率就要引起足够的重视，一般IO设备利用率高于70%，响应时间将大幅上升。资源等待队列大于0意味着可能存在问题。资源的任何错误计数，都值得仔细调查，特别是当性能变差时，错误计数在上升。\n要使用这个方法，你还需要一份完整的资源列表，一般的系统资源包括：\nCPUs: sockets, cores, hardware threads (virtual CPUs) Memory: capacity Network interfaces Storage devices: I/O, capacity Controllers: storage, network cards Interconnects: CPUs, memory, I/O 作者很厚道的按照每种操作系统给出了checklist，重点关注《USE Method: Linux Performance Checklist》，不仅列出了资源，而且告诉你如何进行测量。例如CPU运行队列的测量：\nsystem-wide: vmstat 1, \u0026ldquo;r\u0026rdquo; \u0026gt; CPU count [2]; sar -q, \u0026ldquo;runq-sz\u0026rdquo; \u0026gt; CPU count; dstat -p, \u0026ldquo;run\u0026rdquo; \u0026gt; CPU count; per-process: /proc/PID/schedstat 2nd field (sched_info.run_delay); perf sched latency (shows \u0026ldquo;Average\u0026rdquo; and \u0026ldquo;Maximum\u0026rdquo; delay per-schedule); dynamic tracing, eg, SystemTap schedtimes.stp \u0026ldquo;queued(us)\u0026rdquo;\n根据作者的实践经验，使用USE方法解决了80%的性能问题，只付出了5%的努力，当考虑了所有的资源，你不太可能忽视任何问题。简单有效！\n","date":"2014-06-07T10:56:00+08:00","permalink":"https://mazhen.tech/p/%E4%BD%BF%E7%94%A8use-method%E5%88%86%E6%9E%90%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98/","title":"使用USE Method分析系统性能问题"},{"content":"/proc是一个伪文件系统，可以像访问普通文件系统一样访问系统内部的数据结构，获取当前运行的进程、统计和硬件等各种信息。例如可以使用cat /proc/cpuinfo获取CPU信息。\n/proc/sys/下的文件和子目录比较特别，它们对应的是系统内核参数，更改文件内容就意味着修改了相应的内核参数，可以简单的使用echo命令来完成修改：\n1 echo 1 \u0026gt; /proc/sys/net/ipv4/tcp_syncookies 上面这个命令启用了TCP SYN Cookie保护。使用echo修改内核参数很方便，但是系统重启后这些修改都会消失，而且不方便配置参数的集中管理。/sbin/sysctl命令就是用来查看和修改内核参数的工具。sysctl -a会列出所有内核参数当前的配置信息，比遍历目录/proc/sys/方便多了。sysctl -w修改单个参数的配置，例如：\n1 sysctl -w net.ipv4.tcp_syncookies=1 和上面echo命令的效果一样。需要注意的是，要把目录分隔符斜杠/替换为点.，并省略proc.sys部分。\n通过sysctl -w修改，还是没有解决重启后修改失效的问题。更常用的方式是，把需要修改的配置集中放在/etc/sysctl.conf文件中，使用sysctl -p重新加载配置使其生效。在系统启动阶段，init程序会运行/etc/rc.d/rc.sysinit脚本，其中包含了执行sysctl命令，并使用了/etc/sysctl.conf中的配置信息。因此放在/etc/sysctl.conf中的系统参数设置在重启后也同样生效，同时也便于集中管理修改过了哪些内核参数。\n最后，哪里有比较完整的内核参数说明文档？我觉得kernel.org的文档比较全。例如我们常会遇到的网络内核参数，net.core 和 net.ipv4 。TCP相关的参数，也可以通过man文档了解。\n","date":"2014-05-30T20:21:08+08:00","permalink":"https://mazhen.tech/p/linux%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%B9%E6%B3%95/","title":"Linux内核参数的配置方法"}]